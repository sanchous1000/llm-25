# Лаба 3 — Развертывание Langfuse для логирования LLM-запросов

## Что нужно сделать

### 1) Развернуть Langfuse

Разверните Langfuse для централизованного логирования запросов к LLM.

- Равзерните Langfuse локально, используя Docker Compose.
- Настройте проект в Langfuse и получите креды для подключения к нему.
- Настройте переменные окружения для подключения вашего LLM-приложения к Langfuse.
- Проверьте доступность интерфейса Langfuse через браузер.

---

### 2) Интегрировать логирование запросов LLM

Интегрируйте Langfuse в ваше LLM-приложение (например, из Лабораторной работы 1 или 2) для логирования пользовательских запросов и ответов модели.

*Примечание: для выполнения этой лабораторной работы можно использовать как проприетарные LLM (например, OpenAI GPT, Anthropic Claude), так и открытые модели (например, LLM, развернутую в Лабораторной работе 1).*

- Логируйте каждый пользовательский запрос к LLM, включая входные параметры (промпт, параметры модели).
- Логируйте ответы LLM, включая сгенерированный текст и метаданные (токены, длительность).
- Обеспечьте сохранение идентификатора пользователя для отслеживания сессий.

---

### 3) Логирование взаимодействия с пользователями и их сохранения

Продумайте и реализуйте логирование полного взаимодействия с пользователями, включая промежуточные шаги и их сохранение.

- Логируйте не только финальный запрос/ответ LLM, но и промежуточные шаги (например, извлечение релевантных документов в RAG-пайплайне).
- Используйте трассировку Langfuse для связывания всех шагов одного взаимодействия.
- Обеспечьте сохранение всех метаданных, необходимых для анализа (например, источник документов, использованные чанки, оценка релевантности).

---

### 4) Выбрать датасет и загрузить его в Langfuse

Выберите любой датасет для оценки качества LLM (например, небольшой датасет вопросов-ответов) и загрузите его в Langfuse.

- Выберите датасет (например, 10-20 пар вопрос-ответ). Пример: SQuAD, CoQA или собственный набор вопросов-ответов по выбранной вами предметной области.
- Загрузите его в Langfuse как набор тестов (Dataset), где каждый элемент (Dataset Item) содержит `input` (вопрос) и `expected_output` (эталонный ответ или релевантные документы).

---

### 5) Провести оценку и логирование метрик по датасету

Используйте загруженный датасет для оценки вашего RAG-приложения и логирования метрик в Langfuse через механизм Experiment Run.

- Настройте [Experiment Run через Langfuse SDK](https://langfuse.com/docs/evaluation/experiments/experiments-via-sdk) для запуска оценки на загруженном датасете.
- Реализуйте кастомный `run_evaluator` для вашего RAG-пайплайна, который:
  - Принимает на вход элемент датасета (`input`, `expected_output`).
  - Выполняет retrieval (извлечение релевантных документов/чанков) и генерацию ответа LLM.
  - Вычисляет retrieval-метрики из Лабы 2: `Recall@k`, `Precision@k`, `MRR` (сравнивая извлечённые чанки с эталонными).
  - Возвращает `list[Evaluation]` с рассчитанными метриками для каждого элемента датасета.
- Опционально: интегрируйте дополнительные фреймворки для оценки качества ответов LLM (например, RAGAS для метрик `faithfulness`, `answer_relevancy` и т.д.).
- Запустите эксперимент через Langfuse SDK, чтобы все метрики автоматически логировались и были доступны в интерфейсе Langfuse.
- Проанализируйте результаты в интерфейсе Langfuse: сравните метрики по разным конфигурациям (например, разные параметры retrieval из Лабы 2), сделайте выводы о качестве работы RAG-системы и возможностях улучшения.
