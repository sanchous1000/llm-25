# Отчет по лабораторной работе №2

## 1. Описание задания

В рамках лабораторной работы была реализована система RAG (Retrieval-Augmented Generation) для работы с документацией vLLM. Система позволяет:

- Парсить Markdown-файлы документации с сохранением структуры заголовков
- Разбивать документы на семантически осмысленные чанки
- Создавать векторные представления (эмбеддинги) текстовых фрагментов
- Сохранять эмбеддинги в векторную базу данных Qdrant
- Выполнять семантический поиск по документации
- Генерировать ответы на вопросы с использованием найденного контекста
- Оценивать качество retrieval-системы с помощью стандартных метрик

Работа включает пять основных компонентов:
1. **Парсинг Markdown** (`parse_md_to_jsonl.py`) - преобразование документации в структурированный датасет
2. **Построение эмбеддингов** (`build_embeddings.py`) - создание векторных представлений и загрузка в Qdrant
3. **Семантический поиск** (`search_qdrant.py`) - поиск релевантных документов по запросу
4. **RAG-запросы** (`rag-llm-request.py`) - генерация ответов с использованием найденного контекста
5. **Оценка качества** (`evaluate_retrieval.py`) - вычисление метрик Recall@k, Precision@k и MRR

## 2. Использованные фреймворки и модели

### LLM и модели эмбеддингов:
- **nomic-embed-text** - модель для создания эмбеддингов текстов (через Ollama)
- **qwen3:0.6b** - языковая модель для генерации ответов (через Ollama)

### Фреймворки и библиотеки:
- **Ollama** - локальный сервер для запуска LLM и моделей эмбеддингов
- **Qdrant** - векторная база данных для хранения и поиска эмбеддингов, **qdrant-client** - Python-клиент для работы с БД
- **langchain** (`langchain-text-splitters`) - библиотека для разбиения текста на чанки с учетом структуры Markdown

## 3. Результаты работы

### Архитектура системы
Система реализует классический пайплайн RAG:

1. **Подготовка данных**: Markdown-файлы разбиваются на чанки с учетом иерархии заголовков (H1-H3). Каждый чанк содержит метаданные: путь к файлу, заголовок секции, уровень вложенности.

2. **Векторизация**: Текстовые чанки преобразуются в эмбеддинги с помощью модели `nomic-embed-text` и сохраняются в Qdrant с метаданными.

3. **Retrieval**: При запросе создается эмбеддинг запроса, выполняется поиск по косинусному расстоянию, возвращаются top-k наиболее релевантных документов.
4. **Generation**: Найденные документы используются как контекст для LLM, которая генерирует ответ на основе предоставленной информации.

### Особенности реализации

1. **Умное разбиение на чанки**: используется двухуровневое разбиение:
   - Сначала по заголовкам (MarkdownHeaderTextSplitter)
   - Затем на чанки фиксированного размера с перекрытием (MarkdownTextSplitter)
2. **Сохранение структуры**: каждый чанк содержит информацию о пути в иерархии документа (например, "Getting Started/Installation/GPU/CUDA").

## 4. Выводы

Создана полнофункциональная система для работы с документацией, способная находить релевантную информацию и генерировать ответы.
Использование иерархической структуры заголовков позволяет сохранять семантическую целостность фрагментов документации. Реализованы стандартные метрики для оценки качества retrieval, что позволяет количественно оценить работу системы.

### Потенциальные улучшения

1. **Гибридный поиск**: Комбинация семантического и ключевого поиска для повышения точности.
2. **Re-ranking**: Использование более мощной модели для переранжирования результатов поиска.
3. **Метаданные фильтрация**: Добавление фильтрации по типам документов, разделам и т.д.
4. **Кэширование**: Кэширование эмбеддингов запросов для ускорения работы.
5. **Визуализация**: Добавление визуализации результатов поиска и метрик качества.

## 5. Инструкция по запуску

### Предварительные требования

1. Установить и запустить **Ollama**:
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama serve

ollama pull nomic-embed-text
ollama pull qwen3:0.6b
```

2. Установить и запустить **Qdrant**:
```bash
docker run -p 6333:6333 qdrant/qdrant
```

3. Установить зависимости Python:
```bash
pip install langchain-text-splitters qdrant-client requests numpy tqdm
```

### Пошаговая инструкция

#### Шаг 1: парсинг Markdown-файлов и преобразование документации в структурированный датасет:
```bash
python scripts/parse_md_to_jsonl.py \
  --input_dir docs \
  --output_jsonl artifacts/md_dataset.jsonl \
  --levels 1,2,3 \
  --chunk_size 512 \
  --overlap 64 \
  --include_headers_in_text
```


#### Шаг 2: построение эмбеддингов и загрузка в Qdrant
```bash
python scripts/build_embeddings.py \
  --input_jsonl artifacts/md_dataset.jsonl \
  --ollama_host http://localhost:11434 \
  --embed_model nomic-embed-text \
  --qdrant_host localhost \
  --qdrant_port 6333 \
  --collection vllm_docs \
  --distance cosine \
  --recreate
```

#### Шаг 3: тестирование поиска
```bash
export QUERY="How to install vLLM?"
python scripts/search_qdrant.py \
  --query "$QUERY" \
  --ollama_host http://localhost:11434 \
  --embed_model nomic-embed-text \
  --qdrant_host localhost \
  --qdrant_port 6333 \
  --collection vllm_docs \
  --top_k 5
```

**Результат**: выводятся top-5 наиболее релевантных документов с оценками схожести.

#### Шаг 4: RAG-запрос к LLM - генерация ответа на вопрос с использованием найденного контекста:

```bash
python scripts/rag-llm-request.py \
  --question "How to use multiple GPUs to inference a model?" \
  --ollama_host http://localhost:11434 \
  --embed_model nomic-embed-text \
  --llm_model qwen3:0.6b \
  --qdrant_host localhost \
  --qdrant_port 6333 \
  --collection vllm_docs \
  --top_k 5
```

**Результат**: LLM генерирует ответ на основе найденных документов и выводит список использованных источников.

#### Шаг 5: оценка качества retrieval
```bash
python scripts/evaluate_retrieval.py \
  --ground_truth_file artifacts/ground_truth_example.json \
  --ollama_host http://localhost:11434 \
  --embed_model nomic-embed-text \
  --qdrant_host localhost \
  --qdrant_port 6333 \
  --collection vllm_docs \
  --top_k 10
```

**Результат**: выводятся средние значения метрик - Mean Recall@5, Recall@10; Mean Precision@5, Precision@10; Mean MRR
