{"id": "a60fd8fcc968f85dd50305193d1af8e94118afb7439f90c07bb434acd3490fab", "heading": "vLLM CLI Guide", "level": 1, "text": "# vLLM CLI Guide  \nThe vllm command-line tool is used to run and manage vLLM models. You can start by viewing the help message with:  \n```bash\nvllm --help\n```  \nAvailable Commands:  \n```bash\nvllm {chat,complete,serve,bench,collect-env,run-batch}\n```", "file_path": "cli/README.md"}
{"id": "a60fd8fcc968f85dd50305193d1af8e94118afb7439f90c07bb434acd3490fab", "heading": "vLLM CLI Guide/serve", "level": 2, "text": "## serve  \nStarts the vLLM OpenAI Compatible API server.  \nStart with a model:  \n```bash\nvllm serve meta-llama/Llama-2-7b-hf\n```  \nSpecify the port:  \n```bash\nvllm serve meta-llama/Llama-2-7b-hf --port 8100\n```  \nServe over a Unix domain socket:  \n```bash\nvllm serve meta-llama/Llama-2-7b-hf --uds /tmp/vllm.sock\n```  \nCheck with --help for more options:  \n```bash\n# To list all groups\nvllm serve --help=listgroup\n\n# To view a argument group\nvllm serve --help=ModelConfig\n\n# To view a single argument\nvllm serve --help=max-num-seqs\n\n# To search by keyword\nvllm serve --help=max\n\n# To view full help with pager (less/more)\nvllm serve --help=page\n```  \nSee [vllm serve](./serve.md) for the full reference of all available arguments.", "file_path": "cli/README.md"}
{"id": "a60fd8fcc968f85dd50305193d1af8e94118afb7439f90c07bb434acd3490fab", "heading": "vLLM CLI Guide/chat", "level": 2, "text": "## chat  \nGenerate chat completions via the running API server.  \n```bash\n# Directly connect to localhost API without arguments\nvllm chat\n\n# Specify API url\nvllm chat --url http://{vllm-serve-host}:{vllm-serve-port}/v1\n\n# Quick chat with a single prompt\nvllm chat --quick \"hi\"\n```  \nSee [vllm chat](./chat.md) for the full reference of all available arguments.", "file_path": "cli/README.md"}
{"id": "a60fd8fcc968f85dd50305193d1af8e94118afb7439f90c07bb434acd3490fab", "heading": "vLLM CLI Guide/complete", "level": 2, "text": "## complete  \nGenerate text completions based on the given prompt via the running API server.  \n```bash\n# Directly connect to localhost API without arguments\nvllm complete\n\n# Specify API url\nvllm complete --url http://{vllm-serve-host}:{vllm-serve-port}/v1\n\n# Quick complete with a single prompt\nvllm complete --quick \"The future of AI is\"\n```  \nSee [vllm complete](./complete.md) for the full reference of all available arguments.", "file_path": "cli/README.md"}
{"id": "a60fd8fcc968f85dd50305193d1af8e94118afb7439f90c07bb434acd3490fab", "heading": "vLLM CLI Guide/bench", "level": 2, "text": "## bench  \nRun benchmark tests for latency online serving throughput and offline inference throughput.  \nTo use benchmark commands, please install with extra dependencies using `pip install vllm[bench]`.  \nAvailable Commands:  \n```bash\nvllm bench {latency, serve, throughput}\n```", "file_path": "cli/README.md"}
{"id": "a60fd8fcc968f85dd50305193d1af8e94118afb7439f90c07bb434acd3490fab", "heading": "vLLM CLI Guide/bench/latency", "level": 3, "text": "### latency  \nBenchmark the latency of a single batch of requests.  \n```bash\nvllm bench latency \\\n--model meta-llama/Llama-3.2-1B-Instruct \\\n--input-len 32 \\\n--output-len 1 \\\n--enforce-eager \\\n--load-format dummy\n```  \nSee [vllm bench latency](./bench/latency.md) for the full reference of all available arguments.", "file_path": "cli/README.md"}
{"id": "a60fd8fcc968f85dd50305193d1af8e94118afb7439f90c07bb434acd3490fab", "heading": "vLLM CLI Guide/bench/serve", "level": 3, "text": "### serve  \nBenchmark the online serving throughput.  \n```bash\nvllm bench serve \\\n--model meta-llama/Llama-3.2-1B-Instruct \\\n--host server-host \\\n--port server-port \\\n--random-input-len 32 \\\n--random-output-len 4  \\\n--num-prompts  5\n```  \nSee [vllm bench serve](./bench/serve.md) for the full reference of all available arguments.", "file_path": "cli/README.md"}
{"id": "a60fd8fcc968f85dd50305193d1af8e94118afb7439f90c07bb434acd3490fab", "heading": "vLLM CLI Guide/bench/throughput", "level": 3, "text": "### throughput  \nBenchmark offline inference throughput.  \n```bash\nvllm bench throughput \\\n--model meta-llama/Llama-3.2-1B-Instruct \\\n--input-len 32 \\\n--output-len 1 \\\n--enforce-eager \\\n--load-format dummy\n```  \nSee [vllm bench throughput](./bench/throughput.md) for the full reference of all available arguments.", "file_path": "cli/README.md"}
{"id": "a60fd8fcc968f85dd50305193d1af8e94118afb7439f90c07bb434acd3490fab", "heading": "vLLM CLI Guide/collect-env", "level": 2, "text": "## collect-env  \nStart collecting environment information.  \n```bash\nvllm collect-env\n```", "file_path": "cli/README.md"}
{"id": "a60fd8fcc968f85dd50305193d1af8e94118afb7439f90c07bb434acd3490fab", "heading": "vLLM CLI Guide/run-batch", "level": 2, "text": "## run-batch  \nRun batch prompts and write results to file.  \nRunning with a local file:  \n```bash\nvllm run-batch \\\n-i offline_inference/openai_batch/openai_example_batch.jsonl \\\n-o results.jsonl \\\n--model meta-llama/Meta-Llama-3-8B-Instruct\n```  \nUsing remote file:  \n```bash\nvllm run-batch \\\n-i https://raw.githubusercontent.com/vllm-project/vllm/main/examples/offline_inference/openai_batch/openai_example_batch.jsonl \\\n-o results.jsonl \\\n--model meta-llama/Meta-Llama-3-8B-Instruct\n```  \nSee [vllm run-batch](./run-batch.md) for the full reference of all available arguments.", "file_path": "cli/README.md"}
{"id": "a60fd8fcc968f85dd50305193d1af8e94118afb7439f90c07bb434acd3490fab", "heading": "vLLM CLI Guide/More Help", "level": 2, "text": "## More Help  \nFor detailed options of any subcommand, use:  \n```bash\nvllm <subcommand> --help\n```", "file_path": "cli/README.md"}
{"id": "08dfd71e81eb40a97996e76e805eca681c2642c0ffab4634f64acb154243ffef", "heading": "JSON CLI arguments", "level": 1, "text": "# JSON CLI arguments  \nWhen passing JSON CLI arguments, the following sets of arguments are equivalent:  \n- `--json-arg '{\"key1\": \"value1\", \"key2\": {\"key3\": \"value2\"}}'`\n- `--json-arg.key1 value1 --json-arg.key2.key3 value2`  \nAdditionally, list elements can be passed individually using `+`:  \n- `--json-arg '{\"key4\": [\"value3\", \"value4\", \"value5\"]}'`\n- `--json-arg.key4+ value3 --json-arg.key4+='value4,value5'`", "file_path": "cli/json-args.md"}
{"id": "faaad018361dbd006e5687d5d2d8734f7f3ef6215b5715d09a7ffac7fe9becab", "heading": "Conserving Memory", "level": 1, "text": "# Conserving Memory  \nLarge models might cause your machine to run out of memory (OOM). Here are some options that help alleviate this problem.", "file_path": "configuration/conserving_memory.md"}
{"id": "faaad018361dbd006e5687d5d2d8734f7f3ef6215b5715d09a7ffac7fe9becab", "heading": "Conserving Memory/Tensor Parallelism (TP)", "level": 2, "text": "## Tensor Parallelism (TP)  \nTensor parallelism (`tensor_parallel_size` option) can be used to split the model across multiple GPUs.  \nThe following code splits the model across 2 GPUs.  \n```python\nfrom vllm import LLM", "file_path": "configuration/conserving_memory.md"}
{"id": "faaad018361dbd006e5687d5d2d8734f7f3ef6215b5715d09a7ffac7fe9becab", "heading": "Conserving Memory/Tensor Parallelism (TP)", "level": 2, "text": "llm = LLM(model=\"ibm-granite/granite-3.1-8b-instruct\", tensor_parallel_size=2)\n```  \n!!! warning\nTo ensure that vLLM initializes CUDA correctly, you should avoid calling related functions (e.g. [torch.cuda.set_device][])\nbefore initializing vLLM. Otherwise, you may run into an error like `RuntimeError: Cannot re-initialize CUDA in forked subprocess`.  \nTo control which devices are used, please instead set the `CUDA_VISIBLE_DEVICES` environment variable.  \n!!! note\nWith tensor parallelism enabled, each process will read the whole model and split it into chunks, which makes the disk reading time even longer (proportional to the size of tensor parallelism).  \nYou can convert the model checkpoint to a sharded checkpoint using <gh-file:examples/offline_inference/save_sharded_state.py>. The conversion process might take some time, but later you can load the sharded checkpoint much faster. The model loading time should remain constant regardless of the size of tensor parallelism.", "file_path": "configuration/conserving_memory.md"}
{"id": "faaad018361dbd006e5687d5d2d8734f7f3ef6215b5715d09a7ffac7fe9becab", "heading": "Conserving Memory/Quantization", "level": 2, "text": "## Quantization  \nQuantized models take less memory at the cost of lower precision.  \nStatically quantized models can be downloaded from HF Hub (some popular ones are available at [Red Hat AI](https://huggingface.co/RedHatAI))\nand used directly without extra configuration.  \nDynamic quantization is also supported via the `quantization` option -- see [here](../features/quantization/README.md) for more details.", "file_path": "configuration/conserving_memory.md"}
{"id": "faaad018361dbd006e5687d5d2d8734f7f3ef6215b5715d09a7ffac7fe9becab", "heading": "Conserving Memory/Context length and batch size", "level": 2, "text": "## Context length and batch size  \nYou can further reduce memory usage by limiting the context length of the model (`max_model_len` option)\nand the maximum batch size (`max_num_seqs` option).  \n```python\nfrom vllm import LLM\n\nllm = LLM(model=\"adept/fuyu-8b\", max_model_len=2048, max_num_seqs=2)\n```", "file_path": "configuration/conserving_memory.md"}
{"id": "faaad018361dbd006e5687d5d2d8734f7f3ef6215b5715d09a7ffac7fe9becab", "heading": "Conserving Memory/Reduce CUDA Graphs", "level": 2, "text": "## Reduce CUDA Graphs  \nBy default, we optimize model inference using CUDA graphs which take up extra memory in the GPU.  \n!!! warning\nCUDA graph capture takes up more memory in V1 than in V0.  \nYou can adjust `compilation_config` to achieve a better balance between inference speed and memory usage:  \n??? code  \n```python\nfrom vllm import LLM\nfrom vllm.config import CompilationConfig, CompilationMode\n\nllm = LLM(\nmodel=\"meta-llama/Llama-3.1-8B-Instruct\",\ncompilation_config=CompilationConfig(\nmode=CompilationMode.VLLM_COMPILE,\n# By default, it goes up to max_num_seqs\ncudagraph_capture_sizes=[1, 2, 4, 8, 16],\n),\n)\n```  \nYou can disable graph capturing completely via the `enforce_eager` flag:  \n```python\nfrom vllm import LLM\n\nllm = LLM(model=\"meta-llama/Llama-3.1-8B-Instruct\", enforce_eager=True)\n```", "file_path": "configuration/conserving_memory.md"}
{"id": "faaad018361dbd006e5687d5d2d8734f7f3ef6215b5715d09a7ffac7fe9becab", "heading": "Conserving Memory/Adjust cache size", "level": 2, "text": "## Adjust cache size  \nIf you run out of CPU RAM, try the following options:  \n- (Multi-modal models only) you can set the size of multi-modal cache by setting `mm_processor_cache_gb` engine argument (default 4 GiB).\n- (CPU backend only) you can set the size of KV cache using `VLLM_CPU_KVCACHE_SPACE` environment variable (default 4 GiB).", "file_path": "configuration/conserving_memory.md"}
{"id": "faaad018361dbd006e5687d5d2d8734f7f3ef6215b5715d09a7ffac7fe9becab", "heading": "Conserving Memory/Multi-modal input limits", "level": 2, "text": "## Multi-modal input limits  \nYou can allow a smaller number of multi-modal items per prompt to reduce the memory footprint of the model:  \n```python\nfrom vllm import LLM\n\n# Accept up to 3 images and 1 video per prompt\nllm = LLM(\nmodel=\"Qwen/Qwen2.5-VL-3B-Instruct\",\nlimit_mm_per_prompt={\"image\": 3, \"video\": 1},\n)\n```  \nYou can go a step further and disable unused modalities completely by setting its limit to zero.\nFor example, if your application only accepts image input, there is no need to allocate any memory for videos.  \n```python\nfrom vllm import LLM\n\n# Accept any number of images but no videos\nllm = LLM(\nmodel=\"Qwen/Qwen2.5-VL-3B-Instruct\",\nlimit_mm_per_prompt={\"video\": 0},\n)\n```  \nYou can even run a multi-modal model for text-only inference:  \n```python\nfrom vllm import LLM\n\n# Don't accept images. Just text.\nllm = LLM(\nmodel=\"google/gemma-3-27b-it\",\nlimit_mm_per_prompt={\"image\": 0},\n)\n```", "file_path": "configuration/conserving_memory.md"}
{"id": "faaad018361dbd006e5687d5d2d8734f7f3ef6215b5715d09a7ffac7fe9becab", "heading": "Conserving Memory/Multi-modal input limits/Configurable options", "level": 3, "text": "### Configurable options  \n`limit_mm_per_prompt` also accepts configurable options per modality. In the configurable form, you still specify `count`, and you may optionally provide size hints that control how vLLM profiles and reserves memory for your multiâ€‘modal inputs. This helps you tune memory for the actual media you expect, instead of the modelâ€™s absolute maxima.  \nConfigurable options by modality:  \n- `image`: `{\"count\": int, \"width\": int, \"height\": int}`\n- `video`: `{\"count\": int, \"num_frames\": int, \"width\": int, \"height\": int}`\n- `audio`: `{\"count\": int, \"length\": int}`  \nDetails could be found in [`ImageDummyOptions`][vllm.config.multimodal.ImageDummyOptions], [`VideoDummyOptions`][vllm.config.multimodal.VideoDummyOptions], and [`AudioDummyOptions`][vllm.config.multimodal.AudioDummyOptions].  \nExamples:  \n```python\nfrom vllm import LLM", "file_path": "configuration/conserving_memory.md"}
{"id": "faaad018361dbd006e5687d5d2d8734f7f3ef6215b5715d09a7ffac7fe9becab", "heading": "Conserving Memory/Multi-modal input limits/Configurable options", "level": 3, "text": "# Up to 5 images per prompt, profile with 512x512.\n# Up to 1 video per prompt, profile with 32 frames at 640x640.\nllm = LLM(\nmodel=\"Qwen/Qwen2.5-VL-3B-Instruct\",\nlimit_mm_per_prompt={\n\"image\": {\"count\": 5, \"width\": 512, \"height\": 512},\n\"video\": {\"count\": 1, \"num_frames\": 32, \"width\": 640, \"height\": 640},\n},\n)\n```  \nFor backward compatibility, passing an integer works as before and is interpreted as `{\"count\": <int>}`. For example:  \n- `limit_mm_per_prompt={\"image\": 5}` is equivalent to `limit_mm_per_prompt={\"image\": {\"count\": 5}}`\n- You can mix formats: `limit_mm_per_prompt={\"image\": 5, \"video\": {\"count\": 1, \"num_frames\": 32, \"width\": 640, \"height\": 640}}`  \n!!! note\n- The size hints affect memory profiling only. They shape the dummy inputs used to compute reserved activation sizes. They do not change how inputs are actually processed at inference time.\n- If a hint exceeds what the model can accept, vLLM clamps it to the model's effective maximum and may log a warning.  \n!!! warning", "file_path": "configuration/conserving_memory.md"}
{"id": "faaad018361dbd006e5687d5d2d8734f7f3ef6215b5715d09a7ffac7fe9becab", "heading": "Conserving Memory/Multi-modal input limits/Configurable options", "level": 3, "text": "!!! warning\nThese size hints currently only affect activation memory profiling. Encoder cache size is determined by the actual inputs at runtime and is not limited by these hints.", "file_path": "configuration/conserving_memory.md"}
{"id": "faaad018361dbd006e5687d5d2d8734f7f3ef6215b5715d09a7ffac7fe9becab", "heading": "Conserving Memory/Multi-modal processor arguments", "level": 2, "text": "## Multi-modal processor arguments  \nFor certain models, you can adjust the multi-modal processor arguments to\nreduce the size of the processed multi-modal inputs, which in turn saves memory.  \nHere are some examples:  \n```python\nfrom vllm import LLM\n\n# Available for Qwen2-VL series models\nllm = LLM(\nmodel=\"Qwen/Qwen2.5-VL-3B-Instruct\",\nmm_processor_kwargs={\"max_pixels\": 768 * 768},  # Default is 1280 * 28 * 28\n)\n\n# Available for InternVL series models\nllm = LLM(\nmodel=\"OpenGVLab/InternVL2-2B\",\nmm_processor_kwargs={\"max_dynamic_patch\": 4},  # Default is 12\n)\n```", "file_path": "configuration/conserving_memory.md"}
{"id": "03e686b07d63c28addcd2c3a2fc2c86693446ef3b48ff905f7febf573e93cbdd", "heading": "Model Resolution", "level": 1, "text": "# Model Resolution  \nvLLM loads HuggingFace-compatible models by inspecting the `architectures` field in `config.json` of the model repository\nand finding the corresponding implementation that is registered to vLLM.\nNevertheless, our model resolution may fail for the following reasons:  \n- The `config.json` of the model repository lacks the `architectures` field.\n- Unofficial repositories refer to a model using alternative names which are not recorded in vLLM.\n- The same architecture name is used for multiple models, creating ambiguity as to which model should be loaded.  \nTo fix this, explicitly specify the model architecture by passing `config.json` overrides to the `hf_overrides` option.\nFor example:  \n```python\nfrom vllm import LLM\n\nllm = LLM(\nmodel=\"cerebras/Cerebras-GPT-1.3B\",\nhf_overrides={\"architectures\": [\"GPT2LMHeadModel\"]},  # GPT-2\n)\n```  \nOur [list of supported models](../models/supported_models.md) shows the model architectures that are recognized by vLLM.", "file_path": "configuration/model_resolution.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning", "level": 1, "text": "# Optimization and Tuning  \nThis guide covers optimization strategies and performance tuning for vLLM V1.  \n!!! tip\nRunning out of memory? Consult [this guide](./conserving_memory.md) on how to conserve memory.", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Preemption", "level": 2, "text": "## Preemption  \nDue to the auto-regressive nature of transformer architecture, there are times when KV cache space is insufficient to handle all batched requests.\nIn such cases, vLLM can preempt requests to free up KV cache space for other requests. Preempted requests are recomputed when sufficient KV cache space becomes\navailable again. When this occurs, you may see the following warning:  \n```text\nWARNING 05-09 00:49:33 scheduler.py:1057 Sequence group 0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_cumulative_preemption_cnt=1\n```  \nWhile this mechanism ensures system robustness, preemption and recomputation can adversely affect end-to-end latency.\nIf you frequently encounter preemptions, consider the following actions:", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Preemption", "level": 2, "text": "- Increase `gpu_memory_utilization`. vLLM pre-allocates GPU cache using this percentage of memory. By increasing utilization, you can provide more KV cache space.\n- Decrease `max_num_seqs` or `max_num_batched_tokens`. This reduces the number of concurrent requests in a batch, thereby requiring less KV cache space.\n- Increase `tensor_parallel_size`. This shards model weights across GPUs, allowing each GPU to have more memory available for KV cache. However, increasing this value may cause excessive synchronization overhead.\n- Increase `pipeline_parallel_size`. This distributes model layers across GPUs, reducing the memory needed for model weights on each GPU, indirectly leaving more memory available for KV cache. However, increasing this value may cause latency penalties.  \nYou can monitor the number of preemption requests through Prometheus metrics exposed by vLLM. Additionally, you can log the cumulative number of preemption requests by setting `disable_log_stats=False`.", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Preemption", "level": 2, "text": "In vLLM V1, the default preemption mode is `RECOMPUTE` rather than `SWAP`, as recomputation has lower overhead in the V1 architecture.  \n[](){ #chunked-prefill }", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Chunked Prefill", "level": 2, "text": "## Chunked Prefill  \nChunked prefill allows vLLM to process large prefills in smaller chunks and batch them together with decode requests. This feature helps improve both throughput and latency by better balancing compute-bound (prefill) and memory-bound (decode) operations.  \nIn vLLM V1, **chunked prefill is always enabled by default**. This is different from vLLM V0, where it was conditionally enabled based on model characteristics.  \nWith chunked prefill enabled, the scheduling policy prioritizes decode requests. It batches all pending decode requests before scheduling any prefill operations. When there are available tokens in the `max_num_batched_tokens` budget, it schedules pending prefills. If a pending prefill request cannot fit into `max_num_batched_tokens`, it automatically chunks it.  \nThis policy has two benefits:  \n- It improves ITL and generation decode because decode requests are prioritized.", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Chunked Prefill", "level": 2, "text": "- It helps achieve better GPU utilization by locating compute-bound (prefill) and memory-bound (decode) requests to the same batch.", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Chunked Prefill/Performance Tuning with Chunked Prefill", "level": 3, "text": "### Performance Tuning with Chunked Prefill  \nYou can tune the performance by adjusting `max_num_batched_tokens`:  \n- Smaller values (e.g., 2048) achieve better inter-token latency (ITL) because there are fewer prefills slowing down decodes.\n- Higher values achieve better time to first token (TTFT) as you can process more prefill tokens in a batch.\n- For optimal throughput, we recommend setting `max_num_batched_tokens > 8192` especially for smaller models on large GPUs.\n- If `max_num_batched_tokens` is the same as `max_model_len`, that's almost the equivalent to the V0 default scheduling policy (except that it still prioritizes decodes).  \n```python\nfrom vllm import LLM\n\n# Set max_num_batched_tokens to tune performance\nllm = LLM(model=\"meta-llama/Llama-3.1-8B-Instruct\", max_num_batched_tokens=16384)\n```  \nSee related papers for more details (<https://arxiv.org/pdf/2401.08671> or <https://arxiv.org/pdf/2308.16369>).", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Parallelism Strategies", "level": 2, "text": "## Parallelism Strategies  \nvLLM supports multiple parallelism strategies that can be combined to optimize performance across different hardware configurations.", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Parallelism Strategies/Tensor Parallelism (TP)", "level": 3, "text": "### Tensor Parallelism (TP)  \nTensor parallelism shards model parameters across multiple GPUs within each model layer. This is the most common strategy for large model inference within a single node.  \n**When to use:**  \n- When the model is too large to fit on a single GPU\n- When you need to reduce memory pressure per GPU to allow more KV cache space for higher throughput  \n```python\nfrom vllm import LLM\n\n# Split model across 4 GPUs\nllm = LLM(model=\"meta-llama/Llama-3.3-70B-Instruct\", tensor_parallel_size=4)\n```  \nFor models that are too large to fit on a single GPU (like 70B parameter models), tensor parallelism is essential.", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Parallelism Strategies/Pipeline Parallelism (PP)", "level": 3, "text": "### Pipeline Parallelism (PP)  \nPipeline parallelism distributes model layers across multiple GPUs. Each GPU processes different parts of the model in sequence.  \n**When to use:**  \n- When you've already maxed out efficient tensor parallelism but need to distribute the model further, or across nodes\n- For very deep and narrow models where layer distribution is more efficient than tensor sharding  \nPipeline parallelism can be combined with tensor parallelism for very large models:  \n```python\nfrom vllm import LLM\n\n# Combine pipeline and tensor parallelism\nllm = LLM(\nmodel=\"meta-llama/Llama-3.3-70B-Instruct,\ntensor_parallel_size=4,\npipeline_parallel_size=2,\n)\n```", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Parallelism Strategies/Expert Parallelism (EP)", "level": 3, "text": "### Expert Parallelism (EP)  \nExpert parallelism is a specialized form of parallelism for Mixture of Experts (MoE) models, where different expert networks are distributed across GPUs.  \n**When to use:**  \n- Specifically for MoE models (like DeepSeekV3, Qwen3MoE, Llama-4)\n- When you want to balance the expert computation load across GPUs  \nExpert parallelism is enabled by setting `enable_expert_parallel=True`, which will use expert parallelism instead of tensor parallelism for MoE layers.\nIt will use the same degree of parallelism as what you have set for tensor parallelism.", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Parallelism Strategies/Data Parallelism (DP)", "level": 3, "text": "### Data Parallelism (DP)  \nData parallelism replicates the entire model across multiple GPU sets and processes different batches of requests in parallel.  \n**When to use:**  \n- When you have enough GPUs to replicate the entire model\n- When you need to scale throughput rather than model size\n- In multi-user environments where isolation between request batches is beneficial  \nData parallelism can be combined with the other parallelism strategies and is set by `data_parallel_size=N`.\nNote that MoE layers will be sharded according to the product of the tensor parallel size and data parallel size.", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Parallelism Strategies/Batch-level DP for Multi-Modal Encoders", "level": 3, "text": "### Batch-level DP for Multi-Modal Encoders  \nBy default, TP is used to shard the weights of multi-modal encoders just like for language decoders,\nin order to reduce the memory and compute load on each GPU.  \nHowever, since the size of multi-modal encoders is very small compared to language decoders,\nthere is relatively little gain from TP. On the other hand, TP incurs significant communication\noverhead because of all-reduce being performed after every layer.  \nGiven this, it may be advantageous to instead shard the batched input data using TP, essentially\nperforming batch-level DP. This has been shown to improve the throughput and TTFT by around 10% for\n`tensor_parallel_size=8`. For vision encoders that use hardware-unoptimized Conv3D operations,\nbatch-level DP can provide another 40% improvement compared to regular TP.  \nNevertheless, since the weights of the multi-modal encoder are replicated across each TP rank,", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Parallelism Strategies/Batch-level DP for Multi-Modal Encoders", "level": 3, "text": "there will be a minor increase in memory consumption and may cause OOM if you can barely fit the model already.  \nYou can enable batch-level DP by setting `mm_encoder_tp_mode=\"data\"`, for example:  \n```python\nfrom vllm import LLM", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Parallelism Strategies/Batch-level DP for Multi-Modal Encoders", "level": 3, "text": "llm = LLM(\nmodel=\"Qwen/Qwen2.5-VL-72B-Instruct\",\ntensor_parallel_size=4,\n# When mm_encoder_tp_mode=\"data\",\n# the vision encoder uses TP=4 (not DP=1) to shard the input data,\n# so the TP size becomes the effective DP size.\n# Note that this is independent of the DP size for language decoder which is used in expert parallel setting.\nmm_encoder_tp_mode=\"data\",\n# The language decoder uses TP=4 to shard the weights regardless\n# of the setting of mm_encoder_tp_mode\n)\n```  \n!!! important\nBatch-level DP is not to be confused with API request-level DP\n(which is instead controlled by `data_parallel_size`).  \nBatch-level DP needs to be implemented on a per-model basis,\nand enabled by setting `supports_encoder_tp_data = True` in the model class.\nRegardless, you need to set `mm_encoder_tp_mode=\"data\"` in engine arguments to use this feature.  \nKnown supported models (with corresponding benchmarks):  \n- dots_ocr (<gh-pr:25466>)\n- GLM-4.1V or above (<gh-pr:23168>)\n- InternVL (<gh-pr:23909>)\n- Kimi-VL (<gh-pr:23817>)", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Parallelism Strategies/Batch-level DP for Multi-Modal Encoders", "level": 3, "text": "- InternVL (<gh-pr:23909>)\n- Kimi-VL (<gh-pr:23817>)\n- Llama4 (<gh-pr:18368>)\n- MiniCPM-V-2.5 or above (<gh-pr:23327>, <gh-pr:23948>)\n- Qwen2-VL or above (<gh-pr:22742>, <gh-pr:24955>, <gh-pr:25445>)\n- Step3 (<gh-pr:22697>)", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Input Processing/Parallel Processing", "level": 3, "text": "## Input Processing  \n### Parallel Processing  \nYou can run input processing in parallel via [API server scale-out](../serving/data_parallel_deployment.md#internal-load-balancing).\nThis is useful when input processing (which is run inside the API server)\nbecomes a bottleneck compared to model execution (which is run inside engine core)\nand you have excess CPU capacity.  \n```console\n# Run 4 API processes and 1 engine core process\nvllm serve Qwen/Qwen2.5-VL-3B-Instruct --api-server-count 4", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Input Processing/Parallel Processing", "level": 3, "text": "# Run 4 API processes and 2 engine core processes\nvllm serve Qwen/Qwen2.5-VL-3B-Instruct --api-server-count 4 -dp 2\n```  \n!!! note\nAPI server scale-out is only available for online inference.  \n!!! warning\nBy default, 8 CPU threads are used in each API server to load media items (e.g. images)\nfrom request data.  \nIf you apply API server scale-out, consider adjusting `VLLM_MEDIA_LOADING_THREAD_COUNT`\nto avoid CPU resource exhaustion.  \n!!! note\nAPI server scale-out disables [multi-modal IPC caching](#ipc-caching)\nbecause it requires a one-to-one correspondence between API and engine core processes.  \nThis does not impact [multi-modal processor caching](#processor-caching).", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Multi-Modal Caching", "level": 2, "text": "## Multi-Modal Caching  \nMulti-modal caching avoids repeated transfer or processing of the same multi-modal data,\nwhich commonly occurs in multi-turn conversations.", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Multi-Modal Caching/Processor Caching", "level": 3, "text": "### Processor Caching  \nMulti-modal processor caching is automatically enabled\nto avoid repeatedly processing the same multi-modal inputs in `BaseMultiModalProcessor`.", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Multi-Modal Caching/IPC Caching", "level": 3, "text": "### IPC Caching  \nMulti-modal IPC caching is automatically enabled when\nthere is a one-to-one correspondence between API (`P0`) and engine core (`P1`) processes,\nto avoid repeatedly transferring the same multi-modal inputs between them.  \n#### Key-Replicated Cache  \nBy default, IPC caching uses a **key-replicated cache**, where cache keys exist\nin both the API (`P0`) and engine core (`P1`) processes, but the actual cache\ndata resides only in `P1`.  \n#### Shared Memory Cache  \nWhen multiple worker processes are involved (e.g., when TP > 1), a\n**shared-memory cache** is more efficient. This can be enabled by setting\n`mm_processor_cache_type=\"shm\"`. In this mode, cache keys are stored\non `P0`, while the cache data itself lives in shared memory accessible by all\nprocesses.", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Multi-Modal Caching/Configuration", "level": 3, "text": "### Configuration  \nYou can adjust the size of the cache by setting the value of `mm_processor_cache_gb` (default 4 GiB).  \nIf you do not benefit much from the cache, you can disable both IPC\nand processor caching completely via `mm_processor_cache_gb=0`.  \nExamples:  \n```python\n# Use a larger cache\nllm = LLM(\nmodel=\"Qwen/Qwen2.5-VL-3B-Instruct\",\nmm_processor_cache_gb=8,\n)\n\n# Use a shared-memory based IPC cache\nllm = LLM(\nmodel=\"Qwen/Qwen2.5-VL-3B-Instruct\",\ntensor_parallel_size=2,\nmm_processor_cache_type=\"shm\",\nmm_processor_cache_gb=8,\n)\n\n# Disable the cache\nllm = LLM(\nmodel=\"Qwen/Qwen2.5-VL-3B-Instruct\",\nmm_processor_cache_gb=0,\n)\n```", "file_path": "configuration/optimization.md"}
{"id": "c2208b32f9d357de268dff77afb68586cd8e58ba93e2bd94440f52cd327d4ebd", "heading": "Optimization and Tuning/Multi-Modal Caching/Cache Placement", "level": 3, "text": "### Cache Placement  \nBased on the configuration, the content of the multi-modal caches on `P0` and `P1` are as follows:  \n| mm_processor_cache_type | Cache Type | `P0` Cache | `P1` Engine Cache | `P1` Worker Cache | Max. Memory |\n|-------------------|-------------|------------|------------|-------------|-------------|\n| lru | Processor Caching | K + V | N/A | N/A | `mm_processor_cache_gb * data_parallel_size` |\n| lru | Key-Replicated Caching | K | K + V | N/A | `mm_processor_cache_gb * api_server_count` |\n| shm | Shared Memory Caching | K | N/A | V | `mm_processor_cache_gb * api_server_count` |\n| N/A | Disabled | N/A | N/A | N/A | `0` |  \nK: Stores the hashes of multi-modal items\nV: Stores the processed tensor data of multi-modal items", "file_path": "configuration/optimization.md"}
{"id": "6fec18b9a0e34e736469ad698171f68adb8bcbc88f920289e61cef8ca6b2dbca", "heading": "Server Arguments", "level": 1, "text": "# Server Arguments  \nThe `vllm serve` command is used to launch the OpenAI-compatible server.", "file_path": "configuration/serve_args.md"}
{"id": "6fec18b9a0e34e736469ad698171f68adb8bcbc88f920289e61cef8ca6b2dbca", "heading": "Server Arguments/CLI Arguments", "level": 2, "text": "## CLI Arguments  \nThe `vllm serve` command is used to launch the OpenAI-compatible server.\nTo see the available options, take a look at the [CLI Reference](../cli/README.md#options)!", "file_path": "configuration/serve_args.md"}
{"id": "6fec18b9a0e34e736469ad698171f68adb8bcbc88f920289e61cef8ca6b2dbca", "heading": "Server Arguments/Configuration file", "level": 2, "text": "## Configuration file  \nYou can load CLI arguments via a [YAML](https://yaml.org/) config file.\nThe argument names must be the long form of those outlined [above](serve_args.md).  \nFor example:  \n```yaml\n# config.yaml\n\nmodel: meta-llama/Llama-3.1-8B-Instruct\nhost: \"127.0.0.1\"\nport: 6379\nuvicorn-log-level: \"info\"\n```  \nTo use the above config file:  \n```bash\nvllm serve --config config.yaml\n```  \n!!! note\nIn case an argument is supplied simultaneously using command line and the config file, the value from the command line will take precedence.\nThe order of priorities is `command line > config file values > defaults`.\ne.g. `vllm serve SOME_MODEL --config config.yaml`, SOME_MODEL takes precedence over `model` in config file.", "file_path": "configuration/serve_args.md"}
{"id": "0fc6b514cd71023438f5bb86496a930e3754a8c72a133ea943772dce2a6f1a0e", "heading": "TPU Optimization Tips", "level": 1, "text": "# TPU Optimization Tips  \nThis doc serves as a collection of handy tips for optimizing your vLLM on TPU workload.", "file_path": "configuration/tpu.md"}
{"id": "0fc6b514cd71023438f5bb86496a930e3754a8c72a133ea943772dce2a6f1a0e", "heading": "TPU Optimization Tips/Get started", "level": 2, "text": "## Get started  \nLooking for setup and installation instructions? Find them [here](../getting_started/installation/google_tpu.md).", "file_path": "configuration/tpu.md"}
{"id": "0fc6b514cd71023438f5bb86496a930e3754a8c72a133ea943772dce2a6f1a0e", "heading": "TPU Optimization Tips/Get started/TPU workload sizing", "level": 3, "text": "### TPU workload sizing  \nWhen selecting the ideal number of chips for a single serving instance, it's important to account for both the model size and the average request context length. Adequate HBM for the KV cache is essential to ensure a sufficient number of concurrent requests can be processed.  \nThe following colab [calculator](https://colab.research.google.com/github/ericehanley/rightsize-vllm/blob/main/HBM_Calculator.ipynb) will tell you:  \n- KV cache size requirement per token and per request\n- TPU/GPU memory consumed by the model weights\n- TPU/GPU memory allocated for the KV cache\n- Maximum \\# of requests you can approximately set (--max-num-seqs)  \nThis approach serves as a general rule of thumb.  \n#### Latency-throughput tradeoff  \nAs with rightsizing the number of chips for your workload, consider adjusting `--max-num-seqs` to fine-tune the latency-throughput balance. Decreasing `--max-num-seqs` and/or increasing the number of chips can help reduce latency.", "file_path": "configuration/tpu.md"}
{"id": "0fc6b514cd71023438f5bb86496a930e3754a8c72a133ea943772dce2a6f1a0e", "heading": "TPU Optimization Tips/Get started/TPU workload sizing", "level": 3, "text": "`--max-num-seqs` defines the number of concurrent decode slots, effectively limiting the number of requests the server can process tokens for simultaneously. Increasing this value allows the server to pre-allocate more HBM to handle a higher number of concurrent requests, which can maximize overall throughput. However, this often increases the end-to-end (e2e) latency per request.  \nTherefore, carefully tuning `--max-num-seqs` is crucial to achieving the desired balance between latency and throughput for your specific workload.  \nIn a similar way, `--max-num-batch-tokens` can be adjusted down to improve latency, or adjusted up to improve throughput.  \n#### Compilation and Caching", "file_path": "configuration/tpu.md"}
{"id": "0fc6b514cd71023438f5bb86496a930e3754a8c72a133ea943772dce2a6f1a0e", "heading": "TPU Optimization Tips/Get started/TPU workload sizing", "level": 3, "text": "#### Compilation and Caching  \nComing from a GPU background, one of the key differences you'll notice with TPUs is an initial compilation step. TPUs are specialized accelerators (ASICs) that achieve maximum performance by executing pre-compiled, static computation graphs via the XLA compiler. Unlike GPUs, which can handle dynamic input shapes more flexibly, TPUs require a specific compiled graph for each tensor shape (e.g., batch size and sequence length) they process.  \nTo manage this, vLLM performs a one-time \"warmup\" process when you first launch the server. During this phase, it pre-compiles the model for various common input shapes and saves these compiled graphs to a cache on disk or remote storage (located at `~/.cache/vllm/xla_cache` by default). This process can range significantly, anywhere from a few minutes to an hour depending on the size of the model and context length used.", "file_path": "configuration/tpu.md"}
{"id": "0fc6b514cd71023438f5bb86496a930e3754a8c72a133ea943772dce2a6f1a0e", "heading": "TPU Optimization Tips/Get started/TPU workload sizing", "level": 3, "text": "Although the first compilation can take some time, for all subsequent server launches, vLLM can load these graphs directly from the cache, eliminating the compilation time for future runs.  \nUse `VLLM_XLA_CACHE_PATH` environment variable to write to shareable storage for future deployed nodes (like when using autoscaling).  \n#### Reducing compilation time  \nThis initial compilation time ranges significantly and is impacted by many of the arguments discussed in this optimization doc. Factors that influence the length of time to compile are things like model size and `--max-num-batch-tokens`. Other arguments you can tune are things like `VLLM_TPU_MOST_MODEL_LEN`.", "file_path": "configuration/tpu.md"}
{"id": "0fc6b514cd71023438f5bb86496a930e3754a8c72a133ea943772dce2a6f1a0e", "heading": "TPU Optimization Tips/Get started/Optimize based on your data", "level": 3, "text": "### Optimize based on your data  \n#### max-model-len vs. most-model-len  \n![most_model_len](../assets/design/tpu/most_model_len.png)  \nIf most of your requests are shorter than the maximum model length but you still need to accommodate occasional longer requests, setting a high maximum model length can negatively impact performance. In these cases, you can try introducing most-model-len by specifying the `VLLM_TPU_MOST_MODEL_LEN` environment variable.  \nFor example, 1% requests are 32k length and 99% requests are 2k length. You can pass 32k into `--max-model-len 32768` and use `VLLM_TPU_MOST_MODEL_LEN=2048`.  \nThe requests get subdivided into max-model-len and most-model-len categories, for the latter category, you can gain better performance since the server can process more requests at a time.  \n#### Padding", "file_path": "configuration/tpu.md"}
{"id": "0fc6b514cd71023438f5bb86496a930e3754a8c72a133ea943772dce2a6f1a0e", "heading": "TPU Optimization Tips/Get started/Optimize based on your data", "level": 3, "text": "#### Padding  \nFor online serving with latency requirements, consider switching to bucket padding by setting the `VLLM_TPU_BUCKET_PADDING_GAP` environment variable. Because of the layout of the TPU, try using increments of 128 (e.g., 128, 256, etc.)  \nThe server pads the requests into fixed lengths before sending them to the model to avoid recompilation. To read more about TPU padding, see [here](https://cloud.google.com/tpu/docs/performance-guide#xla-efficiencies). Currently, there are 2 ways to pad the requests:  \n1. the default exponential padding (pad to the nearest power of 2)\n2. bucket padding (pad to the nearest linearly increasing bucket).  \nWhen using bucket padding, the buckets start from 16, end at max_model_len, and increment by `VLLM_TPU_BUCKET_PADDING_GAP`.  \nFor example, max_model_len=512, padding_gap=64, the buckets will be [16, 32, 64, 128, 192, 256, 320, 384, 448, 512].", "file_path": "configuration/tpu.md"}
{"id": "0fc6b514cd71023438f5bb86496a930e3754a8c72a133ea943772dce2a6f1a0e", "heading": "TPU Optimization Tips/Get started/Optimize based on your data", "level": 3, "text": "The fewer tokens you pad, the less unnecessary computation TPU does, the better performance you can get. For example, if num_tokens=300, with exponential padding, you pad to 512, with the bucket_padding above, you pad to 320.  \nHowever, you need to be careful to choose the padding gap. If the gap is too small, it means the number of buckets is large, leading to increased warmup (precompile) time and higher memory to store the compiled graph. Too many compiled graphs may lead to HBM OOM. Conversely, an overly large gap yields no performance improvement compared to the default exponential padding.  \n#### Quantization  \nIf possible, use the precision that matches the chipâ€™s hardware acceleration:  \n- v5e has int4/int8 hardware acceleration in the MXU\n- v6e has int4/int8 hardware acceleration in the MXU  \nSupported quantized formats and features in vLLM on TPU [Jul '25]:  \n- INT8 W8A8\n- INT8 W8A16\n- FP8 KV cache\n- [WIP] FP8 W8A8\n- [WIP] AWQ\n- [WIP] FP4 W4A8  \n#### Parallelization", "file_path": "configuration/tpu.md"}
{"id": "0fc6b514cd71023438f5bb86496a930e3754a8c72a133ea943772dce2a6f1a0e", "heading": "TPU Optimization Tips/Get started/Optimize based on your data", "level": 3, "text": "- [WIP] AWQ\n- [WIP] FP4 W4A8  \n#### Parallelization  \nDon't set TP to be less than the number of chips on a single-host deployment.  \nAlthough itâ€™s common to do this with GPUs, don't try to fragment 2 or 8 different workloads across 8 chips on a single host. If you need 1 or 4 chips, just create an instance with 1 or 4 chips (these are partial-host machine types).", "file_path": "configuration/tpu.md"}
{"id": "0fc6b514cd71023438f5bb86496a930e3754a8c72a133ea943772dce2a6f1a0e", "heading": "TPU Optimization Tips/Get started/Tune your workloads", "level": 3, "text": "### Tune your workloads  \nAlthough we try to have great default configs, we strongly recommend you check out the [vLLM auto-tuner](gh-file:benchmarks/auto_tune/README.md) to optimize your workloads for your use case.", "file_path": "configuration/tpu.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites", "level": 1, "text": "# Benchmark Suites  \nvLLM provides comprehensive benchmarking tools for performance testing and evaluation:  \n- **[Benchmark CLI]**: `vllm bench` CLI tools and specialized benchmark scripts for interactive performance testing\n- **[Performance benchmarks][performance-benchmarks]**: Automated CI benchmarks for development\n- **[Nightly benchmarks][nightly-benchmarks]**: Comparative benchmarks against alternatives  \n[Benchmark CLI]: #benchmark-cli", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI", "level": 2, "text": "## Benchmark CLI  \nThis section guides you through running benchmark tests with the extensive\ndatasets supported on vLLM. It's a living document, updated as new features and datasets\nbecome available.", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Dataset Overview", "level": 3, "text": "### Dataset Overview  \n<style>\nth {\nmin-width: 0 !important;\n}\n</style>  \n| Dataset | Online | Offline | Data Path |\n|---------|--------|---------|-----------|\n| ShareGPT | âœ… | âœ… | `wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json` |\n| ShareGPT4V (Image) | âœ… | âœ… | `wget https://huggingface.co/datasets/Lin-Chen/ShareGPT4V/blob/main/sharegpt4v_instruct_gpt4-vision_cap100k.json`<br>Note that the images need to be downloaded separately. For example, to download COCO's 2017 Train images:<br>`wget http://images.cocodataset.org/zips/train2017.zip` |\n| ShareGPT4Video (Video) | âœ… | âœ… | `git clone https://huggingface.co/datasets/ShareGPT4Video/ShareGPT4Video` |\n| BurstGPT | âœ… | âœ… | `wget https://github.com/HPMLL/BurstGPT/releases/download/v1.1/BurstGPT_without_fails_2.csv` |\n| Sonnet (deprecated) | âœ… | âœ… | Local file: `benchmarks/sonnet.txt` |\n| Random | âœ… | âœ… | `synthetic` |\n| RandomMultiModal (Image/Video) | ðŸŸ¡ | ðŸš§ | `synthetic` |", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Dataset Overview", "level": 3, "text": "| RandomMultiModal (Image/Video) | ðŸŸ¡ | ðŸš§ | `synthetic` |\n| RandomForReranking | âœ… | âœ… | `synthetic` |\n| Prefix Repetition | âœ… | âœ… | `synthetic` |\n| HuggingFace-VisionArena | âœ… | âœ… | `lmarena-ai/VisionArena-Chat` |\n| HuggingFace-MMVU | âœ… | âœ… | `yale-nlp/MMVU` |\n| HuggingFace-InstructCoder | âœ… | âœ… | `likaixin/InstructCoder` |\n| HuggingFace-AIMO | âœ… | âœ… | `AI-MO/aimo-validation-aime`, `AI-MO/NuminaMath-1.5`, `AI-MO/NuminaMath-CoT` |\n| HuggingFace-Other | âœ… | âœ… | `lmms-lab/LLaVA-OneVision-Data`, `Aeala/ShareGPT_Vicuna_unfiltered` |\n| HuggingFace-MTBench | âœ… | âœ… | `philschmid/mt-bench` |\n| HuggingFace-Blazedit | âœ… | âœ… | `vdaita/edit_5k_char`, `vdaita/edit_10k_char` |\n| Spec Bench | âœ… | âœ… | `wget https://raw.githubusercontent.com/hemingkx/Spec-Bench/refs/heads/main/data/spec_bench/question.jsonl` |\n| Custom | âœ… | âœ… | Local file: `data.jsonl` |  \nLegend:  \n- âœ… - supported\n- ðŸŸ¡ - Partial support\n- ðŸš§ - to be supported  \n!!! note\nHuggingFace dataset's `dataset-name` should be set to `hf`.", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Dataset Overview", "level": 3, "text": "HuggingFace dataset's `dataset-name` should be set to `hf`.\nFor local `dataset-path`, please set `hf-name` to its Hugging Face ID like  \n```bash\n--dataset-path /datasets/VisionArena-Chat/ --hf-name lmarena-ai/VisionArena-Chat\n```", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "### Examples  \n#### ðŸš€ Online Benchmark  \n<details class=\"admonition abstract\" markdown=\"1\">\n<summary>Show more</summary>  \nFirst start serving your model:  \n```bash\nvllm serve NousResearch/Hermes-3-Llama-3.1-8B\n```  \nThen run the benchmarking script:  \n```bash\n# download dataset\n# wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\nvllm bench serve \\\n--backend vllm \\\n--model NousResearch/Hermes-3-Llama-3.1-8B \\\n--endpoint /v1/completions \\\n--dataset-name sharegpt \\\n--dataset-path <your data path>/ShareGPT_V3_unfiltered_cleaned_split.json \\\n--num-prompts 10\n```  \nIf successful, you will see the following output:  \n```text\n============ Serving Benchmark Result ============\nSuccessful requests:                     10\nBenchmark duration (s):                  5.78\nTotal input tokens:                      1369\nTotal generated tokens:                  2212\nRequest throughput (req/s):              1.73", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "Request throughput (req/s):              1.73\nOutput token throughput (tok/s):         382.89\nTotal Token throughput (tok/s):          619.85\n---------------Time to First Token----------------\nMean TTFT (ms):                          71.54\nMedian TTFT (ms):                        73.88\nP99 TTFT (ms):                           79.49\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          7.91\nMedian TPOT (ms):                        7.96\nP99 TPOT (ms):                           8.03\n---------------Inter-token Latency----------------\nMean ITL (ms):                           7.74\nMedian ITL (ms):                         7.70\nP99 ITL (ms):                            8.39\n==================================================\n```  \n##### Custom Dataset  \nIf the dataset you want to benchmark is not supported yet in vLLM, even then you can benchmark on it using `CustomDataset`. Your data needs to be in `.jsonl` format and needs to have \"prompt\" field per entry, e.g., data.jsonl", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "```json\n{\"prompt\": \"What is the capital of India?\"}\n{\"prompt\": \"What is the capital of Iran?\"}\n{\"prompt\": \"What is the capital of China?\"}\n```  \n```bash\n# start server\nvllm serve meta-llama/Llama-3.1-8B-Instruct\n```  \n```bash\n# run benchmarking script\nvllm bench serve --port 9001 --save-result --save-detailed \\\n--backend vllm \\\n--model meta-llama/Llama-3.1-8B-Instruct \\\n--endpoint /v1/completions \\\n--dataset-name custom \\\n--dataset-path <path-to-your-data-jsonl> \\\n--custom-skip-chat-template \\\n--num-prompts 80 \\\n--max-concurrency 1 \\\n--temperature=0.3 \\\n--top-p=0.75 \\\n--result-dir \"./log/\"\n```  \nYou can skip applying chat template if your data already has it by using `--custom-skip-chat-template`.  \n##### VisionArena Benchmark for Vision Language Models  \n```bash\n# need a model with vision capability here\nvllm serve Qwen/Qwen2-VL-7B-Instruct\n```  \n```bash\nvllm bench serve \\\n--backend openai-chat \\\n--model Qwen/Qwen2-VL-7B-Instruct \\\n--endpoint /v1/chat/completions \\\n--dataset-name hf \\", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "--endpoint /v1/chat/completions \\\n--dataset-name hf \\\n--dataset-path lmarena-ai/VisionArena-Chat \\\n--hf-split train \\\n--num-prompts 1000\n```  \n##### InstructCoder Benchmark with Speculative Decoding  \n``` bash\nvllm serve meta-llama/Meta-Llama-3-8B-Instruct \\\n--speculative-config $'{\"method\": \"ngram\",\n\"num_speculative_tokens\": 5, \"prompt_lookup_max\": 5,\n\"prompt_lookup_min\": 2}'\n```  \n``` bash\nvllm bench serve \\\n--model meta-llama/Meta-Llama-3-8B-Instruct \\\n--dataset-name hf \\\n--dataset-path likaixin/InstructCoder \\\n--num-prompts 2048\n```  \n##### Spec Bench Benchmark with Speculative Decoding  \n``` bash\nvllm serve meta-llama/Meta-Llama-3-8B-Instruct \\\n--speculative-config $'{\"method\": \"ngram\",\n\"num_speculative_tokens\": 5, \"prompt_lookup_max\": 5,\n\"prompt_lookup_min\": 2}'\n```  \n[SpecBench dataset](https://github.com/hemingkx/Spec-Bench)  \nRun all categories:  \n``` bash\n# Download the dataset using:\n# wget https://raw.githubusercontent.com/hemingkx/Spec-Bench/refs/heads/main/data/spec_bench/question.jsonl", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "vllm bench serve \\\n--model meta-llama/Meta-Llama-3-8B-Instruct \\\n--dataset-name spec_bench \\\n--dataset-path \"<YOUR_DOWNLOADED_PATH>/data/spec_bench/question.jsonl\" \\\n--num-prompts -1\n```  \nAvailable categories include `[writing, roleplay, reasoning, math, coding, extraction, stem, humanities, translation, summarization, qa, math_reasoning, rag]`.  \nRun only a specific category like \"summarization\":  \n``` bash\nvllm bench serve \\\n--model meta-llama/Meta-Llama-3-8B-Instruct \\\n--dataset-name spec_bench \\\n--dataset-path \"<YOUR_DOWNLOADED_PATH>/data/spec_bench/question.jsonl\" \\\n--num-prompts -1\n--spec-bench-category \"summarization\"\n```  \n##### Other HuggingFaceDataset Examples  \n```bash\nvllm serve Qwen/Qwen2-VL-7B-Instruct\n```  \n`lmms-lab/LLaVA-OneVision-Data`:  \n```bash\nvllm bench serve \\\n--backend openai-chat \\\n--model Qwen/Qwen2-VL-7B-Instruct \\\n--endpoint /v1/chat/completions \\\n--dataset-name hf \\\n--dataset-path lmms-lab/LLaVA-OneVision-Data \\\n--hf-split train \\\n--hf-subset \"chart2text(cauldron)\" \\", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "--hf-split train \\\n--hf-subset \"chart2text(cauldron)\" \\\n--num-prompts 10\n```  \n`Aeala/ShareGPT_Vicuna_unfiltered`:  \n```bash\nvllm bench serve \\\n--backend openai-chat \\\n--model Qwen/Qwen2-VL-7B-Instruct \\\n--endpoint /v1/chat/completions \\\n--dataset-name hf \\\n--dataset-path Aeala/ShareGPT_Vicuna_unfiltered \\\n--hf-split train \\\n--num-prompts 10\n```  \n`AI-MO/aimo-validation-aime`:  \n``` bash\nvllm bench serve \\\n--model Qwen/QwQ-32B \\\n--dataset-name hf \\\n--dataset-path AI-MO/aimo-validation-aime \\\n--num-prompts 10 \\\n--seed 42\n```  \n`philschmid/mt-bench`:  \n``` bash\nvllm bench serve \\\n--model Qwen/QwQ-32B \\\n--dataset-name hf \\\n--dataset-path philschmid/mt-bench \\\n--num-prompts 80\n```  \n`vdaita/edit_5k_char` or `vdaita/edit_10k_char`:  \n``` bash\nvllm bench serve \\\n--model Qwen/QwQ-32B \\\n--dataset-name hf \\\n--dataset-path vdaita/edit_5k_char \\\n--num-prompts 90 \\\n--blazedit-min-distance 0.01 \\\n--blazedit-max-distance 0.99\n```  \n##### Running With Sampling Parameters", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "```  \n##### Running With Sampling Parameters  \nWhen using OpenAI-compatible backends such as `vllm`, optional sampling\nparameters can be specified. Example client command:  \n```bash\nvllm bench serve \\\n--backend vllm \\\n--model NousResearch/Hermes-3-Llama-3.1-8B \\\n--endpoint /v1/completions \\\n--dataset-name sharegpt \\\n--dataset-path <your data path>/ShareGPT_V3_unfiltered_cleaned_split.json \\\n--top-k 10 \\\n--top-p 0.9 \\\n--temperature 0.5 \\\n--num-prompts 10\n```  \n##### Running With Ramp-Up Request Rate  \nThe benchmark tool also supports ramping up the request rate over the\nduration of the benchmark run. This can be useful for stress testing the\nserver or finding the maximum throughput that it can handle, given some latency budget.  \nTwo ramp-up strategies are supported:  \n- `linear`: Increases the request rate linearly from a start value to an end value.\n- `exponential`: Increases the request rate exponentially.  \nThe following arguments can be used to control the ramp-up:", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "The following arguments can be used to control the ramp-up:  \n- `--ramp-up-strategy`: The ramp-up strategy to use (`linear` or `exponential`).\n- `--ramp-up-start-rps`: The request rate at the beginning of the benchmark.\n- `--ramp-up-end-rps`: The request rate at the end of the benchmark.  \n</details>  \n#### ðŸ“ˆ Offline Throughput Benchmark  \n<details class=\"admonition abstract\" markdown=\"1\">\n<summary>Show more</summary>  \n```bash\nvllm bench throughput \\\n--model NousResearch/Hermes-3-Llama-3.1-8B \\\n--dataset-name sonnet \\\n--dataset-path vllm/benchmarks/sonnet.txt \\\n--num-prompts 10\n```  \nIf successful, you will see the following output  \n```text\nThroughput: 7.15 requests/s, 4656.00 total tokens/s, 1072.15 output tokens/s\nTotal num prompt tokens:  5014\nTotal num output tokens:  1500\n```  \n##### VisionArena Benchmark for Vision Language Models  \n```bash\nvllm bench throughput \\\n--model Qwen/Qwen2-VL-7B-Instruct \\\n--backend vllm-chat \\\n--dataset-name hf \\\n--dataset-path lmarena-ai/VisionArena-Chat \\", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "--dataset-path lmarena-ai/VisionArena-Chat \\\n--num-prompts 1000 \\\n--hf-split train\n```  \nThe `num prompt tokens` now includes image token counts  \n```text\nThroughput: 2.55 requests/s, 4036.92 total tokens/s, 326.90 output tokens/s\nTotal num prompt tokens:  14527\nTotal num output tokens:  1280\n```  \n##### InstructCoder Benchmark with Speculative Decoding  \n``` bash\nVLLM_WORKER_MULTIPROC_METHOD=spawn \\\nvllm bench throughput \\\n--dataset-name=hf \\\n--dataset-path=likaixin/InstructCoder \\\n--model=meta-llama/Meta-Llama-3-8B-Instruct \\\n--input-len=1000 \\\n--output-len=100 \\\n--num-prompts=2048 \\\n--async-engine \\\n--speculative-config $'{\"method\": \"ngram\",\n\"num_speculative_tokens\": 5, \"prompt_lookup_max\": 5,\n\"prompt_lookup_min\": 2}'\n```  \n```text\nThroughput: 104.77 requests/s, 23836.22 total tokens/s, 10477.10 output tokens/s\nTotal num prompt tokens:  261136\nTotal num output tokens:  204800\n```  \n##### Other HuggingFaceDataset Examples  \n`lmms-lab/LLaVA-OneVision-Data`:  \n```bash\nvllm bench throughput \\", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "```bash\nvllm bench throughput \\\n--model Qwen/Qwen2-VL-7B-Instruct \\\n--backend vllm-chat \\\n--dataset-name hf \\\n--dataset-path lmms-lab/LLaVA-OneVision-Data \\\n--hf-split train \\\n--hf-subset \"chart2text(cauldron)\" \\\n--num-prompts 10\n```  \n`Aeala/ShareGPT_Vicuna_unfiltered`:  \n```bash\nvllm bench throughput \\\n--model Qwen/Qwen2-VL-7B-Instruct \\\n--backend vllm-chat \\\n--dataset-name hf \\\n--dataset-path Aeala/ShareGPT_Vicuna_unfiltered \\\n--hf-split train \\\n--num-prompts 10\n```  \n`AI-MO/aimo-validation-aime`:  \n```bash\nvllm bench throughput \\\n--model Qwen/QwQ-32B \\\n--backend vllm \\\n--dataset-name hf \\\n--dataset-path AI-MO/aimo-validation-aime \\\n--hf-split train \\\n--num-prompts 10\n```  \nBenchmark with LoRA adapters:  \n``` bash\n# download dataset\n# wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\nvllm bench throughput \\\n--model meta-llama/Llama-2-7b-hf \\\n--backend vllm \\", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "--model meta-llama/Llama-2-7b-hf \\\n--backend vllm \\\n--dataset_path <your data path>/ShareGPT_V3_unfiltered_cleaned_split.json \\\n--dataset_name sharegpt \\\n--num-prompts 10 \\\n--max-loras 2 \\\n--max-lora-rank 8 \\\n--enable-lora \\\n--lora-path yard1/llama-2-7b-sql-lora-test\n```  \n</details>  \n#### ðŸ› ï¸ Structured Output Benchmark  \n<details class=\"admonition abstract\" markdown=\"1\">\n<summary>Show more</summary>  \nBenchmark the performance of structured output generation (JSON, grammar, regex).  \n##### Server Setup  \n```bash\nvllm serve NousResearch/Hermes-3-Llama-3.1-8B\n```  \n##### JSON Schema Benchmark  \n```bash\npython3 benchmarks/benchmark_serving_structured_output.py \\\n--backend vllm \\\n--model NousResearch/Hermes-3-Llama-3.1-8B \\\n--dataset json \\\n--structured-output-ratio 1.0 \\\n--request-rate 10 \\\n--num-prompts 1000\n```  \n##### Grammar-based Generation Benchmark  \n```bash\npython3 benchmarks/benchmark_serving_structured_output.py \\\n--backend vllm \\\n--model NousResearch/Hermes-3-Llama-3.1-8B \\\n--dataset grammar \\", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "--dataset grammar \\\n--structure-type grammar \\\n--request-rate 10 \\\n--num-prompts 1000\n```  \n##### Regex-based Generation Benchmark  \n```bash\npython3 benchmarks/benchmark_serving_structured_output.py \\\n--backend vllm \\\n--model NousResearch/Hermes-3-Llama-3.1-8B \\\n--dataset regex \\\n--request-rate 10 \\\n--num-prompts 1000\n```  \n##### Choice-based Generation Benchmark  \n```bash\npython3 benchmarks/benchmark_serving_structured_output.py \\\n--backend vllm \\\n--model NousResearch/Hermes-3-Llama-3.1-8B \\\n--dataset choice \\\n--request-rate 10 \\\n--num-prompts 1000\n```  \n##### XGrammar Benchmark Dataset  \n```bash\npython3 benchmarks/benchmark_serving_structured_output.py \\\n--backend vllm \\\n--model NousResearch/Hermes-3-Llama-3.1-8B \\\n--dataset xgrammar_bench \\\n--request-rate 10 \\\n--num-prompts 1000\n```  \n</details>  \n#### ðŸ“š Long Document QA Benchmark  \n<details class=\"admonition abstract\" markdown=\"1\">\n<summary>Show more</summary>  \nBenchmark the performance of long document question-answering with prefix caching.", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "##### Basic Long Document QA Test  \n```bash\npython3 benchmarks/benchmark_long_document_qa_throughput.py \\\n--model meta-llama/Llama-2-7b-chat-hf \\\n--enable-prefix-caching \\\n--num-documents 16 \\\n--document-length 2000 \\\n--output-len 50 \\\n--repeat-count 5\n```  \n##### Different Repeat Modes  \n```bash\n# Random mode (default) - shuffle prompts randomly\npython3 benchmarks/benchmark_long_document_qa_throughput.py \\\n--model meta-llama/Llama-2-7b-chat-hf \\\n--enable-prefix-caching \\\n--num-documents 8 \\\n--document-length 3000 \\\n--repeat-count 3 \\\n--repeat-mode random", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "# Tile mode - repeat entire prompt list in sequence\npython3 benchmarks/benchmark_long_document_qa_throughput.py \\\n--model meta-llama/Llama-2-7b-chat-hf \\\n--enable-prefix-caching \\\n--num-documents 8 \\\n--document-length 3000 \\\n--repeat-count 3 \\\n--repeat-mode tile", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "# Interleave mode - repeat each prompt consecutively\npython3 benchmarks/benchmark_long_document_qa_throughput.py \\\n--model meta-llama/Llama-2-7b-chat-hf \\\n--enable-prefix-caching \\\n--num-documents 8 \\\n--document-length 3000 \\\n--repeat-count 3 \\\n--repeat-mode interleave\n```  \n</details>  \n#### ðŸ—‚ï¸ Prefix Caching Benchmark  \n<details class=\"admonition abstract\" markdown=\"1\">\n<summary>Show more</summary>  \nBenchmark the efficiency of automatic prefix caching.  \n##### Fixed Prompt with Prefix Caching  \n```bash\npython3 benchmarks/benchmark_prefix_caching.py \\\n--model meta-llama/Llama-2-7b-chat-hf \\\n--enable-prefix-caching \\\n--num-prompts 1 \\\n--repeat-count 100 \\\n--input-length-range 128:256\n```  \n##### ShareGPT Dataset with Prefix Caching  \n```bash\n# download dataset\n# wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "python3 benchmarks/benchmark_prefix_caching.py \\\n--model meta-llama/Llama-2-7b-chat-hf \\\n--dataset-path /path/ShareGPT_V3_unfiltered_cleaned_split.json \\\n--enable-prefix-caching \\\n--num-prompts 20 \\\n--repeat-count 5 \\\n--input-length-range 128:256\n```  \n##### Prefix Repetition Dataset  \n```bash\nvllm bench serve \\\n--backend openai \\\n--model meta-llama/Llama-2-7b-chat-hf \\\n--dataset-name prefix_repetition \\\n--num-prompts 100 \\\n--prefix-repetition-prefix-len 512 \\\n--prefix-repetition-suffix-len 128 \\\n--prefix-repetition-num-prefixes 5 \\\n--prefix-repetition-output-len 128\n```  \n</details>  \n#### âš¡ Request Prioritization Benchmark  \n<details class=\"admonition abstract\" markdown=\"1\">\n<summary>Show more</summary>  \nBenchmark the performance of request prioritization in vLLM.  \n##### Basic Prioritization Test  \n```bash\npython3 benchmarks/benchmark_prioritization.py \\\n--model meta-llama/Llama-2-7b-chat-hf \\\n--input-len 128 \\\n--output-len 64 \\\n--num-prompts 100 \\\n--scheduling-policy priority\n```", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "--num-prompts 100 \\\n--scheduling-policy priority\n```  \n##### Multiple Sequences per Prompt  \n```bash\npython3 benchmarks/benchmark_prioritization.py \\\n--model meta-llama/Llama-2-7b-chat-hf \\\n--input-len 128 \\\n--output-len 64 \\\n--num-prompts 100 \\\n--scheduling-policy priority \\\n--n 2\n```  \n</details>  \n#### ðŸ‘ï¸ Multi-Modal Benchmark  \n<details class=\"admonition abstract\" markdown=\"1\">\n<summary>Show more</summary>  \nBenchmark the performance of multi-modal requests in vLLM.  \n##### Images (ShareGPT4V)  \nStart vLLM:  \n```bash\nvllm serve Qwen/Qwen2.5-VL-7B-Instruct \\\n--dtype bfloat16 \\\n--limit-mm-per-prompt '{\"image\": 1}' \\\n--allowed-local-media-path /path/to/sharegpt4v/images\n```  \nSend requests with images:  \n```bash\nvllm bench serve \\\n--backend openai-chat \\\n--model Qwen/Qwen2.5-VL-7B-Instruct \\\n--dataset-name sharegpt \\\n--dataset-path /path/to/ShareGPT4V/sharegpt4v_instruct_gpt4-vision_cap100k.json \\\n--num-prompts 100 \\\n--save-result \\\n--result-dir ~/vllm_benchmark_results \\\n--save-detailed \\", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "--result-dir ~/vllm_benchmark_results \\\n--save-detailed \\\n--endpoint /v1/chat/completions\n```  \n##### Videos (ShareGPT4Video)  \nStart vLLM:  \n```bash\nvllm serve Qwen/Qwen2.5-VL-7B-Instruct \\\n--dtype bfloat16 \\\n--limit-mm-per-prompt '{\"video\": 1}' \\\n--allowed-local-media-path /path/to/sharegpt4video/videos\n```  \nSend requests with videos:  \n```bash\nvllm bench serve \\\n--backend openai-chat \\\n--model Qwen/Qwen2.5-VL-7B-Instruct \\\n--dataset-name sharegpt \\\n--dataset-path /path/to/ShareGPT4Video/llava_v1_5_mix665k_with_video_chatgpt72k_share4video28k.json \\\n--num-prompts 100 \\\n--save-result \\\n--result-dir ~/vllm_benchmark_results \\\n--save-detailed \\\n--endpoint /v1/chat/completions\n```  \n##### Synthetic Random Images (random-mm)  \nGenerate synthetic image inputs alongside random text prompts to stress-test vision models without external datasets.  \nNotes:  \n- Works only with online benchmark via the OpenAI  backend (`--backend openai-chat`) and endpoint `/v1/chat/completions`.", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "- Video sampling is not yet implemented.  \nStart the server (example):  \n```bash\nvllm serve Qwen/Qwen2.5-VL-3B-Instruct \\\n--dtype bfloat16 \\\n--max-model-len 16384 \\\n--limit-mm-per-prompt '{\"image\": 3, \"video\": 0}' \\\n--mm-processor-kwargs max_pixels=1003520\n```  \nBenchmark. It is recommended to use the flag `--ignore-eos` to simulate real responses. You can set the size of the output via the arg `random-output-len`.  \nEx.1: Fixed number of items and a single image resolution, enforcing generation of approx 40 tokens:  \n```bash\nvllm bench serve \\\n--backend openai-chat \\\n--model Qwen/Qwen2.5-VL-3B-Instruct \\\n--endpoint /v1/chat/completions \\\n--dataset-name random-mm \\\n--num-prompts 100 \\\n--max-concurrency 10 \\\n--random-prefix-len 25 \\\n--random-input-len 300 \\\n--random-output-len 40 \\\n--random-range-ratio 0.2 \\\n--random-mm-base-items-per-request 2 \\\n--random-mm-limit-mm-per-prompt '{\"image\": 3, \"video\": 0}' \\\n--random-mm-bucket-config '{(224, 224, 1): 1.0}' \\\n--request-rate inf \\\n--ignore-eos \\\n--seed 42\n```", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "--request-rate inf \\\n--ignore-eos \\\n--seed 42\n```  \nThe number of items per request can be controlled by passing multiple image buckets:  \n```bash\n--random-mm-base-items-per-request 2 \\\n--random-mm-num-mm-items-range-ratio 0.5 \\\n--random-mm-limit-mm-per-prompt '{\"image\": 4, \"video\": 0}' \\\n--random-mm-bucket-config '{(256, 256, 1): 0.7, (720, 1280, 1): 0.3}' \\\n```  \nFlags specific to `random-mm`:  \n- `--random-mm-base-items-per-request`: base number of multimodal items per request.\n- `--random-mm-num-mm-items-range-ratio`: vary item count uniformly in the closed integer range [floor(nÂ·(1âˆ’r)), ceil(nÂ·(1+r))]. Set r=0 to keep it fixed; r=1 allows 0 items.\n- `--random-mm-limit-mm-per-prompt`: per-modality hard caps, e.g. '{\"image\": 3, \"video\": 0}'.\n- `--random-mm-bucket-config`: dict mapping (H, W, T) â†’ probability. Entries with probability 0 are removed; remaining probabilities are renormalized to sum to 1. Use T=1 for images. Set any T>1 for videos (video sampling not yet supported).  \nBehavioral notes:", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "Behavioral notes:  \n- If the requested base item count cannot be satisfied under the provided per-prompt limits, the tool raises an error rather than silently clamping.  \nHow sampling works:  \n- Determine per-request item count k by sampling uniformly from the integer range defined by `--random-mm-base-items-per-request` and `--random-mm-num-mm-items-range-ratio`, then clamp k to at most the sum of per-modality limits.\n- For each of the k items, sample a bucket (H, W, T) according to the normalized probabilities in `--random-mm-bucket-config`, while tracking how many items of each modality have been added.\n- If a modality (e.g., image) reaches its limit from `--random-mm-limit-mm-per-prompt`, all buckets of that modality are excluded and the remaining bucket probabilities are renormalized before continuing.", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "This should be seen as an edge case, and if this behavior can be avoided by setting `--random-mm-limit-mm-per-prompt` to a large number. Note that this might result in errors due to engine config `--limit-mm-per-prompt`.\n- The resulting request contains synthetic image data in `multi_modal_data` (OpenAI Chat format). When `random-mm` is used with the OpenAI Chat backend, prompts remain text and MM content is attached via `multi_modal_data`.  \n</details>  \n#### Embedding Benchmark  \nBenchmark the performance of embedding requests in vLLM.  \n<details class=\"admonition abstract\" markdown=\"1\">\n<summary>Show more</summary>  \n##### Text Embeddings  \nUnlike generative models which use Completions API or Chat Completions API,\nyou should set `--backend openai-embeddings` and `--endpoint /v1/embeddings` to use the Embeddings API.  \nYou can use any text dataset to benchmark the model, such as ShareGPT.  \nStart the server:  \n```bash\nvllm serve jinaai/jina-embeddings-v3 --trust-remote-code\n```  \nRun the benchmark:", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "```  \nRun the benchmark:  \n```bash\n# download dataset\n# wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\nvllm bench serve \\\n--model jinaai/jina-embeddings-v3 \\\n--backend openai-embeddings \\\n--endpoint /v1/embeddings \\\n--dataset-name sharegpt \\\n--dataset-path <your data path>/ShareGPT_V3_unfiltered_cleaned_split.json\n```  \n##### Multi-modal Embeddings  \nUnlike generative models which use Completions API or Chat Completions API,\nyou should set `--endpoint /v1/embeddings` to use the Embeddings API. The backend to use depends on the model:  \n- CLIP: `--backend openai-embeddings-clip`\n- VLM2Vec: `--backend openai-embeddings-vlm2vec`  \nFor other models, please add your own implementation inside <gh-file:vllm/benchmarks/lib/endpoint_request_func.py> to match the expected instruction format.  \nYou can use any text or multi-modal dataset to benchmark the model, as long as the model supports it.", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "For example, you can use ShareGPT and VisionArena to benchmark vision-language embeddings.  \nServe and benchmark CLIP:  \n```bash\n# Run this in another process\nvllm serve openai/clip-vit-base-patch32", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "# Run these one by one after the server is up\n# download dataset\n# wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\nvllm bench serve \\\n--model openai/clip-vit-base-patch32 \\\n--backend openai-embeddings-clip \\\n--endpoint /v1/embeddings \\\n--dataset-name sharegpt \\\n--dataset-path <your data path>/ShareGPT_V3_unfiltered_cleaned_split.json\n\nvllm bench serve \\\n--model openai/clip-vit-base-patch32 \\\n--backend openai-embeddings-clip \\\n--endpoint /v1/embeddings \\\n--dataset-name hf \\\n--dataset-path lmarena-ai/VisionArena-Chat\n```  \nServe and benchmark VLM2Vec:  \n```bash\n# Run this in another process\nvllm serve TIGER-Lab/VLM2Vec-Full --runner pooling \\\n--trust-remote-code \\\n--chat-template examples/template_vlm2vec_phi3v.jinja", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "# Run these one by one after the server is up\n# download dataset\n# wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\nvllm bench serve \\\n--model TIGER-Lab/VLM2Vec-Full \\\n--backend openai-embeddings-vlm2vec \\\n--endpoint /v1/embeddings \\\n--dataset-name sharegpt \\\n--dataset-path <your data path>/ShareGPT_V3_unfiltered_cleaned_split.json", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "vllm bench serve \\\n--model TIGER-Lab/VLM2Vec-Full \\\n--backend openai-embeddings-vlm2vec \\\n--endpoint /v1/embeddings \\\n--dataset-name hf \\\n--dataset-path lmarena-ai/VisionArena-Chat\n```  \n</details>  \n#### Reranker Benchmark  \nBenchmark the performance of rerank requests in vLLM.  \n<details class=\"admonition abstract\" markdown=\"1\">\n<summary>Show more</summary>  \nUnlike generative models which use Completions API or Chat Completions API,\nyou should set `--backend vllm-rerank` and `--endpoint /v1/rerank` to use the Reranker API.  \nFor reranking, the only supported dataset is `--dataset-name random-rerank`  \nStart the server:  \n```bash\nvllm serve BAAI/bge-reranker-v2-m3\n```  \nRun the benchmark:  \n```bash\nvllm bench serve \\\n--model BAAI/bge-reranker-v2-m3 \\\n--backend vllm-rerank \\\n--endpoint /v1/rerank \\\n--dataset-name random-rerank \\\n--tokenizer BAAI/bge-reranker-v2-m3 \\\n--random-input-len 512 \\\n--num-prompts 10 \\\n--random-batch-size 5\n```", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Benchmark CLI/Examples", "level": 3, "text": "--num-prompts 10 \\\n--random-batch-size 5\n```  \nFor reranker models, this will create `num_prompts / random_batch_size` requests with\n`random_batch_size` \"documents\" where each one has close to `random_input_len` tokens.\nIn the example above, this results in 2 rerank requests with 5 \"documents\" each where\neach document has close to 512 tokens.  \nPlease note that the `/v1/rerank` is also supported by embedding models. So if you're running\nwith an embedding model, also set `--no_reranker`. Because in this case the query is\ntreated as a individual prompt by the server, here we send `random_batch_size - 1` documents\nto account for the extra prompt which is the query. The token accounting to report the\nthroughput numbers correctly is also adjusted.  \n</details>  \n[](){ #performance-benchmarks }", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Performance Benchmarks", "level": 2, "text": "## Performance Benchmarks  \nThe performance benchmarks are used for development to confirm whether new changes improve performance under various workloads. They are triggered on every commit with both the `perf-benchmarks` and `ready` labels, and when a PR is merged into vLLM.", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Performance Benchmarks/Manually Trigger the benchmark", "level": 3, "text": "### Manually Trigger the benchmark  \nUse [vllm-ci-test-repo images](https://gallery.ecr.aws/q9t5s3a7/vllm-ci-test-repo) with vLLM benchmark suite.\nFor CPU environment, please use the image with \"-cpu\" postfix.  \nHere is an example for docker run command for CPU.  \n```bash\ndocker run -it --entrypoint /bin/bash -v /data/huggingface:/root/.cache/huggingface  -e HF_TOKEN=''  --shm-size=16g --name vllm-cpu-ci  public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:1da94e673c257373280026f75ceb4effac80e892-cpu\n```  \nThen, run below command inside the docker instance.  \n```bash\nbash .buildkite/nightly-benchmarks/scripts/run-performance-benchmarks.sh\n```  \nWhen run, benchmark script generates results under **benchmark/results** folder, along with the benchmark_results.md and benchmark_results.json.  \n#### Runtime environment variables  \n- `ON_CPU`: set the value to '1' on IntelÂ® XeonÂ® Processors. Default value is 0.\n- `SERVING_JSON`: JSON file to use for the serving tests. Default value is empty string (use default file).", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Performance Benchmarks/Manually Trigger the benchmark", "level": 3, "text": "- `LATENCY_JSON`: JSON file to use for the latency tests. Default value is empty string (use default file).\n- `THROUGHPUT_JSON`: JSON file to use for the throughout tests. Default value is empty string (use default file).\n- `REMOTE_HOST`: IP for the remote vLLM service to benchmark. Default value is empty string.\n- `REMOTE_PORT`: Port for the remote vLLM service to benchmark. Default value is empty string.  \nFor more results visualization, check the [visualizing the results](https://github.com/intel-ai-tce/vllm/blob/more_cpu_models/.buildkite/nightly-benchmarks/README.md#visualizing-the-results).  \nThe latest performance results are hosted on the public [vLLM Performance Dashboard](https://hud.pytorch.org/benchmark/llms?repoName=vllm-project%2Fvllm).", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Performance Benchmarks/Manually Trigger the benchmark", "level": 3, "text": "More information on the performance benchmarks and their parameters can be found in [Benchmark README](https://github.com/intel-ai-tce/vllm/blob/more_cpu_models/.buildkite/nightly-benchmarks/README.md) and [performance benchmark description](gh-file:.buildkite/nightly-benchmarks/performance-benchmarks-descriptions.md).", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Performance Benchmarks/Continuous Benchmarking", "level": 3, "text": "### Continuous Benchmarking  \nThe continuous benchmarking provides automated performance monitoring for vLLM across different models and GPU devices. This helps track vLLM's performance characteristics over time and identify any performance regressions or improvements.  \n#### How It Works  \nThe continuous benchmarking is triggered via a [GitHub workflow CI](https://github.com/pytorch/pytorch-integration-testing/actions/workflows/vllm-benchmark.yml) in the PyTorch infrastructure repository, which runs automatically every 4 hours. The workflow executes three types of performance tests:  \n- **Serving tests**: Measure request handling and API performance\n- **Throughput tests**: Evaluate token generation rates\n- **Latency tests**: Assess response time characteristics  \n#### Benchmark Configuration", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Performance Benchmarks/Continuous Benchmarking", "level": 3, "text": "#### Benchmark Configuration  \nThe benchmarking currently runs on a predefined set of models configured in the [vllm-benchmarks directory](https://github.com/pytorch/pytorch-integration-testing/tree/main/vllm-benchmarks/benchmarks). To add new models for benchmarking:  \n1. Navigate to the appropriate GPU directory in the benchmarks configuration\n2. Add your model specifications to the corresponding configuration files\n3. The new models will be included in the next scheduled benchmark run  \n#### Viewing Results  \nAll continuous benchmarking results are automatically published to the public [vLLM Performance Dashboard](https://hud.pytorch.org/benchmark/llms?repoName=vllm-project%2Fvllm).  \n[](){ #nightly-benchmarks }", "file_path": "contributing/benchmarks.md"}
{"id": "e148ea8fc3c1bbb6913cd1bfb8ec11c1c526a8d7ac055ef8480dd61e57d05595", "heading": "Benchmark Suites/Nightly Benchmarks", "level": 2, "text": "## Nightly Benchmarks  \nThese compare vLLM's performance against alternatives (`tgi`, `trt-llm`, and `lmdeploy`) when there are major updates of vLLM (e.g., bumping up to a new version). They are primarily intended for consumers to evaluate when to choose vLLM over other options and are triggered on every commit with both the `perf-benchmarks` and `nightly-benchmarks` labels.  \nThe latest nightly benchmark results are shared in major release blog posts such as [vLLM v0.6.0](https://blog.vllm.ai/2024/09/05/perf-update.html).  \nMore information on the nightly benchmarks and their parameters can be found [here](gh-file:.buildkite/nightly-benchmarks/nightly-descriptions.md).", "file_path": "contributing/benchmarks.md"}
{"id": "93379d72aa2468839774ffece87309d8e2c52c310e41cd2a2b5325901f8b9809", "heading": "CI Failures", "level": 1, "text": "# CI Failures  \nWhat should I do when a CI job fails on my PR, but I don't think my PR caused\nthe failure?  \n- Check the dashboard of current CI test failures:\nðŸ‘‰ [CI Failures Dashboard](https://github.com/orgs/vllm-project/projects/20)  \n- If your failure **is already listed**, it's likely unrelated to your PR.\nHelp fixing it is always welcome!\n- Leave comments with links to additional instances of the failure.\n- React with a ðŸ‘ to signal how many are affected.  \n- If your failure **is not listed**, you should **file an issue**.", "file_path": "contributing/ci/failures.md"}
{"id": "93379d72aa2468839774ffece87309d8e2c52c310e41cd2a2b5325901f8b9809", "heading": "CI Failures/Filing a CI Test Failure Issue", "level": 2, "text": "## Filing a CI Test Failure Issue  \n- **File a bug report:**\nðŸ‘‰ [New CI Failure Report](https://github.com/vllm-project/vllm/issues/new?template=450-ci-failure.yml)  \n- **Use this title format:**  \n```text\n[CI Failure]: failing-test-job - regex/matching/failing:test\n```  \n- **For the environment field:**  \n```text\nStill failing on main as of commit abcdef123\n```  \n- **In the description, include failing tests:**  \n```text\nFAILED failing/test.py:failing_test1 - Failure description\nFAILED failing/test.py:failing_test2 - Failure description\nhttps://github.com/orgs/vllm-project/projects/20\nhttps://github.com/vllm-project/vllm/issues/new?template=400-bug-report.yml\nFAILED failing/test.py:failing_test3 - Failure description\n```  \n- **Attach logs** (collapsible section example):\n<details>\n<summary>Logs:</summary>  \n```text\nERROR 05-20 03:26:38 [dump_input.py:68] Dumping input data\n--- Logging error ---\nTraceback (most recent call last):", "file_path": "contributing/ci/failures.md"}
{"id": "93379d72aa2468839774ffece87309d8e2c52c310e41cd2a2b5325901f8b9809", "heading": "CI Failures/Filing a CI Test Failure Issue", "level": 2, "text": "--- Logging error ---\nTraceback (most recent call last):\nFile \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 203, in execute_model\nreturn self.model_executor.execute_model(scheduler_output)\n...\nFAILED failing/test.py:failing_test1 - Failure description\nFAILED failing/test.py:failing_test2 - Failure description\nFAILED failing/test.py:failing_test3 - Failure description\n```  \n</details>", "file_path": "contributing/ci/failures.md"}
{"id": "93379d72aa2468839774ffece87309d8e2c52c310e41cd2a2b5325901f8b9809", "heading": "CI Failures/Logs Wrangling", "level": 2, "text": "## Logs Wrangling  \nDownload the full log file from Buildkite locally.  \nStrip timestamps and colorization:  \n<gh-file:.buildkite/scripts/ci-clean-log.sh>  \n```bash\n./ci-clean-log.sh ci.log\n```  \nUse a tool [wl-clipboard](https://github.com/bugaevc/wl-clipboard) for quick copy-pasting:  \n```bash\ntail -525 ci_build.log | wl-copy\n```", "file_path": "contributing/ci/failures.md"}
{"id": "93379d72aa2468839774ffece87309d8e2c52c310e41cd2a2b5325901f8b9809", "heading": "CI Failures/Investigating a CI Test Failure", "level": 2, "text": "## Investigating a CI Test Failure  \n1. Go to ðŸ‘‰ [Buildkite main branch](https://buildkite.com/vllm/ci/builds?branch=main)\n2. Bisect to find the first build that shows the issue.\n3. Add your findings to the GitHub issue.\n4. If you find a strong candidate PR, mention it in the issue and ping contributors.", "file_path": "contributing/ci/failures.md"}
{"id": "93379d72aa2468839774ffece87309d8e2c52c310e41cd2a2b5325901f8b9809", "heading": "CI Failures/Reproducing a Failure", "level": 2, "text": "## Reproducing a Failure  \nCI test failures may be flaky. Use a bash loop to run repeatedly:  \n<gh-file:.buildkite/scripts/rerun-test.sh>  \n```bash\n./rerun-test.sh tests/v1/engine/test_engine_core_client.py::test_kv_cache_events[True-tcp]\n```", "file_path": "contributing/ci/failures.md"}
{"id": "93379d72aa2468839774ffece87309d8e2c52c310e41cd2a2b5325901f8b9809", "heading": "CI Failures/Submitting a PR", "level": 2, "text": "## Submitting a PR  \nIf you submit a PR to fix a CI failure:  \n- Link the PR to the issue:\nAdd `Closes #12345` to the PR description.\n- Add the `ci-failure` label:\nThis helps track it in the [CI Failures GitHub Project](https://github.com/orgs/vllm-project/projects/20).", "file_path": "contributing/ci/failures.md"}
{"id": "93379d72aa2468839774ffece87309d8e2c52c310e41cd2a2b5325901f8b9809", "heading": "CI Failures/Other Resources", "level": 2, "text": "## Other Resources  \n- ðŸ” [Test Reliability on `main`](https://buildkite.com/organizations/vllm/analytics/suites/ci-1/tests?branch=main&order=ASC&sort_by=reliability)\n- ðŸ§ª [Latest Buildkite CI Runs](https://buildkite.com/vllm/ci/builds?branch=main)", "file_path": "contributing/ci/failures.md"}
{"id": "93379d72aa2468839774ffece87309d8e2c52c310e41cd2a2b5325901f8b9809", "heading": "CI Failures/Daily Triage", "level": 2, "text": "## Daily Triage  \nUse [Buildkite analytics (2-day view)](https://buildkite.com/organizations/vllm/analytics/suites/ci-1/tests?branch=main&period=2days) to:  \n- Identify recent test failures **on `main`**.\n- Exclude legitimate test failures on PRs.\n- (Optional) Ignore tests with 0% reliability.  \nCompare to the [CI Failures Dashboard](https://github.com/orgs/vllm-project/projects/20).", "file_path": "contributing/ci/failures.md"}
{"id": "9024efb77fd5e5800455782221adc7d93e3dcc9ad8b71f214ba53e461ad2a8ae", "heading": "Update PyTorch version on vLLM OSS CI/CD", "level": 1, "text": "# Update PyTorch version on vLLM OSS CI/CD  \nvLLM's current policy is to always use the latest PyTorch stable\nrelease in CI/CD. It is standard practice to submit a PR to update the\nPyTorch version as early as possible when a new [PyTorch stable\nrelease](https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-cadence) becomes available.\nThis process is non-trivial due to the gap between PyTorch\nreleases. Using <gh-pr:16859> as an example, this document outlines common steps to achieve this\nupdate along with a list of potential issues and how to address them.", "file_path": "contributing/ci/update_pytorch_version.md"}
{"id": "9024efb77fd5e5800455782221adc7d93e3dcc9ad8b71f214ba53e461ad2a8ae", "heading": "Update PyTorch version on vLLM OSS CI/CD/Test PyTorch release candidates (RCs)", "level": 2, "text": "## Test PyTorch release candidates (RCs)  \nUpdating PyTorch in vLLM after the official release is not\nideal because any issues discovered at that point can only be resolved\nby waiting for the next release or by implementing hacky workarounds in vLLM.\nThe better solution is to test vLLM with PyTorch release candidates (RC) to ensure\ncompatibility before each release.  \nPyTorch release candidates can be downloaded from [PyTorch test index](https://download.pytorch.org/whl/test).\nFor example, `torch2.7.0+cu12.8` RC can be installed using the following command:  \n```bash\nuv pip install torch torchvision torchaudio \\\n--index-url https://download.pytorch.org/whl/test/cu128\n```  \nWhen the final RC is ready for testing, it will be announced to the community\non the [PyTorch dev-discuss forum](https://dev-discuss.pytorch.org/c/release-announcements).\nAfter this announcement, we can begin testing vLLM integration by drafting a pull request\nfollowing this 3-step process:", "file_path": "contributing/ci/update_pytorch_version.md"}
{"id": "9024efb77fd5e5800455782221adc7d93e3dcc9ad8b71f214ba53e461ad2a8ae", "heading": "Update PyTorch version on vLLM OSS CI/CD/Test PyTorch release candidates (RCs)", "level": 2, "text": "following this 3-step process:  \n1. Update [requirements files](https://github.com/vllm-project/vllm/tree/main/requirements)\nto point to the new releases for `torch`, `torchvision`, and `torchaudio`.  \n2. Use the following option to get the final release candidates' wheels. Some common platforms are `cpu`, `cu128`, and `rocm6.2.4`.  \n```bash\n--extra-index-url https://download.pytorch.org/whl/test/<PLATFORM>\n```  \n3. Since vLLM uses `uv`, ensure the following index strategy is applied:  \n- Via environment variable:  \n```bash\nexport UV_INDEX_STRATEGY=unsafe-best-match\n```  \n- Or via CLI flag:  \n```bash\n--index-strategy unsafe-best-match\n```  \nIf failures are found in the pull request, raise them as issues on vLLM and\ncc the PyTorch release team to initiate discussion on how to address them.", "file_path": "contributing/ci/update_pytorch_version.md"}
{"id": "9024efb77fd5e5800455782221adc7d93e3dcc9ad8b71f214ba53e461ad2a8ae", "heading": "Update PyTorch version on vLLM OSS CI/CD/Update CUDA version", "level": 2, "text": "## Update CUDA version  \nThe PyTorch release matrix includes both stable and experimental [CUDA versions](https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix). Due to limitations, only the latest stable CUDA version (for example, torch `2.7.1+cu126`) is uploaded to PyPI. However, vLLM may require a different CUDA version,\nsuch as 12.8 for Blackwell support.\nThis complicates the process as we cannot use the out-of-the-box\n`pip install torch torchvision torchaudio` command. The solution is to use\n`--extra-index-url` in vLLM's Dockerfiles.  \n- Important indexes at the moment include:  \n| Platform | `--extra-index-url` |\n|----------|-----------------|\n| CUDA 12.8| [https://download.pytorch.org/whl/cu128](https://download.pytorch.org/whl/cu128)|\n| CPU      | [https://download.pytorch.org/whl/cpu](https://download.pytorch.org/whl/cpu)|\n| ROCm 6.2 | [https://download.pytorch.org/whl/rocm6.2.4](https://download.pytorch.org/whl/rocm6.2.4) |", "file_path": "contributing/ci/update_pytorch_version.md"}
{"id": "9024efb77fd5e5800455782221adc7d93e3dcc9ad8b71f214ba53e461ad2a8ae", "heading": "Update PyTorch version on vLLM OSS CI/CD/Update CUDA version", "level": 2, "text": "| ROCm 6.3 | [https://download.pytorch.org/whl/rocm6.3](https://download.pytorch.org/whl/rocm6.3) |\n| XPU      | [https://download.pytorch.org/whl/xpu](https://download.pytorch.org/whl/xpu) |  \n- Update the below files to match the CUDA version from step 1. This makes sure that the release vLLM wheel is tested on CI.\n- `.buildkite/release-pipeline.yaml`\n- `.buildkite/scripts/upload-wheels.sh`", "file_path": "contributing/ci/update_pytorch_version.md"}
{"id": "9024efb77fd5e5800455782221adc7d93e3dcc9ad8b71f214ba53e461ad2a8ae", "heading": "Update PyTorch version on vLLM OSS CI/CD/Address long vLLM build time", "level": 2, "text": "## Address long vLLM build time  \nWhen building vLLM with a new PyTorch/CUDA version, no cache will exist\nin the vLLM sccache S3 bucket, causing the build job on CI to potentially take more than 5 hours\nand timeout. Additionally, since vLLM's fastcheck pipeline runs in read-only mode,\nit doesn't populate the cache, so re-running it to warm up the cache\nis ineffective.  \nWhile ongoing efforts like [#17419](gh-issue:17419)\naddress the long build time at its source, the current workaround is to set `VLLM_CI_BRANCH`\nto a custom branch provided by @khluu (`VLLM_CI_BRANCH=khluu/use_postmerge_q`)\nwhen manually triggering a build on Buildkite. This branch accomplishes two things:  \n1. Increase the timeout limit to 10 hours so that the build doesn't time out.\n2. Allow the compiled artifacts to be written to the vLLM sccache S3 bucket\nto warm it up so that future builds are faster.  \n<p align=\"center\" width=\"100%\">\n<img width=\"60%\" src=\"https://github.com/user-attachments/assets/a8ff0fcd-76e0-4e91-b72f-014e3fdb6b94\">", "file_path": "contributing/ci/update_pytorch_version.md"}
{"id": "9024efb77fd5e5800455782221adc7d93e3dcc9ad8b71f214ba53e461ad2a8ae", "heading": "Update PyTorch version on vLLM OSS CI/CD/Address long vLLM build time", "level": 2, "text": "</p>", "file_path": "contributing/ci/update_pytorch_version.md"}
{"id": "9024efb77fd5e5800455782221adc7d93e3dcc9ad8b71f214ba53e461ad2a8ae", "heading": "Update PyTorch version on vLLM OSS CI/CD/Update dependencies", "level": 2, "text": "## Update dependencies  \nSeveral vLLM dependencies, such as FlashInfer, also depend on PyTorch and need\nto be updated accordingly. Rather than waiting for all of them to publish new\nreleases (which would take too much time), they can be built from\nsource to unblock the update process.", "file_path": "contributing/ci/update_pytorch_version.md"}
{"id": "9024efb77fd5e5800455782221adc7d93e3dcc9ad8b71f214ba53e461ad2a8ae", "heading": "Update PyTorch version on vLLM OSS CI/CD/Update dependencies/FlashInfer", "level": 3, "text": "### FlashInfer  \nHere is how to build and install it from source with `torch2.7.0+cu128` in vLLM [Dockerfile](https://github.com/vllm-project/vllm/blob/27bebcd89792d5c4b08af7a65095759526f2f9e1/docker/Dockerfile#L259-L271):  \n```bash\nexport TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0 10.0+PTX'\nexport FLASHINFER_ENABLE_SM90=1\nuv pip install --system \\\n--no-build-isolation \"git+https://github.com/flashinfer-ai/flashinfer@v0.2.6.post1\"\n```  \nOne caveat is that building FlashInfer from source adds approximately 30\nminutes to the vLLM build time. Therefore, it's preferable to cache the wheel in a\npublic location for immediate installation, such as [this FlashInfer wheel link](https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.6.post1%2Bcu128torch2.7-cp39-abi3-linux_x86_64.whl). For future releases, contact the PyTorch release\nteam if you want to get the package published there.", "file_path": "contributing/ci/update_pytorch_version.md"}
{"id": "9024efb77fd5e5800455782221adc7d93e3dcc9ad8b71f214ba53e461ad2a8ae", "heading": "Update PyTorch version on vLLM OSS CI/CD/Update dependencies/xFormers", "level": 3, "text": "### xFormers  \nSimilar to FlashInfer, here is how to build and install xFormers from source:  \n```bash\nexport TORCH_CUDA_ARCH_LIST='7.0 7.5 8.0 8.9 9.0 10.0+PTX'\nMAX_JOBS=16 uv pip install --system \\\n--no-build-isolation \"git+https://github.com/facebookresearch/xformers@v0.0.30\"\n```", "file_path": "contributing/ci/update_pytorch_version.md"}
{"id": "9024efb77fd5e5800455782221adc7d93e3dcc9ad8b71f214ba53e461ad2a8ae", "heading": "Update PyTorch version on vLLM OSS CI/CD/Update all the different vLLM platforms", "level": 2, "text": "## Update all the different vLLM platforms  \nRather than attempting to update all vLLM platforms in a single pull request, it's more manageable\nto handle some platforms separately. The separation of requirements and Dockerfiles\nfor different platforms in vLLM CI/CD allows us to selectively choose\nwhich platforms to update. For instance, updating XPU requires the corresponding\nrelease from [Intel Extension for PyTorch](https://github.com/intel/intel-extension-for-pytorch) by Intel.\nWhile <gh-pr:16859> updated vLLM to PyTorch 2.7.0 on CPU, CUDA, and ROCm,\n<gh-pr:17444> completed the update for XPU.", "file_path": "contributing/ci/update_pytorch_version.md"}
{"id": "2640282556553341523ef7c13c05ca00a5fd093c3b6f40cf2e09e2cc2a2424ee", "heading": "Deprecation Policy", "level": 1, "text": "# Deprecation Policy  \nThis document outlines the official policy and process for deprecating features\nin the vLLM project.", "file_path": "contributing/deprecation_policy.md"}
{"id": "2640282556553341523ef7c13c05ca00a5fd093c3b6f40cf2e09e2cc2a2424ee", "heading": "Deprecation Policy/Overview", "level": 2, "text": "## Overview  \nvLLM uses a structured \"deprecation pipeline\" to guide the lifecycle of\ndeprecated features. This policy ensures that users are given clear and\nsufficient notice when a feature is deprecated and that deprecations proceed in\na consistent and predictable manner.  \nWe aim to strike a balance between continued innovation and respecting usersâ€™\nreliance on existing functionality. Deprecations are tied to our **minor (Y)\nreleases** following semantic versioning (X.Y.Z), where:  \n- **X** is a major version (rare)\n- **Y** is a minor version (used for significant changes, including deprecations/removals)\n- **Z** is a patch version (used for fixes and safer enhancements)  \nFeatures that fall under this policy include (at a minimum) the following:  \n- CLI flags\n- Environment variables\n- Configuration files\n- APIs in the OpenAI-compatible API server\n- Public Python APIs for the `vllm` library", "file_path": "contributing/deprecation_policy.md"}
{"id": "2640282556553341523ef7c13c05ca00a5fd093c3b6f40cf2e09e2cc2a2424ee", "heading": "Deprecation Policy/Deprecation Pipeline", "level": 2, "text": "## Deprecation Pipeline  \nThe deprecation process consists of several clearly defined stages that span\nmultiple Y releases:", "file_path": "contributing/deprecation_policy.md"}
{"id": "2640282556553341523ef7c13c05ca00a5fd093c3b6f40cf2e09e2cc2a2424ee", "heading": "Deprecation Policy/Deprecation Pipeline/1. Deprecated (Still On By Default)", "level": 3, "text": "### 1. Deprecated (Still On By Default)  \n- **Action**: Feature is marked as deprecated.\n- **Timeline**: A removal version is explicitly stated in the deprecation\nwarning (e.g., \"This will be removed in v0.10.0\").\n- **Communication**: Deprecation is noted in the following, as applicable:\n- Help strings\n- Log output\n- API responses\n- `/metrics` output (for metrics features)\n- User-facing documentation\n- Release notes\n- GitHub Issue (RFC) for feedback\n- Documentation and use of the `@typing_extensions.deprecated` decorator for Python APIs", "file_path": "contributing/deprecation_policy.md"}
{"id": "2640282556553341523ef7c13c05ca00a5fd093c3b6f40cf2e09e2cc2a2424ee", "heading": "Deprecation Policy/Deprecation Pipeline/2.Deprecated (Off By Default)", "level": 3, "text": "### 2.Deprecated (Off By Default)  \n- **Action**: Feature is disabled by default, but can still be re-enabled via a\nCLI flag or environment variable. Feature throws an error when used without\nre-enabling.\n- **Purpose**: Allows users who missed earlier warnings a temporary escape hatch\nwhile signaling imminent removal. Ensures any remaining usage is clearly\nsurfaced and blocks silent breakage before full removal.", "file_path": "contributing/deprecation_policy.md"}
{"id": "2640282556553341523ef7c13c05ca00a5fd093c3b6f40cf2e09e2cc2a2424ee", "heading": "Deprecation Policy/Deprecation Pipeline/3. Removed", "level": 3, "text": "### 3. Removed  \n- **Action**: Feature is completely removed from the codebase.\n- **Note**: Only features that have passed through the previous deprecation\nstages will be removed.", "file_path": "contributing/deprecation_policy.md"}
{"id": "2640282556553341523ef7c13c05ca00a5fd093c3b6f40cf2e09e2cc2a2424ee", "heading": "Deprecation Policy/Example Timeline", "level": 2, "text": "## Example Timeline  \nAssume a feature is deprecated in `v0.9.0`.  \n| Release       | Status                                                                                          |\n|---------------|-------------------------------------------------------------------------------------------------|\n| `v0.9.0`      | Feature is deprecated with clear removal version listed.                                        |\n| `v0.10.0`     | Feature is now off by default, throws an error when used, and can be re-enabled for legacy use. |\n| `v0.11.0`     | Feature is removed.                                                                             |", "file_path": "contributing/deprecation_policy.md"}
{"id": "2640282556553341523ef7c13c05ca00a5fd093c3b6f40cf2e09e2cc2a2424ee", "heading": "Deprecation Policy/Important Guidelines", "level": 2, "text": "## Important Guidelines  \n- **No Removals in Patch Releases**: Removing deprecated features in patch\n(`.Z`) releases is disallowed to avoid surprising users.\n- **Grace Period for Existing Deprecations**: Any feature deprecated **before\nthis policy** will have its grace period start **now**, not retroactively.\n- **Documentation is Critical**: Ensure every stage of the pipeline is\ndocumented clearly for users.", "file_path": "contributing/deprecation_policy.md"}
{"id": "2640282556553341523ef7c13c05ca00a5fd093c3b6f40cf2e09e2cc2a2424ee", "heading": "Deprecation Policy/Final Notes", "level": 2, "text": "## Final Notes  \nThis policy is a living document and may evolve as the needs of the project and\nits users change. Community feedback is welcome and encouraged as we refine the\nprocess.", "file_path": "contributing/deprecation_policy.md"}
{"id": "88e0790edcbdc903e8af5f3f209153e9367369ce3c6a4533cbfa6f4687620a18", "heading": "Incremental Compilation Workflow", "level": 1, "text": "# Incremental Compilation Workflow  \nWhen working on vLLM's C++/CUDA kernels located in the `csrc/` directory, recompiling the entire project with `uv pip install -e .` for every change can be time-consuming. An incremental compilation workflow using CMake allows for faster iteration by only recompiling the necessary components after an initial setup. This guide details how to set up and use such a workflow, which complements your editable Python installation.", "file_path": "contributing/incremental_build.md"}
{"id": "88e0790edcbdc903e8af5f3f209153e9367369ce3c6a4533cbfa6f4687620a18", "heading": "Incremental Compilation Workflow/Prerequisites", "level": 2, "text": "## Prerequisites  \nBefore setting up the incremental build:  \n1. **vLLM Editable Install:** Ensure you have vLLM installed from source in an editable mode. Using pre-compiled wheels for the initial editable setup can be faster, as the CMake workflow will handle subsequent kernel recompilations.  \n```console\nuv venv --python 3.12 --seed\nsource .venv/bin/activate\nVLLM_USE_PRECOMPILED=1 uv pip install -U -e . --torch-backend=auto\n```", "file_path": "contributing/incremental_build.md"}
{"id": "88e0790edcbdc903e8af5f3f209153e9367369ce3c6a4533cbfa6f4687620a18", "heading": "Incremental Compilation Workflow/Prerequisites", "level": 2, "text": "```  \n2. **CUDA Toolkit:** Verify that the NVIDIA CUDA Toolkit is correctly installed and `nvcc` is accessible in your `PATH`. CMake relies on `nvcc` to compile CUDA code. You can typically find `nvcc` in `$CUDA_HOME/bin/nvcc` or by running `which nvcc`. If you encounter issues, refer to the [official CUDA Toolkit installation guides](https://developer.nvidia.com/cuda-toolkit-archive) and vLLM's main [GPU installation documentation](../getting_started/installation/gpu.md#troubleshooting) for troubleshooting. The `CMAKE_CUDA_COMPILER` variable in your `CMakeUserPresets.json` should also point to your `nvcc` binary.  \n3. **Build Tools:** It is highly recommended to install `ccache` for fast rebuilds by caching compilation results (e.g., `sudo apt install ccache` or `conda install ccache`). Also, ensure the core build dependencies like `cmake` and `ninja` are installed. These are installable through `requirements/build.txt` or your system's package manager.  \n```console", "file_path": "contributing/incremental_build.md"}
{"id": "88e0790edcbdc903e8af5f3f209153e9367369ce3c6a4533cbfa6f4687620a18", "heading": "Incremental Compilation Workflow/Prerequisites", "level": 2, "text": "```console\nuv pip install -r requirements/build.txt --torch-backend=auto\n```", "file_path": "contributing/incremental_build.md"}
{"id": "88e0790edcbdc903e8af5f3f209153e9367369ce3c6a4533cbfa6f4687620a18", "heading": "Incremental Compilation Workflow/Setting up the CMake Build Environment", "level": 2, "text": "## Setting up the CMake Build Environment  \nThe incremental build process is managed through CMake. You can configure your build settings using a `CMakeUserPresets.json` file at the root of the vLLM repository.", "file_path": "contributing/incremental_build.md"}
{"id": "88e0790edcbdc903e8af5f3f209153e9367369ce3c6a4533cbfa6f4687620a18", "heading": "Incremental Compilation Workflow/Setting up the CMake Build Environment/Generate `CMakeUserPresets.json` using the helper script", "level": 3, "text": "### Generate `CMakeUserPresets.json` using the helper script  \nTo simplify the setup, vLLM provides a helper script that attempts to auto-detect your system's configuration (like CUDA path, Python environment, and CPU cores) and generates the `CMakeUserPresets.json` file for you.  \n**Run the script:**  \nNavigate to the root of your vLLM clone and execute the following command:  \n```console\npython tools/generate_cmake_presets.py\n```  \nThe script will prompt you if it cannot automatically determine certain paths (e.g., `nvcc` or a specific Python executable for your vLLM development environment). Follow the on-screen prompts. If an existing `CMakeUserPresets.json` is found, the script will ask for confirmation before overwriting it.  \n**Force overwrite existing file:**  \nTo automatically overwrite an existing `CMakeUserPresets.json` without prompting, use the `--force-overwrite` flag:  \n```console\npython tools/generate_cmake_presets.py --force-overwrite\n```", "file_path": "contributing/incremental_build.md"}
{"id": "88e0790edcbdc903e8af5f3f209153e9367369ce3c6a4533cbfa6f4687620a18", "heading": "Incremental Compilation Workflow/Setting up the CMake Build Environment/Generate `CMakeUserPresets.json` using the helper script", "level": 3, "text": "python tools/generate_cmake_presets.py --force-overwrite\n```  \nThis is particularly useful in automated scripts or CI/CD environments where interactive prompts are not desired.  \nAfter running the script, a `CMakeUserPresets.json` file will be created in the root of your vLLM repository.", "file_path": "contributing/incremental_build.md"}
{"id": "88e0790edcbdc903e8af5f3f209153e9367369ce3c6a4533cbfa6f4687620a18", "heading": "Incremental Compilation Workflow/Setting up the CMake Build Environment/Example `CMakeUserPresets.json`", "level": 3, "text": "### Example `CMakeUserPresets.json`  \nBelow is an example of what the generated `CMakeUserPresets.json` might look like. The script will tailor these values based on your system and any input you provide.  \n```json\n{\n\"version\": 6,\n\"cmakeMinimumRequired\": {\n\"major\": 3,\n\"minor\": 26,\n\"patch\": 1\n},\n\"configurePresets\": [\n{\n\"name\": \"release\",\n\"generator\": \"Ninja\",\n\"binaryDir\": \"${sourceDir}/cmake-build-release\",\n\"cacheVariables\": {\n\"CMAKE_CUDA_COMPILER\": \"/usr/local/cuda/bin/nvcc\",\n\"CMAKE_C_COMPILER_LAUNCHER\": \"ccache\",\n\"CMAKE_CXX_COMPILER_LAUNCHER\": \"ccache\",\n\"CMAKE_CUDA_COMPILER_LAUNCHER\": \"ccache\",\n\"CMAKE_BUILD_TYPE\": \"Release\",\n\"VLLM_PYTHON_EXECUTABLE\": \"/home/user/venvs/vllm/bin/python\",\n\"CMAKE_INSTALL_PREFIX\": \"${sourceDir}\",\n\"CMAKE_CUDA_FLAGS\": \"\",\n\"NVCC_THREADS\": \"4\",\n\"CMAKE_JOB_POOLS\": \"compile=32\"\n}\n}\n],\n\"buildPresets\": [\n{\n\"name\": \"release\",\n\"configurePreset\": \"release\",\n\"jobs\": 32\n}\n]\n}\n```  \n**What do the various configurations mean?**", "file_path": "contributing/incremental_build.md"}
{"id": "88e0790edcbdc903e8af5f3f209153e9367369ce3c6a4533cbfa6f4687620a18", "heading": "Incremental Compilation Workflow/Setting up the CMake Build Environment/Example `CMakeUserPresets.json`", "level": 3, "text": "}\n]\n}\n```  \n**What do the various configurations mean?**  \n- `CMAKE_CUDA_COMPILER`: Path to your `nvcc` binary. The script attempts to find this automatically.\n- `CMAKE_C_COMPILER_LAUNCHER`, `CMAKE_CXX_COMPILER_LAUNCHER`, `CMAKE_CUDA_COMPILER_LAUNCHER`: Setting these to `ccache` (or `sccache`) significantly speeds up rebuilds by caching compilation results. Ensure `ccache` is installed (e.g., `sudo apt install ccache` or `conda install ccache`). The script sets these by default.\n- `VLLM_PYTHON_EXECUTABLE`: Path to the Python executable in your vLLM development environment. The script will prompt for this, defaulting to the current Python environment if suitable.\n- `CMAKE_INSTALL_PREFIX: \"${sourceDir}\"`: Specifies that the compiled components should be installed back into your vLLM source directory. This is crucial for the editable install, as it makes the newly built kernels immediately available to your Python environment.", "file_path": "contributing/incremental_build.md"}
{"id": "88e0790edcbdc903e8af5f3f209153e9367369ce3c6a4533cbfa6f4687620a18", "heading": "Incremental Compilation Workflow/Setting up the CMake Build Environment/Example `CMakeUserPresets.json`", "level": 3, "text": "- `CMAKE_JOB_POOLS` and `jobs` in build presets: Control the parallelism of the build. The script sets these based on the number of CPU cores detected on your system.\n- `binaryDir`: Specifies where the build artifacts will be stored (e.g., `cmake-build-release`).", "file_path": "contributing/incremental_build.md"}
{"id": "88e0790edcbdc903e8af5f3f209153e9367369ce3c6a4533cbfa6f4687620a18", "heading": "Incremental Compilation Workflow/Building and Installing with CMake", "level": 2, "text": "## Building and Installing with CMake  \nOnce your `CMakeUserPresets.json` is configured:  \n1. **Initialize the CMake build environment:**\nThis step configures the build system according to your chosen preset (e.g., `release`) and creates the build directory at `binaryDir`  \n```console\ncmake --preset release\n```  \n2. **Build and install the vLLM components:**\nThis command compiles the code and installs the resulting binaries into your vLLM source directory, making them available to your editable Python installation.  \n```console\ncmake --build --preset release --target install\n```  \n3. **Make changes and repeat!**\nNow you start using your editable install of vLLM, testing and making changes as needed. If you need to build again to update based on changes, simply run the CMake command again to build only the affected files.  \n```console\ncmake --build --preset release --target install\n```", "file_path": "contributing/incremental_build.md"}
{"id": "88e0790edcbdc903e8af5f3f209153e9367369ce3c6a4533cbfa6f4687620a18", "heading": "Incremental Compilation Workflow/Verifying the Build", "level": 2, "text": "## Verifying the Build  \nAfter a successful build, you will find a populated build directory (e.g., `cmake-build-release/` if you used the `release` preset and the example configuration).  \n```console\n> ls cmake-build-release/\nbin             cmake_install.cmake      _deps                                machete_generation.log\nbuild.ninja     CPackConfig.cmake        detect_cuda_compute_capabilities.cu  marlin_generation.log\n_C.abi3.so      CPackSourceConfig.cmake  detect_cuda_version.cc               _moe_C.abi3.so\nCMakeCache.txt  ctest                    _flashmla_C.abi3.so                  moe_marlin_generation.log\nCMakeFiles      cumem_allocator.abi3.so  install_local_manifest.txt           vllm-flash-attn\n```  \nThe `cmake --build ... --target install` command copies the compiled shared libraries (like `_C.abi3.so`, `_moe_C.abi3.so`, etc.) into the appropriate `vllm` package directory within your source tree. This updates your editable installation with the newly compiled kernels.", "file_path": "contributing/incremental_build.md"}
{"id": "88e0790edcbdc903e8af5f3f209153e9367369ce3c6a4533cbfa6f4687620a18", "heading": "Incremental Compilation Workflow/Additional Tips", "level": 2, "text": "## Additional Tips  \n- **Adjust Parallelism:** Fine-tune the `CMAKE_JOB_POOLS` in `configurePresets` and `jobs` in `buildPresets` in your `CMakeUserPresets.json`. Too many jobs can overload systems with limited RAM or CPU cores, leading to slower builds or system instability. Too few won't fully utilize available resources.\n- **Clean Builds When Necessary:** If you encounter persistent or strange build errors, especially after significant changes or switching branches, consider removing the CMake build directory (e.g., `rm -rf cmake-build-release`) and re-running the `cmake --preset` and `cmake --build` commands.\n- **Specific Target Builds:** For even faster iterations when working on a specific module, you can sometimes build a specific target instead of the full `install` target, though `install` ensures all necessary components are updated in your Python environment. Refer to CMake documentation for more advanced target management.", "file_path": "contributing/incremental_build.md"}
{"id": "b9d424f4a430761499888bd43e5543f17bafa6b604681f943c5a1189b9482e89", "heading": "Basic Model", "level": 1, "text": "# Basic Model  \nThis guide walks you through the steps to implement a basic vLLM model.", "file_path": "contributing/model/basic.md"}
{"id": "b9d424f4a430761499888bd43e5543f17bafa6b604681f943c5a1189b9482e89", "heading": "Basic Model/1. Bring your model code", "level": 2, "text": "## 1. Bring your model code  \nFirst, clone the PyTorch model code from the source repository.\nFor instance, vLLM's [OPT model](gh-file:vllm/model_executor/models/opt.py) was adapted from\nHuggingFace's [modeling_opt.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py) file.  \n!!! warning\nMake sure to review and adhere to the original code's copyright and licensing terms!", "file_path": "contributing/model/basic.md"}
{"id": "b9d424f4a430761499888bd43e5543f17bafa6b604681f943c5a1189b9482e89", "heading": "Basic Model/2. Make your code compatible with vLLM", "level": 2, "text": "## 2. Make your code compatible with vLLM  \nTo ensure compatibility with vLLM, your model must meet the following requirements:", "file_path": "contributing/model/basic.md"}
{"id": "b9d424f4a430761499888bd43e5543f17bafa6b604681f943c5a1189b9482e89", "heading": "Basic Model/2. Make your code compatible with vLLM/Initialization Code", "level": 3, "text": "### Initialization Code  \nAll vLLM modules within the model must include a `prefix` argument in their constructor. This `prefix` is typically the full name of the module in the model's state dictionary and is crucial for:  \n- Runtime support: vLLM's attention operators are registered in a model's state by their full names. Each attention operator must have a unique prefix as its layer name to avoid conflicts.\n- Non-uniform quantization support: A quantized checkpoint can selectively quantize certain layers while keeping others in full precision. By providing the `prefix` during initialization, vLLM can match the current layer's `prefix` with the quantization configuration to determine if the layer should be initialized in quantized mode.  \nThe initialization code should look like this:  \n??? code  \n```python\nfrom torch import nn\nfrom vllm.config import VllmConfig\nfrom vllm.attention import Attention", "file_path": "contributing/model/basic.md"}
{"id": "b9d424f4a430761499888bd43e5543f17bafa6b604681f943c5a1189b9482e89", "heading": "Basic Model/2. Make your code compatible with vLLM/Initialization Code", "level": 3, "text": "class MyAttention(nn.Module):\ndef __init__(self, vllm_config: VllmConfig, prefix: str):\nsuper().__init__()\nself.attn = Attention(prefix=f\"{prefix}.attn\")\n\nclass MyDecoderLayer(nn.Module):\ndef __init__(self, vllm_config: VllmConfig, prefix: str):\nsuper().__init__()\nself.self_attn = MyAttention(prefix=f\"{prefix}.self_attn\")\n\nclass MyModel(nn.Module):\ndef __init__(self, vllm_config: VllmConfig, prefix: str):\nsuper().__init__()\nself.layers = nn.ModuleList(\n[MyDecoderLayer(vllm_config, prefix=f\"{prefix}.layers.{i}\") for i in range(vllm_config.model_config.hf_config.num_hidden_layers)]\n)\n\nclass MyModelForCausalLM(nn.Module):\ndef __init__(self, vllm_config: VllmConfig, prefix: str = \"\"):\nsuper().__init__()\nself.model = MyModel(vllm_config, prefix=f\"{prefix}.model\")\n```", "file_path": "contributing/model/basic.md"}
{"id": "b9d424f4a430761499888bd43e5543f17bafa6b604681f943c5a1189b9482e89", "heading": "Basic Model/2. Make your code compatible with vLLM/Computation Code", "level": 3, "text": "### Computation Code  \n- Add a `get_input_embeddings` method inside `MyModel` module that returns the text embeddings given `input_ids`. This is equivalent to directly calling the text embedding layer, but provides a unified interface in case `MyModel` is used within a composite multimodal model.  \n```python\nclass MyModel(nn.Module):\n...", "file_path": "contributing/model/basic.md"}
{"id": "b9d424f4a430761499888bd43e5543f17bafa6b604681f943c5a1189b9482e89", "heading": "Basic Model/2. Make your code compatible with vLLM/Computation Code", "level": 3, "text": "def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:\n...\n```  \n- Rewrite the [forward][torch.nn.Module.forward] method of your model to remove any unnecessary code, such as training-specific code. Modify the input parameters to treat `input_ids` and `positions` as flattened tensors with a single batch size dimension, without a max-sequence length dimension.  \n```python\ndef forward(\nself,\ninput_ids: torch.Tensor,\npositions: torch.Tensor,\nintermediate_tensors: IntermediateTensors | None = None,\ninputs_embeds: torch.Tensor | None = None,\n) -> torch.Tensor:\n...\n```  \n!!! note\nCurrently, vLLM supports the basic multi-head attention mechanism and its variant with rotary positional embeddings.\nIf your model employs a different attention mechanism, you will need to implement a new attention layer in vLLM.", "file_path": "contributing/model/basic.md"}
{"id": "b9d424f4a430761499888bd43e5543f17bafa6b604681f943c5a1189b9482e89", "heading": "Basic Model/2. Make your code compatible with vLLM/Computation Code", "level": 3, "text": "For reference, check out our [Llama implementation](gh-file:vllm/model_executor/models/llama.py). vLLM already supports a large number of models. It is recommended to find a model similar to yours and adapt it to your model's architecture. Check out <gh-dir:vllm/model_executor/models> for more examples.", "file_path": "contributing/model/basic.md"}
{"id": "b9d424f4a430761499888bd43e5543f17bafa6b604681f943c5a1189b9482e89", "heading": "Basic Model/3. (Optional) Implement tensor parallelism and quantization support", "level": 2, "text": "## 3. (Optional) Implement tensor parallelism and quantization support  \nIf your model is too large to fit into a single GPU, you can use tensor parallelism to manage it.\nTo do this, substitute your model's linear and embedding layers with their tensor-parallel versions.\nFor the embedding layer, you can simply replace [torch.nn.Embedding][] with `VocabParallelEmbedding`. For the output LM head, you can use `ParallelLMHead`.\nWhen it comes to the linear layers, we provide the following options to parallelize them:  \n- `ReplicatedLinear`: Replicates the inputs and weights across multiple GPUs. No memory saving.\n- `RowParallelLinear`: The input tensor is partitioned along the hidden dimension. The weight matrix is partitioned along the rows (input dimension). An *all-reduce* operation is performed after the matrix multiplication to reduce the results. Typically used for the second FFN layer and the output linear transformation of the attention layer.", "file_path": "contributing/model/basic.md"}
{"id": "b9d424f4a430761499888bd43e5543f17bafa6b604681f943c5a1189b9482e89", "heading": "Basic Model/3. (Optional) Implement tensor parallelism and quantization support", "level": 2, "text": "- `ColumnParallelLinear`: The input tensor is replicated. The weight matrix is partitioned along the columns (output dimension). The result is partitioned along the column dimension. Typically used for the first FFN layer and the separated QKV transformation of the attention layer in the original Transformer.\n- `MergedColumnParallelLinear`: Column-parallel linear that merges multiple `ColumnParallelLinear` operators. Typically used for the first FFN layer with weighted activation functions (e.g., SiLU). This class handles the sharded weight loading logic of multiple weight matrices.\n- `QKVParallelLinear`: Parallel linear layer for the query, key, and value projections of the multi-head and grouped-query attention mechanisms. When number of key/value heads are less than the world size, this class replicates the key/value heads properly. This class handles the weight loading and replication of the weight matrices.", "file_path": "contributing/model/basic.md"}
{"id": "b9d424f4a430761499888bd43e5543f17bafa6b604681f943c5a1189b9482e89", "heading": "Basic Model/3. (Optional) Implement tensor parallelism and quantization support", "level": 2, "text": "Note that all the linear layers above take `linear_method` as an input. vLLM will set this parameter according to different quantization schemes to support weight quantization.", "file_path": "contributing/model/basic.md"}
{"id": "b9d424f4a430761499888bd43e5543f17bafa6b604681f943c5a1189b9482e89", "heading": "Basic Model/4. Implement the weight loading logic", "level": 2, "text": "## 4. Implement the weight loading logic  \nYou now need to implement the `load_weights` method in your `*ForCausalLM` class.\nThis method should load the weights from the HuggingFace's checkpoint file and assign them to the corresponding layers in your model. Specifically, for `MergedColumnParallelLinear` and `QKVParallelLinear` layers, if the original model has separated weight matrices, you need to load the different parts separately.", "file_path": "contributing/model/basic.md"}
{"id": "b9d424f4a430761499888bd43e5543f17bafa6b604681f943c5a1189b9482e89", "heading": "Basic Model/5. Register your model", "level": 2, "text": "## 5. Register your model  \nSee [this page](registration.md) for instructions on how to register your new model to be used by vLLM.", "file_path": "contributing/model/basic.md"}
{"id": "b9d424f4a430761499888bd43e5543f17bafa6b604681f943c5a1189b9482e89", "heading": "Basic Model/Frequently Asked Questions/How to support models with interleaving sliding windows?", "level": 3, "text": "## Frequently Asked Questions  \n### How to support models with interleaving sliding windows?  \nFor models with interleaving sliding windows (e.g. `google/gemma-2-2b-it` and `mistralai/Ministral-8B-Instruct-2410`), the scheduler will treat the model as a full-attention model, i.e., kv-cache of all tokens will not be dropped. This is to make sure prefix caching works with these models. Sliding window only appears as a parameter to the attention kernel computation.  \nTo support a model with interleaving sliding windows, we need to take care of the following details:  \n- Make sure the model's `config.json` contains `layer_types`.\n- In the modeling code, parse the correct sliding window value for every layer, and pass it to the attention layer's `per_layer_sliding_window` argument. For reference, check [this line](https://github.com/vllm-project/vllm/blob/996357e4808ca5eab97d4c97c7d25b3073f46aab/vllm/model_executor/models/llama.py#L171).", "file_path": "contributing/model/basic.md"}
{"id": "b9d424f4a430761499888bd43e5543f17bafa6b604681f943c5a1189b9482e89", "heading": "Basic Model/Frequently Asked Questions/How to support models with interleaving sliding windows?", "level": 3, "text": "With these two steps, interleave sliding windows should work with the model.", "file_path": "contributing/model/basic.md"}
{"id": "b9d424f4a430761499888bd43e5543f17bafa6b604681f943c5a1189b9482e89", "heading": "Basic Model/Frequently Asked Questions/How to support models that use Mamba?", "level": 3, "text": "### How to support models that use Mamba?  \nWe consider 3 different scenarios:  \n1. Models that use Mamba layers (either Mamba-1 or Mamba-2) but do not use attention layers.\n2. Models that combine Mamba layers (either Mamba-1 or Mamba-2) together with attention layers.\n3. Models that combine Mamba-like mechanisms (e.g., Linear Attention, ShortConv) together with attention layers.  \nFor case (1), we recommend looking at the implementation of [`MambaForCausalLM`](gh-file:vllm/model_executor/models/mamba.py) (for Mamba-1) or [`Mamba2ForCausalLM`](gh-file:vllm/model_executor/models/mamba2.py) (for Mamba-2) as a reference.\nThe model should inherit protocol `IsAttentionFree` and also implement class methods `get_mamba_state_dtype_from_config` and `get_mamba_state_shape_from_config` to calculate the state shapes and data types from the config.", "file_path": "contributing/model/basic.md"}
{"id": "b9d424f4a430761499888bd43e5543f17bafa6b604681f943c5a1189b9482e89", "heading": "Basic Model/Frequently Asked Questions/How to support models that use Mamba?", "level": 3, "text": "For the mamba layers themselves, please use the [`MambaMixer`](gh-file:vllm/model_executor/layers/mamba/mamba_mixer.py) (for Mamba-1) or [`MambaMixer2`](gh-file:vllm/model_executor/layers/mamba/mamba_mixer2.py) (for Mamba-2) classes.\nPlease *do not* use the `MambaCacheManager` (deprecated in V1) or replicate any of the V0-specific code paths in the existing model implementations.\nV0-only classes and code will be removed in the very near future.\nThe model should also be added to the `MODELS_CONFIG_MAP` dictionary in <gh-file:vllm/model_executor/models/config.py> to ensure that the runtime defaults are optimized.  \nFor case (2), we recommend using as a reference the implementation of [`JambaForCausalLM`](gh-file:vllm/model_executor/models/jamba.py) (for an example of a model that uses Mamba-1 and attention together) or [`BambaForCausalLM`](gh-file:vllm/model_executor/models/bamba.py) (for an example of a model that uses Mamba-2 and attention together).", "file_path": "contributing/model/basic.md"}
{"id": "b9d424f4a430761499888bd43e5543f17bafa6b604681f943c5a1189b9482e89", "heading": "Basic Model/Frequently Asked Questions/How to support models that use Mamba?", "level": 3, "text": "These models should follow the same instructions as case (1), but they should inherit protocol `IsHybrid` (instead of `IsAttentionFree`) and it is *not* necessary to add them to the `MODELS_CONFIG_MAP` (their runtime defaults will be inferred from the protocol).  \nFor case (3), we recommend looking at the implementation of [`MiniMaxText01ForCausalLM`](gh-file:vllm/model_executor/models/minimax_text_01.py) or [`Lfm2ForCausalLM`](gh-file:vllm/model_executor/models/lfm2.py) as a reference, which use custom \"mamba-like\" layers `MiniMaxText01LinearAttention` and `ShortConv` respectively.\nPlease follow the same guidelines as case (2) for implementing these models.\nWe use \"mamba-like\" to refer to layers that posses a state that is updated in-place, rather than being appended-to (like KV cache for attention).", "file_path": "contributing/model/basic.md"}
{"id": "b9d424f4a430761499888bd43e5543f17bafa6b604681f943c5a1189b9482e89", "heading": "Basic Model/Frequently Asked Questions/How to support models that use Mamba?", "level": 3, "text": "For implementing new custom mamba-like layers, one should inherit from `MambaBase` and implement the methods `get_state_dtype`, `get_state_shape` to calculate the data types and state shapes at runtime, as well as `mamba_type` and `get_attn_backend`.\nIt is also necessary to implement the \"attention meta-data\" class which handles the meta-data that is common across all layers.\nPlease see [`LinearAttentionMetadata`](gh-file:vllm/v1/attention/backends/linear_attn.py) or [`ShortConvAttentionMetadata`](gh-file:v1/attention/backends/short_conv_attn.py) for examples of this.\nFinally, if one wants to support torch compile and CUDA graphs, it necessary to wrap the call to the mamba-like layer inside a custom op and register it.\nPlease see the calls to `direct_register_custom_op` in <gh-file:vllm/model_executor/models/minimax_text_01.py> or <gh-file:vllm/model_executor/layers/mamba/short_conv.py> for examples of this.", "file_path": "contributing/model/basic.md"}
{"id": "b9d424f4a430761499888bd43e5543f17bafa6b604681f943c5a1189b9482e89", "heading": "Basic Model/Frequently Asked Questions/How to support models that use Mamba?", "level": 3, "text": "The new custom op should then be added to the list `_attention_ops` in <gh-file:vllm/config/compilation.py> to ensure that piecewise CUDA graphs works as intended.", "file_path": "contributing/model/basic.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support", "level": 1, "text": "# Multi-Modal Support  \nThis document walks you through the steps to extend a basic model so that it accepts [multi-modal inputs](../../features/multimodal_inputs.md).", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/1. Update the base vLLM model", "level": 2, "text": "## 1. Update the base vLLM model  \nIt is assumed that you have already implemented the model in vLLM according to [these steps](basic.md).\nFurther update the model as follows:  \n- Implement [get_placeholder_str][vllm.model_executor.models.interfaces.SupportsMultiModal.get_placeholder_str] to define the placeholder string which is used to represent the multi-modal item in the text prompt. This should be consistent with the chat template of the model.  \n??? code  \n```python\nclass YourModelForImage2Seq(nn.Module):\n...\n\n@classmethod\ndef get_placeholder_str(cls, modality: str, i: int) -> str | None:\nif modality.startswith(\"image\"):\nreturn \"<image>\"", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/1. Update the base vLLM model", "level": 2, "text": "raise ValueError(\"Only image modality is supported\")\n```  \n- Reserve a keyword parameter in [forward][torch.nn.Module.forward] for each input tensor that corresponds to a multi-modal input, as shown in the following example:  \n```diff\ndef forward(\nself,\ninput_ids: torch.Tensor,\npositions: torch.Tensor,\n+     pixel_values: torch.Tensor,\n) -> SamplerOutput:\n```  \nMore conveniently, you can simply pass `**kwargs` to the [forward][torch.nn.Module.forward] method and retrieve the keyword parameters for multimodal inputs from it.  \n- Implement [get_multimodal_embeddings][vllm.model_executor.models.interfaces.SupportsMultiModal.get_multimodal_embeddings] that returns the embeddings from running the multimodal inputs through the multimodal tokenizer of the model. Below we provide a boilerplate of a typical implementation pattern, but feel free to adjust it to your own needs.  \n??? code  \n```python\nclass YourModelForImage2Seq(nn.Module):\n...", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/1. Update the base vLLM model", "level": 2, "text": "def _process_image_input(self, image_input: YourModelImageInputs) -> torch.Tensor:\nassert self.vision_encoder is not None\nimage_features = self.vision_encoder(image_input)\nreturn self.multi_modal_projector(image_features)\n\ndef get_multimodal_embeddings(\nself,\n**kwargs: object,\n) -> MultiModalEmbeddings | None:\n# Validate the multimodal input keyword arguments\nimage_input = self._parse_and_validate_image_input(**kwargs)\nif image_input is None:\nreturn None", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/1. Update the base vLLM model", "level": 2, "text": "# Run multimodal inputs through encoder and projector\nvision_embeddings = self._process_image_input(image_input)\nreturn vision_embeddings\n```  \n!!! important\nThe returned `multimodal_embeddings` must be either a **3D [torch.Tensor][]** of shape `(num_items, feature_size, hidden_size)`, or a **list / tuple of 2D [torch.Tensor][]'s** of shape `(feature_size, hidden_size)`, so that `multimodal_embeddings[i]` retrieves the embeddings generated from the `i`-th multimodal data item (e.g, image) of the request.  \n!!! note\nBy default, vLLM merges the multimodal embeddings into text embeddings depending on the information of their locations defined in\n[PlaceholderRange][vllm.multimodal.inputs.PlaceholderRange] from input processing.\nThis logic can be found at [get_input_embeddings][vllm.model_executor.models.interfaces.SupportsMultiModal.get_input_embeddings].  \nYou may override this method if additional logic is required for your model when merging embeddings.", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/1. Update the base vLLM model", "level": 2, "text": "- Implement [get_language_model][vllm.model_executor.models.interfaces.SupportsMultiModal.get_language_model] getter to provide stable access to the underlying language model.  \n```python\nclass YourModelForImage2Seq(nn.Module):\n...", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/1. Update the base vLLM model", "level": 2, "text": "def get_language_model(self) -> torch.nn.Module:\n# Change `language_model` according to your implementation.\nreturn self.language_model\n```  \n- Once the above steps are done, update the model class with the [SupportsMultiModal][vllm.model_executor.models.interfaces.SupportsMultiModal] interface.  \n```diff\n+ from vllm.model_executor.models.interfaces import SupportsMultiModal\n\n- class YourModelForImage2Seq(nn.Module):\n+ class YourModelForImage2Seq(nn.Module, SupportsMultiModal):\n```  \n!!! note\nThe model class does not have to be named `*ForCausalLM`.\nCheck out [the HuggingFace Transformers documentation](https://huggingface.co/docs/transformers/model_doc/auto#multimodal) for some examples.", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/2. Specify processing information", "level": 2, "text": "## 2. Specify processing information  \nNext, create a subclass of [BaseProcessingInfo][vllm.multimodal.processing.BaseProcessingInfo]\nto provide basic information related to HF processing.", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/2. Specify processing information/Maximum number of input items", "level": 3, "text": "### Maximum number of input items  \nYou need to override the abstract method [get_supported_mm_limits][vllm.multimodal.processing.BaseProcessingInfo.get_supported_mm_limits]\nto return the maximum number of input items for each modality supported by the model.  \nFor example, if the model supports any number of images but only one video per prompt:  \n```python\ndef get_supported_mm_limits(self) -> Mapping[str, int | None]:\nreturn {\"image\": None, \"video\": 1}\n```", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/3. Specify dummy inputs", "level": 2, "text": "## 3. Specify dummy inputs  \nThen, inherit [BaseDummyInputsBuilder][vllm.multimodal.profiling.BaseDummyInputsBuilder] to construct dummy inputs for\nHF processing as well as memory profiling.", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/3. Specify dummy inputs/For memory profiling", "level": 3, "text": "### For memory profiling  \nOverride the abstract methods [get_dummy_text][vllm.multimodal.profiling.BaseDummyInputsBuilder.get_dummy_text] and [get_dummy_mm_data][vllm.multimodal.profiling.BaseDummyInputsBuilder.get_dummy_mm_data] to construct dummy inputs for memory profiling. These dummy inputs should result in the worst-case memory usage of the model so that vLLM can reserve the correct amount of memory for it.  \nAssuming that the memory usage increases with the number of tokens, the dummy inputs can be constructed to maximize the number of output embeddings, which is the same number as placeholder feature tokens.  \n=== \"Basic example: LLaVA\"  \nLooking at the code of HF's `LlavaForConditionalGeneration`:  \n??? code  \n```python\n# https://github.com/huggingface/transformers/blob/v4.47.1/src/transformers/models/llava/modeling_llava.py#L530-L544\nn_image_tokens = (input_ids == self.config.image_token_index).sum().item()\nn_image_features = image_features.shape[0] * image_features.shape[1]", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/3. Specify dummy inputs/For memory profiling", "level": 3, "text": "if n_image_tokens != n_image_features:\nraise ValueError(\nf\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n)\nspecial_image_mask = (\n(input_ids == self.config.image_token_index)\n.unsqueeze(-1)\n.expand_as(inputs_embeds)\n.to(inputs_embeds.device)\n)\nimage_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\ninputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n```  \nThe number of placeholder feature tokens per image is `image_features.shape[1]`.\n`image_features` is calculated inside the `get_image_features` method:  \n??? code  \n```python\n# https://github.com/huggingface/transformers/blob/v4.47.1/src/transformers/models/llava/modeling_llava.py#L290-L300\nimage_outputs = self.vision_tower(pixel_values, output_hidden_states=True)", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/3. Specify dummy inputs/For memory profiling", "level": 3, "text": "selected_image_feature = image_outputs.hidden_states[vision_feature_layer]\nif vision_feature_select_strategy == \"default\":\nselected_image_feature = selected_image_feature[:, 1:]\nelif vision_feature_select_strategy == \"full\":\nselected_image_feature = selected_image_feature\nelse:\nraise ValueError(f\"Unexpected select feature strategy: {self.config.vision_feature_select_strategy}\")\nimage_features = self.multi_modal_projector(selected_image_feature)\nreturn image_features\n```  \nWe can infer that `image_features.shape[1]` is based on `image_outputs.hidden_states.shape[1]` from the vision tower\n(`CLIPVisionModel` for the [`llava-hf/llava-1.5-7b-hf`](https://huggingface.co/llava-hf/llava-1.5-7b-hf) model).\nMoreover, we only need the sequence length (the second dimension of the tensor) to get `image_features.shape[1]`.\nThe sequence length is determined by the initial hidden states in `CLIPVisionTransformer` since the attention\nmechanism doesn't change the sequence length of the output hidden states.  \n```python", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/3. Specify dummy inputs/For memory profiling", "level": 3, "text": "```python\n# https://github.com/huggingface/transformers/blob/v4.47.1/src/transformers/models/clip/modeling_clip.py#L1094-L1102\nhidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\nhidden_states = self.pre_layrnorm(hidden_states)", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/3. Specify dummy inputs/For memory profiling", "level": 3, "text": "encoder_outputs = self.encoder(\ninputs_embeds=hidden_states,\noutput_attentions=output_attentions,\noutput_hidden_states=output_hidden_states,\nreturn_dict=return_dict,\n)\n```  \nTo find the sequence length, we turn to the code of `CLIPVisionEmbeddings`:  \n??? code  \n```python\n# https://github.com/huggingface/transformers/blob/v4.47.1/src/transformers/models/clip/modeling_clip.py#L247-L257\ntarget_dtype = self.patch_embedding.weight.dtype\npatch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\npatch_embeds = patch_embeds.flatten(2).transpose(1, 2)", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/3. Specify dummy inputs/For memory profiling", "level": 3, "text": "class_embeds = self.class_embedding.expand(batch_size, 1, -1)\nembeddings = torch.cat([class_embeds, patch_embeds], dim=1)\nif interpolate_pos_encoding:\nembeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\nelse:\nembeddings = embeddings + self.position_embedding(self.position_ids)\nreturn embeddings\n```  \nWe can infer that `embeddings.shape[1] == self.num_positions`, where  \n```python\n# https://github.com/huggingface/transformers/blob/v4.47.1/src/transformers/models/clip/modeling_clip.py#L195-L196\nself.num_patches = (self.image_size // self.patch_size) ** 2\nself.num_positions = self.num_patches + 1\n```  \nOverall, the number of placeholder feature tokens for an image can be calculated as:  \n??? code  \n```python\ndef get_num_image_tokens(\nself,\n*,\nimage_width: int,\nimage_height: int,\n) -> int:\nhf_config = self.get_hf_config()\nhf_processor = self.get_hf_processor()\n\nimage_size = hf_config.vision_config.image_size\npatch_size = hf_config.vision_config.patch_size", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/3. Specify dummy inputs/For memory profiling", "level": 3, "text": "num_image_tokens = (image_size // patch_size) ** 2 + 1\nif hf_processor.vision_feature_select_strategy == \"default\":\nnum_image_tokens -= 1\n\nreturn num_image_tokens\n```  \nNotice that the number of image tokens doesn't depend on the image width and height.\nWe can simply use a dummy `image_size` to calculate the multimodal profiling data:  \n??? code  \n```python\n# NOTE: In actuality, this is usually implemented as part of the\n# model's subclass of `BaseProcessingInfo`, but we show it as is\n# here for simplicity.\ndef get_image_size_with_most_features(self) -> ImageSize:\nhf_config = self.get_hf_config()\nwidth = height = hf_config.image_size\nreturn ImageSize(width=width, height=height)\n\ndef get_dummy_mm_data(\nself,\nseq_len: int,\nmm_counts: Mapping[str, int],\nmm_options: Mapping[str, BaseDummyOptions] | None = None,\n) -> MultiModalDataDict:\nnum_images = mm_counts.get(\"image\", 0)\n\ntarget_width, target_height = \\\nself.info.get_image_size_with_most_features()", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/3. Specify dummy inputs/For memory profiling", "level": 3, "text": "image_overrides = mm_options.get(\"image\") if mm_options else None\n\nreturn {\n\"image\":\nself._get_dummy_images(width=target_width,\nheight=target_height,\nnum_images=num_images,\noverrides=image_overrides)\n}\n```  \nFor the text, we simply expand the multimodal image token from the model config to match the desired number of images.  \n```python\ndef get_dummy_text(self, mm_counts: Mapping[str, int]) -> str:\nnum_images = mm_counts.get(\"image\", 0)\n\nprocessor = self.info.get_hf_processor()\nimage_token = processor.image_token", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/3. Specify dummy inputs/For memory profiling", "level": 3, "text": "return image_token * num_images\n```  \n=== \"No input placeholders: Fuyu\"  \nLooking at the code of HF's `FuyuForCausalLM`:  \n??? code  \n```python\n# https://github.com/huggingface/transformers/blob/v4.48.3/src/transformers/models/fuyu/modeling_fuyu.py#L311-L322\nif image_patches is not None and past_key_values is None:\npatch_embeddings = [\nself.vision_embed_tokens(patch.to(self.vision_embed_tokens.weight.dtype))\n.squeeze(0)\n.to(inputs_embeds.device)\nfor patch in image_patches\n]\ninputs_embeds = self.gather_continuous_embeddings(\nword_embeddings=inputs_embeds,\ncontinuous_embeddings=patch_embeddings,\nimage_patch_input_indices=image_patches_indices,\n)\n```  \nThe number of placeholder feature tokens for the `i`th item in the batch is `patch_embeddings[i].shape[0]`,\nwhich is the same as `image_patches[i].shape[0]`, i.e. `num_total_patches`.  \nUnlike LLaVA, Fuyu does not define the number of patches inside the modeling file. Where can we get more information?", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/3. Specify dummy inputs/For memory profiling", "level": 3, "text": "Considering that the model input comes from the output of `FuyuProcessor`, let's **look at the preprocessing files**.  \nThe image outputs are obtained by calling `FuyuImageProcessor.preprocess` and then\n`FuyuImageProcessor.preprocess_with_tokenizer_info` inside `FuyuProcessor`.  \nIn `FuyuImageProcessor.preprocess`, the images are resized and padded to the target `FuyuImageProcessor.size`,\nreturning the dimensions after resizing (but before padding) as metadata.  \n??? code  \n```python\n# https://github.com/huggingface/transformers/blob/v4.48.3/src/transformers/models/fuyu/processing_fuyu.py#L541-L544\nimage_encoding = self.image_processor.preprocess(images, **output_kwargs[\"images_kwargs\"])\nbatch_images = image_encoding[\"images\"]\nimage_unpadded_heights = image_encoding[\"image_unpadded_heights\"]\nimage_unpadded_widths = image_encoding[\"image_unpadded_widths\"]", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/3. Specify dummy inputs/For memory profiling", "level": 3, "text": "# https://github.com/huggingface/transformers/blob/v4.48.3/src/transformers/models/fuyu/image_processing_fuyu.py#L480-L\nif do_resize:\nbatch_images = [\n[self.resize(image, size=size, input_data_format=input_data_format) for image in images]\nfor images in batch_images\n]\n\nimage_sizes = [get_image_size(images[0], channel_dim=input_data_format) for images in batch_images]\nimage_unpadded_heights = [[image_size[0]] for image_size in image_sizes]\nimage_unpadded_widths = [[image_size[1]] for image_size in image_sizes]", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/3. Specify dummy inputs/For memory profiling", "level": 3, "text": "if do_pad:\nbatch_images = [\n[\nself.pad_image(\nimage,\nsize=size,\nmode=padding_mode,\nconstant_values=padding_value,\ninput_data_format=input_data_format,\n)\nfor image in images\n]\nfor images in batch_images\n]\n```  \nIn `FuyuImageProcessor.preprocess_with_tokenizer_info`, the images are split into patches based on this metadata:  \n??? code  \n```python\n# https://github.com/huggingface/transformers/blob/v4.48.3/src/transformers/models/fuyu/processing_fuyu.py#L417-L425\nmodel_image_input = self.image_processor.preprocess_with_tokenizer_info(\nimage_input=tensor_batch_images,\nimage_present=image_present,\nimage_unpadded_h=image_unpadded_heights,\nimage_unpadded_w=image_unpadded_widths,\nimage_placeholder_id=image_placeholder_id,\nimage_newline_id=image_newline_id,\nvariable_sized=True,\n)", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/3. Specify dummy inputs/For memory profiling", "level": 3, "text": "# https://github.com/huggingface/transformers/blob/v4.48.3/src/transformers/models/fuyu/image_processing_fuyu.py#L638-L658\nimage_height, image_width = image.shape[1], image.shape[2]\nif variable_sized:  # variable_sized=True\nnew_h = min(\nimage_height,\nmath.ceil(image_unpadded_h[batch_index, subseq_index] / patch_height) * patch_height,\n)\nnew_w = min(\nimage_width,\nmath.ceil(image_unpadded_w[batch_index, subseq_index] / patch_width) * patch_width,\n)\nimage = image[:, :new_h, :new_w]\nimage_height, image_width = new_h, new_w", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/3. Specify dummy inputs/For memory profiling", "level": 3, "text": "num_patches = self.get_num_patches(image_height=image_height, image_width=image_width)\ntensor_of_image_ids = torch.full(\n[num_patches], image_placeholder_id, dtype=torch.int32, device=image_input.device\n)\npatches = self.patchify_image(image=image.unsqueeze(0)).squeeze(0)\nassert num_patches == patches.shape[0]\n```  \nThe number of patches is in turn defined by `FuyuImageProcessor.get_num_patches`:  \n??? code  \n```python\n# https://github.com/huggingface/transformers/blob/v4.48.3/src/transformers/models/fuyu/image_processing_fuyu.py#L552-L562\npatch_size = patch_size if patch_size is not None else self.patch_size\npatch_height, patch_width = self.patch_size[\"height\"], self.patch_size[\"width\"]\n\nif image_height % patch_height != 0:\nraise ValueError(f\"{image_height=} must be divisible by {patch_height}\")\nif image_width % patch_width != 0:\nraise ValueError(f\"{image_width=} must be divisible by {patch_width}\")", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/3. Specify dummy inputs/For memory profiling", "level": 3, "text": "num_patches_per_dim_h = image_height // patch_height\nnum_patches_per_dim_w = image_width // patch_width\nnum_patches = num_patches_per_dim_h * num_patches_per_dim_w\n```  \nThese image patches correspond to placeholder tokens (`|SPEAKER|`). So, we just need to maximize the number of image patches. Since input images are first resized\nto fit within `image_processor.size`, we can maximize the number of image patches by inputting an image with size equal to `image_processor.size`.  \n```python\ndef get_image_size_with_most_features(self) -> ImageSize:\nimage_processor = self.get_image_processor()\nreturn ImageSize(\nwidth=image_processor.size[\"width\"],\nheight=image_processor.size[\"height\"],\n)\n```  \nFuyu does not expect image placeholders in the inputs to HF processor, so\nthe dummy prompt text is empty regardless of the number of images.  \n```python\ndef get_dummy_text(self, mm_counts: Mapping[str, int]) -> str:\nreturn \"\"\n```  \nFor the multimodal image profiling data, the logic is very similar to LLaVA:  \n??? code", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/3. Specify dummy inputs/For memory profiling", "level": 3, "text": "??? code  \n```python\ndef get_dummy_mm_data(\nself,\nseq_len: int,\nmm_counts: Mapping[str, int],\nmm_options: Optional[Mapping[str, BaseDummyOptions]] = None,\n) -> MultiModalDataDict:\ntarget_width, target_height = \\\nself.info.get_image_size_with_most_features()\nnum_images = mm_counts.get(\"image\", 0)", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/3. Specify dummy inputs/For memory profiling", "level": 3, "text": "image_overrides = mm_options.get(\"image\") if mm_options else None\n\nreturn {\n\"image\":\nself._get_dummy_images(\nwidth=target_width,\nheight=target_height,\nnum_images=num_images,\noverrides=image_overrides,\n)\n}\n```", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/4. Specify processing details", "level": 2, "text": "## 4. Specify processing details  \nAfterwards, create a subclass of [BaseMultiModalProcessor][vllm.multimodal.processing.BaseMultiModalProcessor]\nto fill in the missing details about HF processing.  \n!!! info\n[Multi-Modal Data Processing](../../design/mm_processing.md)", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/4. Specify processing details/Multi-modal fields", "level": 3, "text": "### Multi-modal fields  \nOverride [_get_mm_fields_config][vllm.multimodal.processing.BaseMultiModalProcessor._get_mm_fields_config] to\nreturn a schema of the tensors outputted by the HF processor that are related to the input multi-modal items.  \n=== \"Basic example: LLaVA\"  \nThe output of `CLIPImageProcessor` is a simple tensor with shape\n`(num_images, num_channels, image_height, image_width)`:  \n```python\n# https://github.com/huggingface/transformers/blob/v4.47.1/src/transformers/models/clip/image_processing_clip.py#L339-L345\nimages = [\nto_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\nfor image in all_images\n]", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/4. Specify processing details/Multi-modal fields", "level": 3, "text": "data = {\"pixel_values\": images}\nreturn BatchFeature(data=data, tensor_type=return_tensors)\n```  \nSo, we override [_get_mm_fields_config][vllm.multimodal.processing.BaseMultiModalProcessor._get_mm_fields_config] as follows:  \n```python\ndef _get_mm_fields_config(\nself,\nhf_inputs: BatchFeature,\nhf_processor_mm_kwargs: Mapping[str, object],\n) -> Mapping[str, MultiModalFieldConfig]:\nreturn dict(\npixel_values=MultiModalFieldConfig.batched(\"image\"),\n)\n```  \n!!! note\nOur [actual code](gh-file:vllm/model_executor/models/llava.py) additionally supports\npre-computed image embeddings, which can be passed to be model via the `image_embeds` argument.  \n=== \"With postprocessing: Fuyu\"  \nThe `image_patches` output of `FuyuImageProcessor.preprocess_with_tokenizer_info` concatenates\nthe patches from each image belonging to an item in the batch:  \n```python\n# https://github.com/huggingface/transformers/blob/v4.48.3/src/transformers/models/fuyu/image_processing_fuyu.py#L673-L679\nimage_input_ids.append(tensor_of_image_ids)", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/4. Specify processing details/Multi-modal fields", "level": 3, "text": "image_input_ids.append(tensor_of_image_ids)\nimage_patches.append(patches)\nelse:\nimage_input_ids.append(torch.tensor([], dtype=torch.int32, device=image_input.device))", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/4. Specify processing details/Multi-modal fields", "level": 3, "text": "batch_image_input_ids.append(image_input_ids)\nbatch_image_patches.append(image_patches)\n```  \nThe shape of `image_patches` outputted by `FuyuImageProcessor` is therefore\n`(1, num_images, num_patches, patch_width * patch_height * num_channels)`.  \nIn order to support the use of\n[MultiModalFieldConfig.batched][vllm.multimodal.inputs.MultiModalFieldConfig.batched]\nlike in LLaVA, we remove the extra batch dimension by overriding\n[BaseMultiModalProcessor._call_hf_processor][vllm.multimodal.processing.BaseMultiModalProcessor._call_hf_processor]:  \n??? code  \n```python\ndef _call_hf_processor(\nself,\nprompt: str,\nmm_data: Mapping[str, object],\nmm_kwargs: Mapping[str, object],\ntok_kwargs: Mapping[str, object],\n) -> BatchFeature:\nprocessed_outputs = super()._call_hf_processor(\nprompt=prompt,\nmm_data=mm_data,\nmm_kwargs=mm_kwargs,\ntok_kwargs=tok_kwargs,\n)\n\nimage_patches = processed_outputs.get(\"image_patches\")\nif image_patches is not None:\nimages = mm_data[\"images\"]\nassert isinstance(images, list)", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/4. Specify processing details/Multi-modal fields", "level": 3, "text": "# Original output: (1, num_images, Pn, Px * Py * C)\n# New output: (num_images, Pn, Px * Py * C)\nassert (isinstance(image_patches, list)\nand len(image_patches) == 1)\nassert (isinstance(image_patches[0], torch.Tensor)\nand len(image_patches[0]) == len(images))\n\nprocessed_outputs[\"image_patches\"] = image_patches[0]", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/4. Specify processing details/Multi-modal fields", "level": 3, "text": "processed_outputs[\"image_patches\"] = image_patches[0]\n\nreturn processed_outputs\n```  \n!!! note\nOur [actual code](gh-file:vllm/model_executor/models/fuyu.py) has special handling\nfor text-only inputs to prevent unnecessary warnings from HF processor.  \n!!! note\nThe `_call_hf_processor` method specifies both `mm_kwargs` and `tok_kwargs` for\nprocessing. `mm_kwargs` is used to both initialize and call the huggingface\nprocessor, whereas `tok_kwargs` is only used to call the huggingface processor.  \nThis lets us override [_get_mm_fields_config][vllm.multimodal.processing.BaseMultiModalProcessor._get_mm_fields_config] as follows:  \n```python\ndef _get_mm_fields_config(\nself,\nhf_inputs: BatchFeature,\nhf_processor_mm_kwargs: Mapping[str, object],\n) -> Mapping[str, MultiModalFieldConfig]:\nreturn dict(image_patches=MultiModalFieldConfig.batched(\"image\"))\n```", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/4. Specify processing details/Prompt updates", "level": 3, "text": "### Prompt updates  \nOverride [_get_prompt_updates][vllm.multimodal.processing.BaseMultiModalProcessor._get_prompt_updates] to\nreturn a list of [PromptUpdate][vllm.multimodal.processing.PromptUpdate] instances.  \nEach [PromptUpdate][vllm.multimodal.processing.PromptUpdate] instance specifies an update operation\n(e.g.: insertion, replacement) performed by the HF processor.  \n=== \"Basic example: LLaVA\"  \nLooking at HF's `LlavaProcessor`:  \n```python\n# https://github.com/huggingface/transformers/blob/v4.47.1/src/transformers/models/llava/processing_llava.py#L167-L170\nprompt_strings = []\nfor sample in text:\nsample = sample.replace(self.image_token, self.image_token * num_image_tokens)\nprompt_strings.append(sample)\n```  \nIt simply repeats each input `image_token` a number of times equal to the number of placeholder feature tokens (`num_image_tokens`).\nBased on this, we override [_get_prompt_updates][vllm.multimodal.processing.BaseMultiModalProcessor._get_prompt_updates] as follows:  \n??? code  \n```python", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/4. Specify processing details/Prompt updates", "level": 3, "text": "??? code  \n```python\ndef _get_prompt_updates(\nself,\nmm_items: MultiModalDataItems,\nhf_processor_mm_kwargs: Mapping[str, object],\nout_mm_kwargs: MultiModalKwargsItems,\n) -> Sequence[PromptUpdate]:\nhf_config = self.info.get_hf_config()\nimage_token_id = hf_config.image_token_index", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/4. Specify processing details/Prompt updates", "level": 3, "text": "def get_replacement(item_idx: int):\nimages = mm_items.get_items(\"image\", ImageProcessorItems)\n\nimage_size = images.get_image_size(item_idx)\nnum_image_tokens = self.info.get_num_image_tokens(\nimage_width=image_size.width,\nimage_height=image_size.height,\n)\n\nreturn [image_token_id] * num_image_tokens\n\nreturn [\nPromptReplacement(\nmodality=\"image\",\ntarget=[image_token_id],\nreplacement=get_replacement,\n),\n]\n```  \n=== \"Handling additional tokens: Fuyu\"  \nRecall the layout of feature tokens from Step 2:", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/4. Specify processing details/Prompt updates", "level": 3, "text": "```\n|SPEAKER||SPEAKER|...|SPEAKER||NEWLINE|\n|SPEAKER||SPEAKER|...|SPEAKER||NEWLINE|\n...\n|SPEAKER||SPEAKER|...|SPEAKER||NEWLINE|\n```  \nWe define a helper function to return `ncols` and `nrows` directly:  \n??? code  \n```python\ndef get_image_feature_grid_size(\nself,\n*,\nimage_width: int,\nimage_height: int,\n) -> tuple[int, int]:\nimage_processor = self.get_image_processor()\ntarget_width = image_processor.size[\"width\"]\ntarget_height = image_processor.size[\"height\"]\npatch_width = image_processor.patch_size[\"width\"]\npatch_height = image_processor.patch_size[\"height\"]\n\nif not (image_width <= target_width and image_height <= target_height):\nheight_scale_factor = target_height / image_height\nwidth_scale_factor = target_width / image_width\noptimal_scale_factor = min(height_scale_factor, width_scale_factor)\n\nimage_height = int(image_height * optimal_scale_factor)\nimage_width = int(image_width * optimal_scale_factor)", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/4. Specify processing details/Prompt updates", "level": 3, "text": "ncols = math.ceil(image_width / patch_width)\nnrows = math.ceil(image_height / patch_height)\nreturn ncols, nrows\n```  \nBased on this, we can initially define our replacement tokens as:  \n??? code  \n```python\ndef get_replacement(item_idx: int):\nimages = mm_items.get_items(\"image\", ImageProcessorItems)\nimage_size = images.get_image_size(item_idx)\n\nncols, nrows = self.info.get_image_feature_grid_size(\nimage_width=image_size.width,\nimage_height=image_size.height,\n)", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/4. Specify processing details/Prompt updates", "level": 3, "text": "# `_IMAGE_TOKEN_ID` corresponds to `|SPEAKER|`\n# `_NEWLINE_TOKEN_ID` corresponds to `|NEWLINE|`\nreturn ([_IMAGE_TOKEN_ID] * ncols + [_NEWLINE_TOKEN_ID]) * nrows\n```  \nHowever, this is not entirely correct. After `FuyuImageProcessor.preprocess_with_tokenizer_info` is called,\na BOS token (`<s>`) is also added to the promopt:  \n??? code  \n```python\n# https://github.com/huggingface/transformers/blob/v4.48.3/src/transformers/models/fuyu/processing_fuyu.py#L417-L435\nmodel_image_input = self.image_processor.preprocess_with_tokenizer_info(\nimage_input=tensor_batch_images,\nimage_present=image_present,\nimage_unpadded_h=image_unpadded_heights,\nimage_unpadded_w=image_unpadded_widths,\nimage_placeholder_id=image_placeholder_id,\nimage_newline_id=image_newline_id,\nvariable_sized=True,\n)\nprompt_tokens, prompts_length = _tokenize_prompts_with_image_and_batch(\ntokenizer=self.tokenizer,\nprompts=prompts,\nscale_factors=scale_factors,\nmax_tokens_to_generate=self.max_tokens_to_generate,", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/4. Specify processing details/Prompt updates", "level": 3, "text": "max_tokens_to_generate=self.max_tokens_to_generate,\nmax_position_embeddings=self.max_position_embeddings,\nadd_BOS=True,\nadd_beginning_of_answer_token=True,\n)\n```  \nTo assign the vision embeddings to only the image tokens, instead of a string\nyou can return an instance of [PromptUpdateDetails][vllm.multimodal.processing.PromptUpdateDetails]:  \n??? code  \n```python\nhf_config = self.info.get_hf_config()\nbos_token_id = hf_config.bos_token_id  # `<s>`\nassert isinstance(bos_token_id, int)", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/4. Specify processing details/Prompt updates", "level": 3, "text": "def get_replacement_fuyu(item_idx: int):\nimages = mm_items.get_items(\"image\", ImageProcessorItems)\nimage_size = images.get_image_size(item_idx)\n\nncols, nrows = self.info.get_image_feature_grid_size(\nimage_width=image_size.width,\nimage_height=image_size.height,\n)\nimage_tokens = ([_IMAGE_TOKEN_ID] * ncols + [_NEWLINE_TOKEN_ID]) * nrows\n\nreturn PromptUpdateDetails.select_token_id(\nimage_tokens + [bos_token_id],\nembed_token_id=_IMAGE_TOKEN_ID,\n)\n```  \nFinally, noticing that the HF processor removes the `|ENDOFTEXT|` token from the tokenized prompt,\nwe can search for it to conduct the replacement at the start of the string:  \n??? code  \n```python\ndef _get_prompt_updates(\nself,\nmm_items: MultiModalDataItems,\nhf_processor_mm_kwargs: Mapping[str, object],\nout_mm_kwargs: MultiModalKwargsItems,\n) -> Sequence[PromptUpdate]:\nhf_config = self.info.get_hf_config()\nbos_token_id = hf_config.bos_token_id\nassert isinstance(bos_token_id, int)", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/4. Specify processing details/Prompt updates", "level": 3, "text": "tokenizer = self.info.get_tokenizer()\neot_token_id = tokenizer.bos_token_id\nassert isinstance(eot_token_id, int)\n\ndef get_replacement_fuyu(item_idx: int):\nimages = mm_items.get_items(\"image\", ImageProcessorItems)\nimage_size = images.get_image_size(item_idx)\n\nncols, nrows = self.info.get_image_feature_grid_size(\nimage_width=image_size.width,\nimage_height=image_size.height,\n)\nimage_tokens = ([_IMAGE_TOKEN_ID] * ncols + [_NEWLINE_TOKEN_ID]) * nrows\n\nreturn PromptUpdateDetails.select_token_id(\nimage_tokens + [bos_token_id],\nembed_token_id=_IMAGE_TOKEN_ID,\n)\n\nreturn [\nPromptReplacement(\nmodality=\"image\",\ntarget=[eot_token_id],\nreplacement=get_replacement_fuyu,\n)\n]\n```", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/5. Register processor-related classes", "level": 2, "text": "## 5. Register processor-related classes  \nAfter you have defined [BaseProcessingInfo][vllm.multimodal.processing.BaseProcessingInfo] (Step 2),\n[BaseDummyInputsBuilder][vllm.multimodal.profiling.BaseDummyInputsBuilder] (Step 3),\nand [BaseMultiModalProcessor][vllm.multimodal.processing.BaseMultiModalProcessor] (Step 4),\ndecorate the model class with [MULTIMODAL_REGISTRY.register_processor][vllm.multimodal.registry.MultiModalRegistry.register_processor]\nto register them to the multi-modal registry:  \n```diff\nfrom vllm.model_executor.models.interfaces import SupportsMultiModal\n+ from vllm.multimodal import MULTIMODAL_REGISTRY\n\n+ @MULTIMODAL_REGISTRY.register_processor(\n+     YourMultiModalProcessor,\n+     info=YourProcessingInfo,\n+     dummy_inputs=YourDummyInputsBuilder,\n+ )\nclass YourModelForImage2Seq(nn.Module, SupportsMultiModal):\n```", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/Notes/Inserting feature tokens without replacement", "level": 3, "text": "## Notes  \n### Inserting feature tokens without replacement  \nSome HF processors directly insert feature tokens without replacing anything in the original prompt. In that case, you can use [PromptInsertion][vllm.multimodal.processing.PromptInsertion] instead of [PromptReplacement][vllm.multimodal.processing.PromptReplacement] inside [_get_prompt_updates][vllm.multimodal.processing.BaseMultiModalProcessor._get_prompt_updates].  \nExamples:  \n- BLIP-2 (insert at start of prompt): <gh-file:vllm/model_executor/models/blip2.py>\n- Molmo (insert after `<|endoftext|>` token): <gh-file:vllm/model_executor/models/molmo.py>", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/Notes/Handling prompt updates unrelated to multi-modal data", "level": 3, "text": "### Handling prompt updates unrelated to multi-modal data  \n[_get_prompt_updates][vllm.multimodal.processing.BaseMultiModalProcessor._get_prompt_updates] assumes that each application of prompt update corresponds to one multi-modal item. If the HF processor performs additional processing regardless of how many multi-modal items there are, you should override [_apply_hf_processor_tokens_only][vllm.multimodal.processing.BaseMultiModalProcessor._apply_hf_processor_tokens_only] so that the processed token inputs are consistent with the result of applying the HF processor on text inputs. This is because token inputs bypass the HF processor according to [our design](../../design/mm_processing.md).  \nExamples:  \n- Chameleon (appends `sep_token`): <gh-file:vllm/model_executor/models/chameleon.py>\n- Fuyu (appends `boa_token`): <gh-file:vllm/model_executor/models/fuyu.py>\n- Molmo (applies chat template which is not defined elsewhere): <gh-file:vllm/model_executor/models/molmo.py>", "file_path": "contributing/model/multimodal.md"}
{"id": "c2eda2c4c8b852b73dbc360536c4e678fea2703a2650b452c9eded9f8286f4db", "heading": "Multi-Modal Support/Notes/Custom HF processor", "level": 3, "text": "### Custom HF processor  \nSome models don't define an HF processor class on HF Hub. In that case, you can define a custom HF processor that has the same call signature as HF processors and pass it to [_call_hf_processor][vllm.multimodal.processing.BaseMultiModalProcessor._call_hf_processor].  \nExamples:  \n- DeepSeek-VL2: <gh-file:vllm/model_executor/models/deepseek_vl2.py>\n- InternVL: <gh-file:vllm/model_executor/models/internvl.py>\n- Qwen-VL: <gh-file:vllm/model_executor/models/qwen_vl.py>", "file_path": "contributing/model/multimodal.md"}
{"id": "b9d56bbea217152373b6c4286694fc0738d9d01d3401761e1dcc99fd1794ad73", "heading": "Registering a Model", "level": 1, "text": "# Registering a Model  \nvLLM relies on a model registry to determine how to run each model.\nA list of pre-registered architectures can be found [here](../../models/supported_models.md).  \nIf your model is not on this list, you must register it to vLLM.\nThis page provides detailed instructions on how to do so.", "file_path": "contributing/model/registration.md"}
{"id": "b9d56bbea217152373b6c4286694fc0738d9d01d3401761e1dcc99fd1794ad73", "heading": "Registering a Model/Built-in models", "level": 2, "text": "## Built-in models  \nTo add a model directly to the vLLM library, start by forking our [GitHub repository](https://github.com/vllm-project/vllm) and then [build it from source][build-from-source].\nThis gives you the ability to modify the codebase and test your model.  \nAfter you have implemented your model (see [tutorial](basic.md)), put it into the <gh-dir:vllm/model_executor/models> directory.\nThen, add your model class to `_VLLM_MODELS` in <gh-file:vllm/model_executor/models/registry.py> so that it is automatically registered upon importing vLLM.\nFinally, update our [list of supported models](../../models/supported_models.md) to promote your model!  \n!!! important\nThe list of models in each section should be maintained in alphabetical order.", "file_path": "contributing/model/registration.md"}
{"id": "b9d56bbea217152373b6c4286694fc0738d9d01d3401761e1dcc99fd1794ad73", "heading": "Registering a Model/Out-of-tree models", "level": 2, "text": "## Out-of-tree models  \nYou can load an external model [using a plugin](../../design/plugin_system.md) without modifying the vLLM codebase.  \nTo register the model, use the following code:  \n```python\n# The entrypoint of your plugin\ndef register():\nfrom vllm import ModelRegistry\nfrom your_code import YourModelForCausalLM\n\nModelRegistry.register_model(\"YourModelForCausalLM\", YourModelForCausalLM)\n```  \nIf your model imports modules that initialize CUDA, consider lazy-importing it to avoid errors like `RuntimeError: Cannot re-initialize CUDA in forked subprocess`:  \n```python\n# The entrypoint of your plugin\ndef register():\nfrom vllm import ModelRegistry\n\nModelRegistry.register_model(\n\"YourModelForCausalLM\",\n\"your_code:YourModelForCausalLM\",\n)\n```  \n!!! important\nIf your model is a multimodal model, ensure the model class implements the [SupportsMultiModal][vllm.model_executor.models.interfaces.SupportsMultiModal] interface.\nRead more about that [here](multimodal.md).", "file_path": "contributing/model/registration.md"}
{"id": "e18f0e02c0ca5563e6b8d6d6e98ed569491b85e876d7e96297bf72206e91ae4c", "heading": "Unit Testing", "level": 1, "text": "# Unit Testing  \nThis page explains how to write unit tests to verify the implementation of your model.", "file_path": "contributing/model/tests.md"}
{"id": "e18f0e02c0ca5563e6b8d6d6e98ed569491b85e876d7e96297bf72206e91ae4c", "heading": "Unit Testing/Required Tests", "level": 2, "text": "## Required Tests  \nThese tests are necessary to get your PR merged into vLLM library.\nWithout them, the CI for your PR will fail.", "file_path": "contributing/model/tests.md"}
{"id": "e18f0e02c0ca5563e6b8d6d6e98ed569491b85e876d7e96297bf72206e91ae4c", "heading": "Unit Testing/Required Tests/Model loading", "level": 3, "text": "### Model loading  \nInclude an example HuggingFace repository for your model in <gh-file:tests/models/registry.py>.\nThis enables a unit test that loads dummy weights to ensure that the model can be initialized in vLLM.  \n!!! important\nThe list of models in each section should be maintained in alphabetical order.  \n!!! tip\nIf your model requires a development version of HF Transformers, you can set\n`min_transformers_version` to skip the test in CI until the model is released.", "file_path": "contributing/model/tests.md"}
{"id": "e18f0e02c0ca5563e6b8d6d6e98ed569491b85e876d7e96297bf72206e91ae4c", "heading": "Unit Testing/Optional Tests", "level": 2, "text": "## Optional Tests  \nThese tests are optional to get your PR merged into vLLM library.\nPassing these tests provides more confidence that your implementation is correct, and helps avoid future regressions.", "file_path": "contributing/model/tests.md"}
{"id": "e18f0e02c0ca5563e6b8d6d6e98ed569491b85e876d7e96297bf72206e91ae4c", "heading": "Unit Testing/Optional Tests/Model correctness", "level": 3, "text": "### Model correctness  \nThese tests compare the model outputs of vLLM against [HF Transformers](https://github.com/huggingface/transformers). You can add new tests under the subdirectories of <gh-dir:tests/models>.  \n#### Generative models  \nFor [generative models](../../models/generative_models.md), there are two levels of correctness tests, as defined in <gh-file:tests/models/utils.py>:  \n- Exact correctness (`check_outputs_equal`): The text outputted by vLLM should exactly match the text outputted by HF.\n- Logprobs similarity (`check_logprobs_close`): The logprobs outputted by vLLM should be in the top-k logprobs outputted by HF, and vice versa.  \n#### Pooling models  \nFor [pooling models](../../models/pooling_models.md), we simply check the cosine similarity, as defined in <gh-file:tests/models/utils.py>.  \n[](){ #mm-processing-tests }", "file_path": "contributing/model/tests.md"}
{"id": "e18f0e02c0ca5563e6b8d6d6e98ed569491b85e876d7e96297bf72206e91ae4c", "heading": "Unit Testing/Optional Tests/Multi-modal processing", "level": 3, "text": "### Multi-modal processing  \n#### Common tests  \nAdding your model to <gh-file:tests/models/multimodal/processing/test_common.py> verifies that the following input combinations result in the same outputs:  \n- Text + multi-modal data\n- Tokens + multi-modal data\n- Text + cached multi-modal data\n- Tokens + cached multi-modal data  \n#### Model-specific tests  \nYou can add a new file under <gh-dir:tests/models/multimodal/processing> to run tests that only apply to your model.  \nFor example, if the HF processor for your model accepts user-specified keyword arguments, you can verify that the keyword arguments are being applied correctly, such as in <gh-file:tests/models/multimodal/processing/test_phi3v.py>.", "file_path": "contributing/model/tests.md"}
{"id": "04fadbc7ef9e6816086f4e52752a14ae2bbf45b5792455bc243a9456c76ed28c", "heading": "Speech-to-Text (Transcription/Translation) Support", "level": 1, "text": "# Speech-to-Text (Transcription/Translation) Support  \nThis document walks you through the steps to add support for speech-to-text (ASR) models to vLLMâ€™s transcription and translation APIs by implementing [SupportsTranscription][vllm.model_executor.models.interfaces.SupportsTranscription].\nPlease refer to the [supported models](../../models/supported_models.md#transcription) for further guidance.", "file_path": "contributing/model/transcription.md"}
{"id": "04fadbc7ef9e6816086f4e52752a14ae2bbf45b5792455bc243a9456c76ed28c", "heading": "Speech-to-Text (Transcription/Translation) Support/Update the base vLLM model", "level": 2, "text": "## Update the base vLLM model  \nIt is assumed you have already implemented your model in vLLM according to the basic model guide. Extend your model with the [SupportsTranscription][vllm.model_executor.models.interfaces.SupportsTranscription] interface and implement the following class attributes and methods.", "file_path": "contributing/model/transcription.md"}
{"id": "04fadbc7ef9e6816086f4e52752a14ae2bbf45b5792455bc243a9456c76ed28c", "heading": "Speech-to-Text (Transcription/Translation) Support/Update the base vLLM model/`supported_languages` and `supports_transcription_only`", "level": 3, "text": "### `supported_languages` and `supports_transcription_only`  \nDeclare supported languages and capabilities:  \n- The `supported_languages` mapping is validated at init time.\n- Set `supports_transcription_only=True` if the model should not serve text generation (eg Whisper).  \n??? code \"supported_languages and supports_transcription_only\"  \n```python\nfrom typing import ClassVar, Mapping, Literal\nimport numpy as np\nimport torch\nfrom torch import nn\n\nfrom vllm.config import ModelConfig, SpeechToTextConfig\nfrom vllm.inputs.data import PromptType\nfrom vllm.model_executor.models.interfaces import SupportsTranscription\n\nclass YourASRModel(nn.Module, SupportsTranscription):\n# Map of ISO 639-1 language codes to language names\nsupported_languages: ClassVar[Mapping[str, str]] = {\n\"en\": \"English\",\n\"it\": \"Italian\",\n# ... add more as needed\n}", "file_path": "contributing/model/transcription.md"}
{"id": "04fadbc7ef9e6816086f4e52752a14ae2bbf45b5792455bc243a9456c76ed28c", "heading": "Speech-to-Text (Transcription/Translation) Support/Update the base vLLM model/`supported_languages` and `supports_transcription_only`", "level": 3, "text": "# If your model only supports audio-conditioned generation\n# (no text-only generation), enable this flag.\nsupports_transcription_only: ClassVar[bool] = True\n```  \nProvide an ASR configuration via [get_speech_to_text_config][vllm.model_executor.models.interfaces.SupportsTranscription.get_speech_to_text_config].  \nThis is for controlling general behavior of the API when serving your model:  \n??? code \"get_speech_to_text_config()\"  \n```python\nclass YourASRModel(nn.Module, SupportsTranscription):\n...", "file_path": "contributing/model/transcription.md"}
{"id": "04fadbc7ef9e6816086f4e52752a14ae2bbf45b5792455bc243a9456c76ed28c", "heading": "Speech-to-Text (Transcription/Translation) Support/Update the base vLLM model/`supported_languages` and `supports_transcription_only`", "level": 3, "text": "@classmethod\ndef get_speech_to_text_config(\ncls,\nmodel_config: ModelConfig,\ntask_type: Literal[\"transcribe\", \"translate\"],\n) -> SpeechToTextConfig:\nreturn SpeechToTextConfig(\nsample_rate=16_000,\nmax_audio_clip_s=30,\n# Set to None to disable server-side chunking if your\n# model/processor handles it already\nmin_energy_split_window_size=None,\n)\n```  \nSee [Audio preprocessing and chunking](#audio-preprocessing-and-chunking) for what each field controls.  \nImplement the prompt construction via [get_generation_prompt][vllm.model_executor.models.interfaces.SupportsTranscription.get_generation_prompt]. The server passes you the resampled waveform and task parameters; you return a valid [PromptType][vllm.inputs.data.PromptType]. There are two common patterns:  \n#### Multimodal LLM with audio embeddings (e.g., Voxtral, Gemma3n)  \nReturn a dict containing `multi_modal_data` with the audio, and either a `prompt` string or `prompt_token_ids`:  \n??? code \"get_generation_prompt()\"  \n```python", "file_path": "contributing/model/transcription.md"}
{"id": "04fadbc7ef9e6816086f4e52752a14ae2bbf45b5792455bc243a9456c76ed28c", "heading": "Speech-to-Text (Transcription/Translation) Support/Update the base vLLM model/`supported_languages` and `supports_transcription_only`", "level": 3, "text": "??? code \"get_generation_prompt()\"  \n```python\nclass YourASRModel(nn.Module, SupportsTranscription):\n...", "file_path": "contributing/model/transcription.md"}
{"id": "04fadbc7ef9e6816086f4e52752a14ae2bbf45b5792455bc243a9456c76ed28c", "heading": "Speech-to-Text (Transcription/Translation) Support/Update the base vLLM model/`supported_languages` and `supports_transcription_only`", "level": 3, "text": "@classmethod\ndef get_generation_prompt(\ncls,\naudio: np.ndarray,\nstt_config: SpeechToTextConfig,\nmodel_config: ModelConfig,\nlanguage: str | None,\ntask_type: Literal[\"transcribe\", \"translate\"],\nrequest_prompt: str,\nto_language: str | None,\n) -> PromptType:\n# Example with a free-form instruction prompt\ntask_word = \"Transcribe\" if task_type == \"transcribe\" else \"Translate\"\nprompt = (\n\"<start_of_turn>user\\n\"\nf\"{task_word} this audio: <audio_soft_token>\"\n\"<end_of_turn>\\n<start_of_turn>model\\n\"\n)\n\nreturn {\n\"multi_modal_data\": {\"audio\": (audio, stt_config.sample_rate)},\n\"prompt\": prompt,\n}\n```  \nFor further clarification on multi modal inputs, please refer to [Multi-Modal Inputs](../../features/multimodal_inputs.md).  \n#### Encoderâ€“decoder audio-only (e.g., Whisper)  \nReturn a dict with separate `encoder_prompt` and `decoder_prompt` entries:  \n??? code \"get_generation_prompt()\"  \n```python\nclass YourASRModel(nn.Module, SupportsTranscription):\n...", "file_path": "contributing/model/transcription.md"}
{"id": "04fadbc7ef9e6816086f4e52752a14ae2bbf45b5792455bc243a9456c76ed28c", "heading": "Speech-to-Text (Transcription/Translation) Support/Update the base vLLM model/`supported_languages` and `supports_transcription_only`", "level": 3, "text": "@classmethod\ndef get_generation_prompt(\ncls,\naudio: np.ndarray,\nstt_config: SpeechToTextConfig,\nmodel_config: ModelConfig,\nlanguage: str | None,\ntask_type: Literal[\"transcribe\", \"translate\"],\nrequest_prompt: str,\nto_language: str | None,\n) -> PromptType:\nif language is None:\nraise ValueError(\"Language must be specified\")\n\nprompt = {\n\"encoder_prompt\": {\n\"prompt\": \"\",\n\"multi_modal_data\": {\n\"audio\": (audio, stt_config.sample_rate),\n},\n},\n\"decoder_prompt\": (\n(f\"<|prev|>{request_prompt}\" if request_prompt else \"\")\n+ f\"<|startoftranscript|><|{language}|>\"\n+ f\"<|{task_type}|><|notimestamps|>\"\n),\n}\nreturn cast(PromptType, prompt)\n```", "file_path": "contributing/model/transcription.md"}
{"id": "04fadbc7ef9e6816086f4e52752a14ae2bbf45b5792455bc243a9456c76ed28c", "heading": "Speech-to-Text (Transcription/Translation) Support/Update the base vLLM model/`validate_language` (optional)", "level": 3, "text": "### `validate_language` (optional)  \nLanguage validation via [validate_language][vllm.model_executor.models.interfaces.SupportsTranscription.validate_language]  \nIf your model requires a language and you want a default, override this method (see Whisper):  \n??? code \"validate_language()\"  \n```python\n@classmethod\ndef validate_language(cls, language: str | None) -> str | None:\nif language is None:\nlogger.warning(\n\"Defaulting to language='en'. If you wish to transcribe \"\n\"audio in a different language, pass the `language` field \"\n\"in the TranscriptionRequest.\"\n)\nlanguage = \"en\"\nreturn super().validate_language(language)\n```", "file_path": "contributing/model/transcription.md"}
{"id": "04fadbc7ef9e6816086f4e52752a14ae2bbf45b5792455bc243a9456c76ed28c", "heading": "Speech-to-Text (Transcription/Translation) Support/Update the base vLLM model/`get_num_audio_tokens` (optional)", "level": 3, "text": "### `get_num_audio_tokens` (optional)  \nToken accounting for streaming via [get_num_audio_tokens][vllm.model_executor.models.interfaces.SupportsTranscription.get_num_audio_tokens]  \nProvide a fast durationâ†’token estimate to improve streaming usage statistics:  \n??? code \"get_num_audio_tokens()\"  \n```python\nclass YourASRModel(nn.Module, SupportsTranscription):\n...\n\n@classmethod\ndef get_num_audio_tokens(\ncls,\naudio_duration_s: float,\nstt_config: SpeechToTextConfig,\nmodel_config: ModelConfig,\n) -> int | None:\n# Return None if unknown; otherwise return an estimate.\nreturn int(audio_duration_s * stt_config.sample_rate // 320)  # example\n```", "file_path": "contributing/model/transcription.md"}
{"id": "04fadbc7ef9e6816086f4e52752a14ae2bbf45b5792455bc243a9456c76ed28c", "heading": "Speech-to-Text (Transcription/Translation) Support/Audio preprocessing and chunking", "level": 2, "text": "## Audio preprocessing and chunking  \nThe API server takes care of basic audio I/O and optional chunking before building prompts:  \n- Resampling: Input audio is resampled to `SpeechToTextConfig.sample_rate` using `librosa`.\n- Chunking: If `SpeechToTextConfig.allow_audio_chunking` is True and the duration exceeds `max_audio_clip_s`, the server splits the audio into overlapping chunks and generates a prompt per chunk. Overlap is controlled by `overlap_chunk_second`.\n- Energy-aware splitting: When `min_energy_split_window_size` is set, the server finds low-energy regions to minimize cutting within words.  \nRelevant server logic:  \n??? code \"_preprocess_speech_to_text()\"  \n```python\n# vllm/entrypoints/openai/speech_to_text.py\nasync def _preprocess_speech_to_text(...):\nlanguage = self.model_cls.validate_language(request.language)\n...\ny, sr = librosa.load(bytes_, sr=self.asr_config.sample_rate)\nduration = librosa.get_duration(y=y, sr=sr)\ndo_split_audio = (self.asr_config.allow_audio_chunking", "file_path": "contributing/model/transcription.md"}
{"id": "04fadbc7ef9e6816086f4e52752a14ae2bbf45b5792455bc243a9456c76ed28c", "heading": "Speech-to-Text (Transcription/Translation) Support/Audio preprocessing and chunking", "level": 2, "text": "do_split_audio = (self.asr_config.allow_audio_chunking\nand duration > self.asr_config.max_audio_clip_s)\nchunks = [y] if not do_split_audio else self._split_audio(y, int(sr))\nprompts = []\nfor chunk in chunks:\nprompt = self.model_cls.get_generation_prompt(\naudio=chunk,\nstt_config=self.asr_config,\nmodel_config=self.model_config,\nlanguage=language,\ntask_type=self.task_type,\nrequest_prompt=request.prompt,\nto_language=to_language,\n)\nprompts.append(prompt)\nreturn prompts, duration\n```", "file_path": "contributing/model/transcription.md"}
{"id": "04fadbc7ef9e6816086f4e52752a14ae2bbf45b5792455bc243a9456c76ed28c", "heading": "Speech-to-Text (Transcription/Translation) Support/Exposing tasks automatically", "level": 2, "text": "## Exposing tasks automatically  \nvLLM automatically advertises transcription support if your model implements the interface:  \n```python\nif supports_transcription(model):\nif model.supports_transcription_only:\nreturn [\"transcription\"]\nsupported_tasks.append(\"transcription\")\n```  \nWhen enabled, the server initializes the transcription and translation handlers:  \n```python\nstate.openai_serving_transcription = OpenAIServingTranscription(...) if \"transcription\" in supported_tasks else None\nstate.openai_serving_translation = OpenAIServingTranslation(...) if \"transcription\" in supported_tasks else None\n```  \nNo extra registration is required beyond having your model class available via the model registry and implementing `SupportsTranscription`.", "file_path": "contributing/model/transcription.md"}
{"id": "04fadbc7ef9e6816086f4e52752a14ae2bbf45b5792455bc243a9456c76ed28c", "heading": "Speech-to-Text (Transcription/Translation) Support/Examples in-tree", "level": 2, "text": "## Examples in-tree  \n- Whisper encoderâ€“decoder (audio-only): <gh-file:vllm/model_executor/models/whisper.py>\n- Voxtral decoder-only (audio embeddings + LLM): <gh-file:vllm/model_executor/models/voxtral.py>\n- Gemma3n decoder-only with fixed instruction prompt: <gh-file:vllm/model_executor/models/gemma3n_mm.py>", "file_path": "contributing/model/transcription.md"}
{"id": "04fadbc7ef9e6816086f4e52752a14ae2bbf45b5792455bc243a9456c76ed28c", "heading": "Speech-to-Text (Transcription/Translation) Support/Test with the API", "level": 2, "text": "## Test with the API  \nOnce your model implements `SupportsTranscription`, you can test the endpoints (API mimics OpenAI):  \n- Transcription (ASR):  \n```bash\ncurl -s -X POST \\\n-H \"Authorization: Bearer $VLLM_API_KEY\" \\\n-H \"Content-Type: multipart/form-data\" \\\n-F \"file=@/path/to/audio.wav\" \\\n-F \"model=$MODEL_ID\" \\\nhttp://localhost:8000/v1/audio/transcriptions\n```  \n- Translation (source â†’ English unless otherwise supported):  \n```bash\ncurl -s -X POST \\\n-H \"Authorization: Bearer $VLLM_API_KEY\" \\\n-H \"Content-Type: multipart/form-data\" \\\n-F \"file=@/path/to/audio.wav\" \\\n-F \"model=$MODEL_ID\" \\\nhttp://localhost:8000/v1/audio/translations\n```  \nOr check out more examples in <gh-file:examples/online_serving>.  \n!!! note\n- If your model handles chunking internally (e.g., via its processor or encoder), set `min_energy_split_window_size=None` in the returned `SpeechToTextConfig` to disable server-side chunking.", "file_path": "contributing/model/transcription.md"}
{"id": "04fadbc7ef9e6816086f4e52752a14ae2bbf45b5792455bc243a9456c76ed28c", "heading": "Speech-to-Text (Transcription/Translation) Support/Test with the API", "level": 2, "text": "- Implementing `get_num_audio_tokens` improves accuracy of streaming usage metrics (`prompt_tokens`) without an extra forward pass.\n- For multilingual behavior, keep `supported_languages` aligned with actual model capabilities.", "file_path": "contributing/model/transcription.md"}
{"id": "5c0a87faa7baf28ae43a9838fcd16ff121c42d827fc5da80608c89bb61f5599b", "heading": "Profiling vLLM", "level": 1, "text": "# Profiling vLLM  \n!!! warning\nProfiling is only intended for vLLM developers and maintainers to understand the proportion of time spent in different parts of the codebase. **vLLM end-users should never turn on profiling** as it will significantly slow down the inference.", "file_path": "contributing/profiling.md"}
{"id": "5c0a87faa7baf28ae43a9838fcd16ff121c42d827fc5da80608c89bb61f5599b", "heading": "Profiling vLLM/Profile with PyTorch Profiler", "level": 2, "text": "## Profile with PyTorch Profiler  \nWe support tracing vLLM workers using the `torch.profiler` module. You can enable tracing by setting the `VLLM_TORCH_PROFILER_DIR` environment variable to the directory where you want to save the traces: `VLLM_TORCH_PROFILER_DIR=/mnt/traces/`. Additionally, you can control the profiling content by specifying the following environment variables:  \n- `VLLM_TORCH_PROFILER_RECORD_SHAPES=1` to enable recording Tensor Shapes, off by default\n- `VLLM_TORCH_PROFILER_WITH_PROFILE_MEMORY=1` to record memory, off by default\n- `VLLM_TORCH_PROFILER_WITH_STACK=1` to enable recording stack information, on by default\n- `VLLM_TORCH_PROFILER_WITH_FLOPS=1` to enable recording FLOPs, off by default  \nThe OpenAI server also needs to be started with the `VLLM_TORCH_PROFILER_DIR` environment variable set.  \nWhen using `vllm bench serve`, you can enable profiling by passing the `--profile` flag.  \nTraces can be visualized using <https://ui.perfetto.dev/>.  \n!!! tip", "file_path": "contributing/profiling.md"}
{"id": "5c0a87faa7baf28ae43a9838fcd16ff121c42d827fc5da80608c89bb61f5599b", "heading": "Profiling vLLM/Profile with PyTorch Profiler", "level": 2, "text": "!!! tip\nYou can directly call bench module without installing vLLM using `python -m vllm.entrypoints.cli.main bench`.  \n!!! tip\nOnly send a few requests through vLLM when profiling, as the traces can get quite large. Also, no need to untar the traces, they can be viewed directly.  \n!!! tip\nTo stop the profiler - it flushes out all the profile trace files to the directory. This takes time, for example for about 100 requests worth of data for a llama 70b, it takes about 10 minutes to flush out on a H100.\nSet the env variable VLLM_RPC_TIMEOUT to a big number before you start the server. Say something like 30 minutes.\n`export VLLM_RPC_TIMEOUT=1800000`", "file_path": "contributing/profiling.md"}
{"id": "5c0a87faa7baf28ae43a9838fcd16ff121c42d827fc5da80608c89bb61f5599b", "heading": "Profiling vLLM/Profile with PyTorch Profiler/Example commands and usage", "level": 3, "text": "### Example commands and usage  \n#### Offline Inference  \nRefer to <gh-file:examples/offline_inference/simple_profiling.py> for an example.  \n#### OpenAI Server  \n```bash\nVLLM_TORCH_PROFILER_DIR=./vllm_profile \\\nvllm serve meta-llama/Meta-Llama-3-70B\n```  \nvllm bench command:  \n```bash\nvllm bench serve \\\n--backend vllm \\\n--model meta-llama/Meta-Llama-3-70B \\\n--dataset-name sharegpt \\\n--dataset-path sharegpt.json \\\n--profile \\\n--num-prompts 2\n```", "file_path": "contributing/profiling.md"}
{"id": "5c0a87faa7baf28ae43a9838fcd16ff121c42d827fc5da80608c89bb61f5599b", "heading": "Profiling vLLM/Profile with NVIDIA Nsight Systems", "level": 2, "text": "## Profile with NVIDIA Nsight Systems  \nNsight systems is an advanced tool that exposes more profiling details, such as register and shared memory usage, annotated code regions and low-level CUDA APIs and events.  \n[Install nsight-systems](https://docs.nvidia.com/nsight-systems/InstallationGuide/index.html) using your package manager.\nThe following block is an example for Ubuntu.  \n```bash\napt update\napt install -y --no-install-recommends gnupg\necho \"deb http://developer.download.nvidia.com/devtools/repos/ubuntu$(source /etc/lsb-release; echo \"$DISTRIB_RELEASE\" | tr -d .)/$(dpkg --print-architecture) /\" | tee /etc/apt/sources.list.d/nvidia-devtools.list\napt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\napt update\napt install nsight-systems-cli\n```", "file_path": "contributing/profiling.md"}
{"id": "5c0a87faa7baf28ae43a9838fcd16ff121c42d827fc5da80608c89bb61f5599b", "heading": "Profiling vLLM/Profile with NVIDIA Nsight Systems/Example commands and usage", "level": 3, "text": "### Example commands and usage  \nWhen profiling with `nsys`, it is advisable to set the environment variable `VLLM_WORKER_MULTIPROC_METHOD=spawn`. The default is to use the `fork` method instead of `spawn`. More information on the topic can be found in the [Nsight Systems release notes](https://docs.nvidia.com/nsight-systems/ReleaseNotes/index.html#general-issues).  \n#### Offline Inference  \nFor basic usage, you can just append `nsys profile -o report.nsys-rep --trace-fork-before-exec=true --cuda-graph-trace=node` before any existing script you would run for offline inference.  \nThe following is an example using the `vllm bench latency` script:  \n```bash\nnsys profile -o report.nsys-rep \\\n--trace-fork-before-exec=true \\\n--cuda-graph-trace=node \\\nvllm bench latency \\\n--model meta-llama/Llama-3.1-8B-Instruct \\\n--num-iters-warmup 5 \\\n--num-iters 1 \\\n--batch-size 16 \\\n--input-len 512 \\\n--output-len 8\n```  \n#### OpenAI Server", "file_path": "contributing/profiling.md"}
{"id": "5c0a87faa7baf28ae43a9838fcd16ff121c42d827fc5da80608c89bb61f5599b", "heading": "Profiling vLLM/Profile with NVIDIA Nsight Systems/Example commands and usage", "level": 3, "text": "--input-len 512 \\\n--output-len 8\n```  \n#### OpenAI Server  \nTo profile the server, you will want to prepend your `vllm serve` command with `nsys profile` just like for offline inference, however you must specify `--delay XX --duration YY` parameters according to the needs of your benchmark. After the duration time has been used up, the server will be killed.  \n```bash\n# server\nnsys profile -o report.nsys-rep \\\n--trace-fork-before-exec=true \\\n--cuda-graph-trace=node \\\n--delay 30 \\\n--duration 60 \\\nvllm serve meta-llama/Llama-3.1-8B-Instruct", "file_path": "contributing/profiling.md"}
{"id": "5c0a87faa7baf28ae43a9838fcd16ff121c42d827fc5da80608c89bb61f5599b", "heading": "Profiling vLLM/Profile with NVIDIA Nsight Systems/Example commands and usage", "level": 3, "text": "# client\nvllm bench serve \\\n--backend vllm \\\n--model meta-llama/Llama-3.1-8B-Instruct \\\n--num-prompts 1 \\\n--dataset-name random \\\n--random-input 1024 \\\n--random-output 512\n```  \nIn practice, you should set the `--duration` argument to a large value. Whenever you want the server to stop profiling, run:  \n```bash\nnsys sessions list\n```  \nto get the session id in the form of `profile-XXXXX`, then run:  \n```bash\nnsys stop --session=profile-XXXXX\n```  \nto manually kill the profiler and generate your `nsys-rep` report.  \n#### Analysis  \nYou can view these profiles either as summaries in the CLI, using `nsys stats [profile-file]`, or in the GUI by installing Nsight [locally following the directions here](https://developer.nvidia.com/nsight-systems/get-started).  \n??? console \"CLI example\"  \n```bash\nnsys stats report1.nsys-rep\n...\n** CUDA GPU Kernel Summary (cuda_gpu_kern_sum):", "file_path": "contributing/profiling.md"}
{"id": "5c0a87faa7baf28ae43a9838fcd16ff121c42d827fc5da80608c89bb61f5599b", "heading": "Profiling vLLM/Profile with NVIDIA Nsight Systems/Example commands and usage", "level": 3, "text": "Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)  Max (ns)   StdDev (ns)                                                  Name\n--------  ---------------  ---------  -----------  -----------  --------  ---------  -----------  ----------------------------------------------------------------------------------------------------\n46.3   10,327,352,338     17,505    589,965.9    144,383.0    27,040  3,126,460    944,263.8  sm90_xmma_gemm_bf16bf16_bf16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_ofâ€¦\n14.8    3,305,114,764      5,152    641,520.7    293,408.0   287,296  2,822,716    867,124.9  sm90_xmma_gemm_bf16bf16_bf16f32_f32_tn_n_tilesize256x128x64_warpgroupsize2x1x1_execute_segment_k_ofâ€¦\n12.1    2,692,284,876     14,280    188,535.4     83,904.0    19,328  2,862,237    497,999.9  sm90_xmma_gemm_bf16bf16_bf16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1x1x1_execute_segment_k_offâ€¦", "file_path": "contributing/profiling.md"}
{"id": "5c0a87faa7baf28ae43a9838fcd16ff121c42d827fc5da80608c89bb61f5599b", "heading": "Profiling vLLM/Profile with NVIDIA Nsight Systems/Example commands and usage", "level": 3, "text": "9.5    2,116,600,578     33,920     62,399.8     21,504.0    15,326  2,532,285    290,954.1  sm90_xmma_gemm_bf16bf16_bf16f32_f32_tn_n_tilesize64x64x64_warpgroupsize1x1x1_execute_segment_k_off_â€¦\n5.0    1,119,749,165     18,912     59,208.4      9,056.0     6,784  2,578,366    271,581.7  void vllm::act_and_mul_kernel<c10::BFloat16, &vllm::silu_kernel<c10::BFloat16>, (bool)1>(T1 *, consâ€¦\n4.1      916,662,515     21,312     43,011.6     19,776.0     8,928  2,586,205    199,790.1  void cutlass::device_kernel<flash::enable_sm90_or_later<flash::FlashAttnFwdSm90<flash::CollectiveMaâ€¦\n2.6      587,283,113     37,824     15,526.7      3,008.0     2,719  2,517,756    139,091.1  std::enable_if<T2>(int)0&&vllm::_typeConvert<T1>::exists, void>::type vllm::fused_add_rms_norm_kernâ€¦\n1.9      418,362,605     18,912     22,121.5      3,871.0     3,328  2,523,870    175,248.2  void vllm::rotary_embedding_kernel<c10::BFloat16, (bool)1>(const long *, T1 *, T1 *, const T1 *, inâ€¦", "file_path": "contributing/profiling.md"}
{"id": "5c0a87faa7baf28ae43a9838fcd16ff121c42d827fc5da80608c89bb61f5599b", "heading": "Profiling vLLM/Profile with NVIDIA Nsight Systems/Example commands and usage", "level": 3, "text": "0.7      167,083,069     18,880      8,849.7      2,240.0     1,471  2,499,996    101,436.1  void vllm::reshape_and_cache_flash_kernel<__nv_bfloat16, __nv_bfloat16, (vllm::Fp8KVCacheDataType)0â€¦\n...\n```  \nGUI example:  \n<img width=\"1799\" alt=\"Screenshot 2025-03-05 at 11 48 42AM\" src=\"https://github.com/user-attachments/assets/c7cff1ae-6d6f-477d-a342-bd13c4fc424c\" />", "file_path": "contributing/profiling.md"}
{"id": "5c0a87faa7baf28ae43a9838fcd16ff121c42d827fc5da80608c89bb61f5599b", "heading": "Profiling vLLM/Continuous Profiling", "level": 2, "text": "## Continuous Profiling  \nThere is a [GitHub CI workflow](https://github.com/pytorch/pytorch-integration-testing/actions/workflows/vllm-profiling.yml) in the PyTorch infrastructure repository that provides continuous profiling for different models on vLLM. This automated profiling helps track performance characteristics over time and across different model configurations.", "file_path": "contributing/profiling.md"}
{"id": "5c0a87faa7baf28ae43a9838fcd16ff121c42d827fc5da80608c89bb61f5599b", "heading": "Profiling vLLM/Continuous Profiling/How It Works", "level": 3, "text": "### How It Works  \nThe workflow currently runs weekly profiling sessions for selected models, generating detailed performance traces that can be analyzed using different tools to identify performance regressions or optimization opportunities. But, it can be triggered manually as well, using the Github Action tool.", "file_path": "contributing/profiling.md"}
{"id": "5c0a87faa7baf28ae43a9838fcd16ff121c42d827fc5da80608c89bb61f5599b", "heading": "Profiling vLLM/Continuous Profiling/Adding New Models", "level": 3, "text": "### Adding New Models  \nTo extend the continuous profiling to additional models, you can modify the [profiling-tests.json](https://github.com/pytorch/pytorch-integration-testing/blob/main/vllm-profiling/cuda/profiling-tests.json) configuration file in the PyTorch integration testing repository. Simply add your model specifications to this file to include them in the automated profiling runs.", "file_path": "contributing/profiling.md"}
{"id": "5c0a87faa7baf28ae43a9838fcd16ff121c42d827fc5da80608c89bb61f5599b", "heading": "Profiling vLLM/Continuous Profiling/Viewing Profiling Results", "level": 3, "text": "### Viewing Profiling Results  \nThe profiling traces generated by the continuous profiling workflow are publicly available on the [vLLM Performance Dashboard](https://hud.pytorch.org/benchmark/llms?repoName=vllm-project%2Fvllm). Look for the **Profiling traces** table to access and download the traces for different models and runs.", "file_path": "contributing/profiling.md"}
{"id": "5c0a87faa7baf28ae43a9838fcd16ff121c42d827fc5da80608c89bb61f5599b", "heading": "Profiling vLLM/Profiling vLLM Python Code", "level": 2, "text": "## Profiling vLLM Python Code  \nThe Python standard library includes\n[cProfile](https://docs.python.org/3/library/profile.html) for profiling Python\ncode. vLLM includes a couple of helpers that make it easy to apply it to a section of vLLM.\nBoth the `vllm.utils.cprofile` and `vllm.utils.cprofile_context` functions can be\nused to profile a section of code.", "file_path": "contributing/profiling.md"}
{"id": "5c0a87faa7baf28ae43a9838fcd16ff121c42d827fc5da80608c89bb61f5599b", "heading": "Profiling vLLM/Profiling vLLM Python Code/Example usage - decorator", "level": 3, "text": "### Example usage - decorator  \nThe first helper is a Python decorator that can be used to profile a function.\nIf a filename is specified, the profile will be saved to that file. If no filename is\nspecified, profile data will be printed to stdout.  \n```python\nimport vllm.utils\n\n@vllm.utils.cprofile(\"expensive_function.prof\")\ndef expensive_function():\n# some expensive code\npass\n```", "file_path": "contributing/profiling.md"}
{"id": "5c0a87faa7baf28ae43a9838fcd16ff121c42d827fc5da80608c89bb61f5599b", "heading": "Profiling vLLM/Profiling vLLM Python Code/Example Usage - context manager", "level": 3, "text": "### Example Usage - context manager  \nThe second helper is a context manager that can be used to profile a block of\ncode. Similar to the decorator, the filename is optional.  \n```python\nimport vllm.utils\n\ndef another_function():\n# more expensive code\npass\n\nwith vllm.utils.cprofile_context(\"another_function.prof\"):\nanother_function()\n```", "file_path": "contributing/profiling.md"}
{"id": "5c0a87faa7baf28ae43a9838fcd16ff121c42d827fc5da80608c89bb61f5599b", "heading": "Profiling vLLM/Profiling vLLM Python Code/Analyzing Profile Results", "level": 3, "text": "### Analyzing Profile Results  \nThere are multiple tools available that can help analyze the profile results.\nOne example is [snakeviz](https://jiffyclub.github.io/snakeviz/).  \n```bash\npip install snakeviz\nsnakeviz expensive_function.prof\n```", "file_path": "contributing/profiling.md"}
{"id": "5c0a87faa7baf28ae43a9838fcd16ff121c42d827fc5da80608c89bb61f5599b", "heading": "Profiling vLLM/Profiling vLLM Python Code/Analyzing Garbage Collection Costs", "level": 3, "text": "### Analyzing Garbage Collection Costs  \nLeverage VLLM_GC_DEBUG environment variable to debug GC costs.  \n- VLLM_GC_DEBUG=1: enable GC debugger with gc.collect elpased times\n- VLLM_GC_DEBUG='{\"top_objects\":5}': enable GC debugger to log top 5\ncollected objects for each gc.collect", "file_path": "contributing/profiling.md"}
{"id": "0ed5037833c38458255a8bf1966d4e46dadf3e68f9fedaca5a168029ce8c35ab", "heading": "Vulnerability Management/Reporting Vulnerabilities", "level": 2, "text": "# Vulnerability Management  \n## Reporting Vulnerabilities  \nAs mentioned in the [security\npolicy](https://github.com/vllm-project/vllm/tree/main/SECURITY.md), security\nvulnerabilities may be reported privately to the project via\n[GitHub](https://github.com/vllm-project/vllm/security/advisories/new).", "file_path": "contributing/vulnerability_management.md"}
{"id": "0ed5037833c38458255a8bf1966d4e46dadf3e68f9fedaca5a168029ce8c35ab", "heading": "Vulnerability Management/Vulnerability Management Team", "level": 2, "text": "## Vulnerability Management Team  \nOnce a vulnerability has been reported to the project, the Vulnerability\nManagement Team (VMT) is responsible for managing the vulnerability. The VMT is\nresponsible for:  \n- Triaging the vulnerability.\n- Coordinating with reporters and project maintainers on vulnerability analysis\nand resolution.\n- Drafting of security advisories for confirmed vulnerabilities, as appropriate.\n- Coordination with project maintainers on a coordinated release of the fix and\nsecurity advisory.", "file_path": "contributing/vulnerability_management.md"}
{"id": "0ed5037833c38458255a8bf1966d4e46dadf3e68f9fedaca5a168029ce8c35ab", "heading": "Vulnerability Management/Vulnerability Management Team/Security Advisories", "level": 3, "text": "### Security Advisories  \nAdvisories are published via GitHub through the same system used to report\nvulnerabilities. More information on the process can be found in the [GitHub\ndocumentation](https://docs.github.com/en/code-security/security-advisories/working-with-repository-security-advisories/about-repository-security-advisories).", "file_path": "contributing/vulnerability_management.md"}
{"id": "0ed5037833c38458255a8bf1966d4e46dadf3e68f9fedaca5a168029ce8c35ab", "heading": "Vulnerability Management/Vulnerability Management Team/Team Members", "level": 3, "text": "### Team Members  \nWe prefer to keep all vulnerability-related communication on the security report\non GitHub. However, if you need to contact the VMT directly for an urgent issue,\nyou may contact the following individuals:  \n- Simon Mo - <simon.mo@hey.com>\n- Russell Bryant - <rbryant@redhat.com>\n- Huzaifa Sidhpurwala - <huzaifas@redhat.com>", "file_path": "contributing/vulnerability_management.md"}
{"id": "0ed5037833c38458255a8bf1966d4e46dadf3e68f9fedaca5a168029ce8c35ab", "heading": "Vulnerability Management/Slack Discussion", "level": 2, "text": "## Slack Discussion  \nYou may use the `#security` channel in the [vLLM Slack](https://slack.vllm.ai)\nto discuss security-related topics. However, please do not disclose any\nvulnerabilities in this channel. If you need to report a vulnerability, please\nuse the GitHub security advisory system or contact a VMT member privately.", "file_path": "contributing/vulnerability_management.md"}
{"id": "0ed5037833c38458255a8bf1966d4e46dadf3e68f9fedaca5a168029ce8c35ab", "heading": "Vulnerability Management/Vulnerability Disclosure", "level": 2, "text": "## Vulnerability Disclosure  \nThe process for disclosing vulnerabilities is the following:  \n- The VMT will work with the project maintainers to develop a fix for the\nvulnerability.\n- The VMT will coordinate with the reporter and project maintainers to prepare a\nsecurity advisory that adequately describes the vulnerability and its impact.\n- The VMT will coordinate with the project maintainers to publish a fix and\nrelease an update that includes that fix.\n- The VMT will publish the security advisory on GitHub. Release notes will be\nupdated to include a reference to the security advisory.  \nThe VMT and project maintainers will work to minimize the amount of time in\nbetween disclosing any public information about the vulnerability and making a\nrelease and advisory available.", "file_path": "contributing/vulnerability_management.md"}
{"id": "374f9fcecd3b9cb5f2a2f7cde08d7eb248b1ea23da28f52363cc70349474196a", "heading": "Using Docker", "level": 1, "text": "# Using Docker  \n[](){ #deployment-docker-pre-built-image }", "file_path": "deployment/docker.md"}
{"id": "374f9fcecd3b9cb5f2a2f7cde08d7eb248b1ea23da28f52363cc70349474196a", "heading": "Using Docker/Use vLLM's Official Docker Image", "level": 2, "text": "## Use vLLM's Official Docker Image  \nvLLM offers an official Docker image for deployment.\nThe image can be used to run OpenAI compatible server and is available on Docker Hub as [vllm/vllm-openai](https://hub.docker.com/r/vllm/vllm-openai/tags).  \n```bash\ndocker run --runtime nvidia --gpus all \\\n-v ~/.cache/huggingface:/root/.cache/huggingface \\\n--env \"HF_TOKEN=$HF_TOKEN\" \\\n-p 8000:8000 \\\n--ipc=host \\\nvllm/vllm-openai:latest \\\n--model Qwen/Qwen3-0.6B\n```  \nThis image can also be used with other container engines such as [Podman](https://podman.io/).  \n```bash\npodman run --device nvidia.com/gpu=all \\\n-v ~/.cache/huggingface:/root/.cache/huggingface \\\n--env \"HF_TOKEN=$HF_TOKEN\" \\\n-p 8000:8000 \\\n--ipc=host \\\ndocker.io/vllm/vllm-openai:latest \\\n--model Qwen/Qwen3-0.6B\n```  \nYou can add any other [engine-args](../configuration/engine_args.md) you need after the image tag (`vllm/vllm-openai:latest`).  \n!!! note\nYou can either use the `ipc=host` flag or `--shm-size` flag to allow the", "file_path": "deployment/docker.md"}
{"id": "374f9fcecd3b9cb5f2a2f7cde08d7eb248b1ea23da28f52363cc70349474196a", "heading": "Using Docker/Use vLLM's Official Docker Image", "level": 2, "text": "container to access the host's shared memory. vLLM uses PyTorch, which uses shared\nmemory to share data between processes under the hood, particularly for tensor parallel inference.  \n!!! note\nOptional dependencies are not included in order to avoid licensing issues (e.g. <gh-issue:8030>).  \nIf you need to use those dependencies (having accepted the license terms),\ncreate a custom Dockerfile on top of the base image with an extra layer that installs them:  \n```Dockerfile\nFROM vllm/vllm-openai:v0.9.0", "file_path": "deployment/docker.md"}
{"id": "374f9fcecd3b9cb5f2a2f7cde08d7eb248b1ea23da28f52363cc70349474196a", "heading": "Using Docker/Use vLLM's Official Docker Image", "level": 2, "text": "# e.g. install the `audio` optional dependencies\n# NOTE: Make sure the version of vLLM matches the base image!\nRUN uv pip install --system vllm[audio]==0.9.0\n```  \n!!! tip\nSome new models may only be available on the main branch of [HF Transformers](https://github.com/huggingface/transformers).  \nTo use the development version of `transformers`, create a custom Dockerfile on top of the base image\nwith an extra layer that installs their code from source:  \n```Dockerfile\nFROM vllm/vllm-openai:latest\n\nRUN uv pip install --system git+https://github.com/huggingface/transformers.git\n```  \n[](){ #deployment-docker-build-image-from-source }", "file_path": "deployment/docker.md"}
{"id": "374f9fcecd3b9cb5f2a2f7cde08d7eb248b1ea23da28f52363cc70349474196a", "heading": "Using Docker/Building vLLM's Docker Image from Source", "level": 2, "text": "## Building vLLM's Docker Image from Source  \nYou can build and run vLLM from source via the provided <gh-file:docker/Dockerfile>. To build vLLM:  \n```bash\n# optionally specifies: --build-arg max_jobs=8 --build-arg nvcc_threads=2\nDOCKER_BUILDKIT=1 docker build . \\\n--target vllm-openai \\\n--tag vllm/vllm-openai \\\n--file docker/Dockerfile\n```  \n!!! note\nBy default vLLM will build for all GPU types for widest distribution. If you are just building for the\ncurrent GPU type the machine is running on, you can add the argument `--build-arg torch_cuda_arch_list=\"\"`\nfor vLLM to find the current GPU type and build for that.  \nIf you are using Podman instead of Docker, you might need to disable SELinux labeling by\nadding `--security-opt label=disable` when running `podman build` command to avoid certain [existing issues](https://github.com/containers/buildah/discussions/4184).", "file_path": "deployment/docker.md"}
{"id": "374f9fcecd3b9cb5f2a2f7cde08d7eb248b1ea23da28f52363cc70349474196a", "heading": "Using Docker/Building for Arm64/aarch64", "level": 2, "text": "## Building for Arm64/aarch64  \nA docker container can be built for aarch64 systems such as the Nvidia Grace-Hopper. At time of this writing, this requires the use\nof PyTorch Nightly and should be considered **experimental**. Using the flag `--platform \"linux/arm64\"` will attempt to build for arm64.  \n!!! note\nMultiple modules must be compiled, so this process can take a while. Recommend using `--build-arg max_jobs=` & `--build-arg nvcc_threads=`\nflags to speed up build process. However, ensure your `max_jobs` is substantially larger than `nvcc_threads` to get the most benefits.\nKeep an eye on memory usage with parallel jobs as it can be substantial (see example below).  \n??? console \"Command\"  \n```bash\n# Example of building on Nvidia GH200 server. (Memory usage: ~15GB, Build time: ~1475s / ~25 min, Image size: 6.93GB)\npython3 use_existing_torch.py\nDOCKER_BUILDKIT=1 docker build . \\\n--file docker/Dockerfile \\\n--target vllm-openai \\\n--platform \"linux/arm64\" \\\n-t vllm/vllm-gh200-openai:latest \\", "file_path": "deployment/docker.md"}
{"id": "374f9fcecd3b9cb5f2a2f7cde08d7eb248b1ea23da28f52363cc70349474196a", "heading": "Using Docker/Building for Arm64/aarch64", "level": 2, "text": "--platform \"linux/arm64\" \\\n-t vllm/vllm-gh200-openai:latest \\\n--build-arg max_jobs=66 \\\n--build-arg nvcc_threads=2 \\\n--build-arg torch_cuda_arch_list=\"9.0 10.0+PTX\"\n```  \n!!! note\nIf you are building the `linux/arm64` image on a non-ARM host (e.g., an x86_64 machine), you need to ensure your system is set up for cross-compilation using QEMU. This allows your host machine to emulate ARM64 execution.  \nRun the following command on your host machine to register QEMU user static handlers:  \n```bash\ndocker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n```  \nAfter setting up QEMU, you can use the `--platform \"linux/arm64\"` flag in your `docker build` command.", "file_path": "deployment/docker.md"}
{"id": "374f9fcecd3b9cb5f2a2f7cde08d7eb248b1ea23da28f52363cc70349474196a", "heading": "Using Docker/Use the custom-built vLLM Docker image", "level": 2, "text": "## Use the custom-built vLLM Docker image  \nTo run vLLM with the custom-built Docker image:  \n```bash\ndocker run --runtime nvidia --gpus all \\\n-v ~/.cache/huggingface:/root/.cache/huggingface \\\n-p 8000:8000 \\\n--env \"HF_TOKEN=<secret>\" \\\nvllm/vllm-openai <args...>\n```  \nThe argument `vllm/vllm-openai` specifies the image to run, and should be replaced with the name of the custom-built image (the `-t` tag from the build command).  \n!!! note\n**For version 0.4.1 and 0.4.2 only** - the vLLM docker images under these versions are supposed to be run under the root user since a library under the root user's home directory, i.e. `/root/.config/vllm/nccl/cu12/libnccl.so.2.18.1` is required to be loaded during runtime. If you are running the container under a different user, you may need to first change the permissions of the library (and all the parent directories) to allow the user to access it, then run vLLM with environment variable `VLLM_NCCL_SO_PATH=/root/.config/vllm/nccl/cu12/libnccl.so.2.18.1` .", "file_path": "deployment/docker.md"}
{"id": "2ef36503c84dc9078a15bb8e1bc59175a6cfa0a39acefc4a56ae577914b70ebc", "heading": "Anyscale/Anyscale Deployment", "level": 2, "text": "# Anyscale  \n## Anyscale Deployment  \n[Anyscale](https://www.anyscale.com) is a managed, multi-cloud platform developed by the creators of Ray.  \nAnyscale automates the entire lifecycle of Ray clusters in your AWS, GCP, or Azure account, delivering the flexibility of open-source Ray\nwithout the operational overhead of maintaining Kubernetes control planes, configuring autoscalers, managing observability stacks, or manually managing head and worker nodes with helper scripts like <gh-file:examples/online_serving/run_cluster.sh>.  \nWhen serving large language models with vLLM, Anyscale can rapidly provision [production-ready HTTPS endpoints](https://docs.anyscale.com/examples/deploy-ray-serve-llms) or [fault-tolerant batch inference jobs](https://docs.anyscale.com/examples/ray-data-llm).", "file_path": "deployment/frameworks/anyscale.md"}
{"id": "2ef36503c84dc9078a15bb8e1bc59175a6cfa0a39acefc4a56ae577914b70ebc", "heading": "Anyscale/Production-ready vLLM on Anyscale quickstarts", "level": 2, "text": "## Production-ready vLLM on Anyscale quickstarts  \n- [Offline batch inference](https://console.anyscale.com/template-preview/llm_batch_inference?utm_source=vllm_docs)\n- [Deploy vLLM services](https://console.anyscale.com/template-preview/llm_serving?utm_source=vllm_docs)\n- [Curate a dataset](https://console.anyscale.com/template-preview/audio-dataset-curation-llm-judge?utm_source=vllm_docs)\n- [Finetune an LLM](https://console.anyscale.com/template-preview/entity-recognition-with-llms?utm_source=vllm_docs)", "file_path": "deployment/frameworks/anyscale.md"}
{"id": "35c4208d007813c4f159a6db45b467fc1dcb1dc812cc89a8a80d0527f2733987", "heading": "AnythingLLM", "level": 1, "text": "# AnythingLLM  \n[AnythingLLM](https://github.com/Mintplex-Labs/anything-llm) is a full-stack application that enables you to turn any document, resource, or piece of content into context that any LLM can use as references during chatting.  \nIt allows you to deploy a large language model (LLM) server with vLLM as the backend, which exposes OpenAI-compatible endpoints.", "file_path": "deployment/frameworks/anything-llm.md"}
{"id": "35c4208d007813c4f159a6db45b467fc1dcb1dc812cc89a8a80d0527f2733987", "heading": "AnythingLLM/Prerequisites", "level": 2, "text": "## Prerequisites  \nSet up the vLLM environment:  \n```bash\npip install vllm\n```", "file_path": "deployment/frameworks/anything-llm.md"}
{"id": "35c4208d007813c4f159a6db45b467fc1dcb1dc812cc89a8a80d0527f2733987", "heading": "AnythingLLM/Deploy", "level": 2, "text": "## Deploy  \n1. Start the vLLM server with a supported chat-completion model, for example:  \n```bash\nvllm serve Qwen/Qwen1.5-32B-Chat-AWQ --max-model-len 4096\n```  \n1. Download and install [AnythingLLM Desktop](https://anythingllm.com/desktop).  \n1. Configure the AI provider:  \n- At the bottom, click the ðŸ”§ wrench icon -> **Open settings** -> **AI Providers** -> **LLM**.\n- Enter the following values:\n- LLM Provider: Generic OpenAI\n- Base URL: `http://{vllm server host}:{vllm server port}/v1`\n- Chat Model Name: `Qwen/Qwen1.5-32B-Chat-AWQ`  \n![set AI providers](../../assets/deployment/anything-llm-provider.png)  \n1. Create a workspace:  \n1. At the bottom, click the â†º back icon and back to workspaces.\n1. Create a workspace (e.g., `vllm`) and start chatting.  \n![create a workspace](../../assets/deployment/anything-llm-chat-without-doc.png)  \n1. Add a document.  \n1. Click the ðŸ“Ž attachment icon.\n1. Upload a document.\n1. Select and move the document into your workspace.\n1. Save and embed it.", "file_path": "deployment/frameworks/anything-llm.md"}
{"id": "35c4208d007813c4f159a6db45b467fc1dcb1dc812cc89a8a80d0527f2733987", "heading": "AnythingLLM/Deploy", "level": 2, "text": "1. Save and embed it.  \n![add a document](../../assets/deployment/anything-llm-upload-doc.png)  \n1. Chat using your document as context.  \n![chat with your context](../../assets/deployment/anything-llm-chat-with-doc.png)", "file_path": "deployment/frameworks/anything-llm.md"}
{"id": "71f8b4fd2fa655d16f447a9783bef9df883713161fe247e07d3c71dbefa9bde4", "heading": "AutoGen", "level": 1, "text": "# AutoGen  \n[AutoGen](https://github.com/microsoft/autogen) is a framework for creating multi-agent AI applications that can act autonomously or work alongside humans.", "file_path": "deployment/frameworks/autogen.md"}
{"id": "71f8b4fd2fa655d16f447a9783bef9df883713161fe247e07d3c71dbefa9bde4", "heading": "AutoGen/Prerequisites", "level": 2, "text": "## Prerequisites  \nSet up the vLLM and [AutoGen](https://microsoft.github.io/autogen/0.2/docs/installation/) environment:  \n```bash\npip install vllm\n\n# Install AgentChat and OpenAI client from Extensions\n# AutoGen requires Python 3.10 or later.\npip install -U \"autogen-agentchat\" \"autogen-ext[openai]\"\n```", "file_path": "deployment/frameworks/autogen.md"}
{"id": "71f8b4fd2fa655d16f447a9783bef9df883713161fe247e07d3c71dbefa9bde4", "heading": "AutoGen/Deploy", "level": 2, "text": "## Deploy  \n1. Start the vLLM server with the supported chat completion model, e.g.  \n```bash\nvllm serve mistralai/Mistral-7B-Instruct-v0.2\n```  \n1. Call it with AutoGen:  \n??? code  \n```python\nimport asyncio\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_core.models import ModelFamily\n\n\nasync def main() -> None:\n# Create a model client\nmodel_client = OpenAIChatCompletionClient(\nmodel=\"mistralai/Mistral-7B-Instruct-v0.2\",\nbase_url=\"http://{your-vllm-host-ip}:{your-vllm-host-port}/v1\",\napi_key=\"EMPTY\",\nmodel_info={\n\"vision\": False,\n\"function_calling\": False,\n\"json_output\": False,\n\"family\": ModelFamily.MISTRAL,\n\"structured_output\": True,\n},\n)\n\nmessages = [UserMessage(content=\"Write a very short story about a dragon.\", source=\"user\")]\n\n# Create a stream.\nstream = model_client.create_stream(messages=messages)", "file_path": "deployment/frameworks/autogen.md"}
{"id": "71f8b4fd2fa655d16f447a9783bef9df883713161fe247e07d3c71dbefa9bde4", "heading": "AutoGen/Deploy", "level": 2, "text": "# Iterate over the stream and print the responses.\nprint(\"Streamed responses:\")\nasync for response in stream:\nif isinstance(response, str):\n# A partial response is a string.\nprint(response, flush=True, end=\"\")\nelse:\n# The last response is a CreateResult object with the complete message.\nprint(\"\\n\\n------------\\n\")\nprint(\"The complete response:\", flush=True)\nprint(response.content, flush=True)\n\n# Close the client when done.\nawait model_client.close()\n\n\nasyncio.run(main())\n```  \nFor details, see the tutorial:  \n- [Using vLLM in AutoGen](https://microsoft.github.io/autogen/0.2/docs/topics/non-openai-models/local-vllm/)  \n- [OpenAI-compatible API examples](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html#autogen_ext.models.openai.OpenAIChatCompletionClient)", "file_path": "deployment/frameworks/autogen.md"}
{"id": "6fe72bf2ffdbd05571ab75b82f9a31509883fc6d67b05d73a765abda8b691dc2", "heading": "BentoML", "level": 1, "text": "# BentoML  \n[BentoML](https://github.com/bentoml/BentoML) allows you to deploy a large language model (LLM) server with vLLM as the backend, which exposes OpenAI-compatible endpoints. You can serve the model locally or containerize it as an OCI-compliant image and deploy it on Kubernetes.  \nFor details, see the tutorial [vLLM inference in the BentoML documentation](https://docs.bentoml.com/en/latest/use-cases/large-language-models/vllm.html).", "file_path": "deployment/frameworks/bentoml.md"}
{"id": "6ab7a3a0ced1f529f379f4a9f0d2ecd69313d2566aec60b081e53038db9eb746", "heading": "Cerebrium", "level": 1, "text": "# Cerebrium  \n<p align=\"center\">\n<img src=\"https://i.ibb.co/hHcScTT/Screenshot-2024-06-13-at-10-14-54.png\" alt=\"vLLM_plus_cerebrium\"/>\n</p>  \nvLLM can be run on a cloud based GPU machine with [Cerebrium](https://www.cerebrium.ai/), a serverless AI infrastructure platform that makes it easier for companies to build and deploy AI based applications.  \nTo install the Cerebrium client, run:  \n```bash\npip install cerebrium\ncerebrium login\n```  \nNext, create your Cerebrium project, run:  \n```bash\ncerebrium init vllm-project\n```  \nNext, to install the required packages, add the following to your cerebrium.toml:  \n```toml\n[cerebrium.deployment]\ndocker_base_image_url = \"nvidia/cuda:12.1.1-runtime-ubuntu22.04\"\n\n[cerebrium.dependencies.pip]\nvllm = \"latest\"\n```  \nNext, let us add our code to handle inference for the LLM of your choice (`mistralai/Mistral-7B-Instruct-v0.1` for this example), add the following code to your `main.py`:  \n??? code  \n```python\nfrom vllm import LLM, SamplingParams", "file_path": "deployment/frameworks/cerebrium.md"}
{"id": "6ab7a3a0ced1f529f379f4a9f0d2ecd69313d2566aec60b081e53038db9eb746", "heading": "Cerebrium", "level": 1, "text": "llm = LLM(model=\"mistralai/Mistral-7B-Instruct-v0.1\")\n\ndef run(prompts: list[str], temperature: float = 0.8, top_p: float = 0.95):\n\nsampling_params = SamplingParams(temperature=temperature, top_p=top_p)\noutputs = llm.generate(prompts, sampling_params)\n\n# Print the outputs.\nresults = []\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nresults.append({\"prompt\": prompt, \"generated_text\": generated_text})", "file_path": "deployment/frameworks/cerebrium.md"}
{"id": "6ab7a3a0ced1f529f379f4a9f0d2ecd69313d2566aec60b081e53038db9eb746", "heading": "Cerebrium", "level": 1, "text": "return {\"results\": results}\n```  \nThen, run the following code to deploy it to the cloud:  \n```bash\ncerebrium deploy\n```  \nIf successful, you should be returned a CURL command that you can call inference against. Just remember to end the url with the function name you are calling (in our case`/run`)  \n??? console \"Command\"  \n```bash\ncurl -X POST https://api.cortex.cerebrium.ai/v4/p-xxxxxx/vllm/run \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: <JWT TOKEN>' \\\n--data '{\n\"prompts\": [\n\"Hello, my name is\",\n\"The president of the United States is\",\n\"The capital of France is\",\n\"The future of AI is\"\n]\n}'\n```  \nYou should get a response like:  \n??? console \"Response\"  \n```json\n{\n\"run_id\": \"52911756-3066-9ae8-bcc9-d9129d1bd262\",\n\"result\": {\n\"result\": [\n{\n\"prompt\": \"Hello, my name is\",\n\"generated_text\": \" Sarah, and I'm a teacher. I teach elementary school students. One of\"\n},\n{\n\"prompt\": \"The president of the United States is\",", "file_path": "deployment/frameworks/cerebrium.md"}
{"id": "6ab7a3a0ced1f529f379f4a9f0d2ecd69313d2566aec60b081e53038db9eb746", "heading": "Cerebrium", "level": 1, "text": "},\n{\n\"prompt\": \"The president of the United States is\",\n\"generated_text\": \" elected every four years. This is a democratic system.\\n\\n5. What\"\n},\n{\n\"prompt\": \"The capital of France is\",\n\"generated_text\": \" Paris.\\n\"\n},\n{\n\"prompt\": \"The future of AI is\",\n\"generated_text\": \" bright, but it's important to approach it with a balanced and nuanced perspective.\"\n}\n]\n},\n\"run_time_ms\": 152.53663063049316\n}\n```  \nYou now have an autoscaling endpoint where you only pay for the compute you use!", "file_path": "deployment/frameworks/cerebrium.md"}
{"id": "8c0b8b662774476eb519a437bc0183830df92d4f82fad2af438d43399d13cda8", "heading": "Chatbox", "level": 1, "text": "# Chatbox  \n[Chatbox](https://github.com/chatboxai/chatbox) is a desktop client for LLMs, available on Windows, Mac, Linux.  \nIt allows you to deploy a large language model (LLM) server with vLLM as the backend, which exposes OpenAI-compatible endpoints.", "file_path": "deployment/frameworks/chatbox.md"}
{"id": "8c0b8b662774476eb519a437bc0183830df92d4f82fad2af438d43399d13cda8", "heading": "Chatbox/Prerequisites", "level": 2, "text": "## Prerequisites  \nSet up the vLLM environment:  \n```bash\npip install vllm\n```", "file_path": "deployment/frameworks/chatbox.md"}
{"id": "8c0b8b662774476eb519a437bc0183830df92d4f82fad2af438d43399d13cda8", "heading": "Chatbox/Deploy", "level": 2, "text": "## Deploy  \n1. Start the vLLM server with the supported chat completion model, e.g.  \n```bash\nvllm serve qwen/Qwen1.5-0.5B-Chat\n```  \n1. Download and install [Chatbox desktop](https://chatboxai.app/en#download).  \n1. On the bottom left of settings, Add Custom Provider\n- API Mode: `OpenAI API Compatible`\n- Name: vllm\n- API Host: `http://{vllm server host}:{vllm server port}/v1`\n- API Path: `/chat/completions`\n- Model: `qwen/Qwen1.5-0.5B-Chat`  \n![](../../assets/deployment/chatbox-settings.png)  \n1. Go to `Just chat`, and start to chat:  \n![](../../assets/deployment/chatbox-chat.png)", "file_path": "deployment/frameworks/chatbox.md"}
{"id": "19515f6055faad9ac4c2faf4554528349bd6b253573f40d2018f7f86bbaf4a3e", "heading": "Dify", "level": 1, "text": "# Dify  \n[Dify](https://github.com/langgenius/dify) is an open-source LLM app development platform. Its intuitive interface combines agentic AI workflow, RAG pipeline, agent capabilities, model management, observability features, and more, allowing you to quickly move from prototype to production.  \nIt supports vLLM as a model provider to efficiently serve large language models.  \nThis guide walks you through deploying Dify using a vLLM backend.", "file_path": "deployment/frameworks/dify.md"}
{"id": "19515f6055faad9ac4c2faf4554528349bd6b253573f40d2018f7f86bbaf4a3e", "heading": "Dify/Prerequisites", "level": 2, "text": "## Prerequisites  \nSet up the vLLM environment:  \n```bash\npip install vllm\n```  \nAnd install [Docker](https://docs.docker.com/engine/install/) and [Docker Compose](https://docs.docker.com/compose/install/).", "file_path": "deployment/frameworks/dify.md"}
{"id": "19515f6055faad9ac4c2faf4554528349bd6b253573f40d2018f7f86bbaf4a3e", "heading": "Dify/Deploy", "level": 2, "text": "## Deploy  \n1. Start the vLLM server with the supported chat completion model, e.g.  \n```bash\nvllm serve Qwen/Qwen1.5-7B-Chat\n```  \n1. Start the Dify server with docker compose ([details](https://github.com/langgenius/dify?tab=readme-ov-file#quick-start)):  \n```bash\ngit clone https://github.com/langgenius/dify.git\ncd dify\ncd docker\ncp .env.example .env\ndocker compose up -d\n```  \n1. Open the browser to access `http://localhost/install`, config the basic login information and login.  \n1. In the top-right user menu (under the profile icon), go to Settings, then click `Model Provider`, and locate the `vLLM` provider to install it.  \n1. Fill in the model provider details as follows:  \n- **Model Type**: `LLM`\n- **Model Name**: `Qwen/Qwen1.5-7B-Chat`\n- **API Endpoint URL**: `http://{vllm_server_host}:{vllm_server_port}/v1`\n- **Model Name for API Endpoint**: `Qwen/Qwen1.5-7B-Chat`\n- **Completion Mode**: `Completion`  \n![](../../assets/deployment/dify-settings.png)", "file_path": "deployment/frameworks/dify.md"}
{"id": "19515f6055faad9ac4c2faf4554528349bd6b253573f40d2018f7f86bbaf4a3e", "heading": "Dify/Deploy", "level": 2, "text": "![](../../assets/deployment/dify-settings.png)  \n1. To create a test chatbot, go to `Studio â†’ Chatbot â†’ Create from Blank`, then select Chatbot as the type:  \n![](../../assets/deployment/dify-create-chatbot.png)  \n1. Click the chatbot you just created to open the chat interface and start interacting with the model:  \n![](../../assets/deployment/dify-chat.png)", "file_path": "deployment/frameworks/dify.md"}
{"id": "01c7890f55dbb4305cb049183dff56bfdbe14cb5bb5494fceca8cbd795bcbac1", "heading": "dstack", "level": 1, "text": "# dstack  \n<p align=\"center\">\n<img src=\"https://i.ibb.co/71kx6hW/vllm-dstack.png\" alt=\"vLLM_plus_dstack\"/>\n</p>  \nvLLM can be run on a cloud based GPU machine with [dstack](https://dstack.ai/), an open-source framework for running LLMs on any cloud. This tutorial assumes that you have already configured credentials, gateway, and GPU quotas on your cloud environment.  \nTo install dstack client, run:  \n```bash\npip install dstack[all]\ndstack server\n```  \nNext, to configure your dstack project, run:  \n```bash\nmkdir -p vllm-dstack\ncd vllm-dstack\ndstack init\n```  \nNext, to provision a VM instance with LLM of your choice (`NousResearch/Llama-2-7b-chat-hf` for this example), create the following `serve.dstack.yml` file for the dstack `Service`:  \n??? code \"Config\"  \n```yaml\ntype: service", "file_path": "deployment/frameworks/dstack.md"}
{"id": "01c7890f55dbb4305cb049183dff56bfdbe14cb5bb5494fceca8cbd795bcbac1", "heading": "dstack", "level": 1, "text": "python: \"3.11\"\nenv:\n- MODEL=NousResearch/Llama-2-7b-chat-hf\nport: 8000\nresources:\ngpu: 24GB\ncommands:\n- pip install vllm\n- vllm serve $MODEL --port 8000\nmodel:\nformat: openai\ntype: chat\nname: NousResearch/Llama-2-7b-chat-hf\n```  \nThen, run the following CLI for provisioning:  \n??? console \"Command\"  \n```console\n$ dstack run . -f serve.dstack.yml\n\nâ ¸ Getting run plan...\nConfiguration  serve.dstack.yml\nProject        deep-diver-main\nUser           deep-diver\nMin resources  2..xCPU, 8GB.., 1xGPU (24GB)\nMax price      -\nMax duration   -\nSpot policy    auto\nRetry policy   no\n\n#  BACKEND  REGION       INSTANCE       RESOURCES                               SPOT  PRICE\n1  gcp   us-central1  g2-standard-4  4xCPU, 16GB, 1xL4 (24GB), 100GB (disk)  yes   $0.223804\n2  gcp   us-east1     g2-standard-4  4xCPU, 16GB, 1xL4 (24GB), 100GB (disk)  yes   $0.223804\n3  gcp   us-west1     g2-standard-4  4xCPU, 16GB, 1xL4 (24GB), 100GB (disk)  yes   $0.223804\n...\nShown 3 of 193 offers, $5.876 max", "file_path": "deployment/frameworks/dstack.md"}
{"id": "01c7890f55dbb4305cb049183dff56bfdbe14cb5bb5494fceca8cbd795bcbac1", "heading": "dstack", "level": 1, "text": "Continue? [y/n]: y\nâ ™ Submitting run...\nâ  Launching spicy-treefrog-1 (pulling)\nspicy-treefrog-1 provisioning completed (running)\nService is published at ...\n```  \nAfter the provisioning, you can interact with the model by using the OpenAI SDK:  \n??? code  \n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\nbase_url=\"https://gateway.<gateway domain>\",\napi_key=\"<YOUR-DSTACK-SERVER-ACCESS-TOKEN>\",\n)\n\ncompletion = client.chat.completions.create(\nmodel=\"NousResearch/Llama-2-7b-chat-hf\",\nmessages=[\n{\n\"role\": \"user\",\n\"content\": \"Compose a poem that explains the concept of recursion in programming.\",\n}\n],\n)", "file_path": "deployment/frameworks/dstack.md"}
{"id": "01c7890f55dbb4305cb049183dff56bfdbe14cb5bb5494fceca8cbd795bcbac1", "heading": "dstack", "level": 1, "text": "print(completion.choices[0].message.content)\n```  \n!!! note\ndstack automatically handles authentication on the gateway using dstack's tokens. Meanwhile, if you don't want to configure a gateway, you can provision dstack `Task` instead of `Service`. The `Task` is for development purpose only. If you want to know more about hands-on materials how to serve vLLM using dstack, check out [this repository](https://github.com/dstackai/dstack-examples/tree/main/deployment/vllm)", "file_path": "deployment/frameworks/dstack.md"}
{"id": "c35d6c8aab3c457f1d038048ec5e7e4251efebbd97783583b0eb44efeb21a577", "heading": "Haystack", "level": 1, "text": "# Haystack  \n[Haystack](https://github.com/deepset-ai/haystack) is an end-to-end LLM framework that allows you to build applications powered by LLMs, Transformer models, vector search and more. Whether you want to perform retrieval-augmented generation (RAG), document search, question answering or answer generation, Haystack can orchestrate state-of-the-art embedding models and LLMs into pipelines to build end-to-end NLP applications and solve your use case.  \nIt allows you to deploy a large language model (LLM) server with vLLM as the backend, which exposes OpenAI-compatible endpoints.", "file_path": "deployment/frameworks/haystack.md"}
{"id": "c35d6c8aab3c457f1d038048ec5e7e4251efebbd97783583b0eb44efeb21a577", "heading": "Haystack/Prerequisites", "level": 2, "text": "## Prerequisites  \nSet up the vLLM and Haystack environment:  \n```bash\npip install vllm haystack-ai\n```", "file_path": "deployment/frameworks/haystack.md"}
{"id": "c35d6c8aab3c457f1d038048ec5e7e4251efebbd97783583b0eb44efeb21a577", "heading": "Haystack/Deploy", "level": 2, "text": "## Deploy  \n1. Start the vLLM server with the supported chat completion model, e.g.  \n```bash\nvllm serve mistralai/Mistral-7B-Instruct-v0.1\n```  \n1. Use the `OpenAIGenerator` and `OpenAIChatGenerator` components in Haystack to query the vLLM server.  \n??? code  \n```python\nfrom haystack.components.generators.chat import OpenAIChatGenerator\nfrom haystack.dataclasses import ChatMessage\nfrom haystack.utils import Secret\n\ngenerator = OpenAIChatGenerator(\n# for compatibility with the OpenAI API, a placeholder api_key is needed\napi_key=Secret.from_token(\"VLLM-PLACEHOLDER-API-KEY\"),\nmodel=\"mistralai/Mistral-7B-Instruct-v0.1\",\napi_base_url=\"http://{your-vLLM-host-ip}:{your-vLLM-host-port}/v1\",\ngeneration_kwargs={\"max_tokens\": 512},\n)\n\nresponse = generator.run(\nmessages=[ChatMessage.from_user(\"Hi. Can you help me plan my next trip to Italy?\")]\n)", "file_path": "deployment/frameworks/haystack.md"}
{"id": "c35d6c8aab3c457f1d038048ec5e7e4251efebbd97783583b0eb44efeb21a577", "heading": "Haystack/Deploy", "level": 2, "text": "print(\"-\"*30)\nprint(response)\nprint(\"-\"*30)\n```  \n```console\n------------------------------\n{'replies': [ChatMessage(_role=<ChatRole.ASSISTANT: 'assistant'>, _content=[TextContent(text=' Of course! Where in Italy would you like to go and what type of trip are you looking to plan?')], _name=None, _meta={'model': 'mistralai/Mistral-7B-Instruct-v0.1', 'index': 0, 'finish_reason': 'stop', 'usage': {'completion_tokens': 23, 'prompt_tokens': 21, 'total_tokens': 44, 'completion_tokens_details': None, 'prompt_tokens_details': None}})]}\n------------------------------\n```  \nFor details, see the tutorial [Using vLLM in Haystack](https://github.com/deepset-ai/haystack-integrations/blob/main/integrations/vllm.md).", "file_path": "deployment/frameworks/haystack.md"}
{"id": "a38c2b563e11e817f93a3ddb13342eb2c82886796ad54aa7ff0383be83a46038", "heading": "Helm", "level": 1, "text": "# Helm  \nA Helm chart to deploy vLLM for Kubernetes  \nHelm is a package manager for Kubernetes. It helps automate the deployment of vLLM applications on Kubernetes. With Helm, you can deploy the same framework architecture with different configurations to multiple namespaces by overriding variable values.  \nThis guide will walk you through the process of deploying vLLM with Helm, including the necessary prerequisites, steps for Helm installation and documentation on architecture and values file.", "file_path": "deployment/frameworks/helm.md"}
{"id": "a38c2b563e11e817f93a3ddb13342eb2c82886796ad54aa7ff0383be83a46038", "heading": "Helm/Prerequisites", "level": 2, "text": "## Prerequisites  \nBefore you begin, ensure that you have the following:  \n- A running Kubernetes cluster\n- NVIDIA Kubernetes Device Plugin (`k8s-device-plugin`): This can be found at [https://github.com/NVIDIA/k8s-device-plugin](https://github.com/NVIDIA/k8s-device-plugin)\n- Available GPU resources in your cluster\n- An S3 with the model which will be deployed", "file_path": "deployment/frameworks/helm.md"}
{"id": "a38c2b563e11e817f93a3ddb13342eb2c82886796ad54aa7ff0383be83a46038", "heading": "Helm/Installing the chart", "level": 2, "text": "## Installing the chart  \nTo install the chart with the release name `test-vllm`:  \n```bash\nhelm upgrade --install --create-namespace \\\n--namespace=ns-vllm test-vllm . \\\n-f values.yaml \\\n--set secrets.s3endpoint=$ACCESS_POINT \\\n--set secrets.s3bucketname=$BUCKET \\\n--set secrets.s3accesskeyid=$ACCESS_KEY \\\n--set secrets.s3accesskey=$SECRET_KEY\n```", "file_path": "deployment/frameworks/helm.md"}
{"id": "a38c2b563e11e817f93a3ddb13342eb2c82886796ad54aa7ff0383be83a46038", "heading": "Helm/Uninstalling the chart", "level": 2, "text": "## Uninstalling the chart  \nTo uninstall the `test-vllm` deployment:  \n```bash\nhelm uninstall test-vllm --namespace=ns-vllm\n```  \nThe command removes all the Kubernetes components associated with the\nchart **including persistent volumes** and deletes the release.", "file_path": "deployment/frameworks/helm.md"}
{"id": "a38c2b563e11e817f93a3ddb13342eb2c82886796ad54aa7ff0383be83a46038", "heading": "Helm/Architecture", "level": 2, "text": "## Architecture  \n![helm deployment architecture](../../assets/deployment/architecture_helm_deployment.png)", "file_path": "deployment/frameworks/helm.md"}
{"id": "a38c2b563e11e817f93a3ddb13342eb2c82886796ad54aa7ff0383be83a46038", "heading": "Helm/Values", "level": 2, "text": "## Values  \nThe following table describes configurable parameters of the chart in `values.yaml`:  \n| Key | Type | Default | Description |\n|-----|------|---------|-------------|\n| autoscaling | object | {\"enabled\":false,\"maxReplicas\":100,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":80} | Autoscaling configuration |\n| autoscaling.enabled | bool | false | Enable autoscaling |\n| autoscaling.maxReplicas | int | 100 | Maximum replicas |\n| autoscaling.minReplicas | int | 1 | Minimum replicas |\n| autoscaling.targetCPUUtilizationPercentage | int | 80 | Target CPU utilization for autoscaling |\n| configs | object | {} | Configmap |\n| containerPort | int | 8000 | Container port |\n| customObjects | list | [] | Custom Objects configuration |\n| deploymentStrategy | object | {} | Deployment strategy configuration |\n| externalConfigs | list | [] | External configuration |\n| extraContainers | list | [] | Additional containers configuration |", "file_path": "deployment/frameworks/helm.md"}
{"id": "a38c2b563e11e817f93a3ddb13342eb2c82886796ad54aa7ff0383be83a46038", "heading": "Helm/Values", "level": 2, "text": "| extraInit | object | {\"pvcStorage\":\"1Gi\",\"s3modelpath\":\"relative_s3_model_path/opt-125m\", \"awsEc2MetadataDisabled\": true} | Additional configuration for the init container |\n| extraInit.pvcStorage | string | \"1Gi\" | Storage size of the s3 |\n| extraInit.s3modelpath | string | \"relative_s3_model_path/opt-125m\" | Path of the model on the s3 which hosts model weights and config files |\n| extraInit.awsEc2MetadataDisabled | boolean | true | Disables the use of the Amazon EC2 instance metadata service |\n| extraPorts | list | [] | Additional ports configuration |\n| gpuModels | list | [\"TYPE_GPU_USED\"] | Type of gpu used |\n| image | object | {\"command\":[\"vllm\",\"serve\",\"/data/\",\"--served-model-name\",\"opt-125m\",\"--host\",\"0.0.0.0\",\"--port\",\"8000\"],\"repository\":\"vllm/vllm-openai\",\"tag\":\"latest\"} | Image configuration |\n| image.command | list | [\"vllm\",\"serve\",\"/data/\",\"--served-model-name\",\"opt-125m\",\"--host\",\"0.0.0.0\",\"--port\",\"8000\"] | Container launch command |", "file_path": "deployment/frameworks/helm.md"}
{"id": "a38c2b563e11e817f93a3ddb13342eb2c82886796ad54aa7ff0383be83a46038", "heading": "Helm/Values", "level": 2, "text": "| image.repository | string | \"vllm/vllm-openai\" | Image repository |\n| image.tag | string | \"latest\" | Image tag |\n| livenessProbe | object | {\"failureThreshold\":3,\"httpGet\":{\"path\":\"/health\",\"port\":8000},\"initialDelaySeconds\":15,\"periodSeconds\":10} | Liveness probe configuration |\n| livenessProbe.failureThreshold | int | 3 | Number of times after which if a probe fails in a row, Kubernetes considers that the overall check has failed: the container is not alive |\n| livenessProbe.httpGet | object | {\"path\":\"/health\",\"port\":8000} | Configuration of the kubelet http request on the server |\n| livenessProbe.httpGet.path | string | \"/health\" | Path to access on the HTTP server |\n| livenessProbe.httpGet.port | int | 8000 | Name or number of the port to access on the container, on which the server is listening |\n| livenessProbe.initialDelaySeconds | int | 15 | Number of seconds after the container has started before liveness probe is initiated |", "file_path": "deployment/frameworks/helm.md"}
{"id": "a38c2b563e11e817f93a3ddb13342eb2c82886796ad54aa7ff0383be83a46038", "heading": "Helm/Values", "level": 2, "text": "| livenessProbe.periodSeconds | int | 10 | How often (in seconds) to perform the liveness probe |\n| maxUnavailablePodDisruptionBudget | string | \"\" | Disruption Budget Configuration |\n| readinessProbe | object | {\"failureThreshold\":3,\"httpGet\":{\"path\":\"/health\",\"port\":8000},\"initialDelaySeconds\":5,\"periodSeconds\":5} | Readiness probe configuration |\n| readinessProbe.failureThreshold | int | 3 | Number of times after which if a probe fails in a row, Kubernetes considers that the overall check has failed: the container is not ready |\n| readinessProbe.httpGet | object | {\"path\":\"/health\",\"port\":8000} | Configuration of the kubelet http request on the server |\n| readinessProbe.httpGet.path | string | \"/health\" | Path to access on the HTTP server |\n| readinessProbe.httpGet.port | int | 8000 | Name or number of the port to access on the container, on which the server is listening |", "file_path": "deployment/frameworks/helm.md"}
{"id": "a38c2b563e11e817f93a3ddb13342eb2c82886796ad54aa7ff0383be83a46038", "heading": "Helm/Values", "level": 2, "text": "| readinessProbe.initialDelaySeconds | int | 5 | Number of seconds after the container has started before readiness probe is initiated |\n| readinessProbe.periodSeconds | int | 5 | How often (in seconds) to perform the readiness probe |\n| replicaCount | int | 1 | Number of replicas |\n| resources | object | {\"limits\":{\"cpu\":4,\"memory\":\"16Gi\",\"nvidia.com/gpu\":1},\"requests\":{\"cpu\":4,\"memory\":\"16Gi\",\"nvidia.com/gpu\":1}} | Resource configuration |\n| resources.limits.\"nvidia.com/gpu\" | int | 1 | Number of GPUs used |\n| resources.limits.cpu | int | 4 | Number of CPUs |\n| resources.limits.memory | string | \"16Gi\" | CPU memory configuration |\n| resources.requests.\"nvidia.com/gpu\" | int | 1 | Number of GPUs used |\n| resources.requests.cpu | int | 4 | Number of CPUs |\n| resources.requests.memory | string | \"16Gi\" | CPU memory configuration |\n| secrets | object | {} | Secrets configuration |\n| serviceName | string | \"\" | Service name |\n| servicePort | int | 80 | Service port |", "file_path": "deployment/frameworks/helm.md"}
{"id": "a38c2b563e11e817f93a3ddb13342eb2c82886796ad54aa7ff0383be83a46038", "heading": "Helm/Values", "level": 2, "text": "| servicePort | int | 80 | Service port |\n| labels.environment | string | test | Environment name |", "file_path": "deployment/frameworks/helm.md"}
{"id": "b7ea49ab63143afc0632eccf5b1e1df179296e3a2aff0887bc28f8f30c89281c", "heading": "Hugging Face Inference Endpoints/Overview", "level": 2, "text": "# Hugging Face Inference Endpoints  \n## Overview  \nModels compatible with vLLM can be deployed on Hugging Face Inference Endpoints, either starting from the [Hugging Face Hub](https://huggingface.co) or directly from the [Inference Endpoints](https://endpoints.huggingface.co/) interface. This allows you to serve models in a fully managed environment with GPU acceleration, auto-scaling, and monitoring, without managing the infrastructure manually.  \nFor advanced details on vLLM integration and deployment options, see [Advanced Deployment Details](#advanced-deployment-details).", "file_path": "deployment/frameworks/hf_inference_endpoints.md"}
{"id": "b7ea49ab63143afc0632eccf5b1e1df179296e3a2aff0887bc28f8f30c89281c", "heading": "Hugging Face Inference Endpoints/Deployment Methods", "level": 2, "text": "## Deployment Methods  \n- [**Method 1: Deploy from the Catalog.**](#method-1-deploy-from-the-catalog) One-click deploy models from the Hugging Face Hub with ready-made optimized configurations.\n- [**Method 2: Guided Deployment (Transformers Models).**](#method-2-guided-deployment-transformers-models) Instantly deploy models tagged with `transformers` from the Hub UI using the **Deploy** button.\n- [**Method 3: Manual Deployment (Advanced Models).**](#method-3-manual-deployment-advanced-models) For models that either use custom code with the `transformers` tag, or donâ€™t run with standard `transformers` but are supported by vLLM. This method requires manual configuration.", "file_path": "deployment/frameworks/hf_inference_endpoints.md"}
{"id": "b7ea49ab63143afc0632eccf5b1e1df179296e3a2aff0887bc28f8f30c89281c", "heading": "Hugging Face Inference Endpoints/Deployment Methods/Method 1: Deploy from the Catalog", "level": 3, "text": "### Method 1: Deploy from the Catalog  \nThis is the easiest way to get started with vLLM on Hugging Face Inference Endpoints. You can browse a catalog of models with verified and optimized deployment configuration at [Inference Endpoints](https://endpoints.huggingface.co/catalog) to maximize performance.  \n1. Go to [Endpoints Catalog](https://endpoints.huggingface.co/catalog) and in the **Inference Server** options, select `vLLM`.This will display the current list of models with optimized preconfigured options.  \n![Endpoints Catalog](../../assets/deployment/hf-inference-endpoints-catalog.png)  \n1. Select the desired model and click **Create Endpoint**.  \n![Create Endpoint](../../assets/deployment/hf-inference-endpoints-create-endpoint.png)  \n1. Once the deployment is ready, you can use the endpoint. Update the `DEPLOYMENT_URL` with the URL provided in the console, remembering to append `/v1` as required.  \n```python\n# pip install openai\nfrom openai import OpenAI\nimport os", "file_path": "deployment/frameworks/hf_inference_endpoints.md"}
{"id": "b7ea49ab63143afc0632eccf5b1e1df179296e3a2aff0887bc28f8f30c89281c", "heading": "Hugging Face Inference Endpoints/Deployment Methods/Method 1: Deploy from the Catalog", "level": 3, "text": "client = OpenAI(\nbase_url=DEPLOYMENT_URL,\napi_key=os.environ[\"HF_TOKEN\"],  # https://huggingface.co/settings/tokens\n)\n\nchat_completion = client.chat.completions.create(\nmodel=\"HuggingFaceTB/SmolLM3-3B\",\nmessages=[\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"Give me a brief explanation of gravity in simple terms.\",\n}\n],\n}\n],\nstream=True,\n)\n\nfor message in chat_completion:\nprint(message.choices[0].delta.content, end=\"\")\n```  \n!!! note\nThe catalog provides models optimized for vLLM, including GPU settings and inference engine configurations. You can monitor the endpoint and update the **container or its configuration** from the Inference Endpoints UI.", "file_path": "deployment/frameworks/hf_inference_endpoints.md"}
{"id": "b7ea49ab63143afc0632eccf5b1e1df179296e3a2aff0887bc28f8f30c89281c", "heading": "Hugging Face Inference Endpoints/Deployment Methods/Method 2: Guided Deployment (Transformers Models)", "level": 3, "text": "### Method 2: Guided Deployment (Transformers Models)  \nThis method applies to models with the [`transformers` library tag](https://huggingface.co/models?library=transformers) in their metadata. It allows you to deploy a model directly from the Hub UI without manual configuration.  \n1. Navigate to a model on [Hugging Face Hub](https://huggingface.co/models).\nFor this example we will use the [`ibm-granite/granite-docling-258M`](https://huggingface.co/ibm-granite/granite-docling-258M) model. You can verify that the model is compatible by checking the front matter in the [README](https://huggingface.co/ibm-granite/granite-docling-258M/blob/main/README.md), where the library is tagged as `library: transformers`.  \n2. Locate the **Deploy** button. The button appears for models tagged with `transformers` at the top right of the [model card](https://huggingface.co/ibm-granite/granite-docling-258M).  \n![Locate deploy button](../../assets/deployment/hf-inference-endpoints-locate-deploy-button.png)", "file_path": "deployment/frameworks/hf_inference_endpoints.md"}
{"id": "b7ea49ab63143afc0632eccf5b1e1df179296e3a2aff0887bc28f8f30c89281c", "heading": "Hugging Face Inference Endpoints/Deployment Methods/Method 2: Guided Deployment (Transformers Models)", "level": 3, "text": "3. Click to **Deploy** button > **HF Inference Endpoints**. You will be taken to the Inference Endpoints interface to configure the deployment.  \n![Click deploy button](../../assets/deployment/hf-inference-endpoints-click-deploy-button.png)  \n4. Select the Hardware (we choose AWS>GPU>T4 for the example) and Container Configuration. Choose `vLLM` as the container type and finalize the deployment pressing **Create Endpoint**.  \n![Select Hardware](../../assets/deployment/hf-inference-endpoints-select-hardware.png)  \n5. Use the deployed endpoint. Update the `DEPLOYMENT_URL` with the URL provided in the console (remember to add `/v1` needed). You can then use your endpoint programmatically or via the SDK.  \n```python\n# pip install openai\nfrom openai import OpenAI\nimport os", "file_path": "deployment/frameworks/hf_inference_endpoints.md"}
{"id": "b7ea49ab63143afc0632eccf5b1e1df179296e3a2aff0887bc28f8f30c89281c", "heading": "Hugging Face Inference Endpoints/Deployment Methods/Method 2: Guided Deployment (Transformers Models)", "level": 3, "text": "client = OpenAI(\nbase_url=DEPLOYMENT_URL,\napi_key=os.environ[\"HF_TOKEN\"],  # https://huggingface.co/settings/tokens\n)\n\nchat_completion = client.chat.completions.create(\nmodel=\"ibm-granite/granite-docling-258M\",\nmessages=[\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image_url\",\n\"image_url\": {\n\"url\": \"https://huggingface.co/ibm-granite/granite-docling-258M/resolve/main/assets/new_arxiv.png\",\n},\n},\n{\n\"type\": \"text\",\n\"text\": \"Convert this page to docling.\",\n},\n]\n}\n],\nstream=True,\n)\n\nfor message in chat_completion:\nprint(message.choices[0].delta.content, end=\"\")\n```  \n!!! note\nThis method uses best-guess defaults. You may need to adjust the configuration to fit your specific requirements.", "file_path": "deployment/frameworks/hf_inference_endpoints.md"}
{"id": "b7ea49ab63143afc0632eccf5b1e1df179296e3a2aff0887bc28f8f30c89281c", "heading": "Hugging Face Inference Endpoints/Deployment Methods/Method 3: Manual Deployment (Advanced Models)", "level": 3, "text": "### Method 3: Manual Deployment (Advanced Models)  \nSome models require manual deployment because they:  \n- Use custom code with the `transformers` tag\n- Don't run with standard `transformers` but are supported by `vLLM`  \nThese models cannot be deployed using the **Deploy** button on the model card.  \nIn this guide, we demonstrate manual deployment using the [`rednote-hilab/dots.ocr`](https://huggingface.co/rednote-hilab/dots.ocr) model, an OCR model integrated with vLLM (see vLLM [PR](https://github.com/vllm-project/vllm/pull/24645)).  \n1. Start a new deployment. Go to [Inference Endpoints](https://endpoints.huggingface.co/) and click `New`.  \n![New Endpoint](../../assets/deployment/hf-inference-endpoints-new-endpoint.png)  \n2. Search the model in the Hub. In the dialog, switch to **Hub** and search for the desired model.  \n![Select model](../../assets/deployment/hf-inference-endpoints-select-model.png)", "file_path": "deployment/frameworks/hf_inference_endpoints.md"}
{"id": "b7ea49ab63143afc0632eccf5b1e1df179296e3a2aff0887bc28f8f30c89281c", "heading": "Hugging Face Inference Endpoints/Deployment Methods/Method 3: Manual Deployment (Advanced Models)", "level": 3, "text": "3. Choosing infrastructure. On the configuration page, select the cloud provider and hardware from the available options.\nFor this demo, we choose AWS and L4 GPU. Adjust according to your hardware needs.  \n![Choose Infra](../../assets/deployment/hf-inference-endpoints-choose-infra.png)  \n4. Configure the container. Scroll to the **Container Configuration** and select `vLLM` as the container type.  \n![Configure Container](../../assets/deployment/hf-inference-endpoints-configure-container.png)  \n5. Create the endpoint. Click **Create Endpoint** to deploy the model.  \nOnce the endpoint is ready, you can use it with the OpenAI Completion API, cURL, or other SDKs. Remember to append `/v1` to the deployment URL if needed.  \n!!! note", "file_path": "deployment/frameworks/hf_inference_endpoints.md"}
{"id": "b7ea49ab63143afc0632eccf5b1e1df179296e3a2aff0887bc28f8f30c89281c", "heading": "Hugging Face Inference Endpoints/Deployment Methods/Method 3: Manual Deployment (Advanced Models)", "level": 3, "text": "!!! note\nYou can adjust the **container settings** (Container URI, Container Arguments) from the Inference Endpoints UI and press **Update Endpoint**. This redeploys the endpoint with the updated container configuration. Changes to the model itself require creating a new endpoint or redeploying with a different model. For example, for this demo, you may need to update the Container URI to the nightly image (`vllm/vllm-openai:nightly`) and add the `--trust-remote-code` flag in the container arguments.", "file_path": "deployment/frameworks/hf_inference_endpoints.md"}
{"id": "b7ea49ab63143afc0632eccf5b1e1df179296e3a2aff0887bc28f8f30c89281c", "heading": "Hugging Face Inference Endpoints/Advanced Deployment Details", "level": 2, "text": "## Advanced Deployment Details  \nWith the [transformers backend integration](https://blog.vllm.ai/2025/04/11/transformers-backend.html), vLLM now offers Day 0 support for any model compatible with `transformers`. This means you can deploy such models immediately, leveraging vLLMâ€™s optimized inference without additional backend modifications.  \nHugging Face Inference Endpoints provides a fully managed environment for serving models via vLLM. You can deploy models without configuring servers, installing dependencies, or managing clusters. Endpoints also support deployment across multiple cloud providers (AWS, Azure, GCP) without the need for separate accounts.", "file_path": "deployment/frameworks/hf_inference_endpoints.md"}
{"id": "b7ea49ab63143afc0632eccf5b1e1df179296e3a2aff0887bc28f8f30c89281c", "heading": "Hugging Face Inference Endpoints/Advanced Deployment Details", "level": 2, "text": "The platform integrates seamlessly with the Hugging Face Hub, allowing you to deploy any vLLM- or `transformers`-compatible model, track usage, and update the inference engine directly. The vLLM engine comes preconfigured, enabling optimized inference and easy switching between models or engines without modifying your code. This setup simplifies production deployment: endpoints are ready in minutes, include monitoring and logging, and let you focus on serving models rather than maintaining infrastructure.", "file_path": "deployment/frameworks/hf_inference_endpoints.md"}
{"id": "b7ea49ab63143afc0632eccf5b1e1df179296e3a2aff0887bc28f8f30c89281c", "heading": "Hugging Face Inference Endpoints/Next Steps", "level": 2, "text": "## Next Steps  \n- Explore the [Inference Endpoints](https://endpoints.huggingface.co/catalog) model catalog\n- Read the Inference Endpoints [documentation](https://huggingface.co/docs/inference-endpoints/en/index)\n- Learn about [Inference Endpoints engines](https://huggingface.co/docs/inference-endpoints/en/engines/vllm)\n- Understand the [transformers backend integration](https://blog.vllm.ai/2025/04/11/transformers-backend.html)", "file_path": "deployment/frameworks/hf_inference_endpoints.md"}
{"id": "863b93508c46a83c16f402c7f7f087774b715357a923d1f36e0f0d1852ca78c2", "heading": "LiteLLM", "level": 1, "text": "# LiteLLM  \n[LiteLLM](https://github.com/BerriAI/litellm) call all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.]  \nLiteLLM manages:  \n- Translate inputs to provider's `completion`, `embedding`, and `image_generation` endpoints\n- [Consistent output](https://docs.litellm.ai/docs/completion/output), text responses will always be available at `['choices'][0]['message']['content']`\n- Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - [Router](https://docs.litellm.ai/docs/routing)\n- Set Budgets & Rate limits per project, api key, model [LiteLLM Proxy Server (LLM Gateway)](https://docs.litellm.ai/docs/simple_proxy)  \nAnd LiteLLM supports all models on VLLM.", "file_path": "deployment/frameworks/litellm.md"}
{"id": "863b93508c46a83c16f402c7f7f087774b715357a923d1f36e0f0d1852ca78c2", "heading": "LiteLLM/Prerequisites", "level": 2, "text": "## Prerequisites  \nSet up the vLLM and litellm environment:  \n```bash\npip install vllm litellm\n```", "file_path": "deployment/frameworks/litellm.md"}
{"id": "863b93508c46a83c16f402c7f7f087774b715357a923d1f36e0f0d1852ca78c2", "heading": "LiteLLM/Deploy/Chat completion", "level": 3, "text": "## Deploy  \n### Chat completion  \n1. Start the vLLM server with the supported chat completion model, e.g.  \n```bash\nvllm serve qwen/Qwen1.5-0.5B-Chat\n```  \n1. Call it with litellm:  \n??? code  \n```python\nimport litellm\n\nmessages = [{\"content\": \"Hello, how are you?\", \"role\": \"user\"}]\n\n# hosted_vllm is prefix key word and necessary\nresponse = litellm.completion(\nmodel=\"hosted_vllm/qwen/Qwen1.5-0.5B-Chat\", # pass the vllm model name\nmessages=messages,\napi_base=\"http://{your-vllm-server-host}:{your-vllm-server-port}/v1\",\ntemperature=0.2,\nmax_tokens=80,\n)\n\nprint(response)\n```", "file_path": "deployment/frameworks/litellm.md"}
{"id": "863b93508c46a83c16f402c7f7f087774b715357a923d1f36e0f0d1852ca78c2", "heading": "LiteLLM/Deploy/Embeddings", "level": 3, "text": "### Embeddings  \n1. Start the vLLM server with the supported embedding model, e.g.  \n```bash\nvllm serve BAAI/bge-base-en-v1.5\n```  \n1. Call it with litellm:  \n```python\nfrom litellm import embedding\nimport os\n\nos.environ[\"HOSTED_VLLM_API_BASE\"] = \"http://{your-vllm-server-host}:{your-vllm-server-port}/v1\"\n\n# hosted_vllm is prefix key word and necessary\n# pass the vllm model name\nembedding = embedding(model=\"hosted_vllm/BAAI/bge-base-en-v1.5\", input=[\"Hello world\"])\n\nprint(embedding)\n```  \nFor details, see the tutorial [Using vLLM in LiteLLM](https://docs.litellm.ai/docs/providers/vllm).", "file_path": "deployment/frameworks/litellm.md"}
{"id": "297005ea8e1c46a9ef649efdf8ebc5f8515bd5a0e1f0e67fc25db66cf714872a", "heading": "Lobe Chat", "level": 1, "text": "# Lobe Chat  \n[Lobe Chat](https://github.com/lobehub/lobe-chat) is an open-source, modern-design ChatGPT/LLMs UI/Framework.  \nSupports speech-synthesis, multi-modal, and extensible (function call) plugin system.  \nOne-click FREE deployment of your private OpenAI ChatGPT/Claude/Gemini/Groq/Ollama chat application.  \nIt supports vLLM as an AI model provider to efficiently serve large language models.  \nFor details, see the tutorial [Using vLLM in LobeChat](https://lobehub.com/docs/usage/providers/vllm).", "file_path": "deployment/frameworks/lobe-chat.md"}
{"id": "50bade61ebaa52f554fb6045a0c342730a40515dd7f81902ff8cd67f9df1b454", "heading": "LWS", "level": 1, "text": "# LWS  \nLeaderWorkerSet (LWS) is a Kubernetes API that aims to address common deployment patterns of AI/ML inference workloads.\nA major use case is for multi-host/multi-node distributed inference.  \nvLLM can be deployed with [LWS](https://github.com/kubernetes-sigs/lws) on Kubernetes for distributed model serving.", "file_path": "deployment/frameworks/lws.md"}
{"id": "50bade61ebaa52f554fb6045a0c342730a40515dd7f81902ff8cd67f9df1b454", "heading": "LWS/Prerequisites", "level": 2, "text": "## Prerequisites  \n* At least two Kubernetes nodes, each with 8 GPUs, are required.\n* Install LWS by following the instructions found [here](https://lws.sigs.k8s.io/docs/installation/).", "file_path": "deployment/frameworks/lws.md"}
{"id": "50bade61ebaa52f554fb6045a0c342730a40515dd7f81902ff8cd67f9df1b454", "heading": "LWS/Deploy and Serve", "level": 2, "text": "## Deploy and Serve  \nDeploy the following yaml file `lws.yaml`  \n??? code \"Yaml\"  \n```yaml\napiVersion: leaderworkerset.x-k8s.io/v1\nkind: LeaderWorkerSet\nmetadata:\nname: vllm\nspec:\nreplicas: 1\nleaderWorkerTemplate:\nsize: 2\nrestartPolicy: RecreateGroupOnPodRestart\nleaderTemplate:\nmetadata:\nlabels:\nrole: leader\nspec:\ncontainers:\n- name: vllm-leader\nimage: docker.io/vllm/vllm-openai:latest\nenv:\n- name: HF_TOKEN\nvalue: <your-hf-token>\ncommand:\n- sh\n- -c\n- \"bash /vllm-workspace/examples/online_serving/multi-node-serving.sh leader --ray_cluster_size=$(LWS_GROUP_SIZE);\nvllm serve meta-llama/Meta-Llama-3.1-405B-Instruct --port 8080 --tensor-parallel-size 8 --pipeline_parallel_size 2\"\nresources:\nlimits:\nnvidia.com/gpu: \"8\"\nmemory: 1124Gi\nephemeral-storage: 800Gi\nrequests:\nephemeral-storage: 800Gi\ncpu: 125\nports:\n- containerPort: 8080\nreadinessProbe:\ntcpSocket:\nport: 8080\ninitialDelaySeconds: 15\nperiodSeconds: 10\nvolumeMounts:\n- mountPath: /dev/shm\nname: dshm\nvolumes:\n- name: dshm\nemptyDir:\nmedium: Memory", "file_path": "deployment/frameworks/lws.md"}
{"id": "50bade61ebaa52f554fb6045a0c342730a40515dd7f81902ff8cd67f9df1b454", "heading": "LWS/Deploy and Serve", "level": 2, "text": "name: dshm\nvolumes:\n- name: dshm\nemptyDir:\nmedium: Memory\nsizeLimit: 15Gi\nworkerTemplate:\nspec:\ncontainers:\n- name: vllm-worker\nimage: docker.io/vllm/vllm-openai:latest\ncommand:\n- sh\n- -c\n- \"bash /vllm-workspace/examples/online_serving/multi-node-serving.sh worker --ray_address=$(LWS_LEADER_ADDRESS)\"\nresources:\nlimits:\nnvidia.com/gpu: \"8\"\nmemory: 1124Gi\nephemeral-storage: 800Gi\nrequests:\nephemeral-storage: 800Gi\ncpu: 125\nenv:\n- name: HF_TOKEN\nvalue: <your-hf-token>\nvolumeMounts:\n- mountPath: /dev/shm\nname: dshm\nvolumes:\n- name: dshm\nemptyDir:\nmedium: Memory\nsizeLimit: 15Gi\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: vllm-leader\nspec:\nports:\n- name: http\nport: 8080\nprotocol: TCP\ntargetPort: 8080\nselector:\nleaderworkerset.sigs.k8s.io/name: vllm\nrole: leader\ntype: ClusterIP\n```  \n```bash\nkubectl apply -f lws.yaml\n```  \nVerify the status of the pods:  \n```bash\nkubectl get pods\n```  \nShould get an output similar to this:  \n```bash\nNAME       READY   STATUS    RESTARTS   AGE", "file_path": "deployment/frameworks/lws.md"}
{"id": "50bade61ebaa52f554fb6045a0c342730a40515dd7f81902ff8cd67f9df1b454", "heading": "LWS/Deploy and Serve", "level": 2, "text": "```bash\nNAME       READY   STATUS    RESTARTS   AGE\nvllm-0     1/1     Running   0          2s\nvllm-0-1   1/1     Running   0          2s\n```  \nVerify that the distributed tensor-parallel inference works:  \n```bash\nkubectl logs vllm-0 |grep -i \"Loading model weights took\"\n```  \nShould get something similar to this:  \n```text\nINFO 05-08 03:20:24 model_runner.py:173] Loading model weights took 0.1189 GB\n(RayWorkerWrapper pid=169, ip=10.20.0.197) INFO 05-08 03:20:28 model_runner.py:173] Loading model weights took 0.1189 GB\n```", "file_path": "deployment/frameworks/lws.md"}
{"id": "50bade61ebaa52f554fb6045a0c342730a40515dd7f81902ff8cd67f9df1b454", "heading": "LWS/Access ClusterIP service", "level": 2, "text": "## Access ClusterIP service  \n```bash\n# Listen on port 8080 locally, forwarding to the targetPort of the service's port 8080 in a pod selected by the service\nkubectl port-forward svc/vllm-leader 8080:8080\n```  \nThe output should be similar to the following:  \n```text\nForwarding from 127.0.0.1:8080 -> 8080\nForwarding from [::1]:8080 -> 8080\n```", "file_path": "deployment/frameworks/lws.md"}
{"id": "50bade61ebaa52f554fb6045a0c342730a40515dd7f81902ff8cd67f9df1b454", "heading": "LWS/Serve the model", "level": 2, "text": "## Serve the model  \nOpen another terminal and send a request  \n```text\ncurl http://localhost:8080/v1/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n\"prompt\": \"San Francisco is a\",\n\"max_tokens\": 7,\n\"temperature\": 0\n}'\n```  \nThe output should be similar to the following  \n??? console \"Output\"  \n```text\n{\n\"id\": \"cmpl-1bb34faba88b43f9862cfbfb2200949d\",\n\"object\": \"text_completion\",\n\"created\": 1715138766,\n\"model\": \"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n\"choices\": [\n{\n\"index\": 0,\n\"text\": \" top destination for foodies, with\",\n\"logprobs\": null,\n\"finish_reason\": \"length\",\n\"stop_reason\": null\n}\n],\n\"usage\": {\n\"prompt_tokens\": 5,\n\"total_tokens\": 12,\n\"completion_tokens\": 7\n}\n}\n```", "file_path": "deployment/frameworks/lws.md"}
{"id": "25d6f265f4508d875c03bced16cea2c74f69ed82705ea3b706923693372150c1", "heading": "Modal", "level": 1, "text": "# Modal  \nvLLM can be run on cloud GPUs with [Modal](https://modal.com), a serverless computing platform designed for fast auto-scaling.  \nFor details on how to deploy vLLM on Modal, see [this tutorial in the Modal documentation](https://modal.com/docs/examples/vllm_inference).", "file_path": "deployment/frameworks/modal.md"}
{"id": "bc3d19c935127b989ddcfb121230f8adcdb4448f2179139c15c563a161e87ba4", "heading": "Open WebUI", "level": 1, "text": "# Open WebUI  \n[Open WebUI](https://github.com/open-webui/open-webui) is an extensible, feature-rich,\nand user-friendly self-hosted AI platform designed to operate entirely offline.\nIt supports various LLM runners like Ollama and OpenAI-compatible APIs,\nwith built-in RAG capabilities, making it a powerful AI deployment solution.  \nTo get started with Open WebUI using vLLM, follow these steps:  \n1. Install the [Docker](https://docs.docker.com/engine/install/).  \n2. Start the vLLM server with a supported chat completion model:  \n```console\nvllm serve Qwen/Qwen3-0.6B-Chat\n```  \n!!! note\nWhen starting the vLLM server, be sure to specify the host and port using the `--host` and `--port` flags.\nFor example:  \n```console\nvllm serve <model> --host 0.0.0.0 --port 8000\n```  \n3. Start the Open WebUI Docker container:  \n```console\ndocker run -d \\\n--name open-webui \\\n-p 3000:8080 \\\n-v open-webui:/app/backend/data \\\n-e OPENAI_API_BASE_URL=http://0.0.0.0:8000/v1 \\\n--restart always \\\nghcr.io/open-webui/open-webui:main\n```", "file_path": "deployment/frameworks/open-webui.md"}
{"id": "bc3d19c935127b989ddcfb121230f8adcdb4448f2179139c15c563a161e87ba4", "heading": "Open WebUI", "level": 1, "text": "--restart always \\\nghcr.io/open-webui/open-webui:main\n```  \n4. Open it in the browser: <http://open-webui-host:3000/>  \nAt the top of the page, you should see the model `Qwen/Qwen3-0.6B-Chat`.  \n![Web portal of model Qwen/Qwen3-0.6B-Chat](../../assets/deployment/open_webui.png)", "file_path": "deployment/frameworks/open-webui.md"}
{"id": "15769d61407d6341970b16e83903d25ce1a5eab927695ab0f6b7c1d52ab7af78", "heading": "Retrieval-Augmented Generation", "level": 1, "text": "# Retrieval-Augmented Generation  \n[Retrieval-augmented generation (RAG)](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) is a technique that enables generative artificial intelligence (Gen AI) models to retrieve and incorporate new information. It modifies interactions with a large language model (LLM) so that the model responds to user queries with reference to a specified set of documents, using this information to supplement information from its pre-existing training data. This allows LLMs to use domain-specific and/or updated information. Use cases include providing chatbot access to internal company data or generating responses based on authoritative sources.  \nHere are the integrations:  \n- vLLM + [langchain](https://github.com/langchain-ai/langchain) + [milvus](https://github.com/milvus-io/milvus)\n- vLLM + [llamaindex](https://github.com/run-llama/llama_index) + [milvus](https://github.com/milvus-io/milvus)", "file_path": "deployment/frameworks/retrieval_augmented_generation.md"}
{"id": "15769d61407d6341970b16e83903d25ce1a5eab927695ab0f6b7c1d52ab7af78", "heading": "Retrieval-Augmented Generation/vLLM + langchain/Prerequisites", "level": 3, "text": "## vLLM + langchain  \n### Prerequisites  \nSet up the vLLM and langchain environment:  \n```bash\npip install -U vllm \\\nlangchain_milvus langchain_openai \\\nlangchain_community beautifulsoup4 \\\nlangchain-text-splitters\n```", "file_path": "deployment/frameworks/retrieval_augmented_generation.md"}
{"id": "15769d61407d6341970b16e83903d25ce1a5eab927695ab0f6b7c1d52ab7af78", "heading": "Retrieval-Augmented Generation/vLLM + langchain/Deploy", "level": 3, "text": "### Deploy  \n1. Start the vLLM server with the supported embedding model, e.g.  \n```bash\n# Start embedding service (port 8000)\nvllm serve ssmits/Qwen2-7B-Instruct-embed-base\n```  \n1. Start the vLLM server with the supported chat completion model, e.g.  \n```bash\n# Start chat service (port 8001)\nvllm serve qwen/Qwen1.5-0.5B-Chat --port 8001\n```  \n1. Use the script: <gh-file:examples/online_serving/retrieval_augmented_generation_with_langchain.py>  \n1. Run the script  \n```bash\npython retrieval_augmented_generation_with_langchain.py\n```", "file_path": "deployment/frameworks/retrieval_augmented_generation.md"}
{"id": "15769d61407d6341970b16e83903d25ce1a5eab927695ab0f6b7c1d52ab7af78", "heading": "Retrieval-Augmented Generation/vLLM + llamaindex/Prerequisites", "level": 3, "text": "## vLLM + llamaindex  \n### Prerequisites  \nSet up the vLLM and llamaindex environment:  \n```bash\npip install vllm \\\nllama-index llama-index-readers-web \\\nllama-index-llms-openai-like    \\\nllama-index-embeddings-openai-like \\\nllama-index-vector-stores-milvus \\\n```", "file_path": "deployment/frameworks/retrieval_augmented_generation.md"}
{"id": "15769d61407d6341970b16e83903d25ce1a5eab927695ab0f6b7c1d52ab7af78", "heading": "Retrieval-Augmented Generation/vLLM + llamaindex/Deploy", "level": 3, "text": "### Deploy  \n1. Start the vLLM server with the supported embedding model, e.g.  \n```bash\n# Start embedding service (port 8000)\nvllm serve ssmits/Qwen2-7B-Instruct-embed-base\n```  \n1. Start the vLLM server with the supported chat completion model, e.g.  \n```bash\n# Start chat service (port 8001)\nvllm serve qwen/Qwen1.5-0.5B-Chat --port 8001\n```  \n1. Use the script: <gh-file:examples/online_serving/retrieval_augmented_generation_with_llamaindex.py>  \n1. Run the script:  \n```bash\npython retrieval_augmented_generation_with_llamaindex.py\n```", "file_path": "deployment/frameworks/retrieval_augmented_generation.md"}
{"id": "d604aaee4f7bb1472c379b83982dea6202082b3962089e26b8d9f410125336d3", "heading": "SkyPilot", "level": 1, "text": "# SkyPilot  \n<p align=\"center\">\n<img src=\"https://imgur.com/yxtzPEu.png\" alt=\"vLLM\"/>\n</p>  \nvLLM can be **run and scaled to multiple service replicas on clouds and Kubernetes** with [SkyPilot](https://github.com/skypilot-org/skypilot), an open-source framework for running LLMs on any cloud. More examples for various open models, such as Llama-3, Mixtral, etc, can be found in [SkyPilot AI gallery](https://skypilot.readthedocs.io/en/latest/gallery/index.html).", "file_path": "deployment/frameworks/skypilot.md"}
{"id": "d604aaee4f7bb1472c379b83982dea6202082b3962089e26b8d9f410125336d3", "heading": "SkyPilot/Prerequisites", "level": 2, "text": "## Prerequisites  \n- Go to the [HuggingFace model page](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) and request access to the model `meta-llama/Meta-Llama-3-8B-Instruct`.\n- Check that you have installed SkyPilot ([docs](https://skypilot.readthedocs.io/en/latest/getting-started/installation.html)).\n- Check that `sky check` shows clouds or Kubernetes are enabled.  \n```bash\npip install skypilot-nightly\nsky check\n```", "file_path": "deployment/frameworks/skypilot.md"}
{"id": "d604aaee4f7bb1472c379b83982dea6202082b3962089e26b8d9f410125336d3", "heading": "SkyPilot/Run on a single instance", "level": 2, "text": "## Run on a single instance  \nSee the vLLM SkyPilot YAML for serving, [serving.yaml](https://github.com/skypilot-org/skypilot/blob/master/llm/vllm/serve.yaml).  \n??? code \"Yaml\"  \n```yaml\nresources:\naccelerators: {L4, A10g, A10, L40, A40, A100, A100-80GB} # We can use cheaper accelerators for 8B model.\nuse_spot: True\ndisk_size: 512  # Ensure model checkpoints can fit.\ndisk_tier: best\nports: 8081  # Expose to internet traffic.\n\nenvs:\nPYTHONUNBUFFERED: 1\nMODEL_NAME: meta-llama/Meta-Llama-3-8B-Instruct\nHF_TOKEN: <your-huggingface-token>  # Change to your own huggingface token, or use --env to pass.\n\nsetup: |\nconda create -n vllm python=3.10 -y\nconda activate vllm\n\npip install vllm==0.4.0.post1\n# Install Gradio for web UI.\npip install gradio openai\npip install flash-attn==2.5.7\n\nrun: |\nconda activate vllm\necho 'Starting vllm api server...'\nvllm serve $MODEL_NAME \\\n--port 8081 \\\n--trust-remote-code \\\n--tensor-parallel-size $SKYPILOT_NUM_GPUS_PER_NODE \\\n2>&1 | tee api_server.log &", "file_path": "deployment/frameworks/skypilot.md"}
{"id": "d604aaee4f7bb1472c379b83982dea6202082b3962089e26b8d9f410125336d3", "heading": "SkyPilot/Run on a single instance", "level": 2, "text": "echo 'Waiting for vllm api server to start...'\nwhile ! `cat api_server.log | grep -q 'Uvicorn running on'`; do sleep 1; done", "file_path": "deployment/frameworks/skypilot.md"}
{"id": "d604aaee4f7bb1472c379b83982dea6202082b3962089e26b8d9f410125336d3", "heading": "SkyPilot/Run on a single instance", "level": 2, "text": "echo 'Starting gradio server...'\ngit clone https://github.com/vllm-project/vllm.git || true\npython vllm/examples/online_serving/gradio_openai_chatbot_webserver.py \\\n-m $MODEL_NAME \\\n--port 8811 \\\n--model-url http://localhost:8081/v1 \\\n--stop-token-ids 128009,128001\n```  \nStart the serving the Llama-3 8B model on any of the candidate GPUs listed (L4, A10g, ...):  \n```bash\nHF_TOKEN=\"your-huggingface-token\" sky launch serving.yaml --env HF_TOKEN\n```  \nCheck the output of the command. There will be a shareable gradio link (like the last line of the following). Open it in your browser to use the LLaMA model to do the text completion.  \n```console\n(task, pid=7431) Running on public URL: https://<gradio-hash>.gradio.live\n```  \n**Optional**: Serve the 70B model instead of the default 8B and use more GPU:  \n```bash\nHF_TOKEN=\"your-huggingface-token\" \\\nsky launch serving.yaml \\\n--gpus A100:8 \\\n--env HF_TOKEN \\\n--env MODEL_NAME=meta-llama/Meta-Llama-3-70B-Instruct\n```", "file_path": "deployment/frameworks/skypilot.md"}
{"id": "d604aaee4f7bb1472c379b83982dea6202082b3962089e26b8d9f410125336d3", "heading": "SkyPilot/Scale up to multiple replicas", "level": 2, "text": "## Scale up to multiple replicas  \nSkyPilot can scale up the service to multiple service replicas with built-in autoscaling, load-balancing and fault-tolerance. You can do it by adding a services section to the YAML file.  \n??? code \"Yaml\"  \n```yaml\nservice:\nreplicas: 2\n# An actual request for readiness probe.\nreadiness_probe:\npath: /v1/chat/completions\npost_data:\nmodel: $MODEL_NAME\nmessages:\n- role: user\ncontent: Hello! What is your name?\nmax_completion_tokens: 1\n```  \n??? code \"Yaml\"  \n```yaml\nservice:\nreplicas: 2\n# An actual request for readiness probe.\nreadiness_probe:\npath: /v1/chat/completions\npost_data:\nmodel: $MODEL_NAME\nmessages:\n- role: user\ncontent: Hello! What is your name?\nmax_completion_tokens: 1\n\nresources:\naccelerators: {L4, A10g, A10, L40, A40, A100, A100-80GB} # We can use cheaper accelerators for 8B model.\nuse_spot: True\ndisk_size: 512  # Ensure model checkpoints can fit.\ndisk_tier: best\nports: 8081  # Expose to internet traffic.", "file_path": "deployment/frameworks/skypilot.md"}
{"id": "d604aaee4f7bb1472c379b83982dea6202082b3962089e26b8d9f410125336d3", "heading": "SkyPilot/Scale up to multiple replicas", "level": 2, "text": "envs:\nPYTHONUNBUFFERED: 1\nMODEL_NAME: meta-llama/Meta-Llama-3-8B-Instruct\nHF_TOKEN: <your-huggingface-token>  # Change to your own huggingface token, or use --env to pass.\n\nsetup: |\nconda create -n vllm python=3.10 -y\nconda activate vllm\n\npip install vllm==0.4.0.post1\n# Install Gradio for web UI.\npip install gradio openai\npip install flash-attn==2.5.7\n\nrun: |\nconda activate vllm\necho 'Starting vllm api server...'\nvllm serve $MODEL_NAME \\\n--port 8081 \\\n--trust-remote-code \\\n--tensor-parallel-size $SKYPILOT_NUM_GPUS_PER_NODE \\\n2>&1 | tee api_server.log\n```  \nStart the serving the Llama-3 8B model on multiple replicas:  \n```bash\nHF_TOKEN=\"your-huggingface-token\" \\\nsky serve up -n vllm serving.yaml \\\n--env HF_TOKEN\n```  \nWait until the service is ready:  \n```bash\nwatch -n10 sky serve status vllm\n```  \nExample outputs:  \n```console\nServices\nNAME  VERSION  UPTIME  STATUS  REPLICAS  ENDPOINT\nvllm  1        35s     READY   2/2       xx.yy.zz.100:30001", "file_path": "deployment/frameworks/skypilot.md"}
{"id": "d604aaee4f7bb1472c379b83982dea6202082b3962089e26b8d9f410125336d3", "heading": "SkyPilot/Scale up to multiple replicas", "level": 2, "text": "Service Replicas\nSERVICE_NAME  ID  VERSION  IP            LAUNCHED     RESOURCES                STATUS  REGION\nvllm          1   1        xx.yy.zz.121  18 mins ago  1x GCP([Spot]{'L4': 1})  READY   us-east4\nvllm          2   1        xx.yy.zz.245  18 mins ago  1x GCP([Spot]{'L4': 1})  READY   us-east4\n```  \nAfter the service is READY, you can find a single endpoint for the service and access the service with the endpoint:  \n??? console \"Commands\"  \n```bash\nENDPOINT=$(sky serve status --endpoint 8081 vllm)\ncurl -L http://$ENDPOINT/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n\"messages\": [\n{\n\"role\": \"system\",\n\"content\": \"You are a helpful assistant.\"\n},\n{\n\"role\": \"user\",\n\"content\": \"Who are you?\"\n}\n],\n\"stop_token_ids\": [128009,  128001]\n}'\n```  \nTo enable autoscaling, you could replace the `replicas` with the following configs in `service`:  \n```yaml\nservice:\nreplica_policy:\nmin_replicas: 2\nmax_replicas: 4\ntarget_qps_per_replica: 2\n```", "file_path": "deployment/frameworks/skypilot.md"}
{"id": "d604aaee4f7bb1472c379b83982dea6202082b3962089e26b8d9f410125336d3", "heading": "SkyPilot/Scale up to multiple replicas", "level": 2, "text": "min_replicas: 2\nmax_replicas: 4\ntarget_qps_per_replica: 2\n```  \nThis will scale the service up to when the QPS exceeds 2 for each replica.  \n??? code \"Yaml\"  \n```yaml\nservice:\nreplica_policy:\nmin_replicas: 2\nmax_replicas: 4\ntarget_qps_per_replica: 2\n# An actual request for readiness probe.\nreadiness_probe:\npath: /v1/chat/completions\npost_data:\nmodel: $MODEL_NAME\nmessages:\n- role: user\ncontent: Hello! What is your name?\nmax_completion_tokens: 1", "file_path": "deployment/frameworks/skypilot.md"}
{"id": "d604aaee4f7bb1472c379b83982dea6202082b3962089e26b8d9f410125336d3", "heading": "SkyPilot/Scale up to multiple replicas", "level": 2, "text": "resources:\naccelerators: {L4, A10g, A10, L40, A40, A100, A100-80GB} # We can use cheaper accelerators for 8B model.\nuse_spot: True\ndisk_size: 512  # Ensure model checkpoints can fit.\ndisk_tier: best\nports: 8081  # Expose to internet traffic.\n\nenvs:\nPYTHONUNBUFFERED: 1\nMODEL_NAME: meta-llama/Meta-Llama-3-8B-Instruct\nHF_TOKEN: <your-huggingface-token>  # Change to your own huggingface token, or use --env to pass.\n\nsetup: |\nconda create -n vllm python=3.10 -y\nconda activate vllm\n\npip install vllm==0.4.0.post1\n# Install Gradio for web UI.\npip install gradio openai\npip install flash-attn==2.5.7\n\nrun: |\nconda activate vllm\necho 'Starting vllm api server...'\nvllm serve $MODEL_NAME \\\n--port 8081 \\\n--trust-remote-code \\\n--tensor-parallel-size $SKYPILOT_NUM_GPUS_PER_NODE \\\n2>&1 | tee api_server.log\n```  \nTo update the service with the new config:  \n```bash\nHF_TOKEN=\"your-huggingface-token\" sky serve update vllm serving.yaml --env HF_TOKEN\n```  \nTo stop the service:  \n```bash\nsky serve down vllm\n```", "file_path": "deployment/frameworks/skypilot.md"}
{"id": "d604aaee4f7bb1472c379b83982dea6202082b3962089e26b8d9f410125336d3", "heading": "SkyPilot/Scale up to multiple replicas/**Optional**: Connect a GUI to the endpoint", "level": 3, "text": "### **Optional**: Connect a GUI to the endpoint  \nIt is also possible to access the Llama-3 service with a separate GUI frontend, so the user requests send to the GUI will be load-balanced across replicas.  \n??? code \"Yaml\"  \n```yaml\nenvs:\nMODEL_NAME: meta-llama/Meta-Llama-3-8B-Instruct\nENDPOINT: x.x.x.x:3031 # Address of the API server running vllm.\n\nresources:\ncpus: 2\n\nsetup: |\nconda create -n vllm python=3.10 -y\nconda activate vllm\n\n# Install Gradio for web UI.\npip install gradio openai\n\nrun: |\nconda activate vllm\nexport PATH=$PATH:/sbin", "file_path": "deployment/frameworks/skypilot.md"}
{"id": "d604aaee4f7bb1472c379b83982dea6202082b3962089e26b8d9f410125336d3", "heading": "SkyPilot/Scale up to multiple replicas/**Optional**: Connect a GUI to the endpoint", "level": 3, "text": "run: |\nconda activate vllm\nexport PATH=$PATH:/sbin\n\necho 'Starting gradio server...'\ngit clone https://github.com/vllm-project/vllm.git || true\npython vllm/examples/online_serving/gradio_openai_chatbot_webserver.py \\\n-m $MODEL_NAME \\\n--port 8811 \\\n--model-url http://$ENDPOINT/v1 \\\n--stop-token-ids 128009,128001 | tee ~/gradio.log\n```  \n1. Start the chat web UI:  \n```bash\nsky launch \\\n-c gui ./gui.yaml \\\n--env ENDPOINT=$(sky serve status --endpoint vllm)\n```  \n2. Then, we can access the GUI at the returned gradio link:  \n```console\n| INFO | stdout | Running on public URL: https://6141e84201ce0bb4ed.gradio.live\n```", "file_path": "deployment/frameworks/skypilot.md"}
{"id": "50e40c55056e49c4ccc5270cf1aef653a6a56687081e8f5823f9a1f6e234d108", "heading": "Streamlit", "level": 1, "text": "# Streamlit  \n[Streamlit](https://github.com/streamlit/streamlit) lets you transform Python scripts into interactive web apps in minutes, instead of weeks. Build dashboards, generate reports, or create chat apps.  \nIt can be quickly integrated with vLLM as a backend API server, enabling powerful LLM inference via API calls.", "file_path": "deployment/frameworks/streamlit.md"}
{"id": "50e40c55056e49c4ccc5270cf1aef653a6a56687081e8f5823f9a1f6e234d108", "heading": "Streamlit/Prerequisites", "level": 2, "text": "## Prerequisites  \nSet up the vLLM environment by installing all required packages:  \n```bash\npip install vllm streamlit openai\n```", "file_path": "deployment/frameworks/streamlit.md"}
{"id": "50e40c55056e49c4ccc5270cf1aef653a6a56687081e8f5823f9a1f6e234d108", "heading": "Streamlit/Deploy", "level": 2, "text": "## Deploy  \n1. Start the vLLM server with a supported chat completion model, e.g.  \n```bash\nvllm serve Qwen/Qwen1.5-0.5B-Chat\n```  \n1. Use the script: <gh-file:examples/online_serving/streamlit_openai_chatbot_webserver.py>  \n1. Start the streamlit web UI and start to chat:  \n```bash\nstreamlit run streamlit_openai_chatbot_webserver.py\n\n# or specify the VLLM_API_BASE or VLLM_API_KEY\nVLLM_API_BASE=\"http://vllm-server-host:vllm-server-port/v1\" \\\nstreamlit run streamlit_openai_chatbot_webserver.py\n\n# start with debug mode to view more details\nstreamlit run streamlit_openai_chatbot_webserver.py --logger.level=debug\n```  \n![Chat with vLLM assistant in Streamlit](../../assets/deployment/streamlit-chat.png)", "file_path": "deployment/frameworks/streamlit.md"}
{"id": "afd38a07cd4c5f356b2a14597cefa802592c33d88ffcb2287ce3419734c33f74", "heading": "NVIDIA Triton", "level": 1, "text": "# NVIDIA Triton  \nThe [Triton Inference Server](https://github.com/triton-inference-server) hosts a tutorial demonstrating how to quickly deploy a simple [facebook/opt-125m](https://huggingface.co/facebook/opt-125m) model using vLLM. Please see [Deploying a vLLM model in Triton](https://github.com/triton-inference-server/tutorials/blob/main/Quick_Deploy/vLLM/README.md#deploying-a-vllm-model-in-triton) for more details.", "file_path": "deployment/frameworks/triton.md"}
{"id": "d2f4585925a2e48efb12980436f8c6d40e3bdb7e186e1bdbd9f020b41a7ff631", "heading": "KAITO", "level": 1, "text": "# KAITO  \n[KAITO](https://kaito-project.github.io/kaito/docs/) is a Kubernetes operator that supports deploying and serving LLMs with vLLM. It offers managing large models via container images with built-in OpenAI-compatible inference, auto-provisioning GPU nodes and curated model presets.  \nPlease refer to [quick start](https://kaito-project.github.io/kaito/docs/quick-start) for more details.", "file_path": "deployment/integrations/kaito.md"}
{"id": "5aeb37c78e37412eb4fa319b4410851c6a7c72175d447e31ee9970eff37e356f", "heading": "KServe", "level": 1, "text": "# KServe  \nvLLM can be deployed with [KServe](https://github.com/kserve/kserve) on Kubernetes for highly scalable distributed model serving.  \nPlease see [this guide](https://kserve.github.io/website/latest/modelserving/v1beta1/llm/huggingface/) for more details on using vLLM with KServe.", "file_path": "deployment/integrations/kserve.md"}
{"id": "82b03f5a6b22d0bf15726ba6b04cfd5d94a965a0ab484113d912d5342e29ea15", "heading": "KubeAI", "level": 1, "text": "# KubeAI  \n[KubeAI](https://github.com/substratusai/kubeai) is a Kubernetes operator that enables you to deploy and manage AI models on Kubernetes. It provides a simple and scalable way to deploy vLLM in production. Functionality such as scale-from-zero, load based autoscaling, model caching, and much more is provided out of the box with zero external dependencies.  \nPlease see the Installation Guides for environment specific instructions:  \n- [Any Kubernetes Cluster](https://www.kubeai.org/installation/any/)\n- [EKS](https://www.kubeai.org/installation/eks/)\n- [GKE](https://www.kubeai.org/installation/gke/)  \nOnce you have KubeAI installed, you can\n[configure text generation models](https://www.kubeai.org/how-to/configure-text-generation-models/)\nusing vLLM.", "file_path": "deployment/integrations/kubeai.md"}
{"id": "06f328b5ae13c63137f3dd3733c559775fccae70ddb7f94743e2fa559b7726e0", "heading": "KubeRay", "level": 1, "text": "# KubeRay  \n[KubeRay](https://github.com/ray-project/kuberay) provides a Kubernetes-native way to run vLLM workloads on Ray clusters.\nA Ray cluster can be declared in YAML, and the operator then handles pod scheduling, networking configuration, restarts, and blue-green deployments â€” all while preserving the familiar Kubernetes experience.", "file_path": "deployment/integrations/kuberay.md"}
{"id": "06f328b5ae13c63137f3dd3733c559775fccae70ddb7f94743e2fa559b7726e0", "heading": "KubeRay/Why KubeRay instead of manual scripts?", "level": 2, "text": "## Why KubeRay instead of manual scripts?  \n| Feature | Manual scripts | KubeRay |\n|---------|-----------------------------------------------------------|---------|\n| Cluster bootstrap | Manually SSH into every node and run a script | One command to create or update the whole cluster: `kubectl apply -f cluster.yaml` |\n| Autoscaling | Manual | Automatically patches CRDs for adjusting cluster size |\n| Upgrades | Tear down & re-create manually | Blue/green deployment updates supported |\n| Declarative config | Bash flags & environment variables | Git-ops-friendly YAML CRDs (RayCluster/RayService) |  \nUsing KubeRay reduces the operational burden and simplifies integration of Ray + vLLM with existing Kubernetes workflows (CI/CD, secrets, storage classes, etc.).", "file_path": "deployment/integrations/kuberay.md"}
{"id": "06f328b5ae13c63137f3dd3733c559775fccae70ddb7f94743e2fa559b7726e0", "heading": "KubeRay/Learn more", "level": 2, "text": "## Learn more  \n* [\"Serve a Large Language Model using Ray Serve LLM on Kubernetes\"](https://docs.ray.io/en/master/cluster/kubernetes/examples/rayserve-llm-example.html) - An end-to-end example of how to serve a model using vLLM, KubeRay, and Ray Serve.\n* [KubeRay documentation](https://docs.ray.io/en/latest/cluster/kubernetes/index.html)", "file_path": "deployment/integrations/kuberay.md"}
{"id": "4d2c18ab73167f6796a8ddd9899c225ae07de1f0c821222381d8f056fd3d7281", "heading": "Llama Stack", "level": 1, "text": "# Llama Stack  \nvLLM is also available via [Llama Stack](https://github.com/llamastack/llama-stack).  \nTo install Llama Stack, run  \n```bash\npip install llama-stack -q\n```", "file_path": "deployment/integrations/llamastack.md"}
{"id": "4d2c18ab73167f6796a8ddd9899c225ae07de1f0c821222381d8f056fd3d7281", "heading": "Llama Stack/Inference using OpenAI-Compatible API", "level": 2, "text": "## Inference using OpenAI-Compatible API  \nThen start the Llama Stack server and configure it to point to your vLLM server with the following settings:  \n```yaml\ninference:\n- provider_id: vllm0\nprovider_type: remote::vllm\nconfig:\nurl: http://127.0.0.1:8000\n```  \nPlease refer to [this guide](https://llama-stack.readthedocs.io/en/latest/providers/inference/remote_vllm.html) for more details on this remote vLLM provider.", "file_path": "deployment/integrations/llamastack.md"}
{"id": "4d2c18ab73167f6796a8ddd9899c225ae07de1f0c821222381d8f056fd3d7281", "heading": "Llama Stack/Inference using Embedded vLLM", "level": 2, "text": "## Inference using Embedded vLLM  \nAn [inline provider](https://github.com/llamastack/llama-stack/tree/main/llama_stack/providers/inline/inference)\nis also available. This is a sample of configuration using that method:  \n```yaml\ninference:\n- provider_type: vllm\nconfig:\nmodel: Llama3.1-8B-Instruct\ntensor_parallel_size: 4\n```", "file_path": "deployment/integrations/llamastack.md"}
{"id": "f7b55df726a4bb4994944e704e44a08501af0e1a858eea7ad1fbd29f4e3680bc", "heading": "llmaz", "level": 1, "text": "# llmaz  \n[llmaz](https://github.com/InftyAI/llmaz) is an easy-to-use and advanced inference platform for large language models on Kubernetes, aimed for production use. It uses vLLM as the default model serving backend.  \nPlease refer to the [Quick Start](https://github.com/InftyAI/llmaz?tab=readme-ov-file#quick-start) for more details.", "file_path": "deployment/integrations/llmaz.md"}
{"id": "56cab6f23d8353e452544d785e7155dbfe2706fc9c55d37e07f5aaac12f0879c", "heading": "Production stack", "level": 1, "text": "# Production stack  \nDeploying vLLM on Kubernetes is a scalable and efficient way to serve machine learning models. This guide walks you through deploying vLLM using the [vLLM production stack](https://github.com/vllm-project/production-stack). Born out of a Berkeley-UChicago collaboration, [vLLM production stack](https://github.com/vllm-project/production-stack) is an officially released, production-optimized codebase under the [vLLM project](https://github.com/vllm-project), designed for LLM deployment with:  \n* **Upstream vLLM compatibility** â€“ It wraps around upstream vLLM without modifying its code.\n* **Ease of use** â€“ Simplified deployment via Helm charts and observability through Grafana dashboards.\n* **High performance** â€“ Optimized for LLM workloads with features like multi-model support, model-aware and prefix-aware routing, fast vLLM bootstrapping, and KV cache offloading with [LMCache](https://github.com/LMCache/LMCache), among others.", "file_path": "deployment/integrations/production-stack.md"}
{"id": "56cab6f23d8353e452544d785e7155dbfe2706fc9c55d37e07f5aaac12f0879c", "heading": "Production stack", "level": 1, "text": "If you are new to Kubernetes, don't worry: in the vLLM production stack [repo](https://github.com/vllm-project/production-stack), we provide a step-by-step [guide](https://github.com/vllm-project/production-stack/blob/main/tutorials/00-install-kubernetes-env.md) and a [short video](https://www.youtube.com/watch?v=EsTJbQtzj0g) to set up everything and get started in **4 minutes**!", "file_path": "deployment/integrations/production-stack.md"}
{"id": "56cab6f23d8353e452544d785e7155dbfe2706fc9c55d37e07f5aaac12f0879c", "heading": "Production stack/Pre-requisite", "level": 2, "text": "## Pre-requisite  \nEnsure that you have a running Kubernetes environment with GPU (you can follow [this tutorial](https://github.com/vllm-project/production-stack/blob/main/tutorials/00-install-kubernetes-env.md) to install a Kubernetes environment on a bare-medal GPU machine).", "file_path": "deployment/integrations/production-stack.md"}
{"id": "56cab6f23d8353e452544d785e7155dbfe2706fc9c55d37e07f5aaac12f0879c", "heading": "Production stack/Deployment using vLLM production stack", "level": 2, "text": "## Deployment using vLLM production stack  \nThe standard vLLM production stack is installed using a Helm chart. You can run this [bash script](https://github.com/vllm-project/production-stack/blob/main/utils/install-helm.sh) to install Helm on your GPU server.  \nTo install the vLLM production stack, run the following commands on your desktop:  \n```bash\nsudo helm repo add vllm https://vllm-project.github.io/production-stack\nsudo helm install vllm vllm/vllm-stack -f tutorials/assets/values-01-minimal-example.yaml\n```  \nThis will instantiate a vLLM-production-stack-based deployment named `vllm` that runs a small LLM (Facebook opt-125M model).", "file_path": "deployment/integrations/production-stack.md"}
{"id": "56cab6f23d8353e452544d785e7155dbfe2706fc9c55d37e07f5aaac12f0879c", "heading": "Production stack/Deployment using vLLM production stack/Validate Installation", "level": 3, "text": "### Validate Installation  \nMonitor the deployment status using:  \n```bash\nsudo kubectl get pods\n```  \nAnd you will see that pods for the `vllm` deployment will transit to `Running` state.  \n```text\nNAME                                           READY   STATUS    RESTARTS   AGE\nvllm-deployment-router-859d8fb668-2x2b7        1/1     Running   0          2m38s\nvllm-opt125m-deployment-vllm-84dfc9bd7-vb9bs   1/1     Running   0          2m38s\n```  \n!!! note\nIt may take some time for the containers to download the Docker images and LLM weights.", "file_path": "deployment/integrations/production-stack.md"}
{"id": "56cab6f23d8353e452544d785e7155dbfe2706fc9c55d37e07f5aaac12f0879c", "heading": "Production stack/Deployment using vLLM production stack/Send a Query to the Stack", "level": 3, "text": "### Send a Query to the Stack  \nForward the `vllm-router-service` port to the host machine:  \n```bash\nsudo kubectl port-forward svc/vllm-router-service 30080:80\n```  \nAnd then you can send out a query to the OpenAI-compatible API to check the available models:  \n```bash\ncurl -o- http://localhost:30080/v1/models\n```  \n??? console \"Output\"  \n```json\n{\n\"object\": \"list\",\n\"data\": [\n{\n\"id\": \"facebook/opt-125m\",\n\"object\": \"model\",\n\"created\": 1737428424,\n\"owned_by\": \"vllm\",\n\"root\": null\n}\n]\n}\n```  \nTo send an actual chatting request, you can issue a curl request to the OpenAI `/completion` endpoint:  \n```bash\ncurl -X POST http://localhost:30080/v1/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"facebook/opt-125m\",\n\"prompt\": \"Once upon a time,\",\n\"max_tokens\": 10\n}'\n```  \n??? console \"Output\"  \n```json\n{\n\"id\": \"completion-id\",\n\"object\": \"text_completion\",\n\"created\": 1737428424,\n\"model\": \"facebook/opt-125m\",\n\"choices\": [\n{\n\"text\": \" there was a brave knight who...\",\n\"index\": 0,", "file_path": "deployment/integrations/production-stack.md"}
{"id": "56cab6f23d8353e452544d785e7155dbfe2706fc9c55d37e07f5aaac12f0879c", "heading": "Production stack/Deployment using vLLM production stack/Send a Query to the Stack", "level": 3, "text": "{\n\"text\": \" there was a brave knight who...\",\n\"index\": 0,\n\"finish_reason\": \"length\"\n}\n]\n}\n```", "file_path": "deployment/integrations/production-stack.md"}
{"id": "56cab6f23d8353e452544d785e7155dbfe2706fc9c55d37e07f5aaac12f0879c", "heading": "Production stack/Deployment using vLLM production stack/Uninstall", "level": 3, "text": "### Uninstall  \nTo remove the deployment, run:  \n```bash\nsudo helm uninstall vllm\n```  \n---", "file_path": "deployment/integrations/production-stack.md"}
{"id": "56cab6f23d8353e452544d785e7155dbfe2706fc9c55d37e07f5aaac12f0879c", "heading": "Production stack/Deployment using vLLM production stack/(Advanced) Configuring vLLM production stack", "level": 3, "text": "### (Advanced) Configuring vLLM production stack  \nThe core vLLM production stack configuration is managed with YAML. Here is the example configuration used in the installation above:  \n??? code \"Yaml\"  \n```yaml\nservingEngineSpec:\nruntimeClassName: \"\"\nmodelSpec:\n- name: \"opt125m\"\nrepository: \"vllm/vllm-openai\"\ntag: \"latest\"\nmodelURL: \"facebook/opt-125m\"\n\nreplicaCount: 1\n\nrequestCPU: 6\nrequestMemory: \"16Gi\"\nrequestGPU: 1", "file_path": "deployment/integrations/production-stack.md"}
{"id": "56cab6f23d8353e452544d785e7155dbfe2706fc9c55d37e07f5aaac12f0879c", "heading": "Production stack/Deployment using vLLM production stack/(Advanced) Configuring vLLM production stack", "level": 3, "text": "pvcStorage: \"10Gi\"\n```  \nIn this YAML configuration:  \n* **`modelSpec`** includes:\n* `name`: A nickname that you prefer to call the model.\n* `repository`: Docker repository of vLLM.\n* `tag`: Docker image tag.\n* `modelURL`: The LLM model that you want to use.\n* **`replicaCount`**: Number of replicas.\n* **`requestCPU` and `requestMemory`**: Specifies the CPU and memory resource requests for the pod.\n* **`requestGPU`**: Specifies the number of GPUs required.\n* **`pvcStorage`**: Allocates persistent storage for the model.  \n!!! note\nIf you intend to set up two pods, please refer to this [YAML file](https://github.com/vllm-project/production-stack/blob/main/tutorials/assets/values-01-2pods-minimal-example.yaml).  \n!!! tip", "file_path": "deployment/integrations/production-stack.md"}
{"id": "56cab6f23d8353e452544d785e7155dbfe2706fc9c55d37e07f5aaac12f0879c", "heading": "Production stack/Deployment using vLLM production stack/(Advanced) Configuring vLLM production stack", "level": 3, "text": "!!! tip\nvLLM production stack offers many more features (*e.g.* CPU offloading and a wide range of routing algorithms). Please check out these [examples and tutorials](https://github.com/vllm-project/production-stack/tree/main/tutorials) and our [repo](https://github.com/vllm-project/production-stack) for more details!", "file_path": "deployment/integrations/production-stack.md"}
{"id": "a8ebab7b8f4ea40d32024b42cee83790af502a30b7d9a01fec461f1e15966212", "heading": "Using Kubernetes", "level": 1, "text": "# Using Kubernetes  \nDeploying vLLM on Kubernetes is a scalable and efficient way to serve machine learning models. This guide walks you through deploying vLLM using native Kubernetes.  \n- [Deployment with CPUs](#deployment-with-cpus)\n- [Deployment with GPUs](#deployment-with-gpus)\n- [Troubleshooting](#troubleshooting)\n- [Startup Probe or Readiness Probe Failure, container log contains \"KeyboardInterrupt: terminated\"](#startup-probe-or-readiness-probe-failure-container-log-contains-keyboardinterrupt-terminated)\n- [Conclusion](#conclusion)  \nAlternatively, you can deploy vLLM to Kubernetes using any of the following:  \n- [Helm](frameworks/helm.md)\n- [InftyAI/llmaz](integrations/llmaz.md)\n- [KAITO](integrations/kaito.md)\n- [KServe](integrations/kserve.md)\n- [KubeRay](integrations/kuberay.md)\n- [kubernetes-sigs/lws](frameworks/lws.md)\n- [meta-llama/llama-stack](integrations/llamastack.md)\n- [substratusai/kubeai](integrations/kubeai.md)\n- [vllm-project/aibrix](https://github.com/vllm-project/aibrix)", "file_path": "deployment/k8s.md"}
{"id": "a8ebab7b8f4ea40d32024b42cee83790af502a30b7d9a01fec461f1e15966212", "heading": "Using Kubernetes", "level": 1, "text": "- [vllm-project/aibrix](https://github.com/vllm-project/aibrix)\n- [vllm-project/production-stack](integrations/production-stack.md)", "file_path": "deployment/k8s.md"}
{"id": "a8ebab7b8f4ea40d32024b42cee83790af502a30b7d9a01fec461f1e15966212", "heading": "Using Kubernetes/Deployment with CPUs", "level": 2, "text": "## Deployment with CPUs  \n!!! note\nThe use of CPUs here is for demonstration and testing purposes only and its performance will not be on par with GPUs.  \nFirst, create a Kubernetes PVC and Secret for downloading and storing Hugging Face model:  \n??? console \"Config\"  \n```bash\ncat <<EOF |kubectl apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: vllm-models\nspec:\naccessModes:\n- ReadWriteOnce\nvolumeMode: Filesystem\nresources:\nrequests:\nstorage: 50Gi\n---\napiVersion: v1\nkind: Secret\nmetadata:\nname: hf-token-secret\ntype: Opaque\ndata:\ntoken: $(HF_TOKEN)\nEOF\n```  \nNext, start the vLLM server as a Kubernetes Deployment and Service:  \n??? console \"Config\"  \n```bash\ncat <<EOF |kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: vllm-server\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp.kubernetes.io/name: vllm\ntemplate:\nmetadata:\nlabels:\napp.kubernetes.io/name: vllm\nspec:\ncontainers:\n- name: vllm\nimage: vllm/vllm-openai:latest\ncommand: [\"/bin/sh\", \"-c\"]\nargs: [", "file_path": "deployment/k8s.md"}
{"id": "a8ebab7b8f4ea40d32024b42cee83790af502a30b7d9a01fec461f1e15966212", "heading": "Using Kubernetes/Deployment with CPUs", "level": 2, "text": "command: [\"/bin/sh\", \"-c\"]\nargs: [\n\"vllm serve meta-llama/Llama-3.2-1B-Instruct\"\n]\nenv:\n- name: HF_TOKEN\nvalueFrom:\nsecretKeyRef:\nname: hf-token-secret\nkey: token\nports:\n- containerPort: 8000\nvolumeMounts:\n- name: llama-storage\nmountPath: /root/.cache/huggingface\nvolumes:\n- name: llama-storage\npersistentVolumeClaim:\nclaimName: vllm-models\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: vllm-server\nspec:\nselector:\napp.kubernetes.io/name: vllm\nports:\n- protocol: TCP\nport: 8000\ntargetPort: 8000\ntype: ClusterIP\nEOF\n```  \nWe can verify that the vLLM server has started successfully via the logs (this might take a couple of minutes to download the model):  \n```bash\nkubectl logs -l app.kubernetes.io/name=vllm\n...\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n```", "file_path": "deployment/k8s.md"}
{"id": "a8ebab7b8f4ea40d32024b42cee83790af502a30b7d9a01fec461f1e15966212", "heading": "Using Kubernetes/Deployment with GPUs", "level": 2, "text": "## Deployment with GPUs  \n**Pre-requisite**: Ensure that you have a running [Kubernetes cluster with GPUs](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/).  \n1. Create a PVC, Secret and Deployment for vLLM  \nPVC is used to store the model cache and it is optional, you can use hostPath or other storage options  \n<details>\n<summary>Yaml</summary>  \n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: mistral-7b\nnamespace: default\nspec:\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 50Gi\nstorageClassName: default\nvolumeMode: Filesystem\n```  \n</details>  \nSecret is optional and only required for accessing gated models, you can skip this step if you are not using gated models  \n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\nname: hf-token-secret\nnamespace: default\ntype: Opaque\nstringData:\ntoken: \"REPLACE_WITH_TOKEN\"\n```  \nNext to create the deployment file for vLLM to run the model server. The following example deploys the `Mistral-7B-Instruct-v0.3` model.", "file_path": "deployment/k8s.md"}
{"id": "a8ebab7b8f4ea40d32024b42cee83790af502a30b7d9a01fec461f1e15966212", "heading": "Using Kubernetes/Deployment with GPUs", "level": 2, "text": "Here are two examples for using NVIDIA GPU and AMD GPU.  \nNVIDIA GPU:  \n<details>\n<summary>Yaml</summary>  \n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: mistral-7b\nnamespace: default\nlabels:\napp: mistral-7b\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: mistral-7b\ntemplate:\nmetadata:\nlabels:\napp: mistral-7b\nspec:\nvolumes:\n- name: cache-volume\npersistentVolumeClaim:\nclaimName: mistral-7b\n# vLLM needs to access the host's shared memory for tensor parallel inference.\n- name: shm\nemptyDir:\nmedium: Memory\nsizeLimit: \"2Gi\"\ncontainers:\n- name: mistral-7b\nimage: vllm/vllm-openai:latest\ncommand: [\"/bin/sh\", \"-c\"]\nargs: [\n\"vllm serve mistralai/Mistral-7B-Instruct-v0.3 --trust-remote-code --enable-chunked-prefill --max_num_batched_tokens 1024\"\n]\nenv:\n- name: HF_TOKEN\nvalueFrom:\nsecretKeyRef:\nname: hf-token-secret\nkey: token\nports:\n- containerPort: 8000\nresources:\nlimits:\ncpu: \"10\"\nmemory: 20G\nnvidia.com/gpu: \"1\"\nrequests:\ncpu: \"2\"\nmemory: 6G\nnvidia.com/gpu: \"1\"\nvolumeMounts:", "file_path": "deployment/k8s.md"}
{"id": "a8ebab7b8f4ea40d32024b42cee83790af502a30b7d9a01fec461f1e15966212", "heading": "Using Kubernetes/Deployment with GPUs", "level": 2, "text": "requests:\ncpu: \"2\"\nmemory: 6G\nnvidia.com/gpu: \"1\"\nvolumeMounts:\n- mountPath: /root/.cache/huggingface\nname: cache-volume\n- name: shm\nmountPath: /dev/shm\nlivenessProbe:\nhttpGet:\npath: /health\nport: 8000\ninitialDelaySeconds: 60\nperiodSeconds: 10\nreadinessProbe:\nhttpGet:\npath: /health\nport: 8000\ninitialDelaySeconds: 60\nperiodSeconds: 5\n```  \n</details>  \nAMD GPU:  \nYou can refer to the `deployment.yaml` below if using AMD ROCm GPU like MI300X.  \n<details>\n<summary>Yaml</summary>  \n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: mistral-7b\nnamespace: default\nlabels:\napp: mistral-7b\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: mistral-7b\ntemplate:\nmetadata:\nlabels:\napp: mistral-7b\nspec:\nvolumes:\n# PVC\n- name: cache-volume\npersistentVolumeClaim:\nclaimName: mistral-7b\n# vLLM needs to access the host's shared memory for tensor parallel inference.\n- name: shm\nemptyDir:\nmedium: Memory\nsizeLimit: \"8Gi\"\nhostNetwork: true\nhostIPC: true\ncontainers:\n- name: mistral-7b", "file_path": "deployment/k8s.md"}
{"id": "a8ebab7b8f4ea40d32024b42cee83790af502a30b7d9a01fec461f1e15966212", "heading": "Using Kubernetes/Deployment with GPUs", "level": 2, "text": "hostNetwork: true\nhostIPC: true\ncontainers:\n- name: mistral-7b\nimage: rocm/vllm:rocm6.2_mi300_ubuntu20.04_py3.9_vllm_0.6.4\nsecurityContext:\nseccompProfile:\ntype: Unconfined\nrunAsGroup: 44\ncapabilities:\nadd:\n- SYS_PTRACE\ncommand: [\"/bin/sh\", \"-c\"]\nargs: [\n\"vllm serve mistralai/Mistral-7B-v0.3 --port 8000 --trust-remote-code --enable-chunked-prefill --max_num_batched_tokens 1024\"\n]\nenv:\n- name: HF_TOKEN\nvalueFrom:\nsecretKeyRef:\nname: hf-token-secret\nkey: token\nports:\n- containerPort: 8000\nresources:\nlimits:\ncpu: \"10\"\nmemory: 20G\namd.com/gpu: \"1\"\nrequests:\ncpu: \"6\"\nmemory: 6G\namd.com/gpu: \"1\"\nvolumeMounts:\n- name: cache-volume\nmountPath: /root/.cache/huggingface\n- name: shm\nmountPath: /dev/shm\n```  \n</details>  \nYou can get the full example with steps and sample yaml files from <https://github.com/ROCm/k8s-device-plugin/tree/master/example/vllm-serve>.  \n2. Create a Kubernetes Service for vLLM  \nNext, create a Kubernetes Service file to expose the `mistral-7b` deployment:  \n<details>\n<summary>Yaml</summary>", "file_path": "deployment/k8s.md"}
{"id": "a8ebab7b8f4ea40d32024b42cee83790af502a30b7d9a01fec461f1e15966212", "heading": "Using Kubernetes/Deployment with GPUs", "level": 2, "text": "<details>\n<summary>Yaml</summary>  \n```yaml\napiVersion: v1\nkind: Service\nmetadata:\nname: mistral-7b\nnamespace: default\nspec:\nports:\n- name: http-mistral-7b\nport: 80\nprotocol: TCP\ntargetPort: 8000\n# The label selector should match the deployment labels & it is useful for prefix caching feature\nselector:\napp: mistral-7b\nsessionAffinity: None\ntype: ClusterIP\n```  \n</details>  \n3. Deploy and Test  \nApply the deployment and service configurations using `kubectl apply -f <filename>`:  \n```bash\nkubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\n```  \nTo test the deployment, run the following `curl` command:  \n```bash\ncurl http://mistral-7b.default.svc.cluster.local/v1/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n\"prompt\": \"San Francisco is a\",\n\"max_tokens\": 7,\n\"temperature\": 0\n}'\n```  \nIf the service is correctly deployed, you should receive a response from the vLLM model.", "file_path": "deployment/k8s.md"}
{"id": "a8ebab7b8f4ea40d32024b42cee83790af502a30b7d9a01fec461f1e15966212", "heading": "Using Kubernetes/Troubleshooting/Startup Probe or Readiness Probe Failure, container log contains \"KeyboardInterrupt: terminated\"", "level": 3, "text": "## Troubleshooting  \n### Startup Probe or Readiness Probe Failure, container log contains \"KeyboardInterrupt: terminated\"  \nIf the startup or readiness probe failureThreshold is too low for the time needed to start up the server, Kubernetes scheduler will kill the container. A couple of indications that this has happened:  \n1. container log contains \"KeyboardInterrupt: terminated\"\n2. `kubectl get events` shows message `Container $NAME failed startup probe, will be restarted`  \nTo mitigate, increase the failureThreshold to allow more time for the model server to start serving. You can identify an ideal failureThreshold by removing the probes from the manifest and measuring how much time it takes for the model server to show it's ready to serve.", "file_path": "deployment/k8s.md"}
{"id": "a8ebab7b8f4ea40d32024b42cee83790af502a30b7d9a01fec461f1e15966212", "heading": "Using Kubernetes/Conclusion", "level": 2, "text": "## Conclusion  \nDeploying vLLM with Kubernetes allows for efficient scaling and management of ML models leveraging GPU resources. By following the steps outlined above, you should be able to set up and test a vLLM deployment within your Kubernetes cluster. If you encounter any issues or have suggestions, please feel free to contribute to the documentation.", "file_path": "deployment/k8s.md"}
{"id": "2c51e4fccc8a69434ee0a8e4f72f5dde593a8a5cd28b2a992fe6937d5d54450f", "heading": "Using Nginx", "level": 1, "text": "# Using Nginx  \nThis document shows how to launch multiple vLLM serving containers and use Nginx to act as a load balancer between the servers.  \n[](){ #nginxloadbalancer-nginx-build }", "file_path": "deployment/nginx.md"}
{"id": "2c51e4fccc8a69434ee0a8e4f72f5dde593a8a5cd28b2a992fe6937d5d54450f", "heading": "Using Nginx/Build Nginx Container", "level": 2, "text": "## Build Nginx Container  \nThis guide assumes that you have just cloned the vLLM project and you're currently in the vllm root directory.  \n```bash\nexport vllm_root=`pwd`\n```  \nCreate a file named `Dockerfile.nginx`:  \n```dockerfile\nFROM nginx:latest\nRUN rm /etc/nginx/conf.d/default.conf\nEXPOSE 80\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n```  \nBuild the container:  \n```bash\ndocker build . -f Dockerfile.nginx --tag nginx-lb\n```  \n[](){ #nginxloadbalancer-nginx-conf }", "file_path": "deployment/nginx.md"}
{"id": "2c51e4fccc8a69434ee0a8e4f72f5dde593a8a5cd28b2a992fe6937d5d54450f", "heading": "Using Nginx/Create Simple Nginx Config file", "level": 2, "text": "## Create Simple Nginx Config file  \nCreate a file named `nginx_conf/nginx.conf`. Note that you can add as many servers as you'd like. In the below example we'll start with two. To add more, add another `server vllmN:8000 max_fails=3 fail_timeout=10000s;` entry to `upstream backend`.  \n??? console \"Config\"  \n```console\nupstream backend {\nleast_conn;\nserver vllm0:8000 max_fails=3 fail_timeout=10000s;\nserver vllm1:8000 max_fails=3 fail_timeout=10000s;\n}\nserver {\nlisten 80;\nlocation / {\nproxy_pass http://backend;\nproxy_set_header Host $host;\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_set_header X-Forwarded-Proto $scheme;\n}\n}\n```  \n[](){ #nginxloadbalancer-nginx-vllm-container }", "file_path": "deployment/nginx.md"}
{"id": "2c51e4fccc8a69434ee0a8e4f72f5dde593a8a5cd28b2a992fe6937d5d54450f", "heading": "Using Nginx/Build vLLM Container", "level": 2, "text": "## Build vLLM Container  \n```bash\ncd $vllm_root\ndocker build -f docker/Dockerfile . --tag vllm\n```  \nIf you are behind proxy, you can pass the proxy settings to the docker build command as shown below:  \n```bash\ncd $vllm_root\ndocker build \\\n-f docker/Dockerfile . \\\n--tag vllm \\\n--build-arg http_proxy=$http_proxy \\\n--build-arg https_proxy=$https_proxy\n```  \n[](){ #nginxloadbalancer-nginx-docker-network }", "file_path": "deployment/nginx.md"}
{"id": "2c51e4fccc8a69434ee0a8e4f72f5dde593a8a5cd28b2a992fe6937d5d54450f", "heading": "Using Nginx/Create Docker Network", "level": 2, "text": "## Create Docker Network  \n```bash\ndocker network create vllm_nginx\n```  \n[](){ #nginxloadbalancer-nginx-launch-container }", "file_path": "deployment/nginx.md"}
{"id": "2c51e4fccc8a69434ee0a8e4f72f5dde593a8a5cd28b2a992fe6937d5d54450f", "heading": "Using Nginx/Launch vLLM Containers", "level": 2, "text": "## Launch vLLM Containers  \nNotes:  \n- If you have your HuggingFace models cached somewhere else, update `hf_cache_dir` below.\n- If you don't have an existing HuggingFace cache you will want to start `vllm0` and wait for the model to complete downloading and the server to be ready. This will ensure that `vllm1` can leverage the model you just downloaded and it won't have to be downloaded again.\n- The below example assumes GPU backend used. If you are using CPU backend, remove `--gpus device=ID`, add `VLLM_CPU_KVCACHE_SPACE` and `VLLM_CPU_OMP_THREADS_BIND` environment variables to the docker run command.\n- Adjust the model name that you want to use in your vLLM servers if you don't want to use `Llama-2-7b-chat-hf`.  \n??? console \"Commands\"  \n```console\nmkdir -p ~/.cache/huggingface/hub/\nhf_cache_dir=~/.cache/huggingface/\ndocker run \\\n-itd \\\n--ipc host \\\n--network vllm_nginx \\\n--gpus device=0 \\\n--shm-size=10.24gb \\\n-v $hf_cache_dir:/root/.cache/huggingface/ \\\n-p 8081:8000 \\\n--name vllm0 vllm \\", "file_path": "deployment/nginx.md"}
{"id": "2c51e4fccc8a69434ee0a8e4f72f5dde593a8a5cd28b2a992fe6937d5d54450f", "heading": "Using Nginx/Launch vLLM Containers", "level": 2, "text": "-p 8081:8000 \\\n--name vllm0 vllm \\\n--model meta-llama/Llama-2-7b-chat-hf\ndocker run \\\n-itd \\\n--ipc host \\\n--network vllm_nginx \\\n--gpus device=1 \\\n--shm-size=10.24gb \\\n-v $hf_cache_dir:/root/.cache/huggingface/ \\\n-p 8082:8000 \\\n--name vllm1 vllm \\\n--model meta-llama/Llama-2-7b-chat-hf\n```  \n!!! note\nIf you are behind proxy, you can pass the proxy settings to the docker run command via `-e http_proxy=$http_proxy -e https_proxy=$https_proxy`.  \n[](){ #nginxloadbalancer-nginx-launch-nginx }", "file_path": "deployment/nginx.md"}
{"id": "2c51e4fccc8a69434ee0a8e4f72f5dde593a8a5cd28b2a992fe6937d5d54450f", "heading": "Using Nginx/Launch Nginx", "level": 2, "text": "## Launch Nginx  \n```bash\ndocker run \\\n-itd \\\n-p 8000:80 \\\n--network vllm_nginx \\\n-v ./nginx_conf/:/etc/nginx/conf.d/ \\\n--name nginx-lb nginx-lb:latest\n```  \n[](){ #nginxloadbalancer-nginx-verify-nginx }", "file_path": "deployment/nginx.md"}
{"id": "2c51e4fccc8a69434ee0a8e4f72f5dde593a8a5cd28b2a992fe6937d5d54450f", "heading": "Using Nginx/Verify That vLLM Servers Are Ready", "level": 2, "text": "## Verify That vLLM Servers Are Ready  \n```bash\ndocker logs vllm0 | grep Uvicorn\ndocker logs vllm1 | grep Uvicorn\n```  \nBoth outputs should look like this:  \n```console\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n```", "file_path": "deployment/nginx.md"}
{"id": "c65a98693df8cbef13ec7808030e29f2e90fccb2d8e9b8860ac17984714c25d4", "heading": "Architecture Overview", "level": 1, "text": "# Architecture Overview  \nThis document provides an overview of the vLLM architecture.  \n[TOC]", "file_path": "design/arch_overview.md"}
{"id": "c65a98693df8cbef13ec7808030e29f2e90fccb2d8e9b8860ac17984714c25d4", "heading": "Architecture Overview/Entrypoints", "level": 2, "text": "## Entrypoints  \nvLLM provides a number of entrypoints for interacting with the system. The\nfollowing diagram shows the relationship between them.  \n![Entrypoints Diagram](../assets/design/arch_overview/entrypoints.excalidraw.png)", "file_path": "design/arch_overview.md"}
{"id": "c65a98693df8cbef13ec7808030e29f2e90fccb2d8e9b8860ac17984714c25d4", "heading": "Architecture Overview/Entrypoints/LLM Class", "level": 3, "text": "### LLM Class  \nThe LLM class provides the primary Python interface for doing offline inference,\nwhich is interacting with a model without using a separate model inference\nserver.  \nHere is a sample of `LLM` class usage:  \n??? code  \n```python\nfrom vllm import LLM, SamplingParams\n\n# Define a list of input prompts\nprompts = [\n\"Hello, my name is\",\n\"The capital of France is\",\n\"The largest ocean is\",\n]\n\n# Define sampling parameters\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\n# Initialize the LLM engine with the OPT-125M model\nllm = LLM(model=\"facebook/opt-125m\")\n\n# Generate outputs for the input prompts\noutputs = llm.generate(prompts, sampling_params)", "file_path": "design/arch_overview.md"}
{"id": "c65a98693df8cbef13ec7808030e29f2e90fccb2d8e9b8860ac17984714c25d4", "heading": "Architecture Overview/Entrypoints/LLM Class", "level": 3, "text": "# Print the generated outputs\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```  \nMore API details can be found in the [Offline Inference](#offline-inference-api) section of the API docs.  \nThe code for the `LLM` class can be found in <gh-file:vllm/entrypoints/llm.py>.", "file_path": "design/arch_overview.md"}
{"id": "c65a98693df8cbef13ec7808030e29f2e90fccb2d8e9b8860ac17984714c25d4", "heading": "Architecture Overview/Entrypoints/OpenAI-Compatible API Server", "level": 3, "text": "### OpenAI-Compatible API Server  \nThe second primary interface to vLLM is via its OpenAI-compatible API server.\nThis server can be started using the `vllm serve` command.  \n```bash\nvllm serve <model>\n```  \nThe code for the `vllm` CLI can be found in <gh-file:vllm/entrypoints/cli/main.py>.  \nSometimes you may see the API server entrypoint used directly instead of via the\n`vllm` CLI command. For example:  \n```bash\npython -m vllm.entrypoints.openai.api_server --model <model>\n```  \n!!! warning  \n`python -m vllm.entrypoints.openai.api_server` is deprecated\nand may become unsupported in a future release.  \nThat code can be found in <gh-file:vllm/entrypoints/openai/api_server.py>.  \nMore details on the API server can be found in the [OpenAI-Compatible Server](../serving/openai_compatible_server.md) document.", "file_path": "design/arch_overview.md"}
{"id": "c65a98693df8cbef13ec7808030e29f2e90fccb2d8e9b8860ac17984714c25d4", "heading": "Architecture Overview/LLM Engine", "level": 2, "text": "## LLM Engine  \nThe `LLMEngine` and `AsyncLLMEngine` classes are central to the functioning of\nthe vLLM system, handling model inference and asynchronous request processing.  \n![LLMEngine Diagram](../assets/design/arch_overview/llm_engine.excalidraw.png)", "file_path": "design/arch_overview.md"}
{"id": "c65a98693df8cbef13ec7808030e29f2e90fccb2d8e9b8860ac17984714c25d4", "heading": "Architecture Overview/LLM Engine/LLMEngine", "level": 3, "text": "### LLMEngine  \nThe `LLMEngine` class is the core component of the vLLM engine. It is\nresponsible for receiving requests from clients and generating outputs from the\nmodel. The `LLMEngine` includes input processing, model execution (possibly\ndistributed across multiple hosts and/or GPUs), scheduling, and output\nprocessing.  \n- **Input Processing**: Handles tokenization of input text using the specified\ntokenizer.\n- **Scheduling**: Chooses which requests are processed in each step.\n- **Model Execution**: Manages the execution of the language model, including\ndistributed execution across multiple GPUs.\n- **Output Processing**: Processes the outputs generated by the model, decoding the\ntoken IDs from a language model into human-readable text.  \nThe code for `LLMEngine` can be found in <gh-file:vllm/engine/llm_engine.py>.", "file_path": "design/arch_overview.md"}
{"id": "c65a98693df8cbef13ec7808030e29f2e90fccb2d8e9b8860ac17984714c25d4", "heading": "Architecture Overview/LLM Engine/AsyncLLMEngine", "level": 3, "text": "### AsyncLLMEngine  \nThe `AsyncLLMEngine` class is an asynchronous wrapper for the `LLMEngine` class.\nIt uses `asyncio` to create a background loop that continuously processes\nincoming requests. The `AsyncLLMEngine` is designed for online serving, where it\ncan handle multiple concurrent requests and stream outputs to clients.  \nThe OpenAI-compatible API server uses the `AsyncLLMEngine`. There is also a demo\nAPI server that serves as a simpler example in <gh-file:vllm/entrypoints/api_server.py>.  \nThe code for `AsyncLLMEngine` can be found in <gh-file:vllm/engine/async_llm_engine.py>.", "file_path": "design/arch_overview.md"}
{"id": "c65a98693df8cbef13ec7808030e29f2e90fccb2d8e9b8860ac17984714c25d4", "heading": "Architecture Overview/Worker", "level": 2, "text": "## Worker  \nA worker is a process that runs the model inference. vLLM follows the common\npractice of using one process to control one accelerator device, such as GPUs.\nFor example, if we use tensor parallelism of size 2 and pipeline parallelism of\nsize 2, we will have 4 workers in total. Workers are identified by their\n`rank` and `local_rank`. `rank` is used for global orchestration, while\n`local_rank` is mainly used for assigning the accelerator device and accessing\nlocal resources such as the file system and shared memory.", "file_path": "design/arch_overview.md"}
{"id": "c65a98693df8cbef13ec7808030e29f2e90fccb2d8e9b8860ac17984714c25d4", "heading": "Architecture Overview/Model Runner", "level": 2, "text": "## Model Runner  \nEvery worker has one model runner object, responsible for loading and running\nthe model. Much of the model execution logic resides here, such as preparing\ninput tensors and capturing cudagraphs.", "file_path": "design/arch_overview.md"}
{"id": "c65a98693df8cbef13ec7808030e29f2e90fccb2d8e9b8860ac17984714c25d4", "heading": "Architecture Overview/Model", "level": 2, "text": "## Model  \nEvery model runner object has one model object, which is the actual\n`torch.nn.Module` instance. See [huggingface_integration](huggingface_integration.md) for how various\nconfigurations affect the class we ultimately get.", "file_path": "design/arch_overview.md"}
{"id": "c65a98693df8cbef13ec7808030e29f2e90fccb2d8e9b8860ac17984714c25d4", "heading": "Architecture Overview/Class Hierarchy", "level": 2, "text": "## Class Hierarchy  \nThe following figure shows the class hierarchy of vLLM:  \n> <figure markdown=\"span\">\n>   ![](../assets/design/hierarchy.png){ align=\"center\" alt=\"query\" width=\"100%\" }\n> </figure>  \nThere are several important design choices behind this class hierarchy:  \n1\\. **Extensibility**: All classes in the hierarchy accept a configuration object\ncontaining all the necessary information. The [VllmConfig](https://github.com/vllm-project/vllm/blob/d1c6799b8870e513bf4f2305cbf6cda9fc3d773b/vllm/config.py#L2036)\nclass is the main configuration object that is passed around. The class\nhierarchy is quite deep, and every class needs to read the configuration it is\ninterested in. By encapsulating all configurations in one object, we can easily\npass the configuration object around and access the configuration we need.\nSuppose we want to add a new feature (this is often the case given how fast the\nfield of LLM inference is evolving) that only touches the model runner. We will", "file_path": "design/arch_overview.md"}
{"id": "c65a98693df8cbef13ec7808030e29f2e90fccb2d8e9b8860ac17984714c25d4", "heading": "Architecture Overview/Class Hierarchy", "level": 2, "text": "have to add a new configuration option in the `VllmConfig` class. Since we pass\nthe whole config object around, we only need to add the configuration option to\nthe `VllmConfig` class, and the model runner can access it directly. We don't\nneed to change the constructor of the engine, worker, or model class to pass the\nnew configuration option.  \n2\\. **Uniformity**: The model runner needs a unified interface to create and\ninitialize the model. vLLM supports more than 50 types of popular open-source\nmodels. Each model has its own initialization logic. If the constructor\nsignature varies with models, the model runner does not know how to call the\nconstructor accordingly, without complicated and error-prone inspection logic.\nBy making the constructor of the model class uniform, the model runner can\neasily create and initialize the model without knowing the specific model type.\nThis is also useful for composing models. Vision-language models often consist", "file_path": "design/arch_overview.md"}
{"id": "c65a98693df8cbef13ec7808030e29f2e90fccb2d8e9b8860ac17984714c25d4", "heading": "Architecture Overview/Class Hierarchy", "level": 2, "text": "of a vision model and a language model. By making the constructor uniform, we\ncan easily create a vision model and a language model and compose them into a\nvision-language model.  \n!!! note\nTo support this change, all vLLM models' signatures have been updated to:  \n```python\ndef __init__(self, *, vllm_config: VllmConfig, prefix: str = \"\"):\n```  \nTo avoid accidentally passing incorrect arguments, the constructor is now keyword-only. This ensures that the constructor will raise an error if old configurations are passed. vLLM developers have already made this change for all models within vLLM. For out-of-tree registered models, developers need to update their models, for example by adding shim code to adapt the old constructor signature to the new one:  \n??? code  \n```python\nclass MyOldModel(nn.Module):\ndef __init__(\nself,\nconfig,\ncache_config: Optional[CacheConfig] = None,\nquant_config: Optional[QuantizationConfig] = None,\nlora_config: Optional[LoRAConfig] = None,\nprefix: str = \"\",\n) -> None:\n...", "file_path": "design/arch_overview.md"}
{"id": "c65a98693df8cbef13ec7808030e29f2e90fccb2d8e9b8860ac17984714c25d4", "heading": "Architecture Overview/Class Hierarchy", "level": 2, "text": "from vllm.config import VllmConfig\nclass MyNewModel(MyOldModel):\ndef __init__(self, *, vllm_config: VllmConfig, prefix: str = \"\"):\nconfig = vllm_config.model_config.hf_config\ncache_config = vllm_config.cache_config\nquant_config = vllm_config.quant_config\nlora_config = vllm_config.lora_config\nsuper().__init__(config, cache_config, quant_config, lora_config, prefix)", "file_path": "design/arch_overview.md"}
{"id": "c65a98693df8cbef13ec7808030e29f2e90fccb2d8e9b8860ac17984714c25d4", "heading": "Architecture Overview/Class Hierarchy", "level": 2, "text": "from packaging import version\nif version.parse(__version__) >= version.parse(\"0.6.4\"):\nMyModel = MyNewModel\nelse:\nMyModel = MyOldModel\n```  \nThis way, the model can work with both old and new versions of vLLM.  \n3\\. **Sharding and Quantization at Initialization**: Certain features require\nchanging the model weights. For example, tensor parallelism needs to shard the\nmodel weights, and quantization needs to quantize the model weights. There are\ntwo possible ways to implement this feature. One way is to change the model\nweights after the model is initialized. The other way is to change the model\nweights during the model initialization. vLLM chooses the latter. The first\napproach is not scalable to large models. Suppose we want to run a 405B model\n(with roughly 810GB weights) with 16 H100 80GB GPUs. Ideally, every GPU should\nonly load 50GB weights. If we change the model weights after the model is\ninitialized, we need to load the full 810GB weights to every GPU and then shard", "file_path": "design/arch_overview.md"}
{"id": "c65a98693df8cbef13ec7808030e29f2e90fccb2d8e9b8860ac17984714c25d4", "heading": "Architecture Overview/Class Hierarchy", "level": 2, "text": "the weights, leading to a huge memory overhead. Instead, if we shard the weights\nduring the model initialization, every layer will only create a shard of the\nweights it needs, leading to a much smaller memory overhead. The same idea\napplies to quantization. Note that we also add an additional argument `prefix`\nto the model's constructor so that the model can initialize itself differently\nbased on the prefix. This is useful for non-uniform quantization, where\ndifferent parts of the model are quantized differently. The `prefix` is\nusually an empty string for the top-level model and a string like `\"vision\"`\nor `\"language\"` for the sub-models. In general, it matches the name of the\nmodule's state dict in the checkpoint file.  \nOne disadvantage of this design is that it is hard to write unit tests for\nindividual components in vLLM because every component needs to be initialized by\na complete config object. We solve this problem by providing a default", "file_path": "design/arch_overview.md"}
{"id": "c65a98693df8cbef13ec7808030e29f2e90fccb2d8e9b8860ac17984714c25d4", "heading": "Architecture Overview/Class Hierarchy", "level": 2, "text": "initialization function that creates a default config object with all fields set\nto `None`. If the component we want to test only cares about a few fields in\nthe config object, we can create a default config object and set the fields we\ncare about. This way, we can test the component in isolation. Note that many\ntests in vLLM are end-to-end tests that test the whole system, so this is not a\nbig problem.  \nIn summary, the complete config object `VllmConfig` can be treated as an\nengine-level global state that is shared among all vLLM classes.", "file_path": "design/arch_overview.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs", "level": 1, "text": "# CUDA Graphs  \nThis write-up introduces the new CUDA Graphs modes in vLLM v1 beyond previous [torch.compile integration](torch_compile.md). To summarize, we:  \n1. Added flexible `cudagraph_mode` configuration\n2. Made full CUDA Graphs support orthogonal to compilation\n3. Introduced a CUDA Graphs dispatcher as a central controller that picks the desired runtime mode and CUDA Graphs per batch automatically  \nIn this document we will discuss the:  \n* [Motivation](#motivation)\n* [CUDA Graphs modes](#cudagraphmodes)\n* [Detailed design](#detailed-design)\n* [Example usage of the different CUDA Graphs modes](#usage-guide)  \n!!! note\nIn this document, we refer to pure decode (`max_query_len=1`) or speculative decode (`max_query_len =1+num_spec_tokens`) as **uniform decode** batches, and the opposite would be **non-uniform** batches (i.e., prefill or mixed prefill-decode batches).  \n!!! note\nThe following contents are mostly based on the last commit of <gh-pr:20059>.", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/Motivation", "level": 2, "text": "## Motivation  \nInitial piecewise compilation was built to allow piecewise cudagraph capture, excluding cudagraph-unsupported operations (mainly attention). This allowed some speedup from cudagraphs while maintaining compatibility with all attention backends. We later added support for \"full cudagraphs\" by not compiling piecewise, so that we could further reduce the latency in cases where attention supported cudagraphs. However, this tight coupling between compilation and cudagraph capture led to an all-or-nothing experience with little flexibility. Many attention backends also werenâ€™t ready for unified \"full\" CUDA Graphs capture (e.g., only FlashAttention 3 supports it currently) or only support CUDA Graphs for pure decode batches (e.g., Flashinfer, FlashMLA, and Mamba, etc.). That led to confusing performance/compatibility tradeoffs, inconsistent CUDA Graphs support, and increasingly complex code structure.  \nThis led us to seek a more fine-grained CUDA Graphs solution with the following features:", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/Motivation", "level": 2, "text": "* Explicitly aware of CUDA Graphs for prefill/mixed or (uniform-)decode batch and capture them separately.\n* Separate CUDAGraph capture logic from compilation (as much as feasible) for feature orthogonality, which suggest:\n* Capturing piecewise and full cudagraphs using the same compiled graph, and\n* Full cudagraph capture without compilation.\n* Dispatch between full and piecewise cudagraph at runtime depending on batch composition.\n* Centralized control of CUDAGraph behavior for reduced code complexity and allowed more extendibility.  \nThese features allow the most flexibility for cudagraph capture and compilation for all kinds of startup/performance tradeoffs and feature support.", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/`CudagraphModes`", "level": 2, "text": "## `CudagraphModes`  \n[CUDAGraphMode][vllm.config.compilation.CUDAGraphMode] is the single knob you tune in `CompilationConfig.cudagraph_mode`:  \n* `NONE` â€” turn CUDA Graphs off. Good for debugging.\n* `PIECEWISE` â€”  a single-mode strategy (and past default). It is the most flexible: attention or other CUDA Graphs-incompatible operations stay eager, everything else goes into CUDA Graphs. Requires piecewise compilation.\n* `FULL` â€” a single-mode strategy, which only captures full CUDA Graphs for non-uniform batches, then uniform-decode batches reuse the CUDA Graph of non-uniform batch of the same batch_size, since they are compatible; can be good for small models or workloads with small prompts.\n* `FULL_DECODE_ONLY` â€” full CUDA Graph for uniform decode, no cudagraph for prefill/mixed etc; suitable for decode instances in a P/D setup where prefill is not as important, this way we can save the memory needed for `PIECEWISE` CUDA Graphs.", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/`CudagraphModes`", "level": 2, "text": "* `FULL_AND_PIECEWISE` â€” (default mode) full CUDA Graph for uniform decode, piecewise CUDA Graphs for others; generally the most performant setting, especially for low latency with small models or MoEs, but also requires the most memory and takes the longest to capture.  \nDefaults: If youâ€™re on v1 with piecewise compilation, we default to `FULL_AND_PIECEWISE` for better performance, (for pooling models, it's still `PIECEWISE`). Otherwise, e.g. if piecewise compilation unavailable, we default to `NONE`.  \nWhile `NONE` , `PIECEWISE`, and `FULL` are single-mode configurations and simply equivalent to past implementations of eager execution, piecewise CUDA Graphs, and full CUDA Graphs respectively, `FULL_DECODE_ONLY` and `FULL_AND_PIECEWISE` are newly appended dual-mode configurations, which require dispatching to switch between concrete runtime modes according to runtime batches dynamically.  \n!!! note", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/`CudagraphModes`", "level": 2, "text": "!!! note\nHere, the single-modes `NONE`, `PIECEWISE`, and `FULL` are treated as the runtime modes for CUDA Graphs dispatching. If using a dual-mode, the dispatcher will always dispatch to one of its member modes (plus a potantial `NONE` if no suitable CUDA Graph available), depending on the batch composition.  \nWhile cascade attention is not cudagraph compatible, it is now compatible with all possible cudagraph mode configurations. If a batch uses cascade attention, it always gets dispatched to `PIECEWISE` mode if available (otherwise `NONE`).  \n!!! note\nNot all CUDA Graph modes are compatible with every attention backend. We automatically \"downgrade\" modes to the closest supported mode. For example, if a backend only supports CUDA Graphs for pure decode/uniform batches, we convert `FULL` to `FULL_AND_PIECEWISE` if piecewise compilation is enabled, and `FULL_DECODE_ONLY` otherwise.", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/Detailed Design/Overview", "level": 3, "text": "## Detailed Design  \n### Overview  \nThe new CUDA Graphs logic is built on top of piecewise compilation and supports dual CUDA Graphs runtime mode switching. The system contains the following core components:  \n* [CUDAGraphWrapper][vllm.compilation.cuda_graph.CUDAGraphWrapper]: wrapper that handles CUDAGraph capture & replay on the wrapped callable\n* [CudagraphDispatcher][vllm.v1.cudagraph_dispatcher.CudagraphDispatcher]: the central controller that contains the single source of truth about CUDA Graphs and handles dispatching between them.\n* [CUDAGraphMode][vllm.config.compilation.CUDAGraphMode]: enum describing the supported and runtime modes (introduced above).\n* [BatchDescriptor][vllm.forward_context.BatchDescriptor], serving as a unique representation of the runtime batch used for dispatching.", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/Detailed Design/Overview", "level": 3, "text": "See the following figures for a quick comparison between the previous and current design patterns of CUDA Graphs with inductor compilation. We can see that previously the CUDA Graphs logic and compilation logic were tightly coupled into the vllm `PiecewiseBackend`, and CUDA Graphs was implicitly dispatched by `batch_size` idly. Now the CUDA Graphs logic is separated into the `CUDAGraphWrapper` class, responsible for both full and piecewise CUDA Graphs abilities, and dispatching is **explicitly** done via **runtime mode** plus the `BatchDescriptor` as the **dispatch key** via `CudagraphDispatcher`.  \n**Before:**  \n![previous_design](../assets/design/cuda_graphs/previous_design.png)  \n**After:**  \n![new_design](../assets/design/cuda_graphs/current_design.png)", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/Detailed Design/`BatchDescriptor`", "level": 3, "text": "### `BatchDescriptor`  \n[BatchDescriptor][vllm.forward_context.BatchDescriptor] is a component within `ForwardContext`, alongside the CUDA Graphs runtime modes, serving as the core structure for dispatching keys at runtime. The prototype is:  \n```python\nclass BatchDescriptor(NamedTuple):\nnum_tokens: int\nuniform_decode: bool = False\n```  \nwhere `num_tokens` can be the padded token length, and `uniform_decode` is determined by if `max_query_len` of a batch is equal to the desired `max_query_len` of a uniform_decode, and the num_scheduled_tokens is divisible by that desired `max_query_len`.  \nThe goal of this structure is to uniquely identify a (padded) batch with minimal possible items corresponding to a CUDA Graphs item. We are safe to exclude items like `uniform_query_len` because it is a constant at runtime for a certain setup currently. For example, it should be either `1` for a commonly pure decode or `1+num_spec_tokens` for a validation phase of speculative decode.  \n!!! note", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/Detailed Design/`BatchDescriptor`", "level": 3, "text": "!!! note\nThe prototype of `BatchDescriptor` may be extended for more general situations in the future, e.g., include more items, like `uniform_query_len` to support multiple different uniform decode lengths settings (<gh-pr:23679>), or other modifications needed to support CUDA Graphs for models whose inputs are not necessarily token length aware (for example, some multi-modal inputs).", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/Detailed Design/`CudagraphDispatcher`", "level": 3, "text": "### `CudagraphDispatcher`  \nThe [CudagraphDispatcher][vllm.v1.cudagraph_dispatcher.CudagraphDispatcher] takes responsibility for maintaining two sets of valid dispatching keys, one set for `FULL` runtime mode and one set for `PIECEWISE` runtime mode, and dispatches the correct runtime mode and the dispatching keys before executing the model's forwards. It will take in the initial key (a rough batch_descriptor for the padded input) and return the selected runtime mode and the final batch_descriptor, then tell the CUDAGraphWarpper instances that decision through forward contexts. Notice that `CudagraphDispatcher` is the only source of truth for available CUDA Graph keys and `CUDAGraphWrapper` instances can blindly trust the forward context on what CUDA Graphs to dispatch to. This lets us simplify the wrapper code and centralize the logic in the dispatcher.", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/Detailed Design/`CudagraphDispatcher`", "level": 3, "text": "The dispatching keys are initialized through the dispatcher's `initialize_cudagraph_keys` method, which is called by the gpu_model_runner after all possible attention backends are initialized. This is where we can get much fancier in the future and â€œprepareâ€ all kinds of CUDA Graphs combinations. For now, we just append available keys based on the valid combos of `decode_mode`/`mixed_mode` of `cudagraph_mode` and `cudagraph_capture_sizes` in the compilation config.  \nThe dispatch code looks like:  \n```python\nbatch_descriptor=BatchDescriptor(num_tokens=num_input_tokens, uniform_decode=...)\nruntime_mode, batch_descriptor = cudagraphdispatcher.dispatch(batch_descriptor)\n# execution\nwith set_forward_context(\n...,\ncudagraph_runtime_mode=runtime_mode,\nbatch_descriptor=batch_descriptor,\n):\noutput = self.model(...)\n```", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/Detailed Design/`CudagraphDispatcher`", "level": 3, "text": "):\noutput = self.model(...)\n```  \nInside the `dispatch()` method, the dispatcher will search the proper CUDA Graphs runtime mode and existing dispatching keys for a return. We basically search the existing keys following the priority: `FULL`>`PIECEWISE`>`None`. If the dispatching key does not exist, default to return `NONE` mode for eager execution. The implementations can be found [here](https://github.com/vllm-project/vllm/blob/main/vllm/v1/cudagraph_dispatcher.py#L91).  \nHere is a simplified illustration of the workflow at runtime in the model executor:\n![executor_runtime](../assets/design/cuda_graphs/executor_runtime.png)", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/Detailed Design/`CUDAGraphWrapper`", "level": 3, "text": "### `CUDAGraphWrapper`  \nA [CUDAGraphWrapper][vllm.compilation.cuda_graph.CUDAGraphWrapper] instance wraps a runnable and simply mimics the runnable with appended CUDA Graphs abilities. Each wrapper instance is bound to a specific `runtime_mode`, which is restricted to `PIECEWISE` and `FULL` mode, and takes responsibility for capturing/replaying and passing through (directly calling) the runnable.  At runtime, each wrapper would:  \n1. inspect the runtime_mode and batch_descriptor(dispatching key) from the global forward context.\n2. If runtime_mode is `NONE` or runtime_mode does not match the mode of the wrapper, just call the runnable directly.\n3. Otherwise, i.e., the runtime_mode matches the mode of the wrapper, the wrapper will perform CUDA Graphs capture (if key does not exist, create\na new entry and cache it) or replay (if key exists in the cache).", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/Detailed Design/`CUDAGraphWrapper`", "level": 3, "text": "The above steps are based on the assumption that the CUDA Graphs wrapper would directly trust whatâ€™s in the forward context (controlled by the dispatcher). This lets us simplify and cenralize the logic, reducing the complexity as well as the risk of mismatched state between the wrappers and the dispatcher. It also allows reusing the wrapper class for both `FULL` and `PIECEWISE` runtime modes. See the implementation [here](https://github.com/vllm-project/vllm/blob/f751e50b7a2aae3110d83ed0d88202fc91b3e78a/vllm/compilation/cuda_graph.py#L106).  \n#### Nested Wrapper design  \nThe core mechanism of making a full CUDA Graphs and piecewise CUDA Graphs coexist and compatible is the nested CUDA Graphs wrapper design, building on top of piecewise compilation with only a single piecewise FX graph.  We wrap a FULL mode wrapper outside the entire model for the full CUDA Graphs functionality; meanwhile, each piecewise backend is wrapped via a `PIECEWISE` mode wrapper inside the compilation.", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/Detailed Design/`CUDAGraphWrapper`", "level": 3, "text": "The flow chart below should clearly describe how it works.\n![wrapper_flow](../assets/design/cuda_graphs/wrapper_flow.png)  \nTherefore, for a `FULL` runtime mode, it is safe to capture/replay a full CUDA Graph since the piecewise wrapper is not activated. The situation is similar for `PIECEWISE` mode, as there are no conflicts between the `FULL` mode wrapper and `PIECEWISE` mode wrappers.  For the `NONE` runtime mode, both `FULL` and `PIECEWISE` wrappers would not be activated, so we simply fall through to eager execution.", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/Detailed Design/Full CUDA Graph capturing & warm-up", "level": 3, "text": "### Full CUDA Graph capturing & warm-up  \nThe CUDA Graphs capturing happens when the runner first calls the model forward (using `_dummy_run`) with a non-`NONE` runtime mode. For full CUDA Graph capture, we explicitly capture different cases (i.e., prefill/mixed batch or uniform_decode batch) by properly setting attention metadata to make sure the underlying attention backends launch the desired kernel routines. To distinguish prefill/mixed batch or uniform_decode batch, the most important property is the `max_query_len` in attn_metadata (true for most attention backends). We set it to the desired `uniform_query_len` for uniform_decode otherwise we make it just the `num_tokens` for a non-uniform_decode batch.", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/Detailed Design/Full CUDA Graph capturing & warm-up", "level": 3, "text": "The CUDA Graphs wrapper no longer manages the warm-up logic. The warm-up process is now controlled directly by the GPU model runner, where the `NONE` runtime mode is assigned to play an eager execution for warm-up. When warming up for a full CUDA Graph, it is also important to explicitly run attention during the warmup `dummy_run` call.", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/CUDA Graphs Compatibility of Attention Backends", "level": 2, "text": "## CUDA Graphs Compatibility of Attention Backends  \nTo signal the CUDA Graphs compatibility of the attention backends, we introduce a new enum type [AttentionCGSupport][vllm.v1.attention.backends.utils.AttentionCGSupport], which is an enum type that tracks the capability of the attention backend to support CUDA Graphs. The value is sorted in the order of the capability, i.e., `ALWAYS`> `UNIFORM_BATCH`> `UNIFORM_SINGLE_TOKEN_DECODE`> `NEVER`.  \n```python\nclass AttentionCGSupport(enum.Enum):\n\"\"\" Constants for the CUDA Graphs support of the attention backend\nHere we do not consider the cascade attention, as currently\nit is never CUDA Graphs supported.\"\"\"", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/CUDA Graphs Compatibility of Attention Backends", "level": 2, "text": "ALWAYS = 3\n\"\"\"CUDA Graphs always supported; supports mixed-prefill-decode\"\"\"\nUNIFORM_BATCH = 2\n\"\"\"CUDA Graphs supported for batches the only contain query lengths that are\nthe same, this can be used for spec-decode\ni.e. \"decodes\" are 1 + num_speculative_tokens\"\"\"\nUNIFORM_SINGLE_TOKEN_DECODE = 1\n\"\"\"CUDA Graphs supported for batches the only contain query_len==1 decodes\"\"\"\nNEVER = 0\n\"\"\"NO CUDA Graphs support\"\"\"\n```", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/CUDA Graphs Compatibility of Attention Backends", "level": 2, "text": "NEVER = 0\n\"\"\"NO CUDA Graphs support\"\"\"\n```  \nSuppose we have hybrid attention backends (e.g., in mamba mixer models). In that case, we seek the minimum capability of all backends to determine the final capability of the model, and we might resolve the incompatible CUDA Graphs mode by downgrading the mode to the best fit one. For example, downgrading `FULL` mode to `FULL_AND_PIECEWISE` mode if the minimum capability is `UNIFORM_BATCH`, or `PIECEWISE` mode if the minimum capability is `NEVER` for -O3 compilation mode. For the complete fallback policy, please see the code of [initialize_cudagraph_capture][vllm.v1.worker.gpu_model_runner.GPUModelRunner.initialize_cudagraph_capture].  \nThe following table lists backends that support full CUDA Graphs at the time of writing.  \n| Attention Backend | cudagraph_support | Comments |\n|:---|:---|:---|\n| FlashAttention v2 | `UNIFORM_BATCH` | Actually `ALWAYS` but workaround to fallback to `FULL_AND_PIECEWISE` for performance reason |", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/CUDA Graphs Compatibility of Attention Backends", "level": 2, "text": "| FlashAttention v3 | `ALWAYS` | has unified routine for both batches, so `FULL` mode is good |\n| Triton Attention | `ALWAYS` | prefer `FULL_AND_PIECEWISE` since it has different kernels for prefill/mixed and pure decode batches |\n| AITER FlashAttention | `UNIFORM_BATCH`| |\n| FlashInfer | `UNIFORM_SINGLE_TOKEN_DECODE` | |\n| FlashMLA | `UNIFORM_BATCH` | |\n| AITER MLA | `UNIFORM_SINGLE_TOKEN_DECODE` | |\n| CUTLASS MLA | `UNIFORM_SINGLE_TOKEN_DECODE` | |\n| Mamba attention| `UNIFORM_SINGLE_TOKEN_DECODE` | |  \nUnlisted backends are all declared as `NEVER`.", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/Usage guide", "level": 2, "text": "## Usage guide  \nNow the CLI is directly using the uppercase string of cudagraph_mode for compilation_config: `--compilation-config '{\"cudagraph_mode\": \"...\"}'`, where `...` should be one of `NONE`, `PIECEWISE`, `FULL`, `FULL_DECODE_ONLY`, and `FULL_AND_PIECEWISE`. Note that all `PIECEWISE` related modes require piecewise compilation, and all `FULL` related modes need CUDA Graphs support of attention backends. For example:  \n```bash\nvllm serve --model meta-llama/Llama-3.1-8B-Instruct --compilation-config '{\"cudagraph_mode\": \"FULL_AND_PIECEWISE\"}'\n```", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/Usage guide/Python examples", "level": 3, "text": "### Python examples  \n```python\nimport os\nos.environ.setdefault(\"VLLM_LOGGING_LEVEL\", \"DEBUG\")\n\nimport vllm\nfrom vllm.config import CUDAGraphMode\n\ncompilation_config = {\"mode\": 3, \"cudagraph_mode\": \"FULL_AND_PIECEWISE\"}\nmodel = vllm.LLM(\nmodel=\"meta-llama/Llama-3.1-8B-Instruct\",\ndtype=\"auto\",\ncompilation_config=compilation_config,\n)\nsampling_params = vllm.SamplingParams(\ntemperature=0,  # greedy decoding\nmax_tokens=1024,\n)\noutputs = model.generate(\n[\"My name is John and\"],\nsampling_params=sampling_params,\n)\n```", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/Usage guide/Migration from legacy flags", "level": 3, "text": "### Migration from legacy flags  \nLegacy `use_cudagraph` and `full_cuda_graph` are unified by `cudagraph_mode`:  \n* `use_cudagraph=False` â†’ `NONE`.\n* `use_cudagraph=True` and `full_cuda_graph=False` â†’ `PIECEWISE`.\n* `full_cuda_graph=True` â†’ directly set `FULL` and rely on the graceful fallback policy.  \nAs they are deprecated and will be removed in the next major or minor release, i.e., v0.11.0 or v1.0.0, we recommend using cudagraph_mode instead.", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/Usage guide/Piecewise compilation and full graph custom passes (attention fusion, sequence parallelism)", "level": 3, "text": "### Piecewise compilation and full graph custom passes (attention fusion, sequence parallelism)  \nUnfortunately, some custom compile passes have to see the whole graph to be effective and hence aren't compatible with piecewise compilation. This includes `AttnFusionPass` and `SequenceParallelismPass`. As a short-term solution, we automatically disable piecewise compilation (by setting `splitting_ops=[]`) when attention fusion is enabled. We use CUDA Graph modes `FULL` or `FULL_DECODE_ONLY` (depending on backend support). However, this leads to another optimization incompatibility and confusing performance tradeoffs.", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/Usage guide/Piecewise compilation and full graph custom passes (attention fusion, sequence parallelism)", "level": 3, "text": "Long term, we've added the ability to partition the graph in Inductor instead of right after Dynamo. It can be enabled with `CompilationConfig.use_inductor_graph_partition=True` but is currently experimental and only available with `torch>=2.9`. This also increases compilation time as it has to compile the whole graph and cannot reuse piecewise compilation artifacts. Once vLLM supports 2.9, we plan to make this the default approach as it will also speed up piecewise cudagraph capture.", "file_path": "design/cuda_graphs.md"}
{"id": "afb02acc20b2e50fa3f80011d52437d6a2a37a4eba1739db123bc46b2265673f", "heading": "CUDA Graphs/About the Performance", "level": 2, "text": "## About the Performance  \nSee the following links for examples:  \n* [20059#issuecomment-3160858458](https://github.com/vllm-project/vllm/pull/20059#issuecomment-3160858458)\n* [20059#issuecomment-3188735226](https://github.com/vllm-project/vllm/pull/20059#issuecomment-3188735226)\n* [20059#issuecomment-3219888738](https://github.com/vllm-project/vllm/pull/20059#issuecomment-3219888738)", "file_path": "design/cuda_graphs.md"}
{"id": "c9e834896ff1a9bca71a2cd8c6b58f33ae048bc383f97adc061da1ba753147c8", "heading": "Dual Batch Overlap/Motivation", "level": 2, "text": "# Dual Batch Overlap  \n## Motivation  \nThe core motivation of the DBO system in vLLM is to overlap the sparse all-to-all communication in the MoE layer with the surrounding computation. This system currently only targets DP+EP deployments.", "file_path": "design/dbo.md"}
{"id": "c9e834896ff1a9bca71a2cd8c6b58f33ae048bc383f97adc061da1ba753147c8", "heading": "Dual Batch Overlap/Introduction", "level": 2, "text": "## Introduction  \nThe Dual Batch Overlap system works by splitting the batch in the model runner, creating two worker threads, and then running the model on each of these worker threads. When DBO is enabled, yield points within the `FusedMoEModularKernel` allow the two CPU worker threads (also called UBatch threads) to ping-pong between each other so that when one is running compute, the other is waiting on communication. Throughout the code, ubatch may be used as a short form of microbatch; this is an ASCII-friendly version of the short form Âµ-batch.  \nThe DBO system includes modifications to `GpuModelRunner` and `ModularKernel`, and defines two utility classes: `UBatchWrapper` and `UBatchContext`. `UBatchWrapper` manages thread lifecycle and CUDA graph execution of the model. `UBatchContext` wraps `ForwardContext` to coordinate synchronization between the two UBatch threads.  \nBelow is the overlap schedule that is currently implemented in vLLM.  \n```python\n# Schedule notation legend:\n#    S = Shared expert", "file_path": "design/dbo.md"}
{"id": "c9e834896ff1a9bca71a2cd8c6b58f33ae048bc383f97adc061da1ba753147c8", "heading": "Dual Batch Overlap/Introduction", "level": 2, "text": "```python\n# Schedule notation legend:\n#    S = Shared expert\n#    A0 = MLA qkv proj,\n#    A1 = Core attn + out proj + MoE gate\n#    D = Dispatch\n#    C = Combine", "file_path": "design/dbo.md"}
{"id": "c9e834896ff1a9bca71a2cd8c6b58f33ae048bc383f97adc061da1ba753147c8", "heading": "Dual Batch Overlap/Introduction", "level": 2, "text": "# Comp: |-A0â‚€-A1â‚€-||-MLPâ‚-||-Sâ‚-MLPâ‚€-||-Sâ‚€-A0â‚-A1â‚-|\n# Comm: |----Dâ‚---||--Dâ‚€--||----Câ‚---||-----Câ‚€-----|\n# Order: Dâ‚ send, A0â‚€, A1â‚€, Dâ‚ recv, Dâ‚€ send, MLPâ‚, Dâ‚€ recv,\n#        Câ‚ send, Sâ‚, MLPâ‚€, Câ‚ recv, Câ‚€ send, Sâ‚€, A0â‚, A1â‚, Câ‚€ recv.\n# MLP_SHARED_OVERLAP = \"mlp_shared_overlap\"\n```", "file_path": "design/dbo.md"}
{"id": "c9e834896ff1a9bca71a2cd8c6b58f33ae048bc383f97adc061da1ba753147c8", "heading": "Dual Batch Overlap/Running with DBO", "level": 2, "text": "## Running with DBO  \nTo enable the DBO system pass in the `--enable-dbo` argument to your vllm serve command. This must be run in conjunction with `--data-parallel-size N` where N is greater than 1 and `--enable-expert-parallel`. Additionally, there are two configuration knobs.  \n* `--dbo-decode-token-threshold` the minimum number of tokens in a decode-only batch required to enable DBO for that batch\n* `--dbo-prefill-token-threshold` the minimum number of tokens in a batch containing at least one prefill required to enable DBO for that batch  \nCurrently, DBO is only supported with DeepEP, so DeepEP must be installed and the `--all2all-backend` argument must be set to `deepep_low_latency` if your workload is primarily decode requests, or `deepep_high_throughput` if your workload is primarily prefill requests.  \nBelow is a command that will spin up a two DP rank server with expert parallelism and DBO enabled.", "file_path": "design/dbo.md"}
{"id": "c9e834896ff1a9bca71a2cd8c6b58f33ae048bc383f97adc061da1ba753147c8", "heading": "Dual Batch Overlap/Running with DBO", "level": 2, "text": "EX: `vllm serve deepseek-ai/DeepSeek-V2-Lite --trust-remote-code --data-parallel-size 2 --enable-expert-parallel --enable-dbo --all2all-backend deepep_low_latency`  \nNote that there must be at least two GPUs visible in `CUDA_VISIBLE_DEVICES`", "file_path": "design/dbo.md"}
{"id": "c9e834896ff1a9bca71a2cd8c6b58f33ae048bc383f97adc061da1ba753147c8", "heading": "Dual Batch Overlap/DBO Components", "level": 2, "text": "## DBO Components  \n* GPUModelRunner\n* UBatchWrapper\n* UBatchContext", "file_path": "design/dbo.md"}
{"id": "c9e834896ff1a9bca71a2cd8c6b58f33ae048bc383f97adc061da1ba753147c8", "heading": "Dual Batch Overlap/DBO Components/GPU Model Runner", "level": 3, "text": "### GPU Model Runner  \nThe batch is split into microbatches by the `GPUModelRunner` class. This is accomplished in two steps. First, coordination across all DP ranks is performed to determine whether microbatching will be applied. Microbatching must be uniform across all DP ranks. If microbatching is not feasible for any DP rank, it is disabled for all ranks. If all DP ranks are going to microbatch, the total number of tokens is padded up to the max number of tokens amongst all ranks. If any rank would end up with an empty second microbatch after the padding is applied, microbatching will be aborted and no ranks will microbatch. Once microbatching has been initiated by all ranks, the second step is performed. The `CommonAttentionMetadata` is sliced in half by the `GPUModelRunner` so that there is one attention metadata per-microbatch.", "file_path": "design/dbo.md"}
{"id": "c9e834896ff1a9bca71a2cd8c6b58f33ae048bc383f97adc061da1ba753147c8", "heading": "Dual Batch Overlap/DBO Components/UBatchWrapper", "level": 3, "text": "### UBatchWrapper  \ngpu_ubatch_wrapper  \nThe `UBatchWrapper` class is a model wrapper that's responsible for all of the thread, UBatchContext, and CUDA graph management for DBO. It's designed to be relatively transparent to the GPU Model Runner.  \nThe implementation runs the model twice, once for each microbatch. Each model invocation occurs within a UBatch thread. These threads are launched in parallel and are synchronized using the `UBatchContext`. Each thread is provided with a sliced version of the attention metadata that is used to run its half of the batch.  \nCUDA graphs for DBO are entirely managed by the `UBatchWrapper`. Because of this, DBO only supports running with Full CUDA graphs. However, once a DBO CUDA graph has been captured, it can be replayed without any multithreading or CPU synchronization.  \n#### Interfaces  \nThe `__init__` method takes in the model, VllmConfig, CUDAGraphMode, and device.", "file_path": "design/dbo.md"}
{"id": "c9e834896ff1a9bca71a2cd8c6b58f33ae048bc383f97adc061da1ba753147c8", "heading": "Dual Batch Overlap/DBO Components/UBatchWrapper", "level": 3, "text": "The `forward` method exclusively takes in model arguments. It determines whether or not to run with DBO based on whether a `ubatch_slices` object is present in the `forward_context`. Otherwise, the model is run without DBO.", "file_path": "design/dbo.md"}
{"id": "c9e834896ff1a9bca71a2cd8c6b58f33ae048bc383f97adc061da1ba753147c8", "heading": "Dual Batch Overlap/DBO Components/UBatchContext", "level": 3, "text": "### UBatchContext  \nubatch_context  \nThe `UBatchContext` class is a `ForwardContext` wrapper class that is used by the `UBatchWrapper` class to synchronize the two UBatch threads. It should only be instantiated by using `make_ubatch_contexts`.  \nWhen one of the UBatch threads reaches a `dbo_yield` call, it pauses, and starts the other thread which will run until it reaches the same `dbo_yield` call. This \"ping-pong\" dynamic continues, with threads swapping at each `dbo_yield call`, until the model's execution is complete.  \nThe current implementation has all `dbo_yield` and `dbo_maybe_run_recv_hook` calls in the `FusedMoEModularKernel.forward` method.  \n#### Interfaces  \nThe `make_ubatch_context` function initializes two `UBatchContexts`, one for each UBatch thread. It takes two CUDA streams, the preexisting `ForwardContexts` and a CPU thread barrier. This function should be used exclusively to instantiate `UBatchContexts`. It will handle all of the event initialization.", "file_path": "design/dbo.md"}
{"id": "c9e834896ff1a9bca71a2cd8c6b58f33ae048bc383f97adc061da1ba753147c8", "heading": "Dual Batch Overlap/DBO Components/UBatchContext", "level": 3, "text": "The `dbo_register_recv_hook` method registers a callback that can be returned by the `FusedMoEPrepareAndFinalize` class in the other UBatch threadâ€™s `UBatchContext`. The callback will be run when the other thread calls `dbo_maybe_run_recv_hook`. This is typically used to wait on an all-to-all kernel.  \nThe `dbo_maybe_run_recv_hook` method runs a callback thatâ€™s set by the `dbo_register_recv_hook` function if that callback exists.  \nThe `dbo_yield` method puts the current thread to sleep and wakes up the other UBatch thread.", "file_path": "design/dbo.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/Introduction", "level": 2, "text": "# Fused MoE Modular Kernel  \n## Introduction  \nFusedMoEModularKernel is implemented [here](gh-file:/vllm/model_executor/layers/fused_moe/modular_kernel.py)  \nBased on the format of the input activations, FusedMoE implementations are broadly classified into 2 types.  \n* Contiguous / Standard / Non-Batched, and\n* Batched  \n!!! note\nThe terms Contiguous, Standard, and Non-Batched are used interchangeably throughout the document.  \nThe input activation format completely depends on the All2All Dispatch being used.  \n* In the Contiguous variant, the All2All Dispatch returns the activations as a contiguous tensor of shape (M, K) along with TopK Ids and TopK weights of shape (M, num_topk). Look at `DeepEPHTPrepareAndFinalize` for an example.", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/Introduction", "level": 2, "text": "* In the Batched variant, the All2All Dispatch returns the activations as a tensor of shape (num_experts, max_tokens, K). Here, the activations/tokens that subscribe to the same expert are batched together. Note that not all entries of the tensor are valid. The activations tensor is typically accompanied by an `expert_num_tokens` tensor of size `num_experts`, where `expert_num_tokens[i]` indicates the number of valid tokens that subscribe to the ith expert. Look at `PplxPrepareAndFinalize` or `DeepEPLLPrepareAndFinalize` for an example.  \nThe FusedMoE operation is generally made of multiple operations, in both the Contiguous and Batched variants, as described in the diagrams below  \n![](../assets/design/fused_moe_modular_kernel/fused_moe_non_batched.png \"FusedMoE Non-Batched\")  \n![](../assets/design/fused_moe_modular_kernel/fused_moe_batched.png \"FusedMoE Batched\")  \n!!! note", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/Introduction", "level": 2, "text": "!!! note\nThe main difference, in terms of operations, between the Batched and Non-Batched cases is the Permute / Unpermute operations. All other operations remain.", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/Motivation", "level": 2, "text": "## Motivation  \nAs can be seen from the diagrams, there are a lot of operations and there can be a variety of implementations for each operation. The set of ways the operations can be put together to make a valid FusedMoE implementation quickly becomes intractable. The Modular Kernel framework addresses this issue,  by grouping the operations into logical components. This broad categorization makes the combinations manageable and prevents code-duplication. This also decouples the All2All Dispatch & Combine implementations from the FusedMoE implementations and allows for their independent development and testing. Furthermore, the Modular Kernel framework introduces Abstract classes for the different components thus providing a well-defined skeleton for future implementations.  \nThe rest of the document will focus on the Contiguous / Non-Batched case. Extrapolating to the Batched case should be straight-forward.", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/ModularKernel Components", "level": 2, "text": "## ModularKernel Components  \nFusedMoEModularKernel splits the FusedMoE operation into 3 parts,  \n1. TopKWeightAndReduce\n2. FusedMoEPrepareAndFinalize\n3. FusedMoEPermuteExpertsUnpermute", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/ModularKernel Components/TopKWeightAndReduce", "level": 3, "text": "### TopKWeightAndReduce  \nThe TopK Weight Application and Reduction components happen right after the Unpermute operation and before the All2All Combine. Note that the `FusedMoEPermuteExpertsUnpermute` is responsible for the Unpermute and `FusedMoEPrepareAndFinalize` is responsible for the All2All Combine. There is value in doing the TopK Weight Application and Reduction in the `FusedMoEPermuteExpertsUnpermute`. But some implementations choose to do it `FusedMoEPrepareAndFinalize`. In order to enable this flexibility, we have a TopKWeightAndReduce abstract class.  \nPlease find the implementations of TopKWeightAndReduce [here](gh-file:vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py).  \n`FusedMoEPrepareAndFinalize::finalize()` method accepts a `TopKWeightAndReduce` argument that is invoked inside the method.", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/ModularKernel Components/TopKWeightAndReduce", "level": 3, "text": "The `FusedMoEModularKernel` acts as a bridge between the `FusedMoEPermuteExpertsUnpermute` and `FusedMoEPerpareAndFinalize` implementations to determine where the TopK Weight Application and Reduction happens.  \n* `FusedMoEPermuteExpertsUnpermute::finalize_weight_and_reduce_impl` method returns `TopKWeightAndReduceNoOp` if the `FusedMoEPermuteExpertsUnpermute` implementation does the weight application and reduction itself.\n* `FusedMoEPermuteExpertsUnpermute::finalize_weight_and_reduce_impl` method returns `TopKWeightAndReduceContiguous` / `TopKWeightAndReduceNaiveBatched` / `TopKWeightAndReduceDelegate` if the `FusedMoEPermuteExpertsUnpermute` implementation needs the `FusedMoEPrepareAndFinalize::finalize()` to do the weight application and reduction.", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/ModularKernel Components/FusedMoEPrepareAndFinalize", "level": 3, "text": "### FusedMoEPrepareAndFinalize  \nThe `FusedMoEPrepareAndFinalize` abstract class exposes `prepare`, `prepare_no_receive`  and `finalize` functions.\nThe `prepare` function is responsible for input activation Quantization and All2All Dispatch. If implemented, The `prepare_no_receive` is like `prepare` except it does not wait to receive results from other workers.  Instead it returns a \"receiver\" callback that must be invoked to wait for the final results of worker. It is not required that this method is supported by all `FusedMoEPrepareAndFinalize` classes, but if it is available, it can be used to interleave work with the initial all to all communication, e.g. interleaving shared experts with fused experts.  The `finalize` function is responsible for invoking the All2All Combine. Additionally the `finalize` function may or may not do the TopK weight application and reduction (Please refer to the TopKWeightAndReduce section)", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/ModularKernel Components/FusedMoEPrepareAndFinalize", "level": 3, "text": "![](../assets/design/fused_moe_modular_kernel/prepare_and_finalize_blocks.png \"FusedMoEPrepareAndFinalize Blocks\")", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/ModularKernel Components/FusedMoEPermuteExpertsUnpermute", "level": 3, "text": "### FusedMoEPermuteExpertsUnpermute  \nThe `FusedMoEPermuteExpertsUnpermute` class is where the crux of the MoE operations happen. The `FusedMoEPermuteExpertsUnpermute` abstract class exposes a few important functions,  \n* apply()\n* workspace_shapes()\n* finalize_weight_and_reduce_impl()  \n#### apply()  \nThe `apply` method is where the implementations perform  \n* Permute\n* Matmul with weight W1\n* Act + Mul\n* Quantization\n* Matmul with weight W2\n* Unpermute\n* Maybe TopK Weight Application + Reduction  \n#### workspace_shapes()", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/ModularKernel Components/FusedMoEPermuteExpertsUnpermute", "level": 3, "text": "#### workspace_shapes()  \nThe core FusedMoE implementation performs a series of operations. It would be inefficient to create output memory for each of these operations separately. To that effect, implementations are required to declare 2 workspace shapes, the workspace datatype and the FusedMoE output shape as outputs of the workspace_shapes() method. This information is used to allocate the workspace tensors and the output tensor in `FusedMoEModularKernel::forward()` and passed on to the `FusedMoEPermuteExpertsUnpermute::apply()` method. The workspaces could then be used as intermediate buffers in the FusedMoE implementation.  \n#### finalize_weight_and_reduce_impl()  \nIt is sometimes efficient to perform TopK weight application and Reduction inside the `FusedMoEPermuteExpertsUnpermute::apply()`. Find an example [here](https://github.com/vllm-project/vllm/pull/20228). We have a `TopKWeightAndReduce` abstract class to facilitate such implementations. Please refer to the TopKWeightAndReduce section.", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/ModularKernel Components/FusedMoEPermuteExpertsUnpermute", "level": 3, "text": "`FusedMoEPermuteExpertsUnpermute::finalize_weight_and_reduce_impl()` returns the `TopKWeightAndReduce` object that the implementation wants the `FusedMoEPrepareAndFinalize::finalize()` to use.  \n![](../assets/design/fused_moe_modular_kernel/fused_experts_blocks.png \"FusedMoEPermuteExpertsUnpermute Blocks\")", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/ModularKernel Components/FusedMoEModularKernel", "level": 3, "text": "### FusedMoEModularKernel  \n`FusedMoEModularKernel` is composed of the `FusedMoEPrepareAndFinalize` and `FusedMoEPermuteExpertsUnpermute` objects.\n`FusedMoEModularKernel` pseudocode/sketch,  \n```py\nclass FusedMoEModularKernel:\ndef __init__(self,\nprepare_finalize: FusedMoEPrepareAndFinalize,\nfused_experts: FusedMoEPermuteExpertsUnpermute):\n\nself.prepare_finalize = prepare_finalize\nself.fused_experts = fused_experts\n\ndef forward(self, DP_A):\n\nAq, A_scale, _, _, _ = self.prepare_finalize.prepare(DP_A, ...)\n\nworkspace13_shape, workspace2_shape, _, _ = self.fused_experts.workspace_shapes(...)\n\n# allocate workspaces\nworkspace_13 = torch.empty(workspace13_shape, ...)\nworkspace_2 = torch.empty(workspace2_shape, ...)\n\n# execute fused_experts\nfe_out = self.fused_experts.apply(Aq, A_scale, workspace13, workspace2, ...)", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/ModularKernel Components/FusedMoEModularKernel", "level": 3, "text": "# war_impl is an object of type TopKWeightAndReduceNoOp if the fused_experts implementations\n# performs the TopK Weight Application and Reduction.\nwar_impl = self.fused_experts.finalize_weight_and_reduce_impl()\n\noutput = self.prepare_finalize.finalize(fe_out, war_impl,...)\n\nreturn output\n```", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/How-To/How To Add a FusedMoEPrepareAndFinalize Type", "level": 3, "text": "## How-To  \n### How To Add a FusedMoEPrepareAndFinalize Type  \nTypically a FusedMoEPrepareAndFinalize type is backed by an All2All Dispatch & Combine implementation / kernel. For example,  \n* PplxPrepareAndFinalize type is backed by Pplx All2All kernels,\n* DeepEPHTPrepareAndFinalize type is backed by DeepEP High-Throughput All2All kernels, and\n* DeepEPLLPrepareAndFinalize type is backed by DeepEP Low-Latency All2All kernels.  \n#### Step 1: Add an All2All manager  \nThe purpose of the All2All Manager is to set up the All2All kernel implementations. The `FusedMoEPrepareAndFinalize` implementations typically fetch a kernel-implementation \"handle\" from the All2All Manager to invoke the Dispatch and Combine functions. Please look at the All2All Manager implementations [here](gh-file:vllm/distributed/device_communicators/all2all.py).  \n#### Step 2: Add a FusedMoEPrepareAndFinalize Type  \nThis section describes the significance of the various functions exposed by the `FusedMoEPrepareAndFinalize` abstract class.", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/How-To/How To Add a FusedMoEPrepareAndFinalize Type", "level": 3, "text": "`FusedMoEPrepareAndFinalize::prepare()`: The prepare method implements the Quantization and All2All Dispatch. Typically the Dispatch function from the relevant All2All Manager is invoked.  \n`FusedMoEPrepareAndFinalize::has_prepare_no_receive()`: Indicates whether or not this subclass implements `prepare_no_receive`. Defaults to False.  \n`FusedMoEPrepareAndFinalize::prepare_no_receive()`: The prepare_no_receive method implements the Quantization and All2All Dispatch. It does not wait for the result of the dispatch operation but instead returns a thunk that can be invoked to wait for the final results. Typically the Dispatch function from the relevant All2All Manager is invoked.  \n`FusedMoEPrepareAndFinalize::finalize()`: Maybe perform TopK Weight Application and Reduction and All2All Combine. Typically the Combine function from the relevant All2AllManager is invoked.", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/How-To/How To Add a FusedMoEPrepareAndFinalize Type", "level": 3, "text": "`FusedMoEPrepareAndFinalize::activation_format()`: Return `FusedMoEActivationFormat.BatchedExperts` if the output of the prepare method (i.e. the All2All dispatch) is Batched. Return `FusedMoEActivationFormat.Standard` otherwise.  \n`FusedMoEPrepareAndFinalize::topk_indices_dtype()`: Data type of the TopK ids. Some All2All kernels have strict requirements pertaining to the data type of the TopK ids. This requirement is passed on to the `FusedMoe::select_experts` function so it could be respected. If there are no strict requirements return None.  \n`FusedMoEPrepareAndFinalize::max_num_tokens_per_rank()`: This is the maximum number of tokens that would be submitted to the All2All Dispatch at once.  \n`FusedMoEPrepareAndFinalize::num_dispatchers()`: Total number of dispatching units. This value determines the size of the Dispatch output. The Dispatch output is of shape (num_local_experts, max_num_tokens, K). Here max_num_tokens = num_dispatchers() * max_num_tokens_per_rank().", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/How-To/How To Add a FusedMoEPrepareAndFinalize Type", "level": 3, "text": "We suggest picking an already existing `FusedMoEPrepareAndFinalize` implementation that matches your All2All implementation closely and using it as a reference.", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/How-To/How To Add a FusedMoEPermuteExpertsUnpermute Type", "level": 3, "text": "### How To Add a FusedMoEPermuteExpertsUnpermute Type  \nFusedMoEPermuteExpertsUnpermute performs the core of the FusedMoE operations. The various functions exposed by the abstract class and their significance is as follows,  \n`FusedMoEPermuteExpertsUnpermute::activation_formats()`: Return the supported Input and Output activation formats. i.e. Contiguous / Batched format.  \n`FusedMoEPermuteExpertsUnpermute::supports_chunking()`: Return True if the implementation supports chunking. Typically\nimplementations that input `FusedMoEActivationFormat.Standard` support chunking and `FusedMoEActivationFormat.BatchedExperts` do not.  \n`FusedMoEPermuteExpertsUnpermute::supports_expert_map()`: Return True if the implementation supports expert map.  \n`FusedMoEPermuteExpertsUnpermute::workspace_shapes()` /\n`FusedMoEPermuteExpertsUnpermute::finalize_weight_and_reduce_impl` /\n`FusedMoEPermuteExpertsUnpermute::apply`: Refer to `FusedMoEPermuteExpertsUnpermute` section above.", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/How-To/FusedMoEModularKernel Initialization", "level": 3, "text": "### FusedMoEModularKernel Initialization  \n`FusedMoEMethodBase` class has 3 methods that are collectively responsible in creating the `FusedMoEModularKernel` object. They are,  \n* maybe_make_prepare_finalize,\n* select_gemm_impl, and\n* init_prepare_finalize  \n#### maybe_make_prepare_finalize  \nThe `maybe_make_prepare_finalize` method is responsible for constructing an instance of `FusedMoEPrepareAndFinalize` when appropriate based on the current all2all backend, e.g. when EP + DP is enabled.  The base class method currently constructs all the `FusedMoEPrepareAndFinalize` objects for the EP+DP case.  Derived classes can override this method to construct prepare/finalize objects for different scenarios, e.g. `ModelOptNvFp4FusedMoE` can construct a `FlashInferCutlassMoEPrepareAndFinalize` for the EP+TP case.\nPlease refer to the implementations in,  \n* `ModelOptNvFp4FusedMoE`  \n#### select_gemm_impl", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/How-To/FusedMoEModularKernel Initialization", "level": 3, "text": "* `ModelOptNvFp4FusedMoE`  \n#### select_gemm_impl  \nThe `select_gemm_impl` method is undefined in the base class. It is the responsibility of the derived class to implement a method that constructs a valid/appropriate `FusedMoEPermuteExpertsUnpermute` object.\nPlease refer to the implementations in,  \n* `UnquantizedFusedMoEMethod`\n* `CompressedTensorsW8A8Fp8MoEMethod`\n* `CompressedTensorsW8A8Fp8MoECutlassMethod`\n* `Fp8MoEMethod`\n* `ModelOptNvFp4FusedMoE`\nderived classes.  \n#### init_prepare_finalize  \nBased on the input and env settings, the `init_prepare_finalize` method creates the appropriate `FusedMoEPrepareAndFinalize` object. The method then queries `select_gemm_impl` for the appropriate `FusedMoEPermuteExpertsUnpermute` object and builds the `FusedMoEModularKernel` object  \nPlease take a look at [init_prepare_finalize](https://github.com/vllm-project/vllm/blob/1cbf951ba272c230823b947631065b826409fa62/vllm/model_executor/layers/fused_moe/layer.py#L188).", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/How-To/FusedMoEModularKernel Initialization", "level": 3, "text": "**Important**: The `FusedMoEMethodBase` derived classes use the `FusedMoEMethodBase::fused_experts` object in their `apply` methods. When settings permit the construction of a valid `FusedMoEModularKernel` object, we override `FusedMoEMethodBase::fused_experts` with it. This essentially makes the derived classes agnostic to what FusedMoE implementation is used.", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/How-To/How To Unit Test", "level": 3, "text": "### How To Unit Test  \nWe have `FusedMoEModularKernel` unit tests at [test_modular_kernel_combinations.py](gh-file:tests/kernels/moe/test_modular_kernel_combinations.py).  \nThe unit test iterates through all combinations of `FusedMoEPrepareAndFinalize` and `FusedMoEPremuteExpertsUnpermute` types and if they are\ncompatible, runs some correctness tests.\nIf you are adding some `FusedMoEPrepareAndFinalize` / `FusedMoEPermuteExpertsUnpermute` implementations,  \n1. Add the implementation type to `MK_ALL_PREPARE_FINALIZE_TYPES` and `MK_FUSED_EXPERT_TYPES` in [mk_objects.py](gh-file:tests/kernels/moe/modular_kernel_tools/mk_objects.py) respectively.\n2. Update `Config::is_batched_prepare_finalize()`, `Config::is_batched_fused_experts()`, `Config::is_standard_fused_experts()`,\n`Config::is_fe_16bit_supported()`,  `Config::is_fe_fp8_supported()`, `Config::is_fe_block_fp8_supported()`,", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/How-To/How To Unit Test", "level": 3, "text": "`Config::is_fe_supports_chunking()` methods in [/tests/kernels/moe/modular_kernel_tools/common.py](gh-file:tests/kernels/moe/modular_kernel_tools/common.py)  \nDoing this will add the new implementation to the test suite.", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/How-To/How To Check `FusedMoEPrepareAndFinalize` & `FusedMoEPermuteExpertsUnpermute` Compatibility", "level": 3, "text": "### How To Check `FusedMoEPrepareAndFinalize` & `FusedMoEPermuteExpertsUnpermute` Compatibility  \nThe unit test file [test_modular_kernel_combinations.py](gh-file:tests/kernels/moe/test_modular_kernel_combinations.py) can also be executed as a standalone script.\nExample: `python3 -m tests.kernels.moe.test_modular_kernel_combinations --pf-type PplxPrepareAndFinalize --experts-type BatchedTritonExperts`\nAs a side effect, this script can be used to test `FusedMoEPrepareAndFinalize` & `FusedMoEPermuteExpertsUnpermute` compatibility. When invoked\nwith incompatible types, the script will error.", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/How-To/How To Profile", "level": 3, "text": "### How To Profile  \nPlease take a look at [profile_modular_kernel.py](gh-file:tests/kernels/moe/modular_kernel_tools/profile_modular_kernel.py)\nThe script can be used to generate Torch traces for a single `FusedMoEModularKernel::forward()` call for any compatible\n`FusedMoEPrepareAndFinalize` and `FusedMoEPermuteExpertsUnpermute` types.\nExample: `python3 -m tests.kernels.moe.modular_kernel_tools.profile_modular_kernel --pf-type PplxPrepareAndFinalize --experts-type BatchedTritonExperts`", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/FusedMoEPrepareAndFinalize Implementations", "level": 2, "text": "## FusedMoEPrepareAndFinalize Implementations  \nSee [Fused MoE Kernel features](./moe_kernel_features.md#fused-moe-modular-all2all-backends) for a list of all the available modular prepare and finalize subclasses.", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "b13c12f08b6f124d4247ea315739b46c714d231ceb40ba772ba9dd4b143e2dac", "heading": "Fused MoE Modular Kernel/FusedMoEPermuteExpertsUnpermute", "level": 2, "text": "## FusedMoEPermuteExpertsUnpermute  \nSee [Fused MoE Kernel features](./moe_kernel_features.md#fused-moe-experts-kernels) for a list of all the available modular experts.", "file_path": "design/fused_moe_modular_kernel.md"}
{"id": "379f53afd946ec894870d9c56afcad36b99b118b6d5a8934aff3b03ed5aa1687", "heading": "Integration with Hugging Face", "level": 1, "text": "# Integration with Hugging Face  \nThis document describes how vLLM integrates with Hugging Face libraries. We will explain step by step what happens under the hood when we run `vllm serve`.  \nLet's say we want to serve the popular Qwen model by running `vllm serve Qwen/Qwen2-7B`.  \n1. The `model` argument is `Qwen/Qwen2-7B`. vLLM determines whether this model exists by checking for the corresponding config file `config.json`. See this [code snippet](https://github.com/vllm-project/vllm/blob/10b67d865d92e376956345becafc249d4c3c0ab7/vllm/transformers_utils/config.py#L162-L182) for the implementation. Within this process:\n- If the `model` argument corresponds to an existing local path, vLLM will load the config file directly from this path.", "file_path": "design/huggingface_integration.md"}
{"id": "379f53afd946ec894870d9c56afcad36b99b118b6d5a8934aff3b03ed5aa1687", "heading": "Integration with Hugging Face", "level": 1, "text": "- If the `model` argument is a Hugging Face model ID consisting of a username and model name, vLLM will first try to use the config file from the Hugging Face local cache, using the `model` argument as the model name and the `--revision` argument as the revision. See [their website](https://huggingface.co/docs/huggingface_hub/en/package_reference/environment_variables#hfhome) for more information on how the Hugging Face cache works.", "file_path": "design/huggingface_integration.md"}
{"id": "379f53afd946ec894870d9c56afcad36b99b118b6d5a8934aff3b03ed5aa1687", "heading": "Integration with Hugging Face", "level": 1, "text": "- If the `model` argument is a Hugging Face model ID but it is not found in the cache, vLLM will download the config file from the Hugging Face model hub. Refer to [this function](https://github.com/vllm-project/vllm/blob/10b67d865d92e376956345becafc249d4c3c0ab7/vllm/transformers_utils/config.py#L91) for the implementation. The input arguments include the `model` argument as the model name, the `--revision` argument as the revision, and the environment variable `HF_TOKEN` as the token to access the model hub. In our case, vLLM will download the [config.json](https://huggingface.co/Qwen/Qwen2-7B/blob/main/config.json) file.  \n2. After confirming the existence of the model, vLLM loads its config file and converts it into a dictionary. See this [code snippet](https://github.com/vllm-project/vllm/blob/10b67d865d92e376956345becafc249d4c3c0ab7/vllm/transformers_utils/config.py#L185-L186) for the implementation.", "file_path": "design/huggingface_integration.md"}
{"id": "379f53afd946ec894870d9c56afcad36b99b118b6d5a8934aff3b03ed5aa1687", "heading": "Integration with Hugging Face", "level": 1, "text": "3. Next, vLLM [inspects](https://github.com/vllm-project/vllm/blob/10b67d865d92e376956345becafc249d4c3c0ab7/vllm/transformers_utils/config.py#L189) the `model_type` field in the config dictionary to [generate](https://github.com/vllm-project/vllm/blob/10b67d865d92e376956345becafc249d4c3c0ab7/vllm/transformers_utils/config.py#L190-L216) the config object to use. There are some `model_type` values that vLLM directly supports; see [here](https://github.com/vllm-project/vllm/blob/10b67d865d92e376956345becafc249d4c3c0ab7/vllm/transformers_utils/config.py#L48) for the list. If the `model_type` is not in the list, vLLM will use [AutoConfig.from_pretrained](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoConfig.from_pretrained) to load the config class, with `model`, `--revision`, and `--trust_remote_code` as the arguments. Please note that:", "file_path": "design/huggingface_integration.md"}
{"id": "379f53afd946ec894870d9c56afcad36b99b118b6d5a8934aff3b03ed5aa1687", "heading": "Integration with Hugging Face", "level": 1, "text": "- Hugging Face also has its own logic to determine the config class to use. It will again use the `model_type` field to search for the class name in the transformers library; see [here](https://github.com/huggingface/transformers/tree/main/src/transformers/models) for the list of supported models. If the `model_type` is not found, Hugging Face will use the `auto_map` field from the config JSON file to determine the class name. Specifically, it is the `AutoConfig` field under `auto_map`. See [DeepSeek](https://huggingface.co/deepseek-ai/DeepSeek-V2.5/blob/main/config.json) for an example.\n- The `AutoConfig` field under `auto_map` points to a module path in the model's repository. To create the config class, Hugging Face will import the module and use the `from_pretrained` method to load the config class. This can generally cause arbitrary code execution, so it is only executed when `--trust_remote_code` is enabled.", "file_path": "design/huggingface_integration.md"}
{"id": "379f53afd946ec894870d9c56afcad36b99b118b6d5a8934aff3b03ed5aa1687", "heading": "Integration with Hugging Face", "level": 1, "text": "4. Subsequently, vLLM applies some historical patches to the config object. These are mostly related to RoPE configuration; see [here](https://github.com/vllm-project/vllm/blob/127c07480ecea15e4c2990820c457807ff78a057/vllm/transformers_utils/config.py#L244) for the implementation.", "file_path": "design/huggingface_integration.md"}
{"id": "379f53afd946ec894870d9c56afcad36b99b118b6d5a8934aff3b03ed5aa1687", "heading": "Integration with Hugging Face", "level": 1, "text": "5. Finally, vLLM can reach the model class we want to initialize. vLLM uses the `architectures` field in the config object to determine the model class to initialize, as it maintains the mapping from architecture name to model class in [its registry](https://github.com/vllm-project/vllm/blob/127c07480ecea15e4c2990820c457807ff78a057/vllm/model_executor/models/registry.py#L80). If the architecture name is not found in the registry, it means this model architecture is not supported by vLLM. For `Qwen/Qwen2-7B`, the `architectures` field is `[\"Qwen2ForCausalLM\"]`, which corresponds to the `Qwen2ForCausalLM` class in [vLLM's code](https://github.com/vllm-project/vllm/blob/127c07480ecea15e4c2990820c457807ff78a057/vllm/model_executor/models/qwen2.py#L364). This class will initialize itself depending on various configs.  \nBeyond that, there are two more things vLLM depends on Hugging Face for.", "file_path": "design/huggingface_integration.md"}
{"id": "379f53afd946ec894870d9c56afcad36b99b118b6d5a8934aff3b03ed5aa1687", "heading": "Integration with Hugging Face", "level": 1, "text": "1. **Tokenizer**: vLLM uses the tokenizer from Hugging Face to tokenize the input text. The tokenizer is loaded using [AutoTokenizer.from_pretrained](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained) with the `model` argument as the model name and the `--revision` argument as the revision. It is also possible to use a tokenizer from another model by specifying the `--tokenizer` argument in the `vllm serve` command. Other relevant arguments are `--tokenizer-revision` and `--tokenizer-mode`. Please check Hugging Face's documentation for the meaning of these arguments. This part of the logic can be found in the [get_tokenizer](https://github.com/vllm-project/vllm/blob/127c07480ecea15e4c2990820c457807ff78a057/vllm/transformers_utils/tokenizer.py#L87) function. After obtaining the tokenizer, notably, vLLM will cache some expensive attributes of the tokenizer in", "file_path": "design/huggingface_integration.md"}
{"id": "379f53afd946ec894870d9c56afcad36b99b118b6d5a8934aff3b03ed5aa1687", "heading": "Integration with Hugging Face", "level": 1, "text": "vLLM will cache some expensive attributes of the tokenizer in [get_cached_tokenizer](https://github.com/vllm-project/vllm/blob/127c07480ecea15e4c2990820c457807ff78a057/vllm/transformers_utils/tokenizer.py#L24).", "file_path": "design/huggingface_integration.md"}
{"id": "379f53afd946ec894870d9c56afcad36b99b118b6d5a8934aff3b03ed5aa1687", "heading": "Integration with Hugging Face", "level": 1, "text": "2. **Model weight**: vLLM downloads the model weight from the Hugging Face model hub using the `model` argument as the model name and the `--revision` argument as the revision. vLLM provides the argument `--load-format` to control what files to download from the model hub. By default, it will try to load the weights in the safetensors format and fall back to the PyTorch bin format if the safetensors format is not available. We can also pass `--load-format dummy` to skip downloading the weights.\n- It is recommended to use the safetensors format, as it is efficient for loading in distributed inference and also safe from arbitrary code execution. See the [documentation](https://huggingface.co/docs/safetensors/en/index) for more information on the safetensors format. This part of the logic can be found [here](https://github.com/vllm-project/vllm/blob/10b67d865d92e376956345becafc249d4c3c0ab7/vllm/model_executor/model_loader/loader.py#L385). Please note that:", "file_path": "design/huggingface_integration.md"}
{"id": "379f53afd946ec894870d9c56afcad36b99b118b6d5a8934aff3b03ed5aa1687", "heading": "Integration with Hugging Face", "level": 1, "text": "This completes the integration between vLLM and Hugging Face.  \nIn summary, vLLM reads the config file `config.json`, tokenizer, and model weight from the Hugging Face model hub or a local directory. It uses the config class from either vLLM, Hugging Face transformers, or loads the config class from the model's repository.", "file_path": "design/huggingface_integration.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager", "level": 1, "text": "# Hybrid KV Cache Manager  \n!!! warning\nThis document was written based on commit [458e74](https://github.com/vllm-project/vllm/commit/458e74eb907f96069e6d8a4f3c9f457001fef2ea). This feature is still in its early stage and things may change.", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/What is a hybrid model?", "level": 2, "text": "## What is a hybrid model?  \nMany recent \"hybrid\" LLMs combine multiple attention types within one model. For example:  \n1. Sliding window attention (sw) + full attention (full): gpt-oss, Gemma 2/3, Ministral, cohere, etc.\n2. Mamba + full: Bamba, Jamba, Minimax, etc.\n3. Local chunked attention + full: Llama4  \nTo serve these models efficiently, our [KVCacheManager][vllm.v1.core.kv_cache_manager.KVCacheManager] must:  \n1. Allocate different slots to different layer type, for example:\n- Full attention layers: reserve slots for **all** tokens.\n- Sliding window layers: reserve slots only for the most recent **`sliding_window_size`** tokens.\n2. Support layer-specific prefix-cache rules, for example:\n- Full attention: a cache hit prefix requires **all** tokens remain in the KV cache.\n- Sliding window: a cache hit prefix only requires the last **`sliding_window_size`** tokens remain in the KV cache.", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Definitions", "level": 2, "text": "## Definitions  \n1. **kv hidden size**: The number of bytes to store one token's KV cache for a single layer.\n2. **block**: the memory reserved for kv cache are divided into multiple *blocks* with the same *page size* (defined below)\n3. **block size**: number of tokens inside a block\n4. **page size**: the physical memory size of a block, defined as:  \n$$\n\\text{num_layers} \\times \\text{block_size} \\times \\text{kv_hidden_size}\n$$  \n`num_layers` doesn't mean the total number of layers in the model. The exact number depends on the context in this doc.  \n!!! note\nThis is different from `KVCacheSpec.page_size_bytes` in the code, which is defined as:  \n$$\n\\text{block_size} \\times \\text{kv_hidden_size}\n$$", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Allocation/High level idea", "level": 3, "text": "## Allocation  \n### High level idea  \nWe use a single memory pool for all layer types. The memory pool is split into multiple blocks with the same page size. [KVCacheManager][vllm.v1.core.kv_cache_manager.KVCacheManager] allocates different numbers of blocks to different layers according to its attention type.  \nThe core challenge is ensuring every layer type uses the same **page size**.  For full-attention-only models, the page size is straightforward, defined as:  \n$$\n\\text{page_size} = \\text{block_size} \\times \\text{num_hidden_layers} \\times \\text{kv_hidden_size}\n$$  \nHowever, in hybrid models, `num_hidden_layers` varies by attention type, which would normally produce mismatched page sizes. The cases below show how we unify them.", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Allocation/Case 1: toy model", "level": 3, "text": "### Case 1: toy model  \nLet's start with a toy example: a model has 1 full attention layer and 3 sliding window attention layers. All layers have the same `kv_hidden_size`.  \nWe let each block to hold `block_size` tokens for one layer, so:  \n$$\n\\text{page_size} = \\text{kv_hidden_size} \\times \\text{block_size}\n$$  \n[KVCacheManager][vllm.v1.core.kv_cache_manager.KVCacheManager] allocates a different number of blocks to each layer.  \nThis case is only a toy example. For real models, please refer to the following cases.", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Allocation/Case 2: same `kv_hidden_size` and a regular pattern", "level": 3, "text": "### Case 2: same `kv_hidden_size` and a regular pattern  \nWhen the model has more layers, e.g., 20 sliding window attention layers and 10 full attention layers with the same `kv_hidden_size`. Calling the allocator once per layer (30 calls) is OK but becomes inefficient. As a solution, we group the allocation of layers that need the same number of blocks to reduce the number of calls.  \nThe grouping is feasible because there is usually a beautiful ratio between the number of different types of layers. For example:  \n- Gemma-2: 1 sw : 1 full\n- Llama 4: 3 local : 1 full  \nOur example can be regarded as 2 sw : 1 full. We can allocate blocks as if there are 2 sw and 1 full in the model, and repeat the result by 10 times to generate the `block_ids` for the 30 layers. The page size becomes:  \n$$\n10 \\times \\text{kv_hidden_size} \\times \\text{block_size}\n$$", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Allocation/Case 2: same `kv_hidden_size` and a regular pattern", "level": 3, "text": "10 \\times \\text{kv_hidden_size} \\times \\text{block_size}\n$$  \nAssume `block_size` 16, sliding window size 32, request length 112, then for the above example model, we need to allocate 11 blocks (0-6 for full, 7-8 for sw group 1, 9-10 for sw group 2).  \n![Allocation Result](../assets/design/hybrid_kv_cache_manager/basic_grouping_example.png)  \nHere, \"/\" denotes no block needed (slidingâ€‘window layers don't need slots for early tokens).  \nSee the formal definition below. The layers are divided into multiple *KV Cache Groups* so that there is:  \n1. **Identical attention type inside each group**: Each group only contains layers with the same attention type and thus need the same number of blocks for a given request. This enables layers in the same group share the same block ids without memory waste.\n2. **Identical page size across groups**: Because our memory pool only have one page size.  \nOur example model is divided into 3 KV cache groups:  \n- Group 0: 10 full attention layers (full.0 - full.9)", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Allocation/Case 2: same `kv_hidden_size` and a regular pattern", "level": 3, "text": "- Group 0: 10 full attention layers (full.0 - full.9)\n- Group 1: 10 sliding window attention layers (sw.0 - sw.9)\n- Group 2: 10 sliding window attention layers (sw.10 - sw.19)  \nObviously, it satisfies rule 1. For rule 2, all 3 groups have  \n$$\n10 \\times \\text{kv_hidden_size} \\times \\text{block_size}\n$$  \nas their page size.", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Allocation/Case 3: same `kv_hidden_size` and no regular pattern", "level": 3, "text": "### Case 3: same `kv_hidden_size` and no regular pattern  \nUnfortunately, not all models have such a beautiful ratio, and approach in Case 2 will produce too many small groups. For example, Gemma-3-27b has 52 sliding window attention layers and 10 full attention layers. With the constraints in case 2, it would be 26 sliding window groups and 5 full attention groups, each contains 2 layers. The allocation is still inefficient. To reduce the number of kv cache groups, we group layers using the smallest layer count among all attention types. For example, min(52, 10)=10 layers per group in Gemma-3-27b. Then the grouping result is:  \n- Group 0: 10 full attention layers (full.0 - full.9)\n- Group 1: 10 sliding window attention layers (sw.0 - sw.9)\n- Group 2: 10 sliding window attention layers (sw.10 - sw.19)\n- ...\n- Group 6: 10 sliding window attention layers (sw.40 - sw.49)\n- Group 7: 2 sliding window attention layers (sw.50 - sw.51) and 8 padding layers", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Allocation/Case 3: same `kv_hidden_size` and no regular pattern", "level": 3, "text": "We will update this algorithm if this heuristic leads to a bad result when a new model comes out (e.g., 20 full + 30 sw, the group size should be 10 instead of 20).  \nThis case happens in Gemma-3 series models, and models in case 2 but with eagle speculative decoding which introduce one full attention layer. The solution has some memory waste and is not perfect. Please report any cases where padding overhead becomes unacceptable so we can refine the algorithm.", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Allocation/Case 4: different `kv_hidden_size` (mainly hybrid mamba models)", "level": 3, "text": "### Case 4: different `kv_hidden_size` (mainly hybrid mamba models)  \nSome architectures (e.g., Bamba, Jamba, Minimax) interleave standard attention layers with Mamba layers, where each Mamba layer's state size per token can be much larger than the attention layers' `kv_hidden_size`. Because we only support a single page size across all groups, we must reconcile these differing hidden sizes.  \nThe current algorithm is:  \n1. Increase the `block_size` of attention layers until\n$$\n\\text{block_size} \\times \\text{kv_hidden_size}_{\\text{att}} \\ge \\text{state_size}_{\\text{mamba}}\n$$\n2. Pad the mamba state per layer to\n$$\n\\text{block_size} \\times \\text{kv_hidden_size}_{\\text{att}}\n$$\n3. Apply the grouping strategy in case 3.  \n!!! note\nThis can lead to more than 400 `block_size` for attention layers, which is too large. Another padding strategy is to increase `block_size` until  \n$$\n\\text{block_size} \\times \\text{kv_hidden_size}_{\\text{att}} \\times \\text{num_attn_layers} \\ge \\text{state_size}_{\\text{mamba}}\n$$", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Allocation/Case 4: different `kv_hidden_size` (mainly hybrid mamba models)", "level": 3, "text": "$$  \nThis padding strategy is still a work in progress.", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Allocation/Case 5: KV sharing", "level": 3, "text": "### Case 5: KV sharing  \nKV sharing refers to a layer using the KV cache of another layer, e.g., gemma-3n.\nIn these models, [KVCacheManager][vllm.v1.core.kv_cache_manager.KVCacheManager] ignores all layers with kv sharing and only allocates KV cache for layers that need kv cache, and some patches are made in model runner to apply the allocation result to kv sharing layers.", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Prefix caching", "level": 2, "text": "## Prefix caching  \nFor simplicity, we assume `block_size=1` in this section.", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Prefix caching/High level idea", "level": 3, "text": "### High level idea  \nThe block pool uses a dict similar to `tuple(block_hash, group_id) -> block` to catch the full blocks. That means the same tokens of different groups are cached and evicted independently.  \nWhen a new request comes in, we check the cache hit prefix of each group, and return the intersection of these groups as the cached prefix of the request. See below for the detailed algorithm for checking the cache hit of one group & performing the intersection.", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Prefix caching/Case 0: full attention only models", "level": 3, "text": "### Case 0: full attention only models  \nFor full attention layers, blocks are allocated for all tokens in the request. For details on the underlying design, see [Prefix Caching](prefix_caching.md)  \nTo find the longest cache hit prefix of a request, we enumerate from left (the first block) to right (the last block), checking whether the block is cached, and exit when cache misses. For example, we will return the first 7 tokens (0-6) as the cache hit prefix in the below example (blue blocks are cached):  \n![Prefix Caching of Full Attention](../assets/design/hybrid_kv_cache_manager/full_attn.png)", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Prefix caching/Case 1: sliding window attention only models", "level": 3, "text": "### Case 1: sliding window attention only models  \nFor sliding window attention layers, a naive implementation for memory allocation is to allocate `sliding_window_size` blocks and fill in the blocks in a round-robin way. But this naive implementation is not compatible with prefix caching so we didn't pick this design. In vLLM,  we allocate different blocks for different tokens and free blocks that are outside the sliding window.  \nFor a new request, the cache hit prefix only requires the last `sliding_window_size - 1` tokens being cached.\nLet's say `sliding_window_size = 4` and `block_size = 1`, and the request is a 15-token prompt (blue blocks are cached):  \n![Prefix Caching of Sliding Window Attention](../assets/design/hybrid_kv_cache_manager/sw_attn.png)  \nThere are 3 possible cache hit prefixes:  \n- cache hit length 5, compute prefill with [2, 3, 4] â†’ [5, 6, â€¦, 14]\n- cache hit length 6, compute prefill with [3, 4, 5] â†’ [6, 7, â€¦, 14]", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Prefix caching/Case 1: sliding window attention only models", "level": 3, "text": "- cache hit length 14, compute prefill with [11, 12, 13] â†’ [14] (most efficient)  \nWe can check the cache hit from right to left, and early exit when we find a match.This is opposite from full attention, where we check from left to right and early exit when the match fails. One potential cons (compared to full attention) is that we end up iterating over the entire list of tokens when there's no match, which is often a common case. This could potentially cause non-negligible overheads, but fine with full + swa, as discussed below.", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Prefix caching/Case 2: sliding window attention + full attention models", "level": 3, "text": "### Case 2: sliding window attention + full attention models  \nThe first problem is how to find the cache hit prefix. We need to \"intersect\" the cache hits of global and sliding window attention layers by:  \n1. Get the longest cache hit for full attention (scanning from left to right)\n2. Get the longest cache hit for sliding window attention that is within that length. Implemented by checking cache hits from right to left starting from the cache hit length of full attention.  \nIt can be ensured that the resulting cache hit of sliding window attention layers is also a cache hit of full attention layers. This is more efficient than finding all possible prefixes of each group and doing the intersection, because our approach can exit early if there is no cache hit.", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Prefix caching/Case 2: sliding window attention + full attention models", "level": 3, "text": "The algorithm applies to models with exactly two attention types full attention + X, where X can be an arbitrary efficient attention algorithm like sliding window, llama 4 local attention, and mamba. It doesn't support models without full attention layers, and models with more than 2 types of attention. This is enough for most hybrid models at the moment of writing this doc.  \nThe second question is the cache eviction policy. For now, we use one LRU queue for all kv cache groups. The blocks are added to the LRU queue when freed, either because the request is finished or the block is out of the sliding window.", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Prefix caching/Case 3: mamba models", "level": 3, "text": "### Case 3: mamba models  \nThe prefix caching support of the mamba model is work in progress. Once implemented, models with mamba layer + full attention layer can be supported via the full attention + X algorithm in case 2.", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Implementation/Overview", "level": 3, "text": "## Implementation  \n### Overview  \n![Overview of Hybrid KV Cache Manager](../assets/design/hybrid_kv_cache_manager/overview.png)  \nThe `KVCacheManager` is organized into 3 layers:  \n- **[KVCacheManager][vllm.v1.core.kv_cache_manager.KVCacheManager]**: The interface between the scheduler and kv cache management system.\n- **[KVCacheCoordinator][vllm.v1.core.kv_cache_coordinator.KVCacheCoordinator]**: coordinate per-group SingleTypeKVCacheManagers to generate the allocation result of a request. Depending on the model's configuration, one of these coordinators is chosen:\n- **[KVCacheCoordinatorNoPrefixCache][vllm.v1.core.kv_cache_coordinator.KVCacheCoordinatorNoPrefixCache]**: Used when prefix caching is disabled.\n- **[UnitaryKVCacheCoordinator][vllm.v1.core.kv_cache_coordinator.UnitaryKVCacheCoordinator]**: If only one KV cache group. The prefix caching logic is simplified as no intersection is needed.", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Implementation/Overview", "level": 3, "text": "- **[HybridKVCacheCoordinator][vllm.v1.core.kv_cache_coordinator.HybridKVCacheCoordinator]**: Handles exactly two KV cache groups (must include one fullâ€‘attention group plus one other efficientâ€‘attention group). Other cases are not implemented. You can disable prefix caching to use the KVCacheCoordinatorNoPrefixCache.\n- **[SingleTypeKVCacheManager][vllm.v1.core.single_type_kv_cache_manager.SingleTypeKVCacheManager]**: Each instance manages allocation and prefix caching for one KV cache group, implementing the attentionâ€‘typeâ€“specific logic (e.g., full attention, sliding window, Mamba).  \nThe blue box in the above figure shows the case with 10 full attention layers and 20 sliding window attention layers, thus:  \n- use `HybridKVCacheCoordinator`\n- use 1 `FullAttentionManager` and 2 `SlidingWindowManager` for the 3 `KVCacheGroup`s.", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Implementation/Memory Layout", "level": 3, "text": "### Memory Layout  \nFor a model with n `KVCacheGroup`s, each with m layers, we allocate m buffers. Each buffer is shared by n layers, one from each group.  \nThe following figure is for a model with 10 full attention layers (full.0 - full.9) and 20 sliding window attention layers (sw.0-sw.19). It follows \"case 2\" in \"Allocation\" section and is divided into 3 groups:  \n- Group 0: 10 full attention layers (full.0 - full.9)\n- Group 1: 10 sliding window attention layers (sw.0 - sw.9)\n- Group 2: 10 sliding window attention layers (sw.10 - sw.19)  \nAnd for a request, we allocate 11 blocks with `block_id` 0-6 to group 0, 7-8 to group 1, and 9-10 to group 2.", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "8af5dcc596aa9bb8668824ed396fbf8d8d44b660e32ac031319394996de028e3", "heading": "Hybrid KV Cache Manager/Implementation/Memory Layout", "level": 3, "text": "With such an example, the physical memory is divided into 10 buffers (`KVCacheTensor` 0 - `KVCacheTensor` 9). Each buffer is shared by 3 layers (e.g., `KVCacheTensor` 0 is shared by full.0 from group 0, sw.0 from group 1, and sw.10 from group 2) and is divided into pieces with size `block_size * kv_hidden_size`. The KV cache of these 3 attention layers are saved to different pieces of the buffer based on the allocated `block_ids`:  \n![Example Memory Layout](../assets/design/hybrid_kv_cache_manager/memory_layout.png)  \n!!! note\nOne logic \"block\" is mapped to 10 pieces in the 10 buffers of the physical memory.", "file_path": "design/hybrid_kv_cache_manager.md"}
{"id": "87a53f8239037a5f56676c464a116582d953457abfc9a2b9a5fcb08b2d946556", "heading": "IO Processor Plugins", "level": 1, "text": "# IO Processor Plugins  \nIO Processor plugins are a feature that allows pre and post processing of the model input and output for pooling models. The idea is that users are allowed to pass a custom input to vLLM that is converted into one or more model prompts and fed to the model `encode` method. One potential use-case of such plugins is that of using vLLM for generating multi-modal data. Say users feed an image to vLLM and get an image in output.  \nWhen performing an inference with IO Processor plugins, the prompt type is defined by the plugin and the same is valid for the final request output. vLLM does not perform any validation of input/output data, and it is up to the plugin to ensure the correct data is being fed to the model and returned to the user. As of now these plugins support only pooling models and can be triggered via the `encode` method in `LLM` and `AsyncLLM`, or in online serving mode via the `/pooling` endpoint.", "file_path": "design/io_processor_plugins.md"}
{"id": "87a53f8239037a5f56676c464a116582d953457abfc9a2b9a5fcb08b2d946556", "heading": "IO Processor Plugins/Writing an IO Processor Plugin", "level": 2, "text": "## Writing an IO Processor Plugin  \nIO Processor plugins implement the `IOProcessor` interface (<gh-file:vllm/plugins/io_processors/interface.py>):  \n```python\nIOProcessorInput = TypeVar(\"IOProcessorInput\")\nIOProcessorOutput = TypeVar(\"IOProcessorOutput\")\n\nclass IOProcessor(ABC, Generic[IOProcessorInput, IOProcessorOutput]):\n\ndef __init__(self, vllm_config: VllmConfig):\nself.vllm_config = vllm_config\n\n@abstractmethod\ndef pre_process(\nself,\nprompt: IOProcessorInput,\nrequest_id: str | None = None,\n**kwargs,\n) -> PromptType | Sequence[PromptType]:\nraise NotImplementedError\n\nasync def pre_process_async(\nself,\nprompt: IOProcessorInput,\nrequest_id: str | None = None,\n**kwargs,\n) -> PromptType | Sequence[PromptType]:\nreturn self.pre_process(prompt, request_id, **kwargs)\n\n@abstractmethod\ndef post_process(\nself,\nmodel_output: Sequence[PoolingRequestOutput],\nrequest_id: str | None = None,\n**kwargs,\n) -> IOProcessorOutput:\nraise NotImplementedError", "file_path": "design/io_processor_plugins.md"}
{"id": "87a53f8239037a5f56676c464a116582d953457abfc9a2b9a5fcb08b2d946556", "heading": "IO Processor Plugins/Writing an IO Processor Plugin", "level": 2, "text": "async def post_process_async(\nself,\nmodel_output: AsyncGenerator[tuple[int, PoolingRequestOutput]],\nrequest_id: str | None = None,\n**kwargs,\n) -> IOProcessorOutput:\ncollected_output = [item async for i, item in model_output]\nreturn self.post_process(collected_output, request_id, **kwargs)\n\n@abstractmethod\ndef parse_request(self, request: Any) -> IOProcessorInput:\nraise NotImplementedError", "file_path": "design/io_processor_plugins.md"}
{"id": "87a53f8239037a5f56676c464a116582d953457abfc9a2b9a5fcb08b2d946556", "heading": "IO Processor Plugins/Writing an IO Processor Plugin", "level": 2, "text": "@abstractmethod\ndef output_to_response(\nself, plugin_output: IOProcessorOutput\n) -> IOProcessorResponse:\nraise NotImplementedError\n```  \nThe `parse_request` method is used for validating the user prompt and converting it into the input expected by the `pre_process`/`pre_process_async` methods.\nThe `pre_process*` methods take the validated plugin input to generate vLLM's model prompts for regular inference.\nThe `post_process*` methods take `PoolingRequestOutput` objects as input and generate a custom plugin output.  \nThe `output_to_response` method is used only for online serving and converts the plugin output to the `IOProcessorResponse` type that is then returned by the API Server. The implementation of the `/io_processor_pooling` serving endpoint is available here <gh-file:vllm/entrypoints/openai/serving_pooling_with_io_plugin.py>.", "file_path": "design/io_processor_plugins.md"}
{"id": "87a53f8239037a5f56676c464a116582d953457abfc9a2b9a5fcb08b2d946556", "heading": "IO Processor Plugins/Writing an IO Processor Plugin", "level": 2, "text": "An example implementation of a plugin that enables generating geotiff images with the PrithviGeospatialMAE model is available [here](https://github.com/christian-pinto/prithvi_io_processor_plugin). Please, also refer to our online (<gh-file:examples/online_serving/prithvi_geospatial_mae.py>) and offline (<gh-file:examples/offline_inference/prithvi_geospatial_mae_io_processor.py>) inference examples.", "file_path": "design/io_processor_plugins.md"}
{"id": "87a53f8239037a5f56676c464a116582d953457abfc9a2b9a5fcb08b2d946556", "heading": "IO Processor Plugins/Using an IO Processor plugin", "level": 2, "text": "## Using an IO Processor plugin  \nIO Processor plugins are loaded at engine startup and there are two methods for specifying the name of the plugin to be loaded:  \n1. Via vLLM's `EngineArgs`: setting the `io_processor_plugin` argument in the `EngineArgs` used to initialize the `AsyncLLM`. The same can be achieved by passing the `io_processor_plugin` argument to `LLM` in offline mode, or by passing the `--io-processor-plugin` argument in serving mode.\n2. Via the model HF configuration: adding an `io_processor_plugin` field to the model config (config.json).  \nThe order also determines method priority. i.e., setting the plugin name via `EngineArgs` will override any plugin name specified in the model HF config (config.json).", "file_path": "design/io_processor_plugins.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors", "level": 1, "text": "# Logits Processors  \n!!! important\nSome logits processors design changes are still in progress and the API may\nchange in the near future. We hope to stabilize this part of the API soon  \nThis document describes how the vLLM engine interacts with logits processors, and the programming model which vLLM supports for implementing logits processors.", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processors Background", "level": 2, "text": "## Logits Processors Background  \nA logits processor adjusts the next-token probability distribution, usually with the intention of steering the model towards a desired type of behavior.  \nIn vLLM, logits processors operate at batch granularity. During a given engine step, the logits processor consumes a `(num_requests) x (vocab_size)` tensor of raw logits output by the model. For all requests which enable the logits processor, the logits processor applies a transformation to the corresponding row of the logits tensor, while leaving other rows unmodified. The transformed logits tensor is then passed to softmax.", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processors in the vLLM engine", "level": 2, "text": "## Logits Processors in the vLLM engine  \nThe vLLM engine's persistent batch data structure maintains a list of loaded logits processors.  \nIn order to operate on the entire batch at once, each logits processor may maintain metadata about the requests in the batch (i.e. each request's logits-processor-specific configuration settings). Therefore, logits processors are stateful.  \nIn each engine step, the vLLM engine will (1) update each logits processor's internal state and (2) apply logits processors to the model output logits.", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processors in the vLLM engine/Updating Logits Processor Internal State", "level": 3, "text": "### Updating Logits Processor Internal State  \nAt the beginning of each engine step, the persistent batch may add, discard and/or reorder requests in response to the scheduler output. After the persistent batch has reorganized, the vLLM engine invokes each logits processor's `update_state()` method. This is necessary to ensure that logits processors' internal states are reorganized to match the new persistent batch state at the beginning of the engine step.  \nThe pseudocode below shows the process by which the vLLM persistent batch notifies each logits processor of changes in batch state:  \n??? code \"Model Runner Updates Logits Processor States\"  \n``` python\n# gpu_model_runner.py\n\nclass GPUModelRunner(...):\n\n...\n\ndef execute_model(self, scheduler_output, ...):\nself._update_states(scheduler_output)\n\n...\n\ndef _update_states(...):\n\n...\n\n# ...update persistent batch to reflect new/finished requests & reordering\n# of requests within batch...\n\n...\n\nself.input_batch.refresh_metadata()\n\n\n# gpu_input_batch.py", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processors in the vLLM engine/Updating Logits Processor Internal State", "level": 3, "text": "self.input_batch.refresh_metadata()\n\n\n# gpu_input_batch.py\n\nclass InputBatch:\n\n...\n\ndef refresh_metadata(self):\n\n...\n\n# Update each logits processor's state to reflect persistent batch state\nbatch_update = self.batch_update_builder.get_and_reset(self.num_reqs)\nfor logit_proc in self.logitsprocs.all:\nlogit_proc.update_state(batch_update)\n\n...\n\n\n# vllm/v1/sample/logits_processor/interface.py\n\n@dataclass(frozen=True)\nclass BatchUpdate:\n# Batch state-change data structure which is passed to logits processors'\n# update_state() methods\n\nbatch_size: int\n\nremoved: Sequence[RemovedRequest]\nadded: Sequence[AddedRequest]\nmoved: Sequence[MovedRequest]\n\n```", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processors in the vLLM engine/Applying Logits Processors to the Model Output Logits", "level": 3, "text": "### Applying Logits Processors to the Model Output Logits  \nAfter updating persistent batch state, the vLLM model runner performs model inference to obtain logits. Then, the model runner invokes the sampler against the logits. In turn, part of the sampler's operation is to invoke the logits processors' `apply()` methods against the model output logit processors, yielding transformed logits (the `apply()` methods may modify the logits in-place or out-of-place, although in-place is more memory-efficient). This process is shown in the pseudocode below.  \nNote that the sampler will access the logits processors via `SamplingMetadata.logitsprocs`. When the vLLM engine constructs `SamplingMetadata` (not shown in the code below), the reference to the list of logits processors is passed from the persistent batch data structure to `SamplingMetadata`.  \n??? code \"Apply logits processors to model output logits\"  \n``` python\n# gpu_model_runner.py\n\nclass GPUModelRunner(...):\n\n...", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processors in the vLLM engine/Applying Logits Processors to the Model Output Logits", "level": 3, "text": "class GPUModelRunner(...):\n\n...\n\ndef execute_model(self, scheduler_output, ...):\n# (discussed in previous section)\nself._update_states(scheduler_output)\n\n...\n\n# ...run model inference to obtain logits...\n\n...\n\n# Invoke sampler, which applies logits processors\nsampler_output = self.sampler(logits=logits,\nsampling_metadata=sampling_metadata)\n\n...\n\n\n# sampler.py\n\nclass Sampler(nn.Module):\n\n...\n\ndef forward(self, logits, sampling_metadata):\n\n...\n\n# Apply non-argmax-invariant logits processors to model output logits\nfor processor in (sampling_metadata.logitsprocs.non_argmax_invariant):\nlogits = processor.apply(logits)\n\nsampled = self.sample(logits, sampling_metadata)\n\n...\n\n# ...return sampler output data structure...\n\n\ndef sample(self, logits, sampling_metadta)\n\n...\n\n# ...exit early if all requests are greedy-sampling...\n\n...\n\n# Apply argmax-invariant logits processors\nfor processor in sampling_metadata.logitsprocs.argmax_invariant:\nlogits = processor.apply(logits)\n\n...", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processors in the vLLM engine/Applying Logits Processors to the Model Output Logits", "level": 3, "text": "# ...perform sampling and return sampling result...\n```  \nAt sampling time, the sampler checks whether all requests in the persistent batch employ greedy sampling. If that is the case, the sampler saves compute by skipping \"argmax-invariant\" logits processors. Here, \"argmax\" is shorthand for the token ID with the highest logit value in a given row of the logits tensor (i.e. the token which the model weighted the highest for a given request).  \n* An **argmax-invariant logits processor** is a logits processor (such as Min-P) which does not modify the argmax. For example, a logits processor which masks out the lowest-probability tokens will not change which token ID has the max logit. Greedy sampling always picks the highest-logit-value token ID, and so conceptually an argmax-invariant logits processor can be skipped for greedy sampling requests.", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processors in the vLLM engine/Applying Logits Processors to the Model Output Logits", "level": 3, "text": "* A **non-argmax-invariant logits processor** is a logits processor which may modify the argmax. For example, a logits processor which masks all tokens except for EOS after a certain number of steps in order to force decoding to terminate might end up masking the max-logit-value token and therefore change the argmax. Conceptually, these logits processors cannot be skipped for greedy sampling requests.  \nThe vLLM logits processor abstraction requires the engine to apply logits processors at batch granularity; therefore in practice the argmax-invariant logits processors can only be skipped when the entire batch uses greedy sampling.", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processor Programming Model", "level": 2, "text": "## Logits Processor Programming Model  \nThe previous sections alluded to the interfaces which vLLM logits processors must support. This section introduces in full the programming model for implementing logits processors that are compatible with the vLLM engine, including the `LogitsProcessor` base class and its interface methods as well as the `BatchUpdate` data structure for representing persistent batch state changes, both of which are shown in the code below:  \n??? code \"`LogitsProcessor` base class and `BatchUpdate` data structure\"  \n``` python\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom dataclasses import dataclass\nfrom enum import Enum, auto\nfrom typing import TYPE_CHECKING\n\nimport torch\n\nfrom vllm import SamplingParams\n\nif TYPE_CHECKING:\nfrom vllm.config import VllmConfig\n\n\nclass MoveDirectionality(Enum):\n# One-way i1->i2 req move within batch\nUNIDIRECTIONAL = auto()\n# Two-way i1<->i2 req swap within batch\nSWAP = auto()", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processor Programming Model", "level": 2, "text": "# (index, params, prompt_tok_ids, output_tok_ids) tuples for new\n# requests added to the batch.\nAddedRequest = tuple[int, SamplingParams, list[int], list[int]]\n\n# (index 1, index 2, directionality) tuples representing\n# one-way moves or two-way swaps of requests in batch\nMovedRequest = tuple[int, int, MoveDirectionality]\n\n# Batch indices of any removed requests.\nRemovedRequest = int\n\n\n@dataclass(frozen=True)\nclass BatchUpdate:\n\"\"\"Persistent batch state change info for logitsprocs\"\"\"\nbatch_size: int  # Current num reqs in batch\n\n# Metadata for requests added to, removed from, and moved\n# within the persistent batch.\n#\n# Key assumption: the `output_tok_ids` list (which is an element of each\n# tuple in `added`) is a reference to the request's running output tokens\n# list; via this reference, the logits processors always see the latest\n# list of generated output tokens\nremoved: Sequence[RemovedRequest]\nmoved: Sequence[MovedRequest]\nadded: Sequence[AddedRequest]\n\n\nclass LogitsProcessor(ABC):", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processor Programming Model", "level": 2, "text": "class LogitsProcessor(ABC):\n\n@abstractmethod\ndef __init__(self, vllm_config: \"VllmConfig\", device: torch.device,\nis_pin_memory: bool) -> None:\nraise NotImplementedError\n\n@abstractmethod\ndef apply(self, logits: torch.Tensor) -> torch.Tensor:\nraise NotImplementedError\n\n@abstractmethod\ndef is_argmax_invariant(self) -> bool:\n\"\"\"True if logits processor has no impact on the\nargmax computation in greedy sampling.\nNOTE: may or may not have the same value for all\ninstances of a given LogitsProcessor subclass,\ndepending on subclass implementation.\n\"\"\"\nraise NotImplementedError\n\n@abstractmethod\ndef update_state(\nself,\nbatch_update: \"BatchUpdate\" | None,\n) -> None:\n\"\"\"Called when there are new output tokens, prior\nto each forward pass.\n\nArgs:\nbatch_update is non-None iff there have been\nchanges to the batch makeup.\n\"\"\"\nraise NotImplementedError", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processor Programming Model", "level": 2, "text": "```  \nA vLLM logits processor must subclass `LogitsProcessor` and define (at minimum) the following methods:  \n* `__init__(self, vllm_config: VllmConfig, device: torch.device, is_pin_memory: bool)`\n* `vllm_config`: engine configuration data structure\n* `device`: hardware accelerator device info\n* `is_pin_memory`: flag indicating whether pin memory is available to support logits processor implementation  \n* `apply(self, logits: torch.Tensor) -> torch.Tensor`:\n* Consume a `(num_requests) x (vocab_size)` logits tensor (`logits`)\n* Apply logits processor transformation at batch granularity\n* Return a transformed `(num_requests) x (vocab_size)` logits tensor\n* You can modify the input logits processors in-place or out-of-place; in-place is more memory-efficient  \n* `is_argmax_invariant(self) -> bool`:\n* Return `True` if the logits processor is argmax invariant (never changes what is the highest-logit-value token ID for a given request), `False` if the logits processor may modify argmax", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processor Programming Model", "level": 2, "text": "* `is_argmax_invariant()` is evaluated once at startup; if `True`, vLLM will skip applying this logits processor in a given step when all requests use greedy sampling  \n* `update_state(self, batch_update: \"BatchUpdate\" | None) -> None`:\n* Consume a `BatchUpdate` data structure representing persistent batch state changes at the beginning of the current engine step\n* Use the `BatchUpdate` members to update logits processor internal state\n* **Note:** batch update data structure may be `None`, signaling no change to the batch constituents. In this case, the LogitsProcessor might still want to update its state based on the updated `output_token_ids` lists that it could have retained when they were added.", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processor Programming Model/`BatchUpdate` data structure", "level": 3, "text": "### `BatchUpdate` data structure  \nThe `BatchUpdate` abstraction models the persistent batch as a list of requests, supporting the following operations to change batch state (note that the order in which the operations are mentioned below reflects the order in which they should be processed in `update_state()`):  \n* **Remove:** remove (without replacement) request at index `i`  \n* A Remove is represented in `Batchupdate.removed` by an `int` (representing `i`)  \n* Effect of remove-at-index on batch:  \n``` text\nBatch: [A,B,C]\nRemove @ i:  1\n\n=>", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processor Programming Model/`BatchUpdate` data structure", "level": 3, "text": "New Batch: [A,x,C] # Discard B and leave an empty slot\n```  \n* **Add:** add (or replace existing request with) a new request at index `i`. If a request is replaced, its associated state should be discarded.  \n* An Add is represented in `Batchupdate.added` as a tuple of  \n``` text\n(index, new request SamplingParams, prompt token ids, output token ids)\n```  \n* `prompt token ids` and `output token ids` are references to the request's prompt token ids and output token ids lists, respectively. Note that the output token ids list grows with each engine step, and this growth is visible to the logits processor because output token ids are passed by reference. **This is important for LogitsProcessors that take into account the tokens generated so far**.", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processor Programming Model/`BatchUpdate` data structure", "level": 3, "text": "* The implementation of the particular logits processor subclass determines whether or how the fields in the added request tuple are digested into an internal representation. For example, a logits processor that does not utilize prompt or output token ids may only need to utilize `index` and `SamplingParams` and discard the other tuple fields  \n* If index `i` currently holds a request, a replacement occurs:  \n``` text\nBatch: [A,B,C]\nNew request to be added @ i: D @ 1", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processor Programming Model/`BatchUpdate` data structure", "level": 3, "text": "=>\n\nNew Batch: [A,D,C] # Add D, discard B\n```  \n* If index `i` does not currently hold a request (because `i` is out of bounds of the current batch size):  \n``` text\nBatch: [A,B,C]\nNew request to be added @ i: D @ 3\n\n=>\n\nNew Batch: [A,B,C,D] # Add D, extending batch\n```  \n* **Move:** move request at index `s` to index `d` OR swap requests at indices `s` and `d`  \n* A Move is represented in `Batchupdate.moved` as a tuple of  \n``` text\n(s, d, UNIDIRECTIONAL or SWAP)\n```  \n* If the Move specifies `UNIDRECTIONAL`:  \n* The request at index `s` is moved to index `d`; index `s` becomes an empty slot  \n``` text\nBatch: [A,x,C,D]\nUnidirectionally Move s -> d:  3 -> 1\n\n=>\n\nNew Batch: [A,D,C,x] # Move D to 1, leaving empty slot at 3\n```  \n* If another request already resided at index `d`, it is replaced and discarded  \n``` text\nBatch: [A,B,C,D]\nUnidirectionally Move s -> d:  3 -> 1\n\n=>", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processor Programming Model/`BatchUpdate` data structure", "level": 3, "text": "=>\n\nNew Batch: [A,D,C,x] # Move D to 1, discarding B and leaving empty slot at 3\n```  \n* If the Move specifies `SWAP`, the requests at `s` and `d` exchange indices  \n``` text\nBatch: [A,B,C,D]\nSwap Move s <-> d:  3 <-> 1\n\n=>\n\nNew Batch: [A,D,C,B] # Swap B and D\n```  \nAdditionally, the `BatchUpdate` data structure includes a representation (`batch_size`) of the size of the persistent batch at the beginning of the engine step.", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processor Programming Model/How the vLLM engine builds the `BatchUpdate` data structure", "level": 3, "text": "### How the vLLM engine builds the `BatchUpdate` data structure  \nLogits processor `update_state()` implementations should assume the following model for how the model runner updates persistent batch state (expressed here in terms of the `BatchUpdate` abstraction):  \n1. Identify indices of requests which finished in the current engine step  \n2. Identify new requests introduced in the current step  \n3. Use Add operations to replace as many finished requests with new requests, in order of increasing index of the replaced request starting with the lowest index  \n4. Based on the relative number of new and finished requests:  \n1. If the numbers of new and finished requests are the same, proceed to next step  \n2. *If there are more new requests than finished requests:* apply Add operations to extend the batch with the remaining new requests which did not replace finished requests. Assign consecutive indices to these new requests, starting with `current_max_batch_index + 1`", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processor Programming Model/How the vLLM engine builds the `BatchUpdate` data structure", "level": 3, "text": "3. *If there are fewer new requests than finished requests:*  \n* Apply Remove operations to finished requests which were not replaced with new requests. These removed request indices will necessarily be greater than the greatest index of the finished requests which were replaced in the previous step. The Removes may leave the batch in a non-contiguous state  \n* **\"Condense\" the batch to be contiguous:** starting with the lowest-index empty slot (which was caused by a Remove), apply a Unidirectional Move from the current highest non-empty slot in the batch to fill the empty slot. Proceed with additional Unidirectional Move operations in order of increasing empty slot destination index and decreasing non-empty slot source index until the batch is contiguous", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processor Programming Model/How the vLLM engine builds the `BatchUpdate` data structure", "level": 3, "text": "* **Shrink the batch:** a side-effect of condensing the batch is that empty slots resulting from Remove operations are grouped in a contiguous block at the end of the batch array. Thus, after condensing, update `BatchUpdate.batch_size` to reflect the number of non-empty slots  \n5. Reorder the batch for improved efficiency. Depending on the attention backend implementation and the current characteristics of the batch, zero or more Swap Move operations may be applied to reorder the batch  \nNotes:  \n* A logits processor `update_state()` method must process batch update operations in the following order: removes, adds, moves  \n* The index argument for Add operations refers to the index *at the time the Add occurred*, i.e. before any Move operations\n* Example: if a request is Added at index 5 and then swapped with index 3, the Add operation in `BatchUpdate.added` will be associated with index 5 not 3\n* In other words Move operations can be assumed to be applied after Adds and Removes", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processor Programming Model/How the vLLM engine builds the `BatchUpdate` data structure", "level": 3, "text": "* Move operations can be assumed to be applied in the order in which they appear in `BatchUpdate.moved`  \n* If there are no new/finished requests and there is no batch reordering, then the batch update for the logits processors will be `None`  \n#### Example: Batch Update with Fewer New Requests Than Finished Requests  \nThe following example models an engine step where 1 new request is introduced and 2 finished requests are eliminated, additionally the attention backend performs a swap to optimize the batch ordering.  \n``` text\nBatch state (beginning of engine step): [A,B,C,D]\nBatch size: 4", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processor Programming Model/How the vLLM engine builds the `BatchUpdate` data structure", "level": 3, "text": "New requests: E\n\nFinished requests: A, C\n\nProcessing steps (using BatchUpdate abstraction):\n\n1. Add E at index 0\n\n[E,B,C,D] # Discard A\nBatch size: 4\n\n2. Remove at index 2\n\n[E,B,x,D] # Discard C, empty slot at index 2\nBatch size: 4\n\n3. Condense batch with a Unidirectional Move 3 -> 2 operation and shrink batch\n\n[E,B,D] x # Empty slot is now outside batch\nBatch size: 3\n\n4. Attention backend optimization: reorder batch with Swap 0 <-> 1\n\n[B,E,D]\nBatch size: 3", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processor Programming Model/How the vLLM engine builds the `BatchUpdate` data structure", "level": 3, "text": "[B,E,D]\nBatch size: 3\n\n```  \nThe resulting `BatchUpdate` data structure will look like  \n``` text\nBatchUpdate instance\n* added: [(0,E's SamplingParams,E's prompt tokens ref,E's output tokens ref)]\n* removed: [2] # request C was removed without replacement\n* moved: [(3,2,UNIDIRECTIONAL),(0,1,SWAP)]\n```  \n#### Example: Batch Update with More New Requests Than Finished Requests  \nThe following example models an engine step where 2 new requests are introduced and 1 finished request is eliminated, additionally the attention backend performs a swap to optimize the batch ordering.  \n``` text\nBatch state (beginning of engine step): [A,B,C,D]\nBatch size: 4\n\nNew requests: E,F\n\nFinished requests: C\n\nProcessing steps (using BatchUpdate abstraction):\n\n1. Add E at index 2\n\n[A,B,E,D] # Discard C\nBatch size: 4\n\n2. Add F at index 4 (current max batch index + 1)\n\n[A,B,E,D,F] # Extend batch by 1\nBatch size: 5\n\n4. Attention backend optimization: reorder batch with Swap 0 <-> 1\n\n[B,A,E,D,F]\nBatch size: 5", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/Logits Processor Programming Model/How the vLLM engine builds the `BatchUpdate` data structure", "level": 3, "text": "[B,A,E,D,F]\nBatch size: 5\n\n```  \nNote that batch condensation is skipped because there are no empty slots left behind by Remove operations.  \nThe resulting `BatchUpdate` data structure will look like  \n``` text\nBatchUpdate instance\n* added: [(2,E's SamplingParams,E's prompt tokens ref,E's output tokens ref),(4,F's SamplingParams,F's prompt tokens ref,F's output tokens ref)]\n* removed: [] # no requests were removed without replacement\n* moved: [(0,1,SWAP)]\n```", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/How to Introduce a New Logits Processor to vLLM/Best Practices for Writing Built-In Logits Processors", "level": 3, "text": "## How to Introduce a New Logits Processor to vLLM  \n### Best Practices for Writing Built-In Logits Processors  \n* Write efficient `apply()` and `update_state()` implementations in light of the fact that logits processors operate at batch granularity\n* For example, you may be able to use efficient vectorized operations to implement `apply()` or update internal state vectors in `update_state()`\n* However, if you think that a logits processor may be used infrequently, it may be appropriate to use a \"sparse\" representation of request state i.e. the class can represent request configuration using a dictionary which only stores metadata about requests that enable the logits processor  \n* It is up to the logits processor author to determine:  \n1. **The per-request attributes which configure the logits processor's behavior against that request.** For example, if you are writing a new built-in logits processor for vLLM, you may or may not need to add additional fields to `SamplingParams` and the vLLM REST API", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/How to Introduce a New Logits Processor to vLLM/Best Practices for Writing Built-In Logits Processors", "level": 3, "text": "2. **The conditions under which the logits processor is or is not enabled on a per-request basis.** Unless your intention is for the built-in logits processor to act on all requests all the time, you should write your logits processor in such a way that it is possible to disable the logits processor for a given request, i.e. by defaulting an argument to `None` or by passing in a specific do-nothing argument value i.e. `0.0`. Try to save compute and memory for requests which disable the logits processor", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/How to Introduce a New Logits Processor to vLLM/Best Practices for Writing Built-In Logits Processors", "level": 3, "text": "3. **The conditions under which the logits processor is short-circuited at the batch level.** Even if you have defined a way to disable the built-in logits processor at the request level, it may be difficult to translate this into compute savings i.e. if your `update_state()` and `apply()` implementations use efficient vectorized implementations that operate on the whole persistent batch in a single command. For example, you cannot skip an entire vectorized operation in `apply()` just because one request disabled the logits processor. To save compute in the edge-case where no running requests utilize the built-in logits processor, we recommend designing `apply()` to return the unmodified input tensor if all requests have the logits processor disabled. Similarly, consider whether steps can be skipped in `update_state()` if no requests enable the logits processor  \n* Additionally, an easy way to save compute in `update_state()` is to exit early when the batch_update is `None`", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/How to Introduce a New Logits Processor to vLLM/Best Practices for Writing Built-In Logits Processors", "level": 3, "text": "* Ensure that the logits processor `update_state` method discards information about finished requests (i.e. requests which are replaced by an Add or which are subject to a Remove)  \n* `is_argmax_invariant()` can be hard-coded to `True` or `False` if the logits processor has consistent behavior. However the argmax invariance may also be determined programmatically (i.e. if your logits processor is user-customizable in some way that impacts whether the logits processor is argmax invariant). For this reason, `is_argmax_invariant()` is not a class method", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/How to Introduce a New Logits Processor to vLLM/Built-In Logits Processors", "level": 3, "text": "### Built-In Logits Processors  \nBuilt-in logits processors are always loaded when the vLLM engine starts. See the existing vLLM built-in logits processors in `vllm/v1/sample/logits_processor/builtin.py` for examples of how to write a new built-in vLLM logits processor. It makes sense to write a PR to introduce a new logits processor as a built-in if it is likely to be useful to a wide audience. vLLM currently employs the following built-in logits processors based on the programming model described above:  \n* Min-P  \n* Logit bias  \n* Min-tokens  \nReview these logits processor implementations for guidance on writing built-in logits processors.  \nAdditionally, the following logits-processor-like functionalities are hard-coded into the sampler and do not yet utilize the programming model described above. Most of them will be refactored to use the aforemented logits processor programming model.  \n* Allowed token IDs  \n* Bad words  \n* Repetition penalty  \n* Frequency penalty  \n* Presence penalty  \n* Temperature", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/How to Introduce a New Logits Processor to vLLM/Built-In Logits Processors", "level": 3, "text": "* Frequency penalty  \n* Presence penalty  \n* Temperature  \n* Top-K  \n* Top-P", "file_path": "design/logits_processors.md"}
{"id": "6ae3a235a56228dc0d048f0192f49bbdd9fa66c76cc54da135d4cc5137c731e9", "heading": "Logits Processors/How to Introduce a New Logits Processor to vLLM/Custom Logits Processors", "level": 3, "text": "### Custom Logits Processors  \nvLLM can be augmented with [user-provided custom logits processors](../features/custom_logitsprocs.md).", "file_path": "design/logits_processors.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics", "level": 1, "text": "# Metrics  \nEnsure the v1 LLM Engine exposes a superset of the metrics available in v0.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Objectives", "level": 2, "text": "## Objectives  \n- Achieve parity of metrics between v0 and v1.\n- The priority use case is accessing these metrics via Prometheus, as this is what we expect to be used in production environments.\n- Logging support (i.e. printing metrics to the info log) is provided for more ad-hoc testing, debugging, development, and exploratory use cases.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Background", "level": 2, "text": "## Background  \nMetrics in vLLM can be categorized as follows:  \n1. Server-level metrics: Global metrics that track the state and performance of the LLM engine. These are typically exposed as Gauges or Counters in Prometheus.\n2. Request-level metrics: Metrics that track the characteristics (e.g. size and timing) of individual requests. These are typically exposed as Histograms in Prometheus and are often the SLOs that an SRE monitoring vLLM will be tracking.  \nThe mental model is that server-level metrics help explain the values of request-level metrics.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Background/v0 Metrics", "level": 3, "text": "### v0 Metrics  \nIn v0, the following metrics are exposed via a Prometheus-compatible `/metrics` endpoint using the `vllm:` prefix:  \n- `vllm:num_requests_running` (Gauge)\n- `vllm:num_requests_swapped` (Gauge)\n- `vllm:num_requests_waiting` (Gauge)\n- `vllm:gpu_cache_usage_perc` (Gauge)\n- `vllm:cpu_cache_usage_perc` (Gauge)\n- `vllm:gpu_prefix_cache_hit_rate` (Gauge)\n- `vllm:cpu_prefix_cache_hit_rate` (Gauge)\n- `vllm:prompt_tokens_total` (Counter)\n- `vllm:generation_tokens_total` (Counter)\n- `vllm:request_success_total` (Counter)\n- `vllm:request_prompt_tokens` (Histogram)\n- `vllm:request_generation_tokens` (Histogram)\n- `vllm:time_to_first_token_seconds` (Histogram)\n- `vllm:time_per_output_token_seconds` (Histogram)\n- `vllm:e2e_request_latency_seconds` (Histogram)\n- `vllm:request_queue_time_seconds` (Histogram)\n- `vllm:request_inference_time_seconds` (Histogram)\n- `vllm:request_prefill_time_seconds` (Histogram)\n- `vllm:request_decode_time_seconds` (Histogram)", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Background/v0 Metrics", "level": 3, "text": "- `vllm:request_decode_time_seconds` (Histogram)\n- `vllm:request_max_num_generation_tokens` (Histogram)\n- `vllm:num_preemptions_total` (Counter)\n- `vllm:cache_config_info` (Gauge)\n- `vllm:lora_requests_info` (Gauge)\n- `vllm:tokens_total` (Counter)\n- `vllm:iteration_tokens_total` (Histogram)\n- `vllm:time_in_queue_requests` (Histogram)\n- `vllm:model_forward_time_milliseconds` (Histogram)\n- `vllm:model_execute_time_milliseconds` (Histogram)\n- `vllm:request_params_n` (Histogram)\n- `vllm:request_params_max_tokens` (Histogram)\n- `vllm:spec_decode_draft_acceptance_rate` (Gauge)\n- `vllm:spec_decode_efficiency` (Gauge)\n- `vllm:spec_decode_num_accepted_tokens_total` (Counter)\n- `vllm:spec_decode_num_draft_tokens_total` (Counter)\n- `vllm:spec_decode_num_emitted_tokens_total` (Counter)  \nThese are documented under [Inferencing and Serving -> Production Metrics](../usage/metrics.md).", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Background/Grafana Dashboard", "level": 3, "text": "### Grafana Dashboard  \nvLLM also provides [a reference example](../examples/online_serving/prometheus_grafana.md) for how to collect and store these metrics using Prometheus and visualize them using a Grafana dashboard.  \nThe subset of metrics exposed in the Grafana dashboard gives us an indication of which metrics are especially important:  \n- `vllm:e2e_request_latency_seconds_bucket` - End to end request latency measured in seconds.\n- `vllm:prompt_tokens_total` - Prompt tokens.\n- `vllm:generation_tokens_total` - Generation tokens.\n- `vllm:time_per_output_token_seconds` - Inter-token latency (Time Per Output Token, TPOT) in seconds.\n- `vllm:time_to_first_token_seconds` - Time to First Token (TTFT) latency in seconds.\n- `vllm:num_requests_running` (also, `_swapped` and `_waiting`) - Number of requests in the RUNNING, WAITING, and SWAPPED states.\n- `vllm:gpu_cache_usage_perc` - Percentage of used cache blocks by vLLM.\n- `vllm:request_prompt_tokens` - Request prompt length.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Background/Grafana Dashboard", "level": 3, "text": "- `vllm:request_prompt_tokens` - Request prompt length.\n- `vllm:request_generation_tokens` - Request generation length.\n- `vllm:request_success_total` - Number of finished requests by their finish reason: either an EOS token was generated or the max sequence length was reached.\n- `vllm:request_queue_time_seconds` - Queue time.\n- `vllm:request_prefill_time_seconds` - Requests prefill time.\n- `vllm:request_decode_time_seconds` - Requests decode time.\n- `vllm:request_max_num_generation_tokens` - Max generation tokens in a sequence group.  \nSee [the PR which added this Dashboard](gh-pr:2316) for interesting and useful background on the choices made here.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Background/Prometheus Client Library", "level": 3, "text": "### Prometheus Client Library  \nPrometheus support was initially added [using the aioprometheus library](gh-pr:1890), but a switch was made quickly to [prometheus_client](gh-pr:2730). The rationale is discussed in both linked PRs.  \nWith the switch to `aioprometheus`, we lost a `MetricsMiddleware` to track HTTP metrics, but this was reinstated [using prometheus_fastapi_instrumentator](gh-pr:15657):  \n```bash\n$ curl http://0.0.0.0:8000/metrics 2>/dev/null  | grep -P '^http_(?!.*(_bucket|_created|_sum)).*'\nhttp_requests_total{handler=\"/v1/completions\",method=\"POST\",status=\"2xx\"} 201.0\nhttp_request_size_bytes_count{handler=\"/v1/completions\"} 201.0\nhttp_response_size_bytes_count{handler=\"/v1/completions\"} 201.0\nhttp_request_duration_highr_seconds_count 201.0\nhttp_request_duration_seconds_count{handler=\"/v1/completions\",method=\"POST\"} 201.0\n```", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Background/Multi-process Mode", "level": 3, "text": "### Multi-process Mode  \nIn v0, metrics are collected in the engine core process and we use multiprocess mode to make them available in the API server process. See <gh-pr:7279>.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Background/Built in Python/Process Metrics", "level": 3, "text": "### Built in Python/Process Metrics  \nThe following metrics are supported by default by `prometheus_client`, but they are not exposed when multiprocess mode is used:  \n- `python_gc_objects_collected_total`\n- `python_gc_objects_uncollectable_total`\n- `python_gc_collections_total`\n- `python_info`\n- `process_virtual_memory_bytes`\n- `process_resident_memory_bytes`\n- `process_start_time_seconds`\n- `process_cpu_seconds_total`\n- `process_open_fds`\n- `process_max_fds`  \nThis is relevant because if we move away from multiprocess mode in v1,\nwe get these back. However, it's questionable how relevant these are\nif they don't aggregate these stats for all processes that make up a\nvLLM instance.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Background/v0 PRs and Issues", "level": 3, "text": "### v0 PRs and Issues  \nFor background, these are some of the relevant PRs which added the v0 metrics:  \n- <gh-pr:1890>\n- <gh-pr:2316>\n- <gh-pr:2730>\n- <gh-pr:4464>\n- <gh-pr:7279>  \nAlso note the [\"Even Better Observability\"](gh-issue:3616) feature where e.g. [a detailed roadmap was laid out](gh-issue:3616#issuecomment-2030858781).", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/v1 Design/v1 PRs", "level": 3, "text": "## v1 Design  \n### v1 PRs  \nFor background, here are the relevant v1 PRs relating to the v1\nmetrics issue <gh-issue:10582>:  \n- <gh-pr:11962>\n- <gh-pr:11973>\n- <gh-pr:10907>\n- <gh-pr:12416>\n- <gh-pr:12478>\n- <gh-pr:12516>\n- <gh-pr:12530>\n- <gh-pr:12561>\n- <gh-pr:12579>\n- <gh-pr:12592>\n- <gh-pr:12644>", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/v1 Design/Metrics Collection", "level": 3, "text": "### Metrics Collection  \nIn v1, we wish to move computation and overhead out of the engine core\nprocess to minimize the time between each forward pass.  \nThe overall idea of V1 EngineCore design is:  \n- EngineCore is the inner loop. Performance is most critical here\n- AsyncLLM is the outer loop. This is overlapped with GPU execution\n(ideally), so this is where any \"overheads\" should be if\npossible. So AsyncLLM.output_handler_loop is the ideal place for the\nmetrics bookkeeping if possible.  \nWe will achieve this by collecting metrics in the frontend API server,\nand base these metrics on information we can glean from the\n`EngineCoreOutputs` returned by the engine core process to the\nfrontend.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/v1 Design/Interval Calculations", "level": 3, "text": "### Interval Calculations  \nMany of our metrics are the time interval between various events in\nthe processing of a request. It is best practice to use timestamps\nbased on \"monotonic time\" (`time.monotonic()`) rather than \"wall-clock\ntime\" (`time.time()`) to calculate intervals as the former is\nunaffected by system clock changes (e.g. from NTP).  \nIt's also important to note that monotonic clocks differ between\nprocesses - each process has its own reference point. So it is\nmeaningless to compare monotonic timestamps from different processes.  \nTherefore, in order to calculate an interval, we must compare two\nmonotonic timestamps from the same process.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/v1 Design/Scheduler Stats", "level": 3, "text": "### Scheduler Stats  \nThe engine core process will collect some key statistics from the\nscheduler - e.g. the number of requests that were scheduled or waiting\nafter the last scheduler pass - and include those statistics in\n`EngineCoreOutputs`.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/v1 Design/Engine Core Events", "level": 3, "text": "### Engine Core Events  \nThe engine core will also record the timestamp of certain per-request\nevents so that the frontend can calculate the interval between these\nevents.  \nThe events are:  \n- `QUEUED` - when the request was received by the engine core and\nadded to the scheduler queue.\n- `SCHEDULED` - when the request was first scheduled for execution.\n- `PREEMPTED` - the request has been put back in the waiting queue\nin order to make room for other requests to complete. It will be\nre-scheduled in future and re-start its prefill phase.\n- `NEW_TOKENS` - when the output included in `EngineCoreOutput` was\ngenerated. Since this is common to all requests in a given\niteration, we use a single timestamp on `EngineCoreOutputs` to\nrecord this event.  \nAnd the calculated intervals are:  \n- Queue interval - between `QUEUED` and most recent `SCHEDULED`.\n- Prefill interval - between most recent `SCHEDULED` and the subsequent\nfirst `NEW_TOKENS`.\n- Decode interval - between first (after the most recent `SCHEDULED`) and", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/v1 Design/Engine Core Events", "level": 3, "text": "last `NEW_TOKENS`.\n- Inference interval - between most recent `SCHEDULED` and last `NEW_TOKENS`.\n- Inter-token interval - between successive `NEW_TOKENS`.  \nPut another way:  \n![Interval calculations - common case](../assets/design/metrics/intervals-1.png)  \nWe explored the possibility of having the frontend calculate these\nintervals using the timing of events visible by the frontend. However,\nthe frontend does not have visibility into the timing of the `QUEUED`\nand `SCHEDULED` events and, since we need to calculate intervals based\non monotonic timestamps from the same process ... we need the engine\ncore to record timestamps for all of these events.  \n#### Interval Calculations vs Preemptions  \nWhen a preemption occurs during decode, since any already generated\ntokens are reused, we consider the preemption as affecting the\ninter-token, decode, and inference intervals.  \n![Interval calculations - preempted decode](../assets/design/metrics/intervals-2.png)", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/v1 Design/Engine Core Events", "level": 3, "text": "When a preemption occurs during prefill (assuming such an event\nis possible), we consider the preemption as affecting the\ntime-to-first-token and prefill intervals.  \n![Interval calculations - preempted prefill](../assets/design/metrics/intervals-3.png)", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/v1 Design/Frontend Stats Collection", "level": 3, "text": "### Frontend Stats Collection  \nAs the frontend processes a single `EngineCoreOutputs` - i.e. the\noutput from a single engine core iteration - it collects various\nstatistics relating to that iteration:  \n- The total number of new tokens generated in this iteration.\n- The total number of prompt tokens processed by the prefills that\ncompleted in this iteration.\n- The queue intervals for any requests that were scheduled in this\niteration.\n- The prefill intervals for any requests that completed prefill in\nthis iteration.\n- The inter-token intervals (Time Per Output Token, TPOT), for all\nrequests included in this iteration.\n- The Time-To-First-Token (TTFT) for any requests that completed\nprefill in this iteration. However, we calculate this interval\nrelative to when the request was first received by the frontend\n(`arrival_time`) in order to account for input processing time.  \nFor any requests that were completed in a given iteration, we also\nrecord:", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/v1 Design/Frontend Stats Collection", "level": 3, "text": "record:  \n- The inference and decode intervals - relative to the scheduled and\nfirst token events, as described above.\n- End-to-end latency - the interval between frontend `arrival_time`\nand the frontend receiving the final token.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/v1 Design/Metrics Publishing - Logging", "level": 3, "text": "### Metrics Publishing - Logging  \nThe `LoggingStatLogger` metrics publisher outputs a log `INFO` message\nevery 5 seconds with some key metrics:  \n- The current number of running/waiting requests\n- The current GPU cache usage\n- The number of prompt tokens processed per second over the past 5\nseconds\n- The number of new tokens generated per second over the past 5\nseconds\n- The prefix cache hit rate over the most recent 1k kv-cache block queries", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/v1 Design/Metrics Publishing - Prometheus", "level": 3, "text": "### Metrics Publishing - Prometheus  \nThe `PrometheusStatLogger` metrics publisher makes the metrics\navailable via a `/metrics` HTTP endpoint in a Prometheus-compatible\nformat. A Prometheus instance can then be configured to poll this\nendpoint (e.g. every second) and record the values in its time-series\ndatabase. Prometheus is often used via Grafana, allowing these metrics\nto be graphed over time.  \nPrometheus supports the following metric types:  \n- Counter: a value that will increase over time, never reducing, and\ngenerally reset to zero when the vLLM instance restarts. For\nexample, the number of tokens generated over the lifetime of the\ninstance.\n- Gauge: a value that goes up and down, for example the number of\nrequests currently scheduled for execution.\n- Histogram: a count of metric samples, recorded in buckets. For\nexample, the number of requests whose TTFT was <1ms, <5ms, <10ms,\n<20ms, and so on.  \nPrometheus metrics can also be labelled, allowing metrics to be", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/v1 Design/Metrics Publishing - Prometheus", "level": 3, "text": "Prometheus metrics can also be labelled, allowing metrics to be\ncombined according to matching labels. In vLLM, we add a `model_name`\nlabel to every metric which includes the name of the model served by\nthat instance.  \nExample output:  \n```bash\n$ curl http://0.0.0.0:8000/metrics\n# HELP vllm:num_requests_running Number of requests in model execution batches.\n# TYPE vllm:num_requests_running gauge\nvllm:num_requests_running{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 8.0\n...\n# HELP vllm:generation_tokens_total Number of generation tokens processed.\n# TYPE vllm:generation_tokens_total counter\nvllm:generation_tokens_total{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 27453.0\n...\n# HELP vllm:request_success_total Count of successfully processed requests.\n# TYPE vllm:request_success_total counter\nvllm:request_success_total{finished_reason=\"stop\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 1.0\nvllm:request_success_total{finished_reason=\"length\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 131.0", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/v1 Design/Metrics Publishing - Prometheus", "level": 3, "text": "vllm:request_success_total{finished_reason=\"abort\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\n...\n# HELP vllm:time_to_first_token_seconds Histogram of time to first token in seconds.\n# TYPE vllm:time_to_first_token_seconds histogram\nvllm:time_to_first_token_seconds_bucket{le=\"0.001\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\nvllm:time_to_first_token_seconds_bucket{le=\"0.005\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\nvllm:time_to_first_token_seconds_bucket{le=\"0.01\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 0.0\nvllm:time_to_first_token_seconds_bucket{le=\"0.02\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 13.0\nvllm:time_to_first_token_seconds_bucket{le=\"0.04\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 97.0\nvllm:time_to_first_token_seconds_bucket{le=\"0.06\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 123.0\nvllm:time_to_first_token_seconds_bucket{le=\"0.08\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 138.0", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/v1 Design/Metrics Publishing - Prometheus", "level": 3, "text": "vllm:time_to_first_token_seconds_bucket{le=\"0.1\",model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 140.0\nvllm:time_to_first_token_seconds_count{model_name=\"meta-llama/Llama-3.1-8B-Instruct\"} 140.0\n```  \n!!! note\nThe choice of histogram buckets to be most useful to users\nacross a broad set of use cases is not straightforward and will\nrequire refinement over time.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/v1 Design/Cache Config Info", "level": 3, "text": "### Cache Config Info  \n`prometheus_client` has support for\n[Info metrics](https://prometheus.github.io/client_python/instrumenting/info/)\nwhich are equivalent to a `Gauge` whose value is permanently set to 1,\nbut exposes interesting key/value pair information via labels. This is\nused for information about an instance that does not change - so it\nonly needs to be observed at startup - and allows comparing across\ninstances in Prometheus.  \nWe use this concept for the `vllm:cache_config_info` metric:  \n```text\n# HELP vllm:cache_config_info Information of the LLMEngine CacheConfig\n# TYPE vllm:cache_config_info gauge\nvllm:cache_config_info{block_size=\"16\",cache_dtype=\"auto\",calculate_kv_scales=\"False\",cpu_offload_gb=\"0\",enable_prefix_caching=\"False\",gpu_memory_utilization=\"0.9\",...} 1.0\n```  \nHowever, `prometheus_client` has\n[never supported Info metrics in multiprocessing mode](https://github.com/prometheus/client_python/pull/300) -\nfor [unclear reasons](gh-pr:7279#discussion_r1710417152). We", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/v1 Design/Cache Config Info", "level": 3, "text": "for [unclear reasons](gh-pr:7279#discussion_r1710417152). We\nsimply use a `Gauge` metric set to 1 and\n`multiprocess_mode=\"mostrecent\"` instead.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/v1 Design/LoRA Metrics", "level": 3, "text": "### LoRA Metrics  \nThe `vllm:lora_requests_info` `Gauge` is somewhat similar, except the\nvalue is the current wall-clock time, and is updated every iteration.  \nThe label names used are:  \n- `running_lora_adapters`: a per-adapter count of the number requests\nrunning using that adapter, formatted as a comma-separated string.\n- `waiting_lora_adapters`: similar, except counting requests that are\nwaiting to be scheduled.\n- `max_lora` - the static \"max number of LoRAs in a single batch.\"\nconfiguration.  \nEncoding a running/waiting counts for multiple adapters in a\ncomma-separated string seems quite misguided - we could use labels to\ndistinguish between per-adapter counts. This should be revisited.  \nNote that `multiprocess_mode=\"livemostrecent\"` is used - the most\nrecent metric is used, but only from currently running processes.  \nThis was added in <gh-pr:9477> and there is\n[at least one known user](https://github.com/kubernetes-sigs/gateway-api-inference-extension/pull/54).", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/v1 Design/LoRA Metrics", "level": 3, "text": "If we revisit this design and deprecate the old metric, we should reduce\nthe need for a significant deprecation period by making the change in\nv0 also and asking this project to move to the new metric.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/v1 Design/Prefix Cache metrics", "level": 3, "text": "### Prefix Cache metrics  \nThe discussion in <gh-issue:10582> about adding prefix cache metrics yielded\nsome interesting points which may be relevant to how we approach\nfuture metrics.  \nEvery time the prefix cache is queried, we record the number of tokens\nqueried and the number of queried tokens present in the cache\n(i.e. hits).  \nHowever, the metric of interest is the hit rate - i.e. the number of\nhits per query.  \nIn the case of logging, we expect the user is best served by\ncalculating the hit rate over a fixed number of the most recent\nqueries (the interval is fixed to 1k most recent queries for now).  \nIn the case of Prometheus though, we should take advantage of the\ntime-series nature of Prometheus and allow the user to calculate the\nhit rate over an interval of their choosing. For example, a PromQL\nquery to calculate the hit interval of the past 5 minutes:  \n```text\nrate(cache_query_hit[5m]) / rate(cache_query_total[5m])\n```  \nTo achieve this, we should record the queries and hits as counters in", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/v1 Design/Prefix Cache metrics", "level": 3, "text": "Prometheus, rather than recording the hit rate as a gauge.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Deprecated Metrics/How To Deprecate", "level": 3, "text": "## Deprecated Metrics  \n### How To Deprecate  \nDeprecating metrics shouldn't be taken lightly. Users may not notice a\nmetric has been deprecated, and may be quite inconvenienced when it is\nsuddenly (from their perspective) when it is removed, even if there is\nan equivalent metric for them to use.  \nAs an example, see how `vllm:avg_prompt_throughput_toks_per_s` was\n[deprecated](gh-pr:2764) (with a comment in the code),\n[removed](gh-pr:12383), and then [noticed by a user](gh-issue:13218).  \nIn general:  \n1. We should be cautious about deprecating metrics, especially since\nit can be hard to predict the user impact.\n2. We should include a prominent deprecation notice in the help string\nthat is included in the `/metrics' output.\n3. We should list deprecated metrics in user-facing documentation and\nrelease notes.\n4. We should consider hiding deprecated metrics behind a CLI argument\nin order to give administrators", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Deprecated Metrics/How To Deprecate", "level": 3, "text": "in order to give administrators\n[an escape hatch](https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/#show-hidden-metrics)\nfor some time before deleting them.  \nSee the [deprecation policy](../contributing/deprecation_policy.md) for\nthe project-wide deprecation policy.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Deprecated Metrics/Unimplemented - `vllm:tokens_total`", "level": 3, "text": "### Unimplemented - `vllm:tokens_total`  \nAdded by <gh-pr:4464>, but apparently never implemented. This can just be\nremoved.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Deprecated Metrics/Duplicated - Queue Time", "level": 3, "text": "### Duplicated - Queue Time  \nThe `vllm:time_in_queue_requests` Histogram metric was added by\n<gh-pr:9659> and its calculation is:  \n```python\nself.metrics.first_scheduled_time = now\nself.metrics.time_in_queue = now - self.metrics.arrival_time\n```  \nTwo weeks later, <gh-pr:4464> added `vllm:request_queue_time_seconds` leaving\nus with:  \n```python\nif seq_group.is_finished():\nif (\nseq_group.metrics.first_scheduled_time is not None\nand seq_group.metrics.first_token_time is not None\n):\ntime_queue_requests.append(\nseq_group.metrics.first_scheduled_time -\nseq_group.metrics.arrival_time\n)\n...\nif seq_group.metrics.time_in_queue is not None:\ntime_in_queue_requests.append(seq_group.metrics.time_in_queue)\n```  \nThis seems duplicative, and one of them should be removed. The latter\nis used by the Grafana dashboard, so we should deprecate or remove the\nformer from v0.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Deprecated Metrics/Prefix Cache Hit Rate", "level": 3, "text": "### Prefix Cache Hit Rate  \nSee above - we now expose 'queries' and 'hits' counters rather than a\n'hit rate' gauge.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Deprecated Metrics/KV Cache Offloading", "level": 3, "text": "### KV Cache Offloading  \nTwo v0 metrics relate to a \"swapped\" preemption mode that is no\nlonger relevant in v1:  \n- `vllm:num_requests_swapped`\n- `vllm:cpu_cache_usage_perc`  \nIn this mode, when a request is preempted (e.g. to make room in KV\ncache to complete other requests), we swap kv cache blocks out to CPU\nmemory. This is also known as \"KV cache offloading\" and is configured\nwith `--swap-space` and `--preemption-mode`.  \nIn v0, [vLLM has long supported beam search](gh-issue:6226). The\nSequenceGroup encapsulated the idea of N Sequences which\nall shared the same prompt kv blocks. This enabled KV cache block\nsharing between requests, and copy-on-write to do branching. CPU\nswapping was intended for these beam search like cases.  \nLater, the concept of prefix caching was introduced, which allowed KV\ncache blocks to be shared implicitly. This proved to be a better\noption than CPU swapping since blocks can be evicted slowly on demand\nand the part of the prompt that was evicted can be recomputed.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Deprecated Metrics/KV Cache Offloading", "level": 3, "text": "SequenceGroup was removed in V1, although a replacement will be\nrequired for \"parallel sampling\" (`n>1`).\n[Beam search was moved out of the core (in V0)](gh-issue:8306). There was a\nlot of complex code for a very uncommon feature.  \nIn V1, with prefix caching being better (zero over head) and therefore\non by default, the preemption and recompute strategy should work\nbetter.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Future Work/Parallel Sampling", "level": 3, "text": "## Future Work  \n### Parallel Sampling  \nSome v0 metrics are only relevant in the context of \"parallel\nsampling\". This is where the `n` parameter in a request is used to\nrequest multiple completions from the same prompt.  \nAs part of adding parallel sampling support in <gh-pr:10980>, we should\nalso add these metrics.  \n- `vllm:request_params_n` (Histogram)  \nObserves the value of the 'n' parameter of every finished request.  \n- `vllm:request_max_num_generation_tokens` (Histogram)  \nObserves the maximum output length of all sequences in every finished\nsequence group. In the absence of parallel sampling, this is\nequivalent to `vllm:request_generation_tokens`.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Future Work/Speculative Decoding", "level": 3, "text": "### Speculative Decoding  \nSome v0 metrics are specific to \"speculative decoding\". This is where\nwe generate candidate tokens using a faster, approximate method or\nmodel and then validate those tokens with the larger model.  \n- `vllm:spec_decode_draft_acceptance_rate` (Gauge)\n- `vllm:spec_decode_efficiency` (Gauge)\n- `vllm:spec_decode_num_accepted_tokens_total` (Counter)\n- `vllm:spec_decode_num_draft_tokens_total` (Counter)\n- `vllm:spec_decode_num_emitted_tokens_total` (Counter)  \nThere is a PR under review (<gh-pr:12193>) to add \"prompt lookup (ngram)\"\nspeculative decoding to v1. Other techniques will follow. We should\nrevisit the v0 metrics in this context.  \n!!! note\nWe should probably expose acceptance rate as separate accepted\nand draft counters, like we do for prefix caching hit rate. Efficiency\nlikely also needs similar treatment.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Future Work/Autoscaling and Load-balancing", "level": 3, "text": "### Autoscaling and Load-balancing  \nA common use case for our metrics is to support automated scaling of\nvLLM instances.  \nFor related discussion from the\n[Kubernetes Serving Working Group](https://github.com/kubernetes/community/tree/master/wg-serving),\nsee:  \n- [Standardizing Large Model Server Metrics in Kubernetes](https://docs.google.com/document/d/1SpSp1E6moa4HSrJnS4x3NpLuj88sMXr2tbofKlzTZpk)\n- [Benchmarking LLM Workloads for Performance Evaluation and Autoscaling in Kubernetes](https://docs.google.com/document/d/1k4Q4X14hW4vftElIuYGDu5KDe2LtV1XammoG-Xi3bbQ)\n- [Inference Perf](https://github.com/kubernetes-sigs/wg-serving/tree/main/proposals/013-inference-perf)\n- <gh-issue:5041> and <gh-pr:12726>.  \nThis is a non-trivial topic. Consider this comment from Rob:  \n> I think this metric should focus on trying to estimate what the max\n> concurrency that will cause the average request length > queries per\n> second ... since this is really what will \"saturate\" the server.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Future Work/Autoscaling and Load-balancing", "level": 3, "text": "A clear goal is that we should expose the metrics required to detect\nthis saturation point, so administrators can implement auto-scaling\nrules based on those. However, in order to do so, we need to have a\nclear view on how an administrator (and automated monitoring system)\nshould judge an instance as approaching saturation:  \n> To identify, what is the saturation point for model server compute\n> (the inflection point where we cannot get more throughput with a\n> higher request rate, but start to incur additional latency) so we\n> can autoscale effectively?", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Future Work/Metric Naming", "level": 3, "text": "### Metric Naming  \nOur approach to naming metrics probably deserves to be revisited:  \n1. The use of colons in metric names seems contrary to\n[\"colons are reserved for user defined recording rules\"](https://prometheus.io/docs/concepts/data_model/#metric-names-and-labels).\n2. Most of our metrics follow the convention of ending with units, but\nnot all do.\n3. Some of our metric names end with `_total`:  \nIf there is a suffix of `_total` on the metric name, it will be removed. When\nexposing the time series for counter, a `_total` suffix will be added. This is\nfor compatibility between OpenMetrics and the Prometheus text format, as OpenMetrics\nrequires the `_total` suffix.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Future Work/Adding More Metrics", "level": 3, "text": "### Adding More Metrics  \nThere is no shortage of ideas for new metrics:  \n- Examples from other projects like\n[TGI](https://github.com/IBM/text-generation-inference?tab=readme-ov-file#metrics)\n- Proposals arising from specific use cases, like the Kubernetes\nauto-scaling topic above\n- Proposals that might arise out of standardisation efforts like\n[OpenTelemetry Semantic Conventions for Gen AI](https://github.com/open-telemetry/semantic-conventions/tree/main/docs/gen-ai).  \nWe should be cautious in our approach to adding new metrics. While\nmetrics are often relatively straightforward to add:  \n1. They can be difficult to remove - see the section on deprecation\nabove.\n2. They can have a meaningful performance impact when enabled. And\nmetrics are usually of very limited use unless they can be enabled\nby default and in production.\n3. They have an impact on development and maintenance of the\nproject. Every metric added to v0 has made this v1 effort more", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Future Work/Adding More Metrics", "level": 3, "text": "project. Every metric added to v0 has made this v1 effort more\ntime-consuming, and perhaps not all metrics justify this ongoing\ninvestment in their maintenance.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Tracing - OpenTelemetry", "level": 2, "text": "## Tracing - OpenTelemetry  \nMetrics provide an aggregated view over time of the system's\nperformance and health. Tracing, on the other hand, tracks individual\nrequests as they move through different services and components. Both\nfall under the more general heading of \"Observability\".  \nv0 has support for OpenTelemetry tracing:  \n- Added by <gh-pr:4687>\n- Configured with `--oltp-traces-endpoint` and `--collect-detailed-traces`\n- [OpenTelemetry blog post](https://opentelemetry.io/blog/2024/llm-observability/)\n- [User-facing docs](../examples/online_serving/opentelemetry.md)\n- [Blog post](https://medium.com/@ronen.schaffer/follow-the-trail-supercharging-vllm-with-opentelemetry-distributed-tracing-aa655229b46f)\n- [IBM product docs](https://www.ibm.com/docs/en/instana-observability/current?topic=mgaa-monitoring-large-language-models-llms-vllm-public-preview)  \nOpenTelemetry has a\n[Gen AI Working Group](https://github.com/open-telemetry/community/blob/main/projects/gen-ai.md).", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Tracing - OpenTelemetry", "level": 2, "text": "Since metrics is a big enough topic on its own, we are going to tackle\nthe topic of tracing in v1 separately.", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Tracing - OpenTelemetry/OpenTelemetry Model Forward vs Execute Time", "level": 3, "text": "### OpenTelemetry Model Forward vs Execute Time  \nIn v0, we have the following two metrics:  \n- `vllm:model_forward_time_milliseconds` (Histogram) - The time spent\nin the model forward pass when this request was in the batch.\n- `vllm:model_execute_time_milliseconds` (Histogram) - The time spent\nin the model execute function. This will include model forward,\nblock/sync across workers, cpu-gpu sync time and sampling time.  \nThese metrics are only enabled when OpenTelemetry tracing is enabled\nand if `--collect-detailed-traces=all/model/worker` is used. The\ndocumentation for this option states:  \n> collect detailed traces for the specified modules. This involves\n> use of possibly costly and or blocking operations and hence might\n> have a performance impact.  \nThe metrics were added by <gh-pr:7089> and who up in an OpenTelemetry trace\nas:  \n```text\n-> gen_ai.latency.time_in_scheduler: Double(0.017550230026245117)\n-> gen_ai.latency.time_in_model_forward: Double(3.151565277099609)", "file_path": "design/metrics.md"}
{"id": "bfc0f6e6ef3c7901135a29f0eb7753360abb77b496b2d3f07d40ec58d83667f0", "heading": "Metrics/Tracing - OpenTelemetry/OpenTelemetry Model Forward vs Execute Time", "level": 3, "text": "-> gen_ai.latency.time_in_model_execute: Double(3.6468167304992676)\n```  \nWe already have `inference_time` and `decode_time` metrics, so the\nquestion is whether there are sufficiently common use cases for the\nhigher-resolution timings to justify the overhead.  \nSince we are going to treat the question of OpenTelemetry support\nseparately, we will include these particular metrics under that topic.", "file_path": "design/metrics.md"}
{"id": "0a63816fc89031c1c5cf08766f13829c009ce8c43c7e5877e90682b78c92acd8", "heading": "Multi-Modal Data Processing", "level": 1, "text": "# Multi-Modal Data Processing  \nTo enable various optimizations in vLLM such as [chunked prefill][chunked-prefill] and [prefix caching](../features/automatic_prefix_caching.md), we use [BaseMultiModalProcessor][vllm.multimodal.processing.BaseMultiModalProcessor] to provide the correspondence between placeholder feature tokens (e.g. `<image>`) and multi-modal inputs (e.g. the raw input image) based on the outputs of HF processor.  \nHere are the main features of [BaseMultiModalProcessor][vllm.multimodal.processing.BaseMultiModalProcessor]:", "file_path": "design/mm_processing.md"}
{"id": "0a63816fc89031c1c5cf08766f13829c009ce8c43c7e5877e90682b78c92acd8", "heading": "Multi-Modal Data Processing/Prompt Update Detection", "level": 2, "text": "## Prompt Update Detection  \nOne of the main responsibilities of HF processor is to update the prompt with placeholder tokens. For example:  \n- Insert feature placeholder tokens (e.g. `<image><image>...<image>`, the number of which equals to the feature size) at the start of the string.\n- Replace existing input placeholder tokens (e.g. `<image>` for a single image) with feature placeholder tokens (e.g. `<image><image>...<image>`, the number of which equals to the feature size).  \nThe information about which tokens have been updated is key to finding the correspondence between placeholder feature tokens and multi-modal inputs.  \nIn vLLM, this information is specified using [PromptUpdate][vllm.multimodal.processing.PromptUpdate] in [_get_prompt_updates][vllm.multimodal.processing.BaseMultiModalProcessor._get_prompt_updates]. We can automatically detect whether HF has updated the prompt by checking the existence of the updated tokens.", "file_path": "design/mm_processing.md"}
{"id": "0a63816fc89031c1c5cf08766f13829c009ce8c43c7e5877e90682b78c92acd8", "heading": "Multi-Modal Data Processing/Tokenized Prompt Inputs", "level": 2, "text": "## Tokenized Prompt Inputs  \nTo enable tokenization in a separate process, we support passing input token IDs alongside multi-modal data.", "file_path": "design/mm_processing.md"}
{"id": "0a63816fc89031c1c5cf08766f13829c009ce8c43c7e5877e90682b78c92acd8", "heading": "Multi-Modal Data Processing/Tokenized Prompt Inputs/The problem", "level": 3, "text": "### The problem  \nConsider that HF processors follow these main steps:  \n1. Tokenize the text\n2. Process multi-modal inputs\n3. Perform prompt updates  \nAnd we require that:  \n- For text + multi-modal inputs, apply all steps 1--3.\n- For tokenized + multi-modal inputs, apply only steps 2--3.  \nHow can we achieve this without rewriting HF processors? We can try to call the HF processor several times on different inputs:  \n- For text + multi-modal inputs, simply call the HF processor directly.\n- For tokenized + multi-modal inputs, call the processor only on the multi-modal inputs.  \nWhile HF processors support text + multi-modal inputs natively, this is not so for tokenized + multi-modal inputs: an error is thrown if the number of input placeholder tokens do not correspond to the number of multi-modal inputs.  \nMoreover, since the tokenized text has not passed through the HF processor, we have to apply Step 3 by ourselves to keep the output tokens and multi-modal data consistent with each other.", "file_path": "design/mm_processing.md"}
{"id": "0a63816fc89031c1c5cf08766f13829c009ce8c43c7e5877e90682b78c92acd8", "heading": "Multi-Modal Data Processing/Tokenized Prompt Inputs/The problem", "level": 3, "text": "[](){ #mm-dummy-text }", "file_path": "design/mm_processing.md"}
{"id": "0a63816fc89031c1c5cf08766f13829c009ce8c43c7e5877e90682b78c92acd8", "heading": "Multi-Modal Data Processing/Tokenized Prompt Inputs/Dummy text", "level": 3, "text": "### Dummy text  \nWe work around the first issue by requiring each model to define how to generate dummy text based on the number of multi-modal inputs, via [get_dummy_text][vllm.multimodal.profiling.BaseDummyInputsBuilder.get_dummy_text]. This lets us generate dummy text corresponding to the multi-modal inputs and input them together to obtain the processed multi-modal data.  \n[](){ #mm-automatic-prompt-updating }", "file_path": "design/mm_processing.md"}
{"id": "0a63816fc89031c1c5cf08766f13829c009ce8c43c7e5877e90682b78c92acd8", "heading": "Multi-Modal Data Processing/Tokenized Prompt Inputs/Automatic prompt updating", "level": 3, "text": "### Automatic prompt updating  \nWe address the second issue by implementing model-agnostic code in\n[_apply_prompt_updates][vllm.multimodal.processing.BaseMultiModalProcessor._apply_prompt_updates] to automatically update the prompt with feature placeholder tokens based on the specification outputted by [_get_prompt_updates][vllm.multimodal.processing.BaseMultiModalProcessor._get_prompt_updates].", "file_path": "design/mm_processing.md"}
{"id": "0a63816fc89031c1c5cf08766f13829c009ce8c43c7e5877e90682b78c92acd8", "heading": "Multi-Modal Data Processing/Tokenized Prompt Inputs/Summary", "level": 3, "text": "### Summary  \nWith the help of dummy text and automatic prompt updating, our multi-modal processor can finally accept both text and token prompts with multi-modal data. The detailed logic is shown in [_apply_hf_processor_main][vllm.multimodal.processing.BaseMultiModalProcessor._apply_hf_processor_main].", "file_path": "design/mm_processing.md"}
{"id": "0a63816fc89031c1c5cf08766f13829c009ce8c43c7e5877e90682b78c92acd8", "heading": "Multi-Modal Data Processing/Processor Output Caching", "level": 2, "text": "## Processor Output Caching  \nSome HF processors, such as the one for Qwen2-VL, are [very slow](gh-issue:9238). To alleviate this problem, we cache the multi-modal outputs of HF processor to avoid processing the same multi-modal input (e.g. image) again.  \nWhen new data is passed in, we first check which items are in the cache, and which ones are missing. The missing items are passed into the HF processor in a single batch and cached, before being merged with the existing items in the cache.", "file_path": "design/mm_processing.md"}
{"id": "0a63816fc89031c1c5cf08766f13829c009ce8c43c7e5877e90682b78c92acd8", "heading": "Multi-Modal Data Processing/Processor Output Caching", "level": 2, "text": "Since we only process the missing multi-modal data items, the number of input placeholder tokens no longer corresponds to the number of the multi-modal inputs, so they can't be passed alongside the text prompt to HF processor. Therefore, we process the text and multi-modal inputs separately, using [dummy text][mm-dummy-text] to avoid HF errors. Since this skips HF's prompt updating code, we apply [automatic prompt updating][mm-automatic-prompt-updating] afterwards to keep the output tokens and multi-modal data consistent with each other.", "file_path": "design/mm_processing.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features", "level": 1, "text": "# Fused MoE Kernel features  \nThe purpose of this document is to provide an overview of the various MoE kernels (both modular and non-modular) so it will be easier to select an appropriate set of kernels for any particular situation. This includes information about the all2all backends used by modular kernels.", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Modular All2All backends", "level": 2, "text": "## Fused MoE Modular All2All backends  \nThere are a number of all2all communication backends that are used to implement expert parallelism (EP) for the `FusedMoE` layer. The different `FusedMoEPrepareAndFinalize` sub-classes provide an interface for each all2all backend.  \nThe following table describes the relevant features of each backend, i.e. activation format, supported quantization schemes and async support.  \nThe output activation format (standard or batched) corresponds to the output of the prepare step of the `FusedMoEPrepareAndFinalize` subclass, the finalize step requires the same format. All the backend `prepare` methods expect activations in standard format and all the `finalize methods return activations in standard format. More details on the formats can be found in the [Fused MoE Modular Kernel](./fused_moe_modular_kernel.md) document.", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Modular All2All backends", "level": 2, "text": "The quantization types and formats enumerate which quantization schemes are supported by each `FusedMoEPrepareAndFinalize` class. The quantization can happen before or after the dispatch based on the format the all2all backend supports. e.g. deepep_high_throughput supports only block-quantized fp8 format, any other format will result in dispatching in higher precision and quantizing afterwards. The output of the prepare step for each backend is the quantized type.  The finalize step generally requires the same input type as the original activations, e.g. if the original input is bfloat16 and the quantization scheme is fp8 w/per-tensor scales, `prepare` will return fp8/per-tensor scale activations and `finalize` will take bfloat16 activations. See the diagrams in [Fused MoE Modular Kernel](./fused_moe_modular_kernel.md) for more details on the types and formats of activations at each step of the MoE process.  If no quantization type is specified, the kernel operates on float16 and/or bfloat16.", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Modular All2All backends", "level": 2, "text": "Async backends support the use of DBO (Dual Batch Overlap) and shared expert overlap (where shared experts are computed during the combine step).  \nCertain models require the topk weights to be applied to the input activations rather than the output activations when topk==1, e.g. llama. For modular kernels, this feature is supported by the `FusedMoEPrepareAndFinalize` subclass, for non-modular kernels, it is up to the experts function to deal with this flag.  \nunless otherwise specified, backends are controlled via `VLLM_ALL2ALL_BACKEND`.  All backends except `flashinfer` only work with EP+DP or EP+TP. `Flashinfer` can work with EP or DP w/o EP.  \n<style>\ntd {\npadding: 0.5rem !important;\nwhite-space: nowrap;\n}  \nth {\npadding: 0.5rem !important;\nmin-width: 0 !important;\n}\n</style>", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Modular All2All backends", "level": 2, "text": "min-width: 0 !important;\n}\n</style>  \n| Backend                               | Output act. format | Quant. types    | Quant. format          | Async | Apply Weight On Input | Sub-class                                                                                                                                                     |\n|---------------------------------------|--------------------|-----------------|------------------------|-------|-----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| naive                                 | standard           | all<sup>1</sup> | G,A,T                  | N     | <sup>6</sup>          | [layer.py][vllm.model_executor.layers.fused_moe.layer.FusedMoE.forward_impl]                                                                                  |", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Modular All2All backends", "level": 2, "text": "| pplx                                  | batched            | fp8,int8        | G,A,T                  | Y     | Y                     | [`PplxPrepareAndFinalize`][vllm.model_executor.layers.fused_moe.pplx_prepare_finalize.PplxPrepareAndFinalize]                                                 |\n| deepep_high_throughput                | standard           | fp8             | G(128),A,T<sup>2</sup> | Y     | Y                     | [`DeepEPLLPrepareAndFinalize`][vllm.model_executor.layers.fused_moe.deepep_ll_prepare_finalize.DeepEPLLPrepareAndFinalize]                                    |\n| deepep_low_latency                    | batched            | fp8             | G(128),A,T<sup>3</sup> | Y     | Y                     | [`DeepEPHTPrepareAndFinalize`][vllm.model_executor.layers.fused_moe.deepep_ht_prepare_finalize.DeepEPHTPrepareAndFinalize]                                    |", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Modular All2All backends", "level": 2, "text": "| flashinfer_all2allv                   | standard           | nvfp4,fp8       | G,A,T                  | N     | N                     | [`FlashInferAllToAllMoEPrepareAndFinalize`][vllm.model_executor.layers.fused_moe.flashinfer_cutlass_prepare_finalize.FlashInferAllToAllMoEPrepareAndFinalize] |\n| flashinfer<sup>4</sup>                | standard           | nvfp4,fp8       | G,A,T                  | N     | N                     | [`FlashInferCutlassMoEPrepareAndFinalize`][vllm.model_executor.layers.fused_moe.flashinfer_cutlass_prepare_finalize.FlashInferCutlassMoEPrepareAndFinalize]   |\n| flashinfer<sup>4</sup>                | standard           | nvfp4,fp8       | G,A,T                  | N     | N                     | [`FlashInferCutlassMoEPrepareAndFinalize`][vllm.model_executor.layers.fused_moe.flashinfer_cutlass_prepare_finalize.FlashInferCutlassMoEPrepareAndFinalize]   |", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Modular All2All backends", "level": 2, "text": "| MoEPrepareAndFinalizeNoEP<sup>5</sup> | standard           | fp8,int8        | G,A,T                  | N     | Y                     | [`MoEPrepareAndFinalizeNoEP`][vllm.model_executor.layers.fused_moe.prepare_finalize.MoEPrepareAndFinalizeNoEP]                                                |\n| BatchedPrepareAndFinalize<sup>5</sup> | batched            | fp8,int8        | G,A,T                  | N     | Y                     | [`BatchedPrepareAndFinalize`][vllm.model_executor.layers.fused_moe.fused_batched_moe.BatchedPrepareAndFinalize]                                               |  \n!!! info \"Table key\"\n1. All types: mxfp4, nvfp4, int4, int8, fp8\n2. A,T quantization occurs after dispatch.\n3. All quantization happens after dispatch.\n4. Controlled by different env vars (`VLLM_FLASHINFER_MOE_BACKEND` \"throughput\" or \"latency\")", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Modular All2All backends", "level": 2, "text": "5. This is a no-op dispatcher that can be used to pair with any modular experts to produce a modular kernel that runs w/o dispatch or combine.  These cannot be selected via environment variable.  These are generally use for testing or adapting an expert subclass to the `fused_experts` API.\n6. This depends on the experts implementation.  \n---  \n- G - Grouped\n- G(N) - Grouped w/block size N\n- A - Per activation token\n- T - Per tensor  \nModular kernels are supported by the following `FusedMoEMethodBase` classes.  \n- [`ModelOptFp8MoEMethod`][vllm.model_executor.layers.quantization.modelopt.ModelOptFp8MoEMethod]\n- [`Fp8MoEMethod`][vllm.model_executor.layers.quantization.fp8.Fp8MoEMethod]\n- [`CompressedTensorsW4A4MoeMethod`][vllm.model_executor.layers.quantization.compressed_tensors.compressed_tensors_moe.CompressedTensorsW4A4MoeMethod]\n- [`CompressedTensorsW8A8Fp8MoEMethod`][vllm.model_executor.layers.quantization.compressed_tensors.compressed_tensors_moe.CompressedTensorsW8A8Fp8MoEMethod]", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Modular All2All backends", "level": 2, "text": "- [`Mxfp4MoEMethod`][vllm.model_executor.layers.quantization.mxfp4.Mxfp4MoEMethod]\n- [`UnquantizedFusedMoEMethod`][vllm.model_executor.layers.fused_moe.layer.UnquantizedFusedMoEMethod]", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Experts Kernels", "level": 2, "text": "## Fused MoE Experts Kernels  \nThe are a number of MoE experts kernel implementations for different quantization types and architectures. Most follow the general API of the base Triton [`fused_experts`][vllm.model_executor.layers.fused_moe.fused_moe.fused_experts] function. Many have modular kernel adatpers so they can be used with compatible all2all backends. This table lists each experts kernel and its particular properties.  \nEach kernel must be provided with one of the supported input activation formats.  Some flavors of kernels support both standard and batched formats through different entry points, e.g. `TritonExperts` and `BatchedTritonExperts`. Batched format kernels are currently only needed for matching with certain all2all backends, e.g. `pplx`, `DeepEPLLPrepareAndFinalize`.", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Experts Kernels", "level": 2, "text": "Similar to the backend kernels, each experts kernel only supports certain quantization formats. For non-modular experts, the activations will be in the original type and quantized internally by the kernel. Modular experts will expect the activations to already be in the quantized format. Both types of experts will yield outputs in the original activation type.  \nEach experts kernel supports one or more activation functions, e.g. silu, gelu that are applied to the intermediate results.  \nAs with the backends, some experts support applying topk weights on the input activations. The entries in the column in this table only apply to the non-modular experts.  \nMost experts flavors include an equivalent modular interface which will be a subclass of `FusedMoEPermuteExpertsUnpermute`.  \nTo be used with a particular `FusedMoEPrepareAndFinalize` sub-class, MoE kernels must have compatible activation formats, quantization types and quantization formats.", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Experts Kernels", "level": 2, "text": "| Kernel                       | Input act. format     | Quant. types     | Quant. format | Activation function                                         | Apply Weight On Input | Modular | Source                                                                                                                                                                                                                                                                                                      |\n|------------------------------|-----------------------|------------------|---------------|-------------------------------------------------------------|-----------------------|---------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Experts Kernels", "level": 2, "text": "| triton                       | standard              | all<sup>1</sup>  | G,A,T         | silu, gelu,</br>swigluoai,</br>silu_no_mul,</br>gelu_no_mul | Y                     | Y       | [`fused_experts`][vllm.model_executor.layers.fused_moe.fused_moe.fused_experts],</br>[`TritonExperts`][vllm.model_executor.layers.fused_moe.fused_moe.TritonExperts]                                                                                                                                        |\n| triton (batched)             | batched               | all<sup>1</sup>  | G,A,T         | silu, gelu                                                  | <sup>6</sup>          | Y       | [`BatchedTritonExperts`][vllm.model_executor.layers.fused_moe.fused_batched_moe.BatchedTritonExperts]                                                                                                                                                                                                       |", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Experts Kernels", "level": 2, "text": "| deep gemm                    | standard,</br>batched | fp8              | G(128),A,T    | silu, gelu                                                  | <sup>6</sup>          | Y       | [`deep_gemm_moe_fp8`][vllm.model_executor.layers.fused_moe.deep_gemm_moe.deep_gemm_moe_fp8],</br>[`DeepGemmExperts`][vllm.model_executor.layers.fused_moe.deep_gemm_moe.DeepGemmExperts],</br>[`BatchedDeepGemmExperts`][vllm.model_executor.layers.fused_moe.batched_deep_gemm_moe.BatchedDeepGemmExperts] |\n| cutlass_fp4                  | standard,</br>batched | nvfp4            | A,T           | silu                                                        | Y                     | Y       | [`cutlass_moe_fp4`][vllm.model_executor.layers.fused_moe.cutlass_moe.cutlass_moe_fp4],</br>[`CutlassExpertsFp4`][vllm.model_executor.layers.fused_moe.cutlass_moe.CutlassExpertsFp4]                                                                                                                        |", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Experts Kernels", "level": 2, "text": "| cutlass_fp8                  | standard,</br>batched | fp8              | A,T           | silu, gelu                                                  | Y                     | Y       | [`cutlass_moe_fp8`][vllm.model_executor.layers.fused_moe.cutlass_moe.cutlass_moe_fp8],</br>[`CutlassExpertsFp8`][vllm.model_executor.layers.fused_moe.cutlass_moe.CutlassExpertsFp8],</br>[`CutlasBatchedExpertsFp8`][vllm.model_executor.layers.fused_moe.cutlass_moe.CutlassBatchedExpertsFp8]            |\n| flashinfer                   | standard              | nvfp4,</br>fp8   | T             | <sup>5</sup>                                                | N                     | Y       | [`flashinfer_cutlass_moe_fp4`][vllm.model_executor.layers.fused_moe.flashinfer_cutlass_moe.flashinfer_cutlass_moe_fp4],</br>[`FlashInferExperts`][vllm.model_executor.layers.fused_moe.flashinfer_cutlass_moe.FlashInferExperts]                                                                            |", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Experts Kernels", "level": 2, "text": "| gpt oss triton               | standard              | N/A              | N/A           | <sup>5</sup>                                                | Y                     | Y       | [`triton_kernel_fused_experts`][vllm.model_executor.layers.fused_moe.gpt_oss_triton_kernels_moe.triton_kernel_fused_experts],</br>[`OAITritonExperts`][vllm.model_executor.layers.fused_moe.gpt_oss_triton_kernels_moe.OAITritonExperts]                                                                    |\n| deep gemm+triton<sup>2</sup> | standard,</br>batched | all<sup>1</sup>  | G(128),A,T    | silu, gelu                                                  | <sup>6</sup>          | Y       | [`TritonOrDeepGemmExperts`][vllm.model_executor.layers.fused_moe.triton_deep_gemm_moe.TritonOrDeepGemmExperts],</br>[`BatchedTritonOrDeepGemmExperts`][vllm.model_executor.layers.fused_moe.batched_triton_or_deep_gemm_moe.BatchedTritonOrDeepGemmExperts]                                                 |", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Experts Kernels", "level": 2, "text": "| marlin                       | standard              | <sup>3</sup>     | <sup>3</sup>  | silu,</br>swigluoai                                         | Y                     | N       | [`fused_marlin_moe`][vllm.model_executor.layers.fused_moe.fused_marlin_moe.fused_marlin_moe]                                                                                                                                                                                                                |\n| marlin experts               | standard              | N/A              | N/A           | silu,</br>swigluoai                                         | Y                     | Y       | [`MarlinExperts`][vllm.model_executor.layers.fused_moe.fused_marlin_moe.MarlinExperts]                                                                                                                                                                                                                      |", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Experts Kernels", "level": 2, "text": "| trtllm                       | standard              | mxfp4,</br>nvfp4 | G(16),G(32)   | <sup>5</sup>                                                | N                     | Y       | [`TrtLlmGenExperts`][vllm.model_executor.layers.fused_moe.trtllm_moe.TrtLlmGenExperts]                                                                                                                                                                                                                      |\n| pallas                       | standard              | N/A              | N/A           | silu                                                        | N                     | N       | [`fused_moe`][vllm.model_executor.layers.fused_moe.moe_pallas.fused_moe]                                                                                                                                                                                                                                    |", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Experts Kernels", "level": 2, "text": "| iterative                    | standard              | N/A              | N/A           | silu                                                        | N                     | N       | [`fused_moe`][vllm.model_executor.layers.fused_moe.moe_torch_iterative.fused_moe]                                                                                                                                                                                                                           |\n| rocm aiter moe               | standard              | fp8              | G(128),A,T    | silu, gelu                                                  | Y                     | N       | [`rocm_aiter_fused_experts`][vllm.model_executor.layers.fused_moe.rocm_aiter_fused_moe.rocm_aiter_fused_moe_impl]                                                                                                                                                                                           |", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Experts Kernels", "level": 2, "text": "| cpu_fused_moe                | standard              | N/A              | N/A           | silu                                                        | N                     | N       | [`CPUFusedMOE`][vllm.model_executor.layers.fused_moe.cpu_fused_moe.CPUFusedMOE]                                                                                                                                                                                                                             |\n| naive batched<sup>4</sup>    | batched               | int8,</br>fp8    | G,A,T         | silu, gelu                                                  | <sup>6</sup>          | Y       | [`NaiveBatchedExperts`][vllm.model_executor.layers.fused_moe.fused_batched_moe.NaiveBatchedExperts]                                                                                                                                                                                                         |  \n!!! info \"Table key\"", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Fused MoE Experts Kernels", "level": 2, "text": "!!! info \"Table key\"\n1. All types: mxfp4, nvfp4, int4, int8, fp8\n2. A dispatcher wrapper around triton and deep gemm experts.  Will select based on type + shape + quantization params\n3. uint4, uint8, fp8, fp4\n4. This is a naive implementation of experts that supports batched format. Mainly used for testing.\n5. The `activation` parameter is ignored and SwiGlu is used by default instead.\n6. Only handled by or supported when used with modular kernels.", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Modular Kernel \"families\"", "level": 2, "text": "## Modular Kernel \"families\"  \nThe following table shows \"families\" of modular kernels that are intended to work together. There are some combinations which may work but have not yet been tested, e.g. flashinfer with other fp8 experts. Note that the \"naive\" backend will work with any non-modular experts.  \n| backend                          | `FusedMoEPrepareAndFinalize` subclasses                    | `FusedMoEPermuteExpertsUnpermute` subclasses                                                                               |\n|----------------------------------|------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------|\n| deepep_high_throughput           | `DeepEPHTPrepareAndFinalize`                               |  `DeepGemmExperts`,</br>`TritonExperts`,</br>`TritonOrDeepGemmExperts`,</br>`CutlassExpertsFp8`, </br>`MarlinExperts`      |", "file_path": "design/moe_kernel_features.md"}
{"id": "c8ad4a5c8763dbf4ff15ad95f721bad468851873f3ce9ee2d0e4f96d10de16b9", "heading": "Fused MoE Kernel features/Modular Kernel \"families\"", "level": 2, "text": "| deepep_low_latency,</br>pplx     | `DeepEPLLPrepareAndFinalize`,</br>`PplxPrepareAndFinalize` |  `BatchedDeepGemmExperts`,</br>`BatchedTritonExperts`,</br>`BatchedTritonOrDeepGemmExperts`,</br>`CutlassBatchedExpertsFp8`|\n| flashinfer                       | `FlashInferCutlassMoEPrepareAndFinalize`                   | `FlashInferExperts`                                                                                                        |", "file_path": "design/moe_kernel_features.md"}
{"id": "09867e8a3bac92fb6d74259bff65c25a960dec3736e8d41d7006a25bf6fddebd", "heading": "Python Multiprocessing/Debugging", "level": 2, "text": "# Python Multiprocessing  \n## Debugging  \nPlease see the [Troubleshooting][troubleshooting-python-multiprocessing]\npage for information on known issues and how to solve them.", "file_path": "design/multiprocessing.md"}
{"id": "09867e8a3bac92fb6d74259bff65c25a960dec3736e8d41d7006a25bf6fddebd", "heading": "Python Multiprocessing/Introduction", "level": 2, "text": "## Introduction  \n!!! important\nThe source code references are to the state of the code at the time of writing in December 2024.  \nThe use of Python multiprocessing in vLLM is complicated by:  \n- The use of vLLM as a library and the inability to control the code using vLLM\n- Varying levels of incompatibilities between multiprocessing methods and vLLM\ndependencies  \nThis document describes how vLLM deals with these challenges.", "file_path": "design/multiprocessing.md"}
{"id": "09867e8a3bac92fb6d74259bff65c25a960dec3736e8d41d7006a25bf6fddebd", "heading": "Python Multiprocessing/Multiprocessing Methods", "level": 2, "text": "## Multiprocessing Methods  \n[Python multiprocessing methods](https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods) include:  \n- `spawn` - spawn a new Python process. The default on Windows and macOS.  \n- `fork` - Use `os.fork()` to fork the Python interpreter. The default on\nLinux for Python versions prior to 3.14.  \n- `forkserver` - Spawn a server process that will fork a new process on request.\nThe default on Linux for Python version 3.14 and newer.", "file_path": "design/multiprocessing.md"}
{"id": "09867e8a3bac92fb6d74259bff65c25a960dec3736e8d41d7006a25bf6fddebd", "heading": "Python Multiprocessing/Multiprocessing Methods/Tradeoffs", "level": 3, "text": "### Tradeoffs  \n`fork` is the fastest method, but is incompatible with dependencies that use\nthreads. If you are under macOS, using `fork` may cause the process to crash.  \n`spawn` is more compatible with dependencies, but can be problematic when vLLM\nis used as a library. If the consuming code does not use a `__main__` guard (`if\n__name__ == \"__main__\":`), the code will be inadvertently re-executed when vLLM\nspawns a new process. This can lead to infinite recursion, among other problems.  \n`forkserver` will spawn a new server process that will fork new processes on\ndemand. This unfortunately has the same problem as `spawn` when vLLM is used as\na library. The server process is created as a spawned new process, which will\nre-execute code not protected by a `__main__` guard.  \nFor both `spawn` and `forkserver`, the process must not depend on inheriting any\nglobal state as would be the case with `fork`.", "file_path": "design/multiprocessing.md"}
{"id": "09867e8a3bac92fb6d74259bff65c25a960dec3736e8d41d7006a25bf6fddebd", "heading": "Python Multiprocessing/Compatibility with Dependencies", "level": 2, "text": "## Compatibility with Dependencies  \nMultiple vLLM dependencies indicate either a preference or requirement for using\n`spawn`:  \n- <https://pytorch.org/docs/stable/notes/multiprocessing.html#cuda-in-multiprocessing>\n- <https://pytorch.org/docs/stable/multiprocessing.html#sharing-cuda-tensors>\n- <https://docs.habana.ai/en/latest/PyTorch/Getting_Started_with_PyTorch_and_Gaudi/Getting_Started_with_PyTorch.html?highlight=multiprocessing#torch-multiprocessing-for-dataloaders>  \nIt is perhaps more accurate to say that there are known problems with using\n`fork` after initializing these dependencies.", "file_path": "design/multiprocessing.md"}
{"id": "09867e8a3bac92fb6d74259bff65c25a960dec3736e8d41d7006a25bf6fddebd", "heading": "Python Multiprocessing/Current State (v0)", "level": 2, "text": "## Current State (v0)  \nThe environment variable `VLLM_WORKER_MULTIPROC_METHOD` can be used to control which method is used by vLLM. The current default is `fork`.  \n- <https://github.com/vllm-project/vllm/blob/d05f88679bedd73939251a17c3d785a354b2946c/vllm/envs.py#L339-L342>  \nWhen we know we own the process because the `vllm` command was used, we use\n`spawn` because it's the most widely compatible.  \n- <https://github.com/vllm-project/vllm/blob/d05f88679bedd73939251a17c3d785a354b2946c/vllm/scripts.py#L123-L140>  \nThe `multiproc_xpu_executor` forces the use of `spawn`.  \n- <https://github.com/vllm-project/vllm/blob/d05f88679bedd73939251a17c3d785a354b2946c/vllm/executor/multiproc_xpu_executor.py#L14-L18>  \nThere are other miscellaneous places hard-coding the use of `spawn`:  \n- <https://github.com/vllm-project/vllm/blob/d05f88679bedd73939251a17c3d785a354b2946c/vllm/distributed/device_communicators/all_reduce_utils.py#L135>", "file_path": "design/multiprocessing.md"}
{"id": "09867e8a3bac92fb6d74259bff65c25a960dec3736e8d41d7006a25bf6fddebd", "heading": "Python Multiprocessing/Current State (v0)", "level": 2, "text": "- <https://github.com/vllm-project/vllm/blob/d05f88679bedd73939251a17c3d785a354b2946c/vllm/entrypoints/openai/api_server.py#L184>  \nRelated PRs:  \n- <gh-pr:8823>", "file_path": "design/multiprocessing.md"}
{"id": "09867e8a3bac92fb6d74259bff65c25a960dec3736e8d41d7006a25bf6fddebd", "heading": "Python Multiprocessing/Prior State in v1", "level": 2, "text": "## Prior State in v1  \nThere was an environment variable to control whether multiprocessing is used in\nthe v1 engine core, `VLLM_ENABLE_V1_MULTIPROCESSING`. This defaulted to off.  \n- <https://github.com/vllm-project/vllm/blob/d05f88679bedd73939251a17c3d785a354b2946c/vllm/envs.py#L452-L454>  \nWhen it was enabled, the v1 `LLMEngine` would create a new process to run the\nengine core.  \n- <https://github.com/vllm-project/vllm/blob/d05f88679bedd73939251a17c3d785a354b2946c/vllm/v1/engine/llm_engine.py#L93-L95>\n- <https://github.com/vllm-project/vllm/blob/d05f88679bedd73939251a17c3d785a354b2946c/vllm/v1/engine/llm_engine.py#L70-L77>\n- <https://github.com/vllm-project/vllm/blob/d05f88679bedd73939251a17c3d785a354b2946c/vllm/v1/engine/core_client.py#L44-L45>  \nIt was off by default for all the reasons mentioned above - compatibility with\ndependencies and code using vLLM as a library.", "file_path": "design/multiprocessing.md"}
{"id": "09867e8a3bac92fb6d74259bff65c25a960dec3736e8d41d7006a25bf6fddebd", "heading": "Python Multiprocessing/Prior State in v1/Changes Made in v1", "level": 3, "text": "### Changes Made in v1  \nThere is not an easy solution with Python's `multiprocessing` that will work\neverywhere. As a first step, we can get v1 into a state where it does \"best\neffort\" choice of multiprocessing method to maximize compatibility.  \n- Default to `fork`.\n- Use `spawn` when we know we control the main process (`vllm` was executed).\n- If we detect `cuda` was previously initialized, force `spawn` and emit a\nwarning. We know `fork` will break, so this is the best we can do.  \nThe case that is known to still break in this scenario is code using vLLM as a\nlibrary that initializes `cuda` before calling vLLM. The warning we emit should\ninstruct users to either add a `__main__` guard or to disable multiprocessing.  \nIf that known-failure case occurs, the user will see two messages that explain\nwhat is happening. First, a log message from vLLM:  \n```console\nWARNING 12-11 14:50:37 multiproc_worker_utils.py:281] CUDA was previously\ninitialized. We must use the `spawn` multiprocessing start method. Setting", "file_path": "design/multiprocessing.md"}
{"id": "09867e8a3bac92fb6d74259bff65c25a960dec3736e8d41d7006a25bf6fddebd", "heading": "Python Multiprocessing/Prior State in v1/Changes Made in v1", "level": 3, "text": "VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See\nhttps://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing\nfor more information.\n```  \nSecond, Python itself will raise an exception with a nice explanation:  \n```console\nRuntimeError:\nAn attempt has been made to start a new process before the\ncurrent process has finished its bootstrapping phase.", "file_path": "design/multiprocessing.md"}
{"id": "09867e8a3bac92fb6d74259bff65c25a960dec3736e8d41d7006a25bf6fddebd", "heading": "Python Multiprocessing/Prior State in v1/Changes Made in v1", "level": 3, "text": "This probably means that you are not using fork to start your\nchild processes and you have forgotten to use the proper idiom\nin the main module:\n\nif __name__ == '__main__':\nfreeze_support()\n...\n\nThe \"freeze_support()\" line can be omitted if the program\nis not going to be frozen to produce an executable.\n\nTo fix this issue, refer to the \"Safe importing of main module\"\nsection in https://docs.python.org/3/library/multiprocessing.html\n```", "file_path": "design/multiprocessing.md"}
{"id": "09867e8a3bac92fb6d74259bff65c25a960dec3736e8d41d7006a25bf6fddebd", "heading": "Python Multiprocessing/Alternatives Considered/Detect if a `__main__` guard is present", "level": 3, "text": "## Alternatives Considered  \n### Detect if a `__main__` guard is present  \nIt has been suggested that we could behave better if we could detect whether\ncode using vLLM as a library has a `__main__` guard in place. This [post on\nstackoverflow](https://stackoverflow.com/questions/77220442/multiprocessing-pool-in-a-python-class-without-name-main-guard)\nwas from a library author facing the same question.  \nIt is possible to detect whether we are in the original, `__main__` process, or\na subsequent spawned process. However, it does not appear to be straight forward\nto detect whether a `__main__` guard is present in the code.  \nThis option has been discarded as impractical.", "file_path": "design/multiprocessing.md"}
{"id": "09867e8a3bac92fb6d74259bff65c25a960dec3736e8d41d7006a25bf6fddebd", "heading": "Python Multiprocessing/Alternatives Considered/Use `forkserver`", "level": 3, "text": "### Use `forkserver`  \nAt first it appears that `forkserver` is a nice solution to the problem.\nHowever, the way it works presents the same challenges that `spawn` does when\nvLLM is used as a library.", "file_path": "design/multiprocessing.md"}
{"id": "09867e8a3bac92fb6d74259bff65c25a960dec3736e8d41d7006a25bf6fddebd", "heading": "Python Multiprocessing/Alternatives Considered/Force `spawn` all the time", "level": 3, "text": "### Force `spawn` all the time  \nOne way to clean this up is to just force the use of `spawn` all the time and\ndocument that the use of a `__main__` guard is required when using vLLM as a\nlibrary. This would unfortunately break existing code and make vLLM harder to\nuse, violating the desire to make the `LLM` class as easy as possible to use.  \nInstead of pushing this on our users, we will retain the complexity to do our\nbest to make things work.", "file_path": "design/multiprocessing.md"}
{"id": "09867e8a3bac92fb6d74259bff65c25a960dec3736e8d41d7006a25bf6fddebd", "heading": "Python Multiprocessing/Future Work", "level": 2, "text": "## Future Work  \nWe may want to consider a different worker management approach in the future\nthat works around these challenges.  \n1. We could implement something `forkserver`-like, but have the process manager\nbe something we initially launch by running our own subprocess and a custom\nentrypoint for worker management (launch a `vllm-manager` process).  \n2. We can explore other libraries that may better suit our needs. Examples to\nconsider:  \n- <https://github.com/joblib/loky>", "file_path": "design/multiprocessing.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector", "level": 1, "text": "# P2P NCCL Connector  \nAn implementation of xPyD with dynamic scaling based on point-to-point communication, partly inspired by Dynamo.", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Detailed Design/Overall Process", "level": 3, "text": "## Detailed Design  \n### Overall Process  \nAs shown in Figure 1, the overall process of this **PD disaggregation** solution is described through a request flow:  \n1. The client sends an HTTP request to the Proxy/Router's `/v1/completions` interface.\n2. The Proxy/Router selects a **1P1D (1 Prefill instance + 1 Decode instance)** through either through round-robin or random selection, generates a `request_id` (rules to be introduced later), modifies the `max_tokens` in the HTTP request message to **1**, and then forwards the request to the **P instance**.\n3. Immediately afterward, the Proxy/Router forwards the **original HTTP request** to the **D instance**.\n4. The **P instance** performs **Prefill** and then **actively sends the generated KV cache** to the D instance (using **PUT_ASYNC** mode). The D instance's `zmq_addr` can be resolved through the `request_id`.", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Detailed Design/Overall Process", "level": 3, "text": "5. The **D instance** has a **dedicated thread** for receiving the KV cache (to avoid blocking the main process). The received KV cache is saved into the **GPU memory buffer**, the size of which is determined by the vLLM startup parameter `kv_buffer_size`. When the GPU buffer is full, the KV cache is stored in the **local Tensor memory pool**.\n6. During the **Decode**, the D instance's main process retrieves the KV cache (transmitted by the P instance) from either the **GPU buffer** or the **memory pool**, thereby **skipping Prefill**.\n7. After completing **Decode**, the D instance returns the result to the **Proxy/Router**, which then forwards it to the **client**.  \n![image1](https://github.com/user-attachments/assets/fb01bde6-755b-49f7-ad45-48a94b1e10a7)", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Detailed Design/Proxy/Router (Demo)", "level": 3, "text": "### Proxy/Router (Demo)  \nA simple HTTP service acts as the entry point for client requests and starts a background thread to listen for P/D instances reporting their HTTP IP and PORT, as well as ZMQ IP and PORT. It maintains a dictionary of `http_addr -> zmq_addr`. The `http_addr` is the IP:PORT for the vLLM instance's request, while the `zmq_addr` is the address for KV cache handshake and metadata reception.  \nThe Proxy/Router is responsible for selecting 1P1D based on the characteristics of the client request, such as the prompt, and generating a corresponding `request_id`, for example:  \n```text\ncmpl-___prefill_addr_10.0.1.2:21001___decode_addr_10.0.1.3:22001_93923d63113b4b338973f24d19d4bf11-0\n```  \nCurrently, to quickly verify whether xPyD can work, a round-robin selection of 1P1D is used. In the future, it is planned to use a trie combined with the load status of instances to select appropriate P and D.", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Detailed Design/Proxy/Router (Demo)", "level": 3, "text": "Each P/D instance periodically sends a heartbeat packet to the Proxy/Router (currently every 3 seconds) to register (i.e., report `http_addr -> zmq_addr`) and keep the connection alive. If an instance crashes and fails to send a ping for a certain period of time, the Proxy/Router will remove the timed-out instance (this feature has not yet been developed).", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Detailed Design/KV Cache Transfer Methods", "level": 3, "text": "### KV Cache Transfer Methods  \nThere are three methods for KVCache transfer: PUT, GET, and PUT_ASYNC. These methods can be specified using the `--kv-transfer-config` and `kv_connector_extra_config` parameters, specifically through the `send_type` field. Both PUT and PUT_ASYNC involve the P instance actively sending KVCache to the D instance. The difference is that PUT is a synchronous transfer method that blocks the main process, while PUT_ASYNC is an asynchronous transfer method. PUT_ASYNC uses a dedicated thread for sending KVCache, which means it does not block the main process. In contrast, the GET method involves the P instance saving the KVCache to the memory buffer after computing the prefill. The D instance then actively retrieves the computed KVCache from the P instance once it has allocated space for the KVCache.  \nExperimental results have shown that the performance of these methods, from highest to lowest, is as follows: PUT_ASYNC â†’ GET â†’ PUT.", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Detailed Design/P2P Communication via ZMQ & NCCL", "level": 3, "text": "### P2P Communication via ZMQ & NCCL  \nAs long as the address of the counterpart is known, point-to-point KV cache transfer (using NCCL) can be performed, without being constrained by rank and world size. To support dynamic scaling (expansion and contraction) of instances with PD disaggregation. This means that adding or removing P/D instances does not require a full system restart.  \nEach P/D instance only needs to create a single `P2pNcclEngine` instance. This instance maintains a ZMQ Server, which runs a dedicated thread to listen on the `zmq_addr` address and receive control flow requests from other instances. These requests include requests to establish an NCCL connection and requests to send KVCache metadata (such as tensor shapes and data types). However, it does not actually transmit the KVCache data itself.", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Detailed Design/P2P Communication via ZMQ & NCCL", "level": 3, "text": "When a P instance and a D instance transmit KVCache for the first time, they need to establish a ZMQ connection and an NCCL group. For subsequent KVCache transmissions, this ZMQ connection and NCCL group are reused. The NCCL group consists of only two ranks, meaning the world size is equal to 2. This design is intended to support dynamic scaling, which means that adding or removing P/D instances does not require a full system restart. As long as the address of the counterpart is known, point-to-point KVCache transmission can be performed, without being restricted by rank or world size.", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Detailed Design/NCCL Group Topology", "level": 3, "text": "### NCCL Group Topology  \nCurrently, only symmetric TP (Tensor Parallelism) methods are supported for KVCache transmission. Asymmetric TP and PP (Pipeline Parallelism) methods will be supported in the future. Figure 2 illustrates the 1P2D setup, where each instance has a TP (Tensor Parallelism) degree of 2. There are a total of 7 NCCL groups: three vLLM instances each have one NCCL group with TP=2. Additionally, the 0th GPU card of the P instance establishes an NCCL group with the 0th GPU card of each D instance. Similarly, the 1st GPU card of the P instance establishes an NCCL group with the 1st GPU card of each D instance.  \n![image2](https://github.com/user-attachments/assets/837e61d6-365e-4cbf-8640-6dd7ab295b36)", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Detailed Design/NCCL Group Topology", "level": 3, "text": "Each NCCL group occupies a certain amount of GPU memory buffer for communication, the size of which is primarily influenced by the `NCCL_MAX_NCHANNELS` environment variable. When `NCCL_MAX_NCHANNELS=16`, an NCCL group typically occupies 100MB, while when `NCCL_MAX_NCHANNELS=8`, it usually takes up 52MB. For large-scale xPyD configurationsâ€”such as DeepSeek's 96P144Dâ€”this implementation is currently not feasible. Moving forward, we are considering using RDMA for point-to-point communication and are also keeping an eye on UCCL.", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Detailed Design/GPU Memory Buffer and Tensor Memory Pool", "level": 3, "text": "### GPU Memory Buffer and Tensor Memory Pool  \nThe trade-off in the size of the memory buffer is as follows: For P instances, the memory buffer is not required in PUT and PUT_ASYNC modes, but it is necessary in GET mode. For D instances, a memory buffer is needed in all three modes. The memory buffer for D instances should not be too large. Similarly, for P instances in GET mode, the memory buffer should also not be too large. The memory buffer of D instances is used to temporarily store KVCache sent by P instances. If it is too large, it will reduce the KVCache space available for normal inference by D instances, thereby decreasing the inference batch size and ultimately leading to a reduction in output throughput. The size of the memory buffer is configured by the parameter `kv_buffer_size`, measured in bytes, and is typically set to 5%ï½ž10% of the memory size.", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Detailed Design/GPU Memory Buffer and Tensor Memory Pool", "level": 3, "text": "If the `--max-num-seqs` parameter for P instances is set to a large value, due to the large batch size, P instances will generate a large amount of KVCache simultaneously. This may exceed the capacity of the memory buffer of D instances, resulting in KVCache loss. Once KVCache is lost, D instances need to recompute Prefill, which is equivalent to performing Prefill twice. Consequently, the time-to-first-token (TTFT) will significantly increase, leading to degraded performance.", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Detailed Design/GPU Memory Buffer and Tensor Memory Pool", "level": 3, "text": "To address the above issues, I have designed and developed a local Tensor memory pool for storing KVCache, inspired by the buddy system used in Linux memory modules. Since the memory is sufficiently large, typically in the TB range on servers, there is no need to consider prefix caching or using block-based designs to reuse memory, thereby saving space. When the memory buffer is insufficient, KVCache can be directly stored in the Tensor memory pool, and D instances can subsequently retrieve KVCache from it. The read and write speed is that of PCIe, with PCIe 4.0 having a speed of approximately 21 GB/s, which is usually faster than the Prefill speed. Otherwise, solutions like Mooncake and lmcache would not be necessary. The Tensor memory pool acts as a flood diversion area, typically unused except during sudden traffic surges. In the worst-case scenario, my solution performs no worse than the normal situation with a Cache store.", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Install vLLM", "level": 2, "text": "## Install vLLM  \n```shell\npip install \"vllm>=0.9.2\"\n```", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Run xPyD/Instructions", "level": 3, "text": "## Run xPyD  \n### Instructions  \n- The following examples are run on an A800 (80GB) device, using the Meta-Llama-3.1-8B-Instruct model.\n- Pay attention to the setting of the `kv_buffer_size` (in bytes). The empirical value is 10% of the GPU memory size. This is related to the kvcache size. If it is too small, the GPU memory buffer for temporarily storing the received kvcache will overflow, causing the kvcache to be stored in the tensor memory pool, which increases latency. If it is too large, the kvcache available for inference will be reduced, leading to a smaller batch size and decreased throughput.\n- For Prefill instances, when using non-GET mode, the `kv_buffer_size` can be set to 1, as Prefill currently does not need to receive kvcache. However, when using GET mode, a larger `kv_buffer_size` is required because it needs to store the kvcache sent to the D instance.\n- You may need to modify the `kv_buffer_size` and `port` in the following commands (if there is a conflict).", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Run xPyD/Instructions", "level": 3, "text": "- `PUT_ASYNC` offers the best performance and should be prioritized.\n- The `--port` must be consistent with the `http_port` in the `--kv-transfer-config`.\n- The `disagg_proxy_p2p_nccl_xpyd.py` script will use port 10001 (for receiving client requests) and port 30001 (for receiving service discovery from P and D instances).\n- The node running the proxy must have `quart` installed.\n- Supports multiple nodes; you just need to modify the `proxy_ip` and `proxy_port` in `--kv-transfer-config`.\n- In the following examples, it is assumed that **the proxy's IP is 10.0.1.1**.", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Run xPyD/Run 1P3D", "level": 3, "text": "### Run 1P3D  \n#### Proxy (e.g. 10.0.1.1)  \n```shell\ncd {your vllm directory}/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/\npython3 disagg_proxy_p2p_nccl_xpyd.py &\n```  \n#### Prefill1 (e.g. 10.0.1.2 or 10.0.1.1)  \n??? console \"Command\"  \n```shell\nCUDA_VISIBLE_DEVICES=0 vllm serve {your model directory} \\\n--host 0.0.0.0 \\\n--port 20001 \\\n--tensor-parallel-size 1 \\\n--seed 1024 \\\n--served-model-name base_model \\\n--dtype float16 \\\n--max-model-len 10000 \\\n--max-num-batched-tokens 10000 \\\n--max-num-seqs 256 \\\n--trust-remote-code \\\n--gpu-memory-utilization 0.9 \\\n--kv-transfer-config \\\n'{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_producer\",\"kv_buffer_size\":\"1e1\",\"kv_port\":\"21001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20001\"}}' > /var/vllm.log 2>&1 &\n```  \n#### Decode1 (e.g. 10.0.1.3 or 10.0.1.1)  \n??? console \"Command\"  \n```shell\nCUDA_VISIBLE_DEVICES=1 vllm serve {your model directory} \\\n--host 0.0.0.0 \\\n--port 20002 \\\n--tensor-parallel-size 1 \\", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Run xPyD/Run 1P3D", "level": 3, "text": "--host 0.0.0.0 \\\n--port 20002 \\\n--tensor-parallel-size 1 \\\n--seed 1024 \\\n--served-model-name base_model \\\n--dtype float16 \\\n--max-model-len 10000 \\\n--max-num-batched-tokens 10000 \\\n--max-num-seqs 256 \\\n--trust-remote-code \\\n--gpu-memory-utilization 0.7 \\\n--kv-transfer-config \\\n'{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_consumer\",\"kv_buffer_size\":\"8e9\",\"kv_port\":\"22001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20002\"}}' > /var/vllm.log 2>&1 &\n```  \n#### Decode2 (e.g. 10.0.1.4 or 10.0.1.1)  \n??? console \"Command\"  \n```shell\nCUDA_VISIBLE_DEVICES=2 vllm serve {your model directory} \\\n--host 0.0.0.0 \\\n--port 20003 \\\n--tensor-parallel-size 1 \\\n--seed 1024 \\\n--served-model-name base_model \\\n--dtype float16 \\\n--max-model-len 10000 \\\n--max-num-batched-tokens 10000 \\\n--max-num-seqs 256 \\\n--trust-remote-code \\\n--gpu-memory-utilization 0.7 \\\n--kv-transfer-config \\", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Run xPyD/Run 1P3D", "level": 3, "text": "--gpu-memory-utilization 0.7 \\\n--kv-transfer-config \\\n'{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_consumer\",\"kv_buffer_size\":\"8e9\",\"kv_port\":\"23001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20003\"}}' > /var/vllm.log 2>&1 &\n```  \n#### Decode3 (e.g. 10.0.1.5 or 10.0.1.1)  \n??? console \"Command\"  \n```shell\nCUDA_VISIBLE_DEVICES=3 vllm serve {your model directory} \\\n--host 0.0.0.0 \\\n--port 20004 \\\n--tensor-parallel-size 1 \\\n--seed 1024 \\\n--served-model-name base_model \\\n--dtype float16 \\\n--max-model-len 10000 \\\n--max-num-batched-tokens 10000 \\\n--max-num-seqs 256 \\\n--trust-remote-code \\\n--gpu-memory-utilization 0.7 \\\n--kv-transfer-config \\\n'{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_consumer\",\"kv_buffer_size\":\"8e9\",\"kv_port\":\"24001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20004\"}}' > /var/vllm.log 2>&1 &\n```", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Run xPyD/Run 3P1D", "level": 3, "text": "### Run 3P1D  \n#### Proxy (e.g. 10.0.1.1)  \n```shell\ncd {your vllm directory}/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/\npython3 disagg_proxy_p2p_nccl_xpyd.py &\n```  \n#### Prefill1 (e.g. 10.0.1.2 or 10.0.1.1)  \n??? console \"Command\"  \n```shell\nCUDA_VISIBLE_DEVICES=0 vllm serve {your model directory} \\\n--host 0.0.0.0 \\\n--port 20001 \\\n--tensor-parallel-size 1 \\\n--seed 1024 \\\n--served-model-name base_model \\\n--dtype float16 \\\n--max-model-len 10000 \\\n--max-num-batched-tokens 10000 \\\n--max-num-seqs 256 \\\n--trust-remote-code \\\n--gpu-memory-utilization 0.9 \\\n--kv-transfer-config \\\n'{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_producer\",\"kv_buffer_size\":\"1e1\",\"kv_port\":\"21001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20001\"}}' > /var/vllm.log 2>&1 &\n```  \n#### Prefill2 (e.g. 10.0.1.3 or 10.0.1.1)  \n??? console \"Command\"  \n```shell\nCUDA_VISIBLE_DEVICES=1 vllm serve {your model directory} \\\n--host 0.0.0.0 \\\n--port 20002 \\\n--tensor-parallel-size 1 \\", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Run xPyD/Run 3P1D", "level": 3, "text": "--host 0.0.0.0 \\\n--port 20002 \\\n--tensor-parallel-size 1 \\\n--seed 1024 \\\n--served-model-name base_model \\\n--dtype float16 \\\n--max-model-len 10000 \\\n--max-num-batched-tokens 10000 \\\n--max-num-seqs 256 \\\n--trust-remote-code \\\n--gpu-memory-utilization 0.9 \\\n--kv-transfer-config \\\n'{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_producer\",\"kv_buffer_size\":\"1e1\",\"kv_port\":\"22001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20002\"}}' > /var/vllm.log 2>&1 &\n```  \n#### Prefill3 (e.g. 10.0.1.4 or 10.0.1.1)  \n??? console \"Command\"  \n```shell\nCUDA_VISIBLE_DEVICES=2 vllm serve {your model directory} \\\n--host 0.0.0.0 \\\n--port 20003 \\\n--tensor-parallel-size 1 \\\n--seed 1024 \\\n--served-model-name base_model \\\n--dtype float16 \\\n--max-model-len 10000 \\\n--max-num-batched-tokens 10000 \\\n--max-num-seqs 256 \\\n--trust-remote-code \\\n--gpu-memory-utilization 0.9 \\\n--kv-transfer-config \\", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Run xPyD/Run 3P1D", "level": 3, "text": "--gpu-memory-utilization 0.9 \\\n--kv-transfer-config \\\n'{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_producer\",\"kv_buffer_size\":\"1e1\",\"kv_port\":\"23001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20003\"}}' > /var/vllm.log 2>&1 &\n```  \n#### Decode1 (e.g. 10.0.1.5 or 10.0.1.1)  \n??? console \"Command\"  \n```shell\nCUDA_VISIBLE_DEVICES=3 vllm serve {your model directory} \\\n--host 0.0.0.0 \\\n--port 20004 \\\n--tensor-parallel-size 1 \\\n--seed 1024 \\\n--served-model-name base_model \\\n--dtype float16 \\\n--max-model-len 10000 \\\n--max-num-batched-tokens 10000 \\\n--max-num-seqs 256 \\\n--trust-remote-code \\\n--gpu-memory-utilization 0.7 \\\n--kv-transfer-config \\\n'{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_consumer\",\"kv_buffer_size\":\"8e9\",\"kv_port\":\"24001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20004\"}}' > /var/vllm.log 2>&1 &\n```", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Single request", "level": 2, "text": "## Single request  \n```shell\ncurl -X POST -s http://10.0.1.1:10001/v1/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"base_model\",\n\"prompt\": \"San Francisco is a\",\n\"max_tokens\": 10,\n\"temperature\": 0\n}'\n```", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Benchmark", "level": 2, "text": "## Benchmark  \n??? console \"Command\"  \n```shell\nvllm bench serve \\\n--backend vllm \\\n--model base_model \\\n--tokenizer meta-llama/Llama-3.1-8B-Instruct \\\n--dataset-name \"random\" \\\n--host 10.0.1.1 \\\n--port 10001 \\\n--random-input-len 1024 \\\n--random-output-len 1024 \\\n--ignore-eos \\\n--burstiness 100 \\\n--percentile-metrics \"ttft,tpot,itl,e2el\" \\\n--metric-percentiles \"90,95,99\" \\\n--seed $(date +%s) \\\n--trust-remote-code \\\n--request-rate 3 \\\n--num-prompts 1000\n```", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Shut down", "level": 2, "text": "## Shut down  \n```shell\npgrep python | xargs kill -9 && pkill -f python\n```", "file_path": "design/p2p_nccl_connector.md"}
{"id": "d1fe72942d1bb65a235a7b866e4146a553116828402734c34db1f32eef4518e9", "heading": "P2P NCCL Connector/Test data/**Scenario**: 1K input & 200 output tokens, E2E P99 latency ~2s", "level": 3, "text": "## Test data  \n### **Scenario**: 1K input & 200 output tokens, E2E P99 latency ~2s  \n![testdata](https://github.com/user-attachments/assets/cef0953b-4567-4bf9-b940-405b92a28eb1)", "file_path": "design/p2p_nccl_connector.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention", "level": 1, "text": "# Paged Attention  \n!!! warning\nThis is a historical document based on the [original paper for vLLM](https://arxiv.org/abs/2309.06180).\nIt no longer describes the code used in vLLM today.  \nCurrently, vLLM utilizes its own implementation of a multi-head query\nattention kernel (`csrc/attention/attention_kernels.cu`).\nThis kernel is designed to be compatible with\nvLLM's paged KV caches, where the key and value cache are stored in\nseparate blocks (note that this block concept differs from the GPU\nthread block. So in a later document, I will refer to vLLM paged\nattention block as \"block\", while refer to GPU thread block as\n\"thread block\").  \nTo achieve high performance, this kernel relies on a specially\ndesigned memory layout and access method, specifically when threads\nread data from global memory to shared memory. The purpose of this\ndocument is to provide a high-level explanation of the kernel\nimplementation step by step, aiding those who wish to learn about the", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention", "level": 1, "text": "vLLM multi-head query attention kernel. After going through this\ndocument, users will likely have a better understanding and feel easier\nto follow the actual implementation.  \nPlease note that this document may not cover all details, such as how\nto calculate the correct index for the corresponding data or the dot\nmultiplication implementation. However, after reading this document\nand becoming familiar with the high-level logic flow, it should be\neasier for you to read the actual code and understand the details.", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Inputs", "level": 2, "text": "## Inputs  \nThe kernel function takes a list of arguments for the current thread\nto perform its assigned work. The three most important arguments are\nthe input pointers `q`, `k_cache`, and `v_cache`, which point\nto query, key, and value data on global memory that need to be read\nand processed. The output pointer `out` points to global memory\nwhere the result should be written. These four pointers actually\nrefer to multi-dimensional arrays, but each thread only accesses the\nportion of data assigned to it. I have omitted all other runtime\nparameters here for simplicity.  \n```cpp\ntemplate<typename scalar_t, int HEAD_SIZE, int BLOCK_SIZE, int NUM_THREADS, int PARTITION_SIZE = 0>\n__device__ void paged_attention_kernel(\n... // Other side args.\nconst scalar_t* __restrict__ out,       // [num_seqs, num_heads, max_num_partitions, head_size]\nconst scalar_t* __restrict__ q,         // [num_seqs, num_heads, head_size]\nconst scalar_t* __restrict__ k_cache,   // [num_blocks, num_kv_heads, head_size/x, block_size, x]", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Inputs", "level": 2, "text": "const scalar_t* __restrict__ v_cache,   // [num_blocks, num_kv_heads, head_size, block_size]\n... // Other side args.\n)\n```  \nThere are also a list of template arguments above the function\nsignature that are determined during compilation time. `scalar_t`\nrepresents the data type of the query, key, and value data elements,\nsuch as FP16. `HEAD_SIZE` indicates the number of elements in each\nhead. `BLOCK_SIZE` refers to the number of tokens in each block.\n`NUM_THREADS` denotes the number of threads in each thread block.\n`PARTITION_SIZE` represents the number of tensor parallel GPUs (For\nsimplicity, we assume this is 0 and tensor parallel is disabled).  \nWith these arguments, we need to perform a sequence of preparations.\nThis includes calculating the current head index, block index, and\nother necessary variables. However, for now, we can ignore these\npreparations and proceed directly to the actual calculations. It will\nbe easier to understand them once we grasp the entire flow.", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Concepts", "level": 2, "text": "## Concepts  \nJust before we dive into the calculation flow, I want to describe a\nfew concepts that are needed for later sections. However, you may\nskip this section and return later if you encounter any confusing\nterminologies.  \n- **Sequence**: A sequence represents a client request. For example,\nthe data pointed to by `q` has a shape of\n`[num_seqs, num_heads, head_size]`. That represents there are total\n`num_seqs` of query sequence data are pointed by `q`. Since this\nkernel is a single query attention kernel, each sequence only has one\nquery token. Hence, the `num_seqs` equals the total number of tokens\nthat are processed in the batch.\n- **Context**: The context consists of the generated tokens from the\nsequence. For instance, `[\"What\", \"is\", \"your\"]` are the context\ntokens, and the input query token is `\"name\"`. The model might\ngenerate the token `\"?\"`.\n- **Vec**: The vec is a list of elements that are fetched and\ncalculated together. For query and key data, the vec size", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Concepts", "level": 2, "text": "calculated together. For query and key data, the vec size\n(`VEC_SIZE`) is determined so that each thread group can fetch and\ncalculate 16 bytes of data at a time. For value data, the vec size\n(`V_VEC_SIZE`) is determined so that each thread can fetch and\ncalculate 16 bytes of data at a time. For example, if the\n`scalar_t` is FP16 (2 bytes) and `THREAD_GROUP_SIZE` is 2, the\n`VEC_SIZE` will be 4, while the `V_VEC_SIZE` will be 8.\n- **Thread group**: The thread group is a small group of\nthreads(`THREAD_GROUP_SIZE`) that fetches and calculates one\nquery token and one key token at a time. Each thread handles only a\nportion of the token data. The total number of elements processed by\none thread group is referred as `x`. For example, if the thread\ngroup contains 2 threads and the head size is 8, then thread 0\nhandles the query and key elements at index 0, 2, 4, 6, while thread\n1 handles the elements at index 1, 3, 5, 7.\n- **Block**: The key and value cache data in vLLM are split into", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Concepts", "level": 2, "text": "blocks. Each block stores data for a fixed number(`BLOCK_SIZE`)\nof tokens at one head. Each block may contain only a portion of the\nwhole context tokens. For example, if the block size is 16 and the\nhead size is 128, then for one head, one block can store 16 * 128 =\n2048 elements.\n- **Warp**: A warp is a group of 32 threads(`WARP_SIZE`) that\nexecute simultaneously on a stream multiprocessor (SM). In this\nkernel, each warp processes the calculation between one query token\nand key tokens of one entire block at a time (it may process multiple\nblocks in multiple iterations). For example, if there are 4 warps and\n6 blocks for one context, the assignment would be like warp 0 handles\nthe 0th, 4th blocks, warp 1 handles the 1st, 5th blocks, warp 2\nhandles the 2nd block and warp 3 handles the 3rd block.\n- **Thread block**: A thread block is a group of\nthreads(`NUM_THREADS`) that can access the same shared memory.\nEach thread block contains multiple warps(`NUM_WARPS`), and in", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Concepts", "level": 2, "text": "Each thread block contains multiple warps(`NUM_WARPS`), and in\nthis kernel, each thread block processes the calculation between one\nquery token and key tokens of a whole context.\n- **Grid**: A grid is a collection of thread blocks and defines the\nshape of the collection. In this kernel, the shape is\n`(num_heads, num_seqs, max_num_partitions)`. Therefore, each thread\nblock only handles the calculation for one head, one sequence, and\none partition.", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Query", "level": 2, "text": "## Query  \nThis section will introduce how query data is stored in memory and\nfetched by each thread. As mentioned above, each thread group fetches\none query token data, while each thread itself only handles a part of\none query token data. Within each warp, every thread group will fetch\nthe same query token data, but will multiply it with different key\ntoken data.  \n```cpp\nconst scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;\n```  \n<figure markdown=\"span\">\n![](../assets/design/paged_attention/query.png){ align=\"center\" alt=\"query\" width=\"70%\" }\n</figure>  \nEach thread defines its own `q_ptr` which points to the assigned\nquery token data on global memory. For example, if `VEC_SIZE` is 4\nand `HEAD_SIZE` is 128, the `q_ptr` points to data that contains\ntotal of 128 elements divided into 128 / 4 = 32 vecs.  \n<figure markdown=\"span\">\n![](../assets/design/paged_attention/q_vecs.png){ align=\"center\" alt=\"q_vecs\" width=\"70%\" }\n</figure>  \n```cpp", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Query", "level": 2, "text": "</figure>  \n```cpp\n__shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];\n```  \nNext, we need to read the global memory data pointed to by `q_ptr`\ninto shared memory as `q_vecs`. It is important to note that each\nvecs is assigned to a different row. For example, if the\n`THREAD_GROUP_SIZE` is 2, thread 0 will handle the 0th row vecs,\nwhile thread 1 handles the 1st row vecs. By reading the query data in\nthis way, neighboring threads like thread 0 and thread 1 can read\nneighbor memory, achieving the memory coalescing to improve\nperformance.", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Key", "level": 2, "text": "## Key  \nSimilar to the \"Query\" section, this section introduces memory layout\nand assignment for keys. While each thread group only handle one\nquery token one kernel run, it may handle multiple key tokens across\nmultiple iterations. Meanwhile, each warp will process multiple blocks\nof key tokens in multiple iterations, ensuring that all context\ntokens are processed by the entire thread group after the kernel run.\nIn this context, \"handle\" refers to performing the dot multiplication\nbetween query data and key data.  \n```cpp\nconst scalar_t* k_ptr = k_cache + physical_block_number * kv_block_stride\n+ kv_head_idx * kv_head_stride\n+ physical_block_offset * x;\n```  \nUnlike to `q_ptr`, `k_ptr` in each thread will point to different\nkey token at different iterations. As shown above, that `k_ptr`\npoints to key token data based on `k_cache` at assigned block,\nassigned head and assigned token.  \n<figure markdown=\"span\">\n![](../assets/design/paged_attention/key.png){ align=\"center\" alt=\"key\" width=\"70%\" }\n</figure>", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Key", "level": 2, "text": "</figure>  \nThe diagram above illustrates the memory layout for key data. It\nassumes that the `BLOCK_SIZE` is 16, `HEAD_SIZE` is 128, `x` is\n8, `THREAD_GROUP_SIZE` is 2, and there are a total of 4 warps. Each\nrectangle represents all the elements for one key token at one head,\nwhich will be processed by one thread group. The left half shows the\ntotal 16 blocks of key token data for warp 0, while the right half\nrepresents the remaining key token data for other warps or\niterations. Inside each rectangle, there are a total 32 vecs (128\nelements for one token) that will be processed by 2 threads (one\nthread group) separately.  \n<figure markdown=\"span\">\n![](../assets/design/paged_attention/k_vecs.png){ align=\"center\" alt=\"k_vecs\" width=\"70%\" }\n</figure>  \n```cpp\nK_vec k_vecs[NUM_VECS_PER_THREAD]\n```  \nNext, we need to read the key token data from `k_ptr` and store\nthem on register memory as `k_vecs`. We use register memory for\n`k_vecs` because it will only be accessed by one thread once,", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Key", "level": 2, "text": "`k_vecs` because it will only be accessed by one thread once,\nwhereas `q_vecs` will be accessed by multiple threads multiple\ntimes. Each `k_vecs` will contain multiple vectors for later\ncalculation. Each vec will be set at each inner iteration. The\nassignment of vecs allows neighboring threads in a warp to read\nneighboring memory together, which again promotes the memory\ncoalescing. For instance, thread 0 will read vec 0, while thread 1\nwill read vec 1. In the next inner loop, thread 0 will read vec 2,\nwhile thread 1 will read vec 3, and so on.  \nYou may still be a little confused about the overall flow. Don't\nworry, please keep reading the next \"QK\" section. It will illustrate\nthe query and key calculation flow in a clearer and higher-level\nmanner.", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/QK", "level": 2, "text": "## QK  \nAs shown the pseudo code below, before the entire for loop block, we\nfetch the query data for one token and store it in `q_vecs`. Then,\nin the outer for loop, we iterate through different `k_ptrs` that\npoint to different tokens and prepare the `k_vecs` in the inner for\nloop. Finally, we perform the dot multiplication between the\n`q_vecs` and each `k_vecs`.  \n```cpp\nq_vecs = ...\nfor ... {\nk_ptr = ...\nfor ... {\nk_vecs[i] = ...\n}\n...\nfloat qk = scale * Qk_dot<scalar_t, THREAD_GROUP_SIZE>::dot(q_vecs[thread_group_offset], k_vecs);\n}\n```  \nAs mentioned before, for each thread, it only fetches part of the\nquery and key token data at a time. However, there will be a cross\nthread group reduction happen in the `Qk_dot<>::dot` . So `qk`\nreturned here is not just between part of the query and key token dot\nmultiplication, but actually a full result between entire query and\nkey token data.  \nFor example, if the value of `HEAD_SIZE` is 128 and\n`THREAD_GROUP_SIZE` is 2, each thread's `k_vecs` will contain", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/QK", "level": 2, "text": "`THREAD_GROUP_SIZE` is 2, each thread's `k_vecs` will contain\ntotal 64 elements. However, the returned `qk` is actually the\nresult of dot multiplication between 128 query elements and 128 key\nelements. If you want to learn more about the details of the dot\nmultiplication and reduction, you may refer to the implementation of\n`Qk_dot<>::dot`. However, for the sake of simplicity, I will not\ncover it in this document.", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Softmax", "level": 2, "text": "## Softmax  \nNext, we need to calculate the normalized softmax for all `qk`s,\nas shown above, where each $x$ represents a `qk`. To do this,\nwe must obtain the reduced value of `qk_max`($m(x)$) and\nthe `exp_sum`($\\ell(x)$) of all `qk`s. The reduction\nshould be performed across the entire thread block, encompassing\nresults between the query token and all context key tokens.  \n$$\n\\begin{gather*}\nm(x):=\\max _i \\quad x_i \\\\ \\quad f(x):=\\left[\\begin{array}{lll}e^{x_1-m(x)} & \\ldots & e^{x_B-m(x)}\\end{array}\\right]\\\\ \\quad \\ell(x):=\\sum_i f(x)_i \\\\\n\\quad \\operatorname{softmax}(x):=\\frac{f(x)}{\\ell(x)}\n\\end{gather*}\n$$", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Softmax/`qk_max` and `logits`", "level": 3, "text": "### `qk_max` and `logits`  \nJust right after we get the `qk` result, we can set the temporary\n`logits` result with `qk` (In the end, the `logits` should\nstore the normalized softmax result). Also we can compare and collect\nthe `qk_max` for all `qk`s that are calculated by current\nthread group.  \n```cpp\nif (thread_group_offset == 0) {\nconst bool mask = token_idx >= context_len;\nlogits[token_idx - start_token_idx] = mask ? 0.f : qk;\nqk_max = mask ? qk_max : fmaxf(qk_max, qk);\n}\n```  \nPlease note that the `logits` here is on shared memory, so each\nthread group will set the fields for its own assigned context tokens.\nOverall, the size of logits should be number of context tokens.  \n```cpp\nfor (int mask = WARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {\nqk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask));\n}", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Softmax/`qk_max` and `logits`", "level": 3, "text": "if (lane == 0) {\nred_smem[warp_idx] = qk_max;\n}\n```  \nThen we need to get the reduced `qk_max` across each warp. The main\nidea is to make threads in warp to communicate with each other and\nget the final max `qk` .  \n```cpp\nfor (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {\nqk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask));\n}\nqk_max = VLLM_SHFL_SYNC(qk_max, 0);\n```  \nFinally, we can get the reduced `qk_max` from whole thread block by\ncompare the `qk_max` from all warps in this thread block. Then we\nneed to broadcast the final result to each thread.", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Softmax/`exp_sum`", "level": 3, "text": "### `exp_sum`  \nSimilar to `qk_max`, we need to get the reduced sum value from the\nentire thread block too.  \n```cpp\nfor (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {\nfloat val = __expf(logits[i] - qk_max);\nlogits[i] = val;\nexp_sum += val;\n}\n...\nexp_sum = block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum);\n```  \nFirstly, sum all exp values from each thread group, and meanwhile,\nconvert each entry of `logits` from `qk` to `exp(qk - qk_max)`.\nPlease note, the `qk_max` here is already the max `qk` across the\nwhole thread block. And then we can do reduction for `exp_sum`\nacross whole thread block just like the `qk_max`.  \n```cpp\nconst float inv_sum = __fdividef(1.f, exp_sum + 1e-6f);\nfor (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {\nlogits[i] *= inv_sum;\n}\n```  \nFinally, with the reduced `qk_max` and `exp_sum`, we can obtain\nthe final normalized softmax result as `logits`. This `logits`\nvariable will be used for dot multiplication with the value data in", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Softmax/`exp_sum`", "level": 3, "text": "later steps. Now, it should store the normalized softmax result of\n`qk` for all assigned context tokens.", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Value", "level": 2, "text": "## Value  \n<figure markdown=\"span\">\n![](../assets/design/paged_attention/value.png){ align=\"center\" alt=\"value\" width=\"70%\" }\n</figure>  \n<figure markdown=\"span\">\n![](../assets/design/paged_attention/logits_vec.png){ align=\"center\" alt=\"logits_vec\" width=\"50%\" }\n</figure>  \n<figure markdown=\"span\">\n![](../assets/design/paged_attention/v_vec.png){ align=\"center\" alt=\"v_vec\" width=\"70%\" }\n</figure>  \nNow we need to retrieve the value data and perform dot multiplication\nwith `logits`. Unlike query and key, there is no thread group\nconcept for value data. As shown in diagram, different from key token\nmemory layout, elements from the same column correspond to the same\nvalue token. For one block of value data, there are `HEAD_SIZE` of\nrows and `BLOCK_SIZE` of columns that are split into multiple\n`v_vecs`.  \nEach thread always fetches `V_VEC_SIZE` elements from the same\n`V_VEC_SIZE` of tokens at a time. As a result, a single thread\nretrieves multiple `v_vec`s from different rows and the same", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Value", "level": 2, "text": "retrieves multiple `v_vec`s from different rows and the same\ncolumns through multiple inner iterations. For each `v_vec`, it\nneeds to be dot multiplied with the corresponding `logits_vec`,\nwhich is also `V_VEC_SIZE` elements from `logits`. Overall, with\nmultiple inner iterations, each warp will process one block of value\ntokens. And with multiple outer iterations, the whole context value\ntokens are processed  \n```cpp\nfloat accs[NUM_ROWS_PER_THREAD];\nfor ... { // Iteration over different blocks.\nlogits_vec = ...\nfor ... { // Iteration over different rows.\nv_vec = ...\n...\naccs[i] += dot(logits_vec, v_vec);\n}\n}\n```  \nAs shown in the above pseudo code, in the outer loop, similar to\n`k_ptr`, `logits_vec` iterates over different blocks and reads\n`V_VEC_SIZE` elements from `logits`. In the inner loop, each\nthread reads `V_VEC_SIZE` elements from the same tokens as a\n`v_vec` and performs dot multiplication. It is important to note\nthat in each inner iteration, the thread fetches different head", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Value", "level": 2, "text": "that in each inner iteration, the thread fetches different head\nposition elements for the same tokens. The dot result is then\naccumulated in `accs`. Therefore, each entry of `accs` is mapped\nto a head position assigned to the current thread.  \nFor example, if `BLOCK_SIZE` is 16 and `V_VEC_SIZE` is 8, each\nthread fetches 8 value elements for 8 tokens at a time. Each element\nis from different tokens at the same head position. If `HEAD_SIZE`\nis 128 and `WARP_SIZE` is 32, for each inner loop, a warp needs to\nfetch `WARP_SIZE * V_VEC_SIZE = 256` elements. This means there are\na total of 128 * 16 / 256 = 8 inner iterations for a warp to handle\na whole block of value tokens. And each `accs` in each thread\ncontains 8 elements that accumulated at 8 different head positions.\nFor the thread 0, the `accs` variable will have 8 elements, which\nare 0th, 32nd â€¦ 224th elements of a value head that are accumulated\nfrom all assigned 8 tokens.", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/LV", "level": 2, "text": "## LV  \nNow, we need to perform reduction for `accs` within each warp. This\nprocess allows each thread to accumulate the `accs` for the\nassigned head positions of all tokens in one block.  \n```cpp\nfor (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {\nfloat acc = accs[i];\nfor (int mask = NUM_V_VECS_PER_ROW / 2; mask >= 1; mask /= 2) {\nacc += VLLM_SHFL_XOR_SYNC(acc, mask);\n}\naccs[i] = acc;\n}\n```  \nNext, we perform reduction for `accs` across all warps, allowing\neach thread to have the accumulation of `accs` for the assigned\nhead positions of all context tokens. Please note that each `accs`\nin every thread only stores the accumulation for a portion of\nelements of the entire head for all context tokens. However, overall,\nall results for output have been calculated but are just stored in\ndifferent thread register memory.  \n??? code  \n```cpp\nfloat* out_smem = reinterpret_cast<float*>(shared_mem);\nfor (int i = NUM_WARPS; i > 1; i /= 2) {\n// Upper warps write to shared memory.\n...", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/LV", "level": 2, "text": "// Upper warps write to shared memory.\n...\nfloat* dst = &out_smem[(warp_idx - mid) * HEAD_SIZE];\nfor (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {\n...\ndst[row_idx] = accs[i];\n}", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/LV", "level": 2, "text": "// Lower warps update the output.\nconst float* src = &out_smem[warp_idx * HEAD_SIZE];\nfor (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {\n...\naccs[i] += src[row_idx];\n}\n\n// Write out the accs.\n}\n```", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Output", "level": 2, "text": "## Output  \nNow we can write all of calculated result from local register memory\nto final output global memory.  \n```cpp\nscalar_t* out_ptr = out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE\n+ head_idx * max_num_partitions * HEAD_SIZE\n+ partition_idx * HEAD_SIZE;\n```  \nFirst, we need to define the `out_ptr` variable, which points to\nthe start address of the assigned sequence and assigned head.  \n```cpp\nfor (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {\nconst int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;\nif (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {\nfrom_float(*(out_ptr + row_idx), accs[i]);\n}\n}\n```  \nFinally, we need to iterate over different assigned head positions\nand write out the corresponding accumulated result based on the\n`out_ptr`.", "file_path": "design/paged_attention.md"}
{"id": "563c912e35a0930e807f197cbc3c054f73586c3b5995fabf4f364191d8896eed", "heading": "Paged Attention/Citation", "level": 2, "text": "## Citation  \n```bibtex\n@inproceedings{kwon2023efficient,\ntitle={Efficient Memory Management for Large Language Model Serving with PagedAttention},\nauthor={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},\nbooktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},\nyear={2023}\n}\n```", "file_path": "design/paged_attention.md"}
{"id": "2fd71c6aad4f705679d6b80a46b02bcb9bb3fb5a15bc73fe9f0ae80c466b7302", "heading": "Plugin System", "level": 1, "text": "# Plugin System  \nThe community frequently requests the ability to extend vLLM with custom features. To facilitate this, vLLM includes a plugin system that allows users to add custom features without modifying the vLLM codebase. This document explains how plugins work in vLLM and how to create a plugin for vLLM.", "file_path": "design/plugin_system.md"}
{"id": "2fd71c6aad4f705679d6b80a46b02bcb9bb3fb5a15bc73fe9f0ae80c466b7302", "heading": "Plugin System/How Plugins Work in vLLM", "level": 2, "text": "## How Plugins Work in vLLM  \nPlugins are user-registered code that vLLM executes. Given vLLM's architecture (see [Arch Overview](arch_overview.md)), multiple processes may be involved, especially when using distributed inference with various parallelism techniques. To enable plugins successfully, every process created by vLLM needs to load the plugin. This is done by the [load_general_plugins](https://github.com/vllm-project/vllm/blob/c76ac49d266e27aa3fea84ef2df1f813d24c91c7/vllm/plugins/__init__.py#L16) function in the `vllm.plugins` module. This function is called for every process created by vLLM before it starts any work.", "file_path": "design/plugin_system.md"}
{"id": "2fd71c6aad4f705679d6b80a46b02bcb9bb3fb5a15bc73fe9f0ae80c466b7302", "heading": "Plugin System/How vLLM Discovers Plugins", "level": 2, "text": "## How vLLM Discovers Plugins  \nvLLM's plugin system uses the standard Python `entry_points` mechanism. This mechanism allows developers to register functions in their Python packages for use by other packages. An example of a plugin:  \n??? code  \n```python\n# inside `setup.py` file\nfrom setuptools import setup\n\nsetup(name='vllm_add_dummy_model',\nversion='0.1',\npackages=['vllm_add_dummy_model'],\nentry_points={\n'vllm.general_plugins':\n[\"register_dummy_model = vllm_add_dummy_model:register\"]\n})\n\n# inside `vllm_add_dummy_model.py` file\ndef register():\nfrom vllm import ModelRegistry", "file_path": "design/plugin_system.md"}
{"id": "2fd71c6aad4f705679d6b80a46b02bcb9bb3fb5a15bc73fe9f0ae80c466b7302", "heading": "Plugin System/How vLLM Discovers Plugins", "level": 2, "text": "if \"MyLlava\" not in ModelRegistry.get_supported_archs():\nModelRegistry.register_model(\n\"MyLlava\",\n\"vllm_add_dummy_model.my_llava:MyLlava\",\n)\n```  \nFor more information on adding entry points to your package, please check the [official documentation](https://setuptools.pypa.io/en/latest/userguide/entry_point.html).  \nEvery plugin has three parts:  \n1. **Plugin group**: The name of the entry point group. vLLM uses the entry point group `vllm.general_plugins` to register general plugins. This is the key of `entry_points` in the `setup.py` file. Always use `vllm.general_plugins` for vLLM's general plugins.\n2. **Plugin name**: The name of the plugin. This is the value in the dictionary of the `entry_points` dictionary. In the example above, the plugin name is `register_dummy_model`. Plugins can be filtered by their names using the `VLLM_PLUGINS` environment variable. To load only a specific plugin, set `VLLM_PLUGINS` to the plugin name.", "file_path": "design/plugin_system.md"}
{"id": "2fd71c6aad4f705679d6b80a46b02bcb9bb3fb5a15bc73fe9f0ae80c466b7302", "heading": "Plugin System/How vLLM Discovers Plugins", "level": 2, "text": "3. **Plugin value**: The fully qualified name of the function to register in the plugin system. In the example above, the plugin value is `vllm_add_dummy_model:register`, which refers to a function named `register` in the `vllm_add_dummy_model` module.", "file_path": "design/plugin_system.md"}
{"id": "2fd71c6aad4f705679d6b80a46b02bcb9bb3fb5a15bc73fe9f0ae80c466b7302", "heading": "Plugin System/Types of supported plugins", "level": 2, "text": "## Types of supported plugins  \n- **General plugins** (with group name `vllm.general_plugins`): The primary use case for these plugins is to register custom, out-of-the-tree models into vLLM. This is done by calling `ModelRegistry.register_model` to register the model inside the plugin function.  \n- **Platform plugins** (with group name `vllm.platform_plugins`): The primary use case for these plugins is to register custom, out-of-the-tree platforms into vLLM. The plugin function should return `None` when the platform is not supported in the current environment, or the platform class's fully qualified name when the platform is supported.  \n- **IO Processor plugins** (with group name `vllm.io_processor_plugins`): The primary use case for these plugins is to register custom pre/post processing of the model prompt and model output for pooling models. The plugin function returns the IOProcessor's class fully qualified name.", "file_path": "design/plugin_system.md"}
{"id": "2fd71c6aad4f705679d6b80a46b02bcb9bb3fb5a15bc73fe9f0ae80c466b7302", "heading": "Plugin System/Guidelines for Writing Plugins", "level": 2, "text": "## Guidelines for Writing Plugins  \n- **Being re-entrant**: The function specified in the entry point should be re-entrant, meaning it can be called multiple times without causing issues. This is necessary because the function might be called multiple times in some processes.", "file_path": "design/plugin_system.md"}
{"id": "2fd71c6aad4f705679d6b80a46b02bcb9bb3fb5a15bc73fe9f0ae80c466b7302", "heading": "Plugin System/Compatibility Guarantee", "level": 2, "text": "## Compatibility Guarantee  \nvLLM guarantees the interface of documented plugins, such as `ModelRegistry.register_model`, will always be available for plugins to register models. However, it is the responsibility of plugin developers to ensure their plugins are compatible with the version of vLLM they are targeting. For example, `\"vllm_add_dummy_model.my_llava:MyLlava\"` should be compatible with the version of vLLM that the plugin targets. The interface for the model may change during vLLM's development.", "file_path": "design/plugin_system.md"}
{"id": "2665f608d74db82719f066875911ce066105d913739948a6820c47c5d53a1f8b", "heading": "Automatic Prefix Caching", "level": 1, "text": "# Automatic Prefix Caching  \nPrefix caching kv-cache blocks is a popular optimization in LLM inference to avoid redundant prompt computations. The core idea is simple â€“ we cache the kv-cache blocks of processed requests, and reuse these blocks when a new request comes in with the same prefix as previous requests. Since prefix caching is almost a free lunch and wonâ€™t change model outputs, it has been widely used by many public endpoints (e.g., OpenAI, Anthropic, etc) and most open source LLM inference frameworks (e.g., SGLang).  \nWhile there are many ways to implement prefix caching, vLLM chooses a hash-based approach. Specifically, we hash each kv-cache block by the tokens in the block and the tokens in the prefix before the block:  \n```text\nBlock 1                  Block 2                  Block 3\n[A gentle breeze stirred] [the leaves as children] [laughed in the distance]\nBlock 1: |<--- block tokens ---->|\nBlock 2: |<------- prefix ------>| |<--- block tokens --->|", "file_path": "design/prefix_caching.md"}
{"id": "2665f608d74db82719f066875911ce066105d913739948a6820c47c5d53a1f8b", "heading": "Automatic Prefix Caching", "level": 1, "text": "Block 2: |<------- prefix ------>| |<--- block tokens --->|\nBlock 3: |<------------------ prefix -------------------->| |<--- block tokens ---->|\n```  \nIn the example above, the KV cache in the first block can be uniquely identified with the token â€œA gentle breeze stirredâ€. The third block can be uniquely identified with the tokens in the block â€œlaughed in the distanceâ€, along with the prefix tokens â€œA gentle breeze stirred the leaves as childrenâ€. Therefore, we can build the block hash of `hash(tuple[components])`, where components are:  \n* Parent hash value: The hash value of the parent hash block.\n* Block tokens: A tuple of tokens in this block. The reason to include the exact tokens is to reduce potential hash value collision.\n* Extra hashes: Other values required to make this block unique, such as LoRA IDs, multi-modality input hashes (see the example below), and cache salts to isolate caches in multi-tenant environments.  \n!!! note \"Note 1\"\nWe only cache full blocks.  \n!!! note \"Note 2\"", "file_path": "design/prefix_caching.md"}
{"id": "2665f608d74db82719f066875911ce066105d913739948a6820c47c5d53a1f8b", "heading": "Automatic Prefix Caching", "level": 1, "text": "We only cache full blocks.  \n!!! note \"Note 2\"\nThe above hash key structure is not 100% collision free. Theoretically itâ€™s still possible for the different prefix tokens to have the same hash value. To avoid any hash collisions **in a multi-tenant setup, we advise to use SHA256** as hash function instead of the default builtin hash.\nSHA256 is supported since vLLM v0.8.3 and must be enabled with a command line argument. It comes with a performance impact of about 100-200ns per token (~6ms for 50k tokens of context).  \n**A hashing example with multi-modality inputs**\nIn this example, we illustrate how prefix caching works with multi-modality inputs (e.g., images). Assuming we have a request with the following messages:  \n```text\nmessages = [\n{\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\",\n\"text\": \"What's in this image?\"\n},\n{\"type\": \"image_url\",\n\"image_url\": {\"url\": image_url},\n},\n]},\n]\n```  \nIt will become the following prompt:  \n```text\nPrompt:\n<s>[INST]What's in this image?\\n[IMG][/INST]", "file_path": "design/prefix_caching.md"}
{"id": "2665f608d74db82719f066875911ce066105d913739948a6820c47c5d53a1f8b", "heading": "Automatic Prefix Caching", "level": 1, "text": "Tokenized prompt:\n[1, 3, 7493, 1681, 1294, 1593, 3937, 9551, 10, 4]", "file_path": "design/prefix_caching.md"}
{"id": "2665f608d74db82719f066875911ce066105d913739948a6820c47c5d53a1f8b", "heading": "Automatic Prefix Caching", "level": 1, "text": "Prompt with placeholders (<P>):\n[1, 3, 7493, 1681, 1294, 1593, 3937, 9551, <P>, <P>, ..., <P>, 4]\n```  \nAs we can see, after the tokenization, the `[IMG]` will be replaced by a sequence of placeholder tokens, and these placeholders will be replaced by image embeddings during prefill. The challenge for prefix caching to support this case is we need to differentiate images from the placeholders. To address this problem, we encode the image hash generated by the frontend image processor. For example, the hash of the blocks in the above prompt would be (assuming block size 16, and we have 41 placeholder tokens):  \n```text\nBlock 0\nParent hash: None\nToken IDs: 1, 3, 7493, 1681, 1294, 1593, 3937, 9551, <p>, ..., <p>\nExtra hash: <image hash>\nBlock 1\nParent hash: Block 0 hash\nToken IDs: <p>, ..., <p>\nExtra hash: <image hash>\nBlock 2\nParent hash: Block 1 hash\nToken IDs: <p>, ..., <p>\nExtra hash: <image hash>\nBlock 3\nParent hash: Block 2 hash\nToken IDs: <p>, ..., <p>, 4\nExtra hash: <image hash>\n```", "file_path": "design/prefix_caching.md"}
{"id": "2665f608d74db82719f066875911ce066105d913739948a6820c47c5d53a1f8b", "heading": "Automatic Prefix Caching", "level": 1, "text": "Token IDs: <p>, ..., <p>, 4\nExtra hash: <image hash>\n```  \nIn the rest of this document, we first introduce the data structure used for prefix caching in vLLM v1, followed by the prefix caching workflow of major KV cache operators (e.g., allocate, append, free, eviction). Finally, we use an example to illustrate the end to end prefix caching workflow.  \n**Cache Isolation for Security**\nTo improve privacy in shared environments, vLLM supports isolating prefix cache reuse through optional per-request salting. By including a `cache_salt` in the request, this value is injected into the hash of the first block, ensuring that only requests with the same salt can reuse cached KV blocks. This prevents timing-based attacks where an adversary could infer cached content by observing latency differences. This offers protection without compromising performance.  \n```json\n{\n\"messages\": [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},", "file_path": "design/prefix_caching.md"}
{"id": "2665f608d74db82719f066875911ce066105d913739948a6820c47c5d53a1f8b", "heading": "Automatic Prefix Caching", "level": 1, "text": "{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Here is a document with details about the world series: ...\"},\n{\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n],\n\"cache_salt\": \"your-cache-salt\"\n}\n```  \nWith this setup, cache sharing is limited to users or requests that explicitly agree on a common salt, enabling cache reuse within a trust group while isolating others.  \n!!! note\nCache isolation is not supported in engine V0.", "file_path": "design/prefix_caching.md"}
{"id": "2665f608d74db82719f066875911ce066105d913739948a6820c47c5d53a1f8b", "heading": "Automatic Prefix Caching/Data Structure", "level": 2, "text": "## Data Structure  \nThe prefix caching in vLLM v1 is implemented in the KV cache manager. The basic building block is the â€œBlockâ€ data class (simplified):  \n```python\nclass KVCacheBlock:\n# The block ID (immutable)\nblock_id: int\n# The block hash (will be assigned when the block is full,\n# and will be reset when the block is evicted).\nblock_hash: BlockHash\n# The number of requests using this block now.\nref_cnt: int", "file_path": "design/prefix_caching.md"}
{"id": "2665f608d74db82719f066875911ce066105d913739948a6820c47c5d53a1f8b", "heading": "Automatic Prefix Caching/Data Structure", "level": 2, "text": "# The pointers to form a doubly linked list for the free queue.\nprev_free_block: \"KVCacheBlock | None\" = None\nnext_free_block: \"KVCacheBlock | None\" = None\n```  \nThere are two design points to highlight:  \n1. We allocate all KVCacheBlock when initializing the KV cache manager to be a block pool. This avoids Python object creation overheads and can easily track all blocks all the time.\n2. We introduce doubly linked list pointers directly in the KVCacheBlock, so that we could construct a free queue directly. This gives us two benefits:\n1. We could have O(1) complexity moving elements in the middle to the tail.\n2. We could avoid introducing another Python queue (e.g., `deque`) which has a wrapper to the elements.  \nAs a result, we will have the following components when the KV cache manager is initialized:  \n![Component Overview](../assets/design/prefix_caching/overview.png)  \n* Block Pool: A list of KVCacheBlock.\n* Free Block Queue: Only store the pointers of head and tail blocks for manipulations.", "file_path": "design/prefix_caching.md"}
{"id": "2665f608d74db82719f066875911ce066105d913739948a6820c47c5d53a1f8b", "heading": "Automatic Prefix Caching/Data Structure", "level": 2, "text": "* Cache blocks: Mapping from hash key to block IDs.\n* Request blocks: Mapping from request ID to allocated block IDs.", "file_path": "design/prefix_caching.md"}
{"id": "2665f608d74db82719f066875911ce066105d913739948a6820c47c5d53a1f8b", "heading": "Automatic Prefix Caching/Operations/Block Allocation", "level": 3, "text": "## Operations  \n### Block Allocation  \n**New request:** Workflow for the scheduler to schedule a new request with KV cache block allocation:  \n1. The scheduler calls `kv_cache_manager.get_computed_blocks()` to get a sequence of blocks that have already been computed. This is done by hashing the prompt tokens in the request and looking up cache blocks.\n2. The scheduler calls `kv_cache_manager.allocate_slots()`. It does the following steps:\n1. Compute the number of new required blocks, and return if there are no sufficient blocks to allocate.\n2. â€œTouchâ€ the computed blocks. It increases the reference count of the computed block by one, and removes the block from the free queue if the block wasnâ€™t used by other requests. This is to avoid these computed blocks being evicted. See the example in the next section for illustration.\n3. Allocate new blocks by popping the heads of the free queue. If the head block is a cached block, this also â€œevictsâ€ the block so that no other requests can reuse it anymore from now on.", "file_path": "design/prefix_caching.md"}
{"id": "2665f608d74db82719f066875911ce066105d913739948a6820c47c5d53a1f8b", "heading": "Automatic Prefix Caching/Operations/Block Allocation", "level": 3, "text": "4. If an allocated block is already full of tokens, we immediately add it to the cache block, so that the block can be reused by other requests in the same batch.  \n**Running request:** Workflow for the scheduler to schedule a running request with KV cache block allocation:  \n1. The scheduler calls `kv_cache_manager.allocate_slots()`. It does the following steps:\n1. Compute the number of new required blocks, and return if there are no sufficient blocks to allocate.\n2. Allocate new blocks by popping the heads of the free queue. If the head block is a cached block, this also â€œevictsâ€ the block so that no other requests can reuse it anymore from now on.\n3. Append token IDs to the slots in existing blocks as well as the new blocks. If a block is full, we add it to the cache block to cache it.  \n**Duplicated blocks**\nAssuming block size is 4 and you send a request (Request 1\\) with prompt ABCDEF and decoding length 3:  \n```text\nPrompt: [A, B, C, D, E, F]\nOutput: [G, H, I]", "file_path": "design/prefix_caching.md"}
{"id": "2665f608d74db82719f066875911ce066105d913739948a6820c47c5d53a1f8b", "heading": "Automatic Prefix Caching/Operations/Block Allocation", "level": 3, "text": "Time 0:\nTokens: [A, B, C, D, E, F, G]\nBlock Table: [0 (ABCD), 1 (EFG)]\nCache Blocks: 0\nTime 1:\nTokens: [A, B, C, D, E, F, G, H]\nBlock Table: [0 (ABCD), 1 (EFGH)]\nCache Blocks: 0, 1\nTime 2:\nTokens: [A, B, C, D, E, F, G, H, I]\nBlock Table: [0 (ABCD), 1 (EFGH), 2 (I)]\nCache Blocks: 0, 1\n```  \nNow block 0 and block 1 are cached, and we send the same request again (Request 2\\) with greedy sampling, so that it will produce exactly the same outputs as the Request 1:  \n```text\nPrompt: [A, B, C, D, E, F]\nOutput: [G, H, I]", "file_path": "design/prefix_caching.md"}
{"id": "2665f608d74db82719f066875911ce066105d913739948a6820c47c5d53a1f8b", "heading": "Automatic Prefix Caching/Operations/Block Allocation", "level": 3, "text": "Time 0:\nTokens: [A, B, C, D, E, F, G]\nBlock Table: [0 (ABCD), 3 (EFG)]\nCache Blocks: 0, 1\nTime 1:\nTokens: [A, B, C, D, E, F, G, H]\nBlock Table: [0 (ABCD), 3 (EFGH)]\nCache Blocks: 0, 1, 3\n```  \nAs can be seen, block 3 is a new full block and is cached. However, it is redundant as block 1, meaning that we cached the same block twice. In v0, when detecting block 3 is duplicated, we free block 3 and let Request 2 use block 1 instead, so its block table becomes `[0, 1]` in Time 1. However, the block table in vLLM v1 is append-only, meaning that changing the block table from `[0, 3]` to `[0, 1]` is not allowed. As a result, we will have duplicated blocks for the hash key E-H. This duplication will be eliminated when the request is freed.", "file_path": "design/prefix_caching.md"}
{"id": "2665f608d74db82719f066875911ce066105d913739948a6820c47c5d53a1f8b", "heading": "Automatic Prefix Caching/Operations/Free", "level": 3, "text": "### Free  \nWhen a request is finished, we free all its blocks if no other requests are using them (reference count = 0). In this example, we free request 1 and block 2, 3, 4, 8 associated with it. We can see that the freed blocks are added to the tail of the free queue in the *reverse* order. This is because the last block of a request must hash more tokens and is less likely to be reused by other requests. As a result, it should be evicted first.  \n![Free queue after a request us freed](../assets/design/prefix_caching/free.png)", "file_path": "design/prefix_caching.md"}
{"id": "2665f608d74db82719f066875911ce066105d913739948a6820c47c5d53a1f8b", "heading": "Automatic Prefix Caching/Operations/Eviction (LRU)", "level": 3, "text": "### Eviction (LRU)  \nWhen the head block (least recently used block) of the free queue is cached, we have to evict the block to prevent it from being used by other requests. Specifically, eviction involves the following steps:  \n1. Pop the block from the head of the free queue. This is the LRU block to be evicted.\n2. Remove the block ID from the cache block.\n3. Remove the block hash.", "file_path": "design/prefix_caching.md"}
{"id": "2665f608d74db82719f066875911ce066105d913739948a6820c47c5d53a1f8b", "heading": "Automatic Prefix Caching/Example", "level": 2, "text": "## Example  \nIn this example, we assume the block size is 4 (each block can cache 4 tokens), and we have 10 blocks in the KV-cache manager in total.  \n**Time 1: The cache is empty and a new request comes in.** We allocate 4 blocks. 3 of them are already full and cached. The fourth block is partially full with 3 of 4 tokens.  \n![Example Time 1](../assets/design/prefix_caching/example-time-1.png)  \n**Time 3: Request 0 makes the block 3 full and asks for a new block to keep decoding.** We cache block 3 and allocate block 4.  \n![Example Time 3](../assets/design/prefix_caching/example-time-3.png)  \n**Time 4: Request 1 comes in with the 14 prompt tokens, where the first 10 tokens are the same as request 0.** We can see that only the first 2 blocks (8 tokens) hit the cache, because the 3rd block only matches 2 of 4 tokens.  \n![Example Time 4](../assets/design/prefix_caching/example-time-4.png)", "file_path": "design/prefix_caching.md"}
{"id": "2665f608d74db82719f066875911ce066105d913739948a6820c47c5d53a1f8b", "heading": "Automatic Prefix Caching/Example", "level": 2, "text": "**Time 5: Request 0 is finished and free.** Blocks 2, 3 and 4 are added to the free queue in the reverse order (but block 2 and 3 are still cached). Block 0 and 1 are not added to the free queue because they are being used by Request 1.  \n![Example Time 5](../assets/design/prefix_caching/example-time-5.png)  \n**Time 6: Request 1 is finished and free.**  \n![Example Time 6](../assets/design/prefix_caching/example-time-6.png)  \n**Time 7: Request 2 comes in with the 29 prompt tokens, where the first 12 tokens are the same as request 0\\.** Note that even the block order in the free queue was `7 - 8 - 9 - 4 - 3 - 2 - 6 - 5 - 1 - 0`, the cache hit blocks (i.e., 0, 1, 2) are touched and removed from the queue before allocation, so the free queue becomes `7 - 8 - 9 - 4 - 3 - 6 - 5`. As a result, the allocated blocks are 0 (cached), 1 (cached), 2 (cached), 7, 8, 9, 4, 3 (evicted).  \n![Example Time 7](../assets/design/prefix_caching/example-time-7.png)", "file_path": "design/prefix_caching.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration", "level": 1, "text": "# `torch.compile` integration  \nIn vLLM's V1 architecture, `torch.compile` is enabled by default and is a critical part of the framework. This document gives a simple walk-through example to show how to understand the `torch.compile` usage.  \nThroughout the example, we will run a common Llama model, and turn on debug level logging to show all the details. The command to be used is `VLLM_LOGGING_LEVEL=DEBUG vllm serve meta-llama/Llama-3.2-1B`.  \n!!! note\nFor more information and the latest progress of `torch.compile` integration, see this [Blog Post](https://blog.vllm.ai/2025/08/20/torch-compile.html).", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Compilation Cache", "level": 2, "text": "## Compilation Cache  \nIn the very verbose logs, we can see:  \n```console\nINFO 03-07 03:06:55 [backends.py:409] Using cache directory: ~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0 for vLLM's torch.compile\n```  \nvLLM will take all the available factors into consideration, and decide a directory to store all the compilation artifact. This means, you can directly copy the whole `~/.cache/vllm/torch_compile_cache` directory in your deployment scenario to save a great amount of compilation time, and hence accelerating the starting time of the vLLM instance.  \nThe factors considered include:  \n- All the related configs (see the `compute_hash` functions in their respective configs in the [config folder](gh-file:vllm/config))\n- PyTorch configs (see the `compute_hash` functions in the [compiler_interface.py](gh-file:vllm/compilation/compiler_interface.py))\n- The model's forward function and the relevant functions called by the forward function (see below)", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Compilation Cache", "level": 2, "text": "With all these factors taken into consideration, usually we can guarantee that the cache is safe to use, and will not cause any unexpected behavior. Therefore, the cache is enabled by default. If you want to debug the compilation process, or if you suspect the cache is causing some issues, you can disable it by setting the environment variable `VLLM_DISABLE_COMPILE_CACHE=1`.  \nA unique aspect of vLLM's `torch.compile` integration, is that we guarantee all the compilation finishes before we serve any requests. No requests will trigger new compilations. Otherwise, the engine would be blocked on that request, and the response time will have unexpected spikes.", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Python Code Compilation", "level": 2, "text": "## Python Code Compilation  \nIn the very verbose logs, we can see:  \n??? console \"Logs\"  \n```text\nDEBUG 03-07 03:06:52 [decorators.py:203] Start compiling function <code object forward at 0x7f08acf40c90, file \"xxx/vllm/model_executor/models/llama.py\", line 339>", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Python Code Compilation", "level": 2, "text": "DEBUG 03-07 03:06:54 [backends.py:370] Traced files (to be considered for compilation cache):\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/torch/_dynamo/polyfills/builtins.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/torch/nn/modules/container.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/torch/nn/modules/module.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/vllm/attention/layer.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/vllm/distributed/communication_op.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/vllm/distributed/parallel_state.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/vllm/model_executor/custom_op.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/vllm/model_executor/layers/activation.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/vllm/model_executor/layers/layernorm.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/vllm/model_executor/layers/linear.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/vllm/model_executor/layers/rotary_embedding.py", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Python Code Compilation", "level": 2, "text": "DEBUG 03-07 03:06:54 [backends.py:370] xxx/vllm/model_executor/layers/vocab_parallel_embedding.py\nDEBUG 03-07 03:06:54 [backends.py:370] xxx/vllm/model_executor/models/llama.py", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Python Code Compilation", "level": 2, "text": "DEBUG 03-07 03:07:07 [backends.py:462] Computation graph saved to ~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0/computation_graph.py\nDEBUG 03-07 03:07:07 [wrapper.py:105] Dynamo transformed code saved to ~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0/transformed_code.py\n```", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Python Code Compilation", "level": 2, "text": "```  \nThis is about the Python code compilation, i.e. graph capture by Dynamo. It tries to trace the function with code `xxx/vllm/model_executor/models/llama.py:339`, which is the `forward` function of the model we compile. During the forward pass, there are also other functions called and inlined by Dynamo, as shown by the logs, including some PyTorch functions from `xxx/torch/nn/modules/module.py` (used by PyTorch `nn.Module`, because module attribute access will trigger a function call), some communication / attention / activation functions from vLLM. All the traced files will be considered when we decide the cache directory to use. This way, any code change in the above files will trigger compilation cache miss, and therefore recompilation.", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Python Code Compilation", "level": 2, "text": "The result of the Dynamo compilation, is a new function stored in `~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0/transformed_code.py`. Usually, this function unpacks tensors from the module, and then pass it to the traced computation graph. The computation graph is stored in `~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0/computation_graph.py`.", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Computation Graph Processing", "level": 2, "text": "## Computation Graph Processing  \nThe computation graph has shape annotations for every tensor. The inputs are input ids, position ids, weights and buffers from the model, and the outputs are the final hidden states. Note that lm head projection and sampling operations are not considered in the graph.  \nMost of the inputs to the computation graph has static shape, since they are model weights and buffers, and will not change during the lifetime of the model. Only the input ids and position ids have symbolic shapes, i.e. the shape can change from batch to batch. However, they will share the same symbolic shapes. That is to say, the only changing size to the computation graph, is the batch size (number of tokens processed in the current forward pass).", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Computation Graph Processing", "level": 2, "text": "The attention operation is complicated, and it needs to interact with kv caches, with complicated shapes. Fortunately, the output of the attention operation just share the same shape as the input query of the attention operation. Therefore, we wrap the whole attention operation into a PyTorch custom op `torch.ops.vllm.unified_attention_with_output`, so that Dynamo will not try to inspect any of the internal operations. This way, although attention operation is complicated, we can still capture the model's computation graph as a full-graph, from Dynamo's perspective.  \nThe computation graph is further split into pieces, by the `splitting_ops` (usually this is the attention operation). Therefore, in the `~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0/computation_graph.py` file, we can see lots of submodules, each submodule is a piece of graph after splitting:  \n- Attention operation itself is a submodule.", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Computation Graph Processing", "level": 2, "text": "- Attention operation itself is a submodule.\n- The part of computation graph, from one attention operation to the next attention operation, is a submodule.  \nEvery submodule can be identified by its index, and will be processed individually.", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Computation Graph Compilation", "level": 2, "text": "## Computation Graph Compilation  \nIn the very verbose logs, we can also see:  \n```console\nDEBUG 03-07 03:52:37 [backends.py:134] store the 0-th graph for shape None from inductor via handle ('fpegyiq3v3wzjzphd45wkflpabggdbjpylgr7tta4hj6uplstsiw', '~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0/inductor_cache/iw/ciwzrk3ittdqatuzwonnajywvno3llvjcs2vfdldzwzozn3zi3iy.py')\nDEBUG 03-07 03:52:39 [backends.py:134] store the 1-th graph for shape None from inductor via handle ('f7fmlodmf3h3by5iiu2c4zarwoxbg4eytwr3ujdd2jphl4pospfd', '~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0/inductor_cache/ly/clyfzxldfsj7ehaluis2mca2omqka4r7mgcedlf6xfjh645nw6k2.py')\n...\nDEBUG 03-07 03:52:45 [backends.py:134] store the 15-th graph for shape None from inductor via handle ('f7fmlodmf3h3by5iiu2c4zarwoxbg4eytwr3ujdd2jphl4pospfd', '~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0/inductor_cache/ly/clyfzxldfsj7ehaluis2mca2omqka4r7mgcedlf6xfjh645nw6k2.py')", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Computation Graph Compilation", "level": 2, "text": "DEBUG 03-07 03:52:45 [backends.py:134] store the 16-th graph for shape None from inductor via handle ('fvj3ccoi7m34f3dnr4itmu55mmun44l5xymwhrjlwisylsk7q6jy', '~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0/inductor_cache/tf/ctfftkglj7b4lcttq5cymx6cew372uoauupqn6ldsvpiucavqcjc.py')\n```  \nThis means the first piece of computation graph (with shape `None` for symbolic shape) is compiled by Inductor (with a key `fpegyiq3v3wzjzphd45wkflpabggdbjpylgr7tta4hj6uplstsiw`). The compiled kernel is stored in  `~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0/inductor_cache/iw/ciwzrk3ittdqatuzwonnajywvno3llvjcs2vfdldzwzozn3zi3iy.py`. You can open the file to see what is the code Inductor finally runs.  \nOne more detail: you can see that the 1-th graph and the 15-th graph have the same key, while the 0-th graph and the 16-th graph are different. This is expected, since we split the graph by the attention op, we get 3 unique subgraphs:  \n- the first layer before attention", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Computation Graph Compilation", "level": 2, "text": "- the first layer before attention\n- every middle layer, from one attention operation to the next attention operation\n- the final layer after attention  \nIf we already have the cache directory (e.g. run the same code for the second time), we will see the following logs:  \n```console\nDEBUG 03-07 04:00:45 [backends.py:86] Directly load the 0-th graph for shape None from inductor via handle ('fpegyiq3v3wzjzphd45wkflpabggdbjpylgr7tta4hj6uplstsiw', '~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0/inductor_cache/iw/ciwzrk3ittdqatuzwonnajywvno3llvjcs2vfdldzwzozn3zi3iy.py')\n```  \nThis time, Inductor compilation is completely bypassed, and we will load from disk to read the compilation artifact we get from the last time.  \nThe above example just uses Inductor to compile for a general shape (i.e. symbolic shape). We can also use Inductor to compile for some of the specific shapes, for example:  \n```bash\nvllm serve meta-llama/Llama-3.2-1B \\\n--compilation_config '{\"compile_sizes\": [1, 2, 4, 8]}'\n```", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Computation Graph Compilation", "level": 2, "text": "--compilation_config '{\"compile_sizes\": [1, 2, 4, 8]}'\n```  \nThen it will also compile a specific kernel just for batch size `1, 2, 4, 8`. At this time, all of the shapes in the computation graph are static and known, and we will turn on auto-tuning to tune for max performance. This can be slow when you run it for the first time, but the next time you run it, we can directly bypass the tuning and run the tuned kernel.  \nWhen all the shapes are known, `torch.compile` can compare different configs, and often find some better configs to run the kernel. For example, we can see the following log:  \n??? console \"Logs\"", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Computation Graph Compilation", "level": 2, "text": "```\nAUTOTUNE mm(8x2048, 2048x3072)\ntriton_mm_4 0.0130 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=2\ntriton_mm_8 0.0134 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4\ntriton_mm_12 0.0148 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4\nmm 0.0160 ms 81.6%\ntriton_mm_16 0.0165 ms 78.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8\ntriton_mm_3 0.0199 ms 65.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=2", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Computation Graph Compilation", "level": 2, "text": "triton_mm_1 0.0203 ms 64.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=2\ntriton_mm_7 0.0203 ms 64.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\ntriton_mm_2 0.0208 ms 62.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4\ntriton_mm_11 0.0215 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\nSingleProcess AUTOTUNE benchmarking takes 2.0428 seconds and 7.5727 seconds precompiling\n```", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Computation Graph Compilation", "level": 2, "text": "```  \nIt means, for a matrix multiplication with shape `8x2048x3072`, `torch.compile` tries triton template with various configs, and it is much faster than the default code (which dispatches to cublas library).  \nUnfortunately, because auto-tuning takes quite a long time (from seconds to minutes, depending on the model size and the batch size), even though it can be cached for later use, for the sake of user-friendliness, we turn it off by default. If you want to have max performance, it is recommended to try it, by compiling specific shapes.", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Cudagraph Capture", "level": 2, "text": "## Cudagraph Capture  \nvLLM's V1 architecture uses piecewise cudagraph that aligns with the piecewise compilation. The full computation graph is split as mentioned above, and we only capture the cudagraph for the piece of graph between attention operations (including the first graph before any attention operation, and the last graph after all the attention operation). This is based on a common observation: computation between attentions are usually token-wise and easy to deal with for cudagraph; while the attention operation is non-trivial to be cudagraph compatible. Thus, by running the attention operation in eager mode while the rest operations in cudagraph, we keep the flexibility of the attention operation.", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Cudagraph Capture", "level": 2, "text": "The piecewise cudagraph also has fine-grained memory management. The purpose is to only exclude the attention kernel from cudagraph, while keeping all the rest modules and the memory allocation operations in the cudagraph. This is why the attention operation in V1 has the output tensor as the input of the attention.  \nThe cudagraphs are captured and managed by the compiler backend, and replayed when the batch size has corresponding cudagraph captured. The caller of the model (model runner) only needs to make sure it manages the input buffers correctly. All of the intermediate buffers are managed automatically by the compiler backend.  \nBy default, vLLM will try to determine a set of sizes to capture cudagraph. You can also override it using the config `cudagraph_capture_sizes`:  \n```bash\nvllm serve meta-llama/Llama-3.2-1B \\\n--compilation-config '{\"cudagraph_capture_sizes\": [1, 2, 4, 8]}'\n```", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Cudagraph Capture", "level": 2, "text": "```  \nThen it will only capture cudagraph for the specified sizes. It can be useful to have fine-grained control over the cudagraph capture.", "file_path": "design/torch_compile.md"}
{"id": "d5f2765396f52e491c937b35b0a06b36f55c09bae57835424f0b37927eaab9a5", "heading": "`torch.compile` integration/Cudagraph Capture/Full Cudagraph capture", "level": 3, "text": "### Full Cudagraph capture  \nIt is possible to include attention as part of the cudagraph if using an attention backend that is cudagraph compatible. This can improve performance in some cases such as decode speed for smaller models or MOEs. See [CUDA Graphs](cuda_graphs.md) for more details.", "file_path": "design/torch_compile.md"}
{"id": "162e3e36b33273a45e72b7d45f8882fb18d90ac845ba67503f656ad45635f4e5", "heading": "Features/Compatibility Matrix", "level": 2, "text": "# Features  \n## Compatibility Matrix  \nThe tables below show mutually exclusive features and the support on some hardware.  \nThe symbols used have the following meanings:  \n- âœ… = Full compatibility\n- ðŸŸ  = Partial compatibility\n- âŒ = No compatibility\n- â” = Unknown or TBD  \n!!! note\nCheck the âŒ or ðŸŸ  with links to see tracking issue for unsupported feature/hardware combination.", "file_path": "features/README.md"}
{"id": "162e3e36b33273a45e72b7d45f8882fb18d90ac845ba67503f656ad45635f4e5", "heading": "Features/Compatibility Matrix/Feature x Feature", "level": 3, "text": "### Feature x Feature  \n<style>\ntd:not(:first-child) {\ntext-align: center !important;\n}\ntd {\npadding: 0.5rem !important;\nwhite-space: nowrap;\n}  \nth {\npadding: 0.5rem !important;\nmin-width: 0 !important;\n}  \nth:not(:first-child) {\nwriting-mode: vertical-lr;\ntransform: rotate(180deg)\n}\n</style>  \n| Feature | [CP][chunked-prefill] | [APC](automatic_prefix_caching.md) | [LoRA](lora.md) | [SD](spec_decode.md) | CUDA graph | [pooling](../models/pooling_models.md) | <abbr title=\"Encoder-Decoder Models\">enc-dec</abbr> | <abbr title=\"Logprobs\">logP</abbr> | <abbr title=\"Prompt Logprobs\">prmpt logP</abbr> | <abbr title=\"Async Output Processing\">async output</abbr> | multi-step | <abbr title=\"Multimodal Inputs\">mm</abbr> | best-of | beam-search | [prompt-embeds](prompt_embeds.md) |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| [CP][chunked-prefill] | âœ… | | | | | | | | | | | | | | |\n| [APC](automatic_prefix_caching.md) | âœ… | âœ… | | | | | | | | | | | | | |", "file_path": "features/README.md"}
{"id": "162e3e36b33273a45e72b7d45f8882fb18d90ac845ba67503f656ad45635f4e5", "heading": "Features/Compatibility Matrix/Feature x Feature", "level": 3, "text": "| [LoRA](lora.md) | âœ… | âœ… | âœ… | | | | | | | | | | | | |\n| [SD](spec_decode.md) | âœ… | âœ… | âŒ | âœ… | | | | | | | | | | | |\n| CUDA graph | âœ… | âœ… | âœ… | âœ… | âœ… | | | | | | | | | | |\n| [pooling](../models/pooling_models.md) | ðŸŸ \\* | ðŸŸ \\* | âœ… | âŒ | âœ… | âœ… | | | | | | | | | |\n| <abbr title=\"Encoder-Decoder Models\">enc-dec</abbr> | âŒ | [âŒ](gh-issue:7366) | âŒ | [âŒ](gh-issue:7366) | âœ… | âœ… | âœ… | | | | | | | | |\n| <abbr title=\"Logprobs\">logP</abbr> | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âœ… | âœ… | | | | | | | |\n| <abbr title=\"Prompt Logprobs\">prmpt logP</abbr> | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âœ… | âœ… | âœ… | | | | | | |\n| <abbr title=\"Async Output Processing\">async output</abbr> | âœ… | âœ… | âœ… | âŒ | âœ… | âŒ | âŒ | âœ… | âœ… | âœ… | | | | | |\n| multi-step | âŒ | âœ… | âŒ | âŒ | âœ… | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | | | | |\n| [mm](multimodal_inputs.md) | âœ… | âœ… | [ðŸŸ ](gh-pr:4194)<sup>^</sup> | â” | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | â” | âœ… | | | |\n| best-of | âœ… | âœ… | âœ… | [âŒ](gh-issue:6137) | âœ… | âŒ | âœ… | âœ… | âœ… | â” | [âŒ](gh-issue:7968) | âœ… | âœ… | | |", "file_path": "features/README.md"}
{"id": "162e3e36b33273a45e72b7d45f8882fb18d90ac845ba67503f656ad45635f4e5", "heading": "Features/Compatibility Matrix/Feature x Feature", "level": 3, "text": "| beam-search | âœ… | âœ… | âœ… | [âŒ](gh-issue:6137) | âœ… | âŒ | âœ… | âœ… | âœ… | â” | [âŒ](gh-issue:7968) | â” | âœ… | âœ… | |\n| [prompt-embeds](prompt_embeds.md) | âœ… | [âŒ](gh-issue:25096) | âœ… | âŒ | âœ… | âŒ | âŒ | âœ… | âŒ | â” | â” | âŒ | â” | â” | âœ… |  \n\\* Chunked prefill and prefix caching are only applicable to last-token pooling.\n<sup>^</sup> LoRA is only applicable to the language backbone of multimodal models.  \n[](){ #feature-x-hardware }", "file_path": "features/README.md"}
{"id": "162e3e36b33273a45e72b7d45f8882fb18d90ac845ba67503f656ad45635f4e5", "heading": "Features/Compatibility Matrix/Feature x Hardware", "level": 3, "text": "### Feature x Hardware  \n| Feature                                                   | Volta               | Turing    | Ampere    | Ada    | Hopper     | CPU                | AMD    | TPU | Intel GPU |\n|-----------------------------------------------------------|---------------------|-----------|-----------|--------|------------|--------------------|--------|-----| ------------|\n| [CP][chunked-prefill]                                     | [âŒ](gh-issue:2729) | âœ…        | âœ…        | âœ…     | âœ…        | âœ…                  | âœ…     | âœ… | âœ…        |\n| [APC](automatic_prefix_caching.md)                        | [âŒ](gh-issue:3687) | âœ…        | âœ…        | âœ…     | âœ…        | âœ…                  | âœ…     | âœ… | âœ…        |\n| [LoRA](lora.md)                                           | âœ…                  | âœ…        | âœ…        | âœ…     | âœ…        | âœ…                  | âœ…     | âœ… | âœ…        |", "file_path": "features/README.md"}
{"id": "162e3e36b33273a45e72b7d45f8882fb18d90ac845ba67503f656ad45635f4e5", "heading": "Features/Compatibility Matrix/Feature x Hardware", "level": 3, "text": "| [SD](spec_decode.md)                                      | âœ…                  | âœ…        | âœ…        | âœ…     | âœ…        | âœ…                  | âœ…     | âŒ | [ðŸŸ ](gh-issue:26963)       |\n| CUDA graph                                                | âœ…                  | âœ…        | âœ…        | âœ…     | âœ…        | âŒ                  | âœ…     | âŒ | [âŒ](gh-issue:26970)        |\n| [pooling](../models/pooling_models.md)                    | âœ…                  | âœ…        | âœ…        | âœ…     | âœ…        | âœ…                  | âœ…     | âŒ | âœ…        |\n| <abbr title=\"Encoder-Decoder Models\">enc-dec</abbr>       | âœ…                  | âœ…        | âœ…        | âœ…     | âœ…        | âœ…                  | âŒ     | âŒ | âœ…        |\n| [mm](multimodal_inputs.md)                                | âœ…                  | âœ…        | âœ…        | âœ…     | âœ…        | âœ…                  | âœ…     | âŒ | [ðŸŸ ](gh-issue:26965)       |", "file_path": "features/README.md"}
{"id": "162e3e36b33273a45e72b7d45f8882fb18d90ac845ba67503f656ad45635f4e5", "heading": "Features/Compatibility Matrix/Feature x Hardware", "level": 3, "text": "| <abbr title=\"Logprobs\">logP</abbr>                        | âœ…                  | âœ…        | âœ…        | âœ…     | âœ…        | âœ…                  | âœ…     | âŒ | âœ…        |\n| <abbr title=\"Prompt Logprobs\">prmpt logP</abbr>           | âœ…                  | âœ…        | âœ…        | âœ…     | âœ…        | âœ…                  | âœ…     | âŒ | âœ…        |\n| <abbr title=\"Async Output Processing\">async output</abbr> | âœ…                  | âœ…        | âœ…        | âœ…     | âœ…        | âŒ                  | âŒ     | âŒ | âœ…        |\n| multi-step                                                | âœ…                  | âœ…        | âœ…        | âœ…     | âœ…        | [âŒ](gh-issue:8477) | âœ…     | âŒ | âœ…        |\n| best-of                                                   | âœ…                  | âœ…        | âœ…        | âœ…     | âœ…        | âœ…                  | âœ…     | âŒ | âœ…        |\n| beam-search                                               | âœ…                  | âœ…        | âœ…        | âœ…     | âœ…        | âœ…                  | âœ…     | âŒ | âœ…        |", "file_path": "features/README.md"}
{"id": "162e3e36b33273a45e72b7d45f8882fb18d90ac845ba67503f656ad45635f4e5", "heading": "Features/Compatibility Matrix/Feature x Hardware", "level": 3, "text": "| [prompt-embeds](prompt_embeds.md)                         | âœ…                  | âœ…        | âœ…        | âœ…     | âœ…        | âœ…                  | ?     | [âŒ](gh-issue:25097) | âœ…       |", "file_path": "features/README.md"}
{"id": "6006b22fdd51d6e84ebd89d20227051605a7eb7e2ec6e52124741558685011f1", "heading": "Automatic Prefix Caching/Introduction", "level": 2, "text": "# Automatic Prefix Caching  \n## Introduction  \nAutomatic Prefix Caching (APC in short) caches the KV cache of existing queries, so that a new query can directly reuse the KV cache if it shares the same prefix with one of the existing queries, allowing the new query to skip the computation of the shared part.  \n!!! note\nTechnical details on how vLLM implements APC can be found [here](../design/prefix_caching.md).", "file_path": "features/automatic_prefix_caching.md"}
{"id": "6006b22fdd51d6e84ebd89d20227051605a7eb7e2ec6e52124741558685011f1", "heading": "Automatic Prefix Caching/Enabling APC in vLLM", "level": 2, "text": "## Enabling APC in vLLM  \nSet `enable_prefix_caching=True` in vLLM engine to enable APC. Here is an example:  \n<gh-file:examples/offline_inference/automatic_prefix_caching.py>", "file_path": "features/automatic_prefix_caching.md"}
{"id": "6006b22fdd51d6e84ebd89d20227051605a7eb7e2ec6e52124741558685011f1", "heading": "Automatic Prefix Caching/Example workloads", "level": 2, "text": "## Example workloads  \nWe describe two example workloads, where APC can provide huge performance benefit:  \n- Long document query, where the user repeatedly queries the same long document (e.g. software manual or annual report) with different queries. In this case, instead of processing the long document again and again, APC allows vLLM to process this long document *only once*, and all future requests can avoid recomputing this long document by reusing its KV cache. This allows vLLM to serve future requests with much higher throughput and much lower latency.\n- Multi-round conversation, where the user may chat with the application multiple times in the same chatting session. In this case, instead of processing the whole chatting history again and again, APC allows vLLM to reuse the processing results of the chat history across all future rounds of conversation, allowing vLLM to serve future requests with much higher throughput and much lower latency.", "file_path": "features/automatic_prefix_caching.md"}
{"id": "6006b22fdd51d6e84ebd89d20227051605a7eb7e2ec6e52124741558685011f1", "heading": "Automatic Prefix Caching/Limits", "level": 2, "text": "## Limits  \nAPC in general does not reduce the performance of vLLM. With that being said, APC only reduces the time of processing the queries (the prefilling phase) and does not reduce the time of generating new tokens (the decoding phase). So APC does not bring performance gain when vLLM spends most of the time generating answers to the queries (e.g. when the length of the answer is long), or new queries do not share the same prefix with any of existing queries (so that the computation cannot be reused).", "file_path": "features/automatic_prefix_caching.md"}
{"id": "4e0797ec74d8be66fd903e600cd7fb7e7cc33f12c519a38bc2dec40fc2983bea", "heading": "Custom Arguments", "level": 1, "text": "# Custom Arguments  \nYou can use vLLM *custom arguments* to pass in arguments which are not part of the vLLM `SamplingParams` and REST API specifications. Adding or removing a vLLM custom argument does not require recompiling vLLM, since the custom arguments are passed in as a dictionary.  \nCustom arguments can be useful if, for example, you want to use a [custom logits processor](./custom_logitsprocs.md) without modifying the vLLM source code.", "file_path": "features/custom_arguments.md"}
{"id": "4e0797ec74d8be66fd903e600cd7fb7e7cc33f12c519a38bc2dec40fc2983bea", "heading": "Custom Arguments/Offline Custom Arguments", "level": 2, "text": "## Offline Custom Arguments  \nCustom arguments passed to `SamplingParams.extra_args` as a `dict` will be visible to any code which has access to `SamplingParams`:  \n``` python\nSamplingParams(extra_args={\"your_custom_arg_name\": 67})\n```  \nThis allows arguments which are not already part of `SamplingParams` to be passed into `LLM` as part of a request.", "file_path": "features/custom_arguments.md"}
{"id": "4e0797ec74d8be66fd903e600cd7fb7e7cc33f12c519a38bc2dec40fc2983bea", "heading": "Custom Arguments/Online Custom Arguments", "level": 2, "text": "## Online Custom Arguments  \nThe vLLM REST API allows custom arguments to be passed to the vLLM server via `vllm_xargs`. The example below integrates custom arguments into a vLLM REST API request:  \n``` bash\ncurl http://localhost:8000/v1/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n...\n\"vllm_xargs\": {\"your_custom_arg\": 67}\n}'\n```  \nFurthermore, OpenAI SDK users can access `vllm_xargs` via the `extra_body` argument:  \n``` python\nbatch = await client.completions.create(\nmodel=\"Qwen/Qwen2.5-1.5B-Instruct\",\n...,\nextra_body={\n\"vllm_xargs\": {\n\"your_custom_arg\": 67\n}\n}\n)\n```  \n!!! note\n`vllm_xargs` is assigned to `SamplingParams.extra_args` under the hood, so code which uses `SamplingParams.extra_args` is compatible with both offline and online scenarios.", "file_path": "features/custom_arguments.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors", "level": 1, "text": "# Custom Logits Processors  \n!!! important\nSome logits processors design changes are still in progress and the API may\nchange in the near future. We hope to stabilize this part of the API soon  \nA \"custom\" logits processor is written by a user of vLLM and is loaded into vLLM at initialization without needing to modify or recompile the vLLM source code. It is the opposite of a built-in logits processor.  \nThis document shows how to write, load and use a custom logits processor.", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Logits Processors Background", "level": 2, "text": "## Logits Processors Background  \nA logits processor adjusts the next-token probability distribution, usually with the intention of steering the model towards a desired type of behavior.  \nIn vLLM, logits processors operate at batch granularity. During a given engine step, the logits processor consumes a `(num_requests) x (vocab_size)` tensor of raw logits output by the model. For all requests which enable the logits processor, the logits processor applies a transformation to the corresponding row of the logits tensor, while leaving other rows unmodified. The transformed logits tensor is then passed to softmax.", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Creating a Custom Logits Processor", "level": 2, "text": "## Creating a Custom Logits Processor  \nCustom logits processors must subclass `vllm.v1.sample.logits_processor.LogitsProcessor` and define (at minimum) the following methods:  \n* `__init__(self, vllm_config: VllmConfig, device: torch.device, is_pin_memory: bool)`\n* `vllm_config`: engine configuration data structure\n* `device`: hardware accelerator device info\n* `is_pin_memory`: flag indicating whether pin memory is available to support logits processor implementation  \n* `apply(self, logits: torch.Tensor) -> torch.Tensor`:\n* Consume a `(num_requests) x (vocab_size)` logits tensor (`logits`)\n* Apply logits processor transformation at batch granularity\n* Return a transformed `(num_requests) x (vocab_size)` logits tensor\n* You can modify the input logits processors in-place or out-of-place; in-place is more memory-efficient  \n* `is_argmax_invariant(self) -> bool`:", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Creating a Custom Logits Processor", "level": 2, "text": "* `is_argmax_invariant(self) -> bool`:\n* Return `True` if the logits processor is argmax invariant (never changes what is the highest-logit-value token ID for a given request), `False` if the logits processor may modify argmax\n* `is_argmax_invariant()` is evaluated once at startup; if `True`, vLLM will skip applying this logits processor in a given step when all requests use greedy sampling  \n* `update_state(self, batch_update: Optional[\"BatchUpdate\"]) -> None`:\n* Consume a `BatchUpdate` data structure representing persistent batch state changes at the beginning of the current engine step\n* Use the `BatchUpdate` members to update logits processor internal state\n* **Note:** batch update data structure may be `None`, signaling no change to the batch constituents. In this case, the LogitsProcessor might still want to update its state based on the updated `output_token_ids` lists that it could have retained when they were added.", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Creating a Custom Logits Processor/How the vLLM engine builds the `BatchUpdate` data structure", "level": 3, "text": "### How the vLLM engine builds the `BatchUpdate` data structure  \n!!! important\nSome logits processors design changes are still in progress. We expect\nthat in the future you will not need to account for batch state changes\nwhen implementing a logits processor, and the information in this section\nwill become irrelevant.  \nLogits processor `update_state()` implementations should assume the following model for how the model runner updates persistent batch state (expressed here in terms of the `BatchUpdate` abstraction):  \n1. Identify indices of requests which finished in the current engine step  \n2. Identify new requests introduced in the current step  \n3. Use Add operations to replace as many finished requests with new requests, in order of increasing index of the replaced request starting with the lowest index  \n4. Based on the relative number of new and finished requests:  \n1. If the numbers of new and finished requests are the same, proceed to next step", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Creating a Custom Logits Processor/How the vLLM engine builds the `BatchUpdate` data structure", "level": 3, "text": "2. *If there are more new requests than finished requests:* apply Add operations to extend the batch with the remaining new requests which did not replace finished requests. Assign consecutive indices to these new requests, starting with `current_max_batch_index + 1`  \n3. *If there are fewer new requests than finished requests:*  \n* Apply Remove operations to finished requests which were not replaced with new requests. These removed request indices will necessarily be greater than the greatest index of the finished requests which were replaced in the previous step. The Removes may leave the batch in a non-contiguous state", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Creating a Custom Logits Processor/How the vLLM engine builds the `BatchUpdate` data structure", "level": 3, "text": "* **\"Condense\" the batch to be contiguous:** starting with the lowest-index empty slot (which was caused by a Remove), apply a Unidirectional Move from the current highest non-empty slot in the batch to fill the empty slot. Proceed with additional Unidirectional Move operations in order of increasing empty slot destination index and decreasing non-empty slot source index until the batch is contiguous  \n* **Shrink the batch:** a side-effect of condensing the batch is that empty slots resulting from Remove operations are grouped in a contiguous block at the end of the batch array. Thus, after condensing, update `BatchUpdate.batch_size` to reflect the number of non-empty slots  \n5. Reorder the batch for improved efficiency. Depending on the attention backend implementation and the current characteristics of the batch, zero or more Swap Move operations may be applied to reorder the batch  \nNotes:", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Creating a Custom Logits Processor/How the vLLM engine builds the `BatchUpdate` data structure", "level": 3, "text": "Notes:  \n* A logits processor `update_state()` method must process batch update operations in the following order: removes, adds, moves  \n* The index argument for Add operations refers to the index *at the time the Add occurred*, i.e. before any Move operations\n* Example: if a request is Added at index 5 and then swapped with index 3, the Add operation in `BatchUpdate.added` will be associated with index 5 not 3\n* In other words Move operations can be assumed to be applied after Adds and Removes  \n* Move operations can be assumed to be applied in the order in which they appear in `BatchUpdate.moved`  \n* If there are no new/finished requests and there is no batch reordering, then the batch update for the logits processors will be `None`", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Creating a Custom Logits Processor/Passing Custom Argument to a Custom Logits Processor", "level": 3, "text": "### Passing Custom Argument to a Custom Logits Processor  \nUnlike built-in logits processors, custom logits processors may require configuration arguments that are not hard-coded into `SamplingParams` or the vLLM server REST API. To solve this problem, custom logits processors may leverage vLLM [custom arguments](./custom_arguments.md) support to receive configuration settings from the user (although you are also free to design a custom logits processor which utilizes the pre-existing fields in `SamplingParams`.)", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Creating a Custom Logits Processor/Example Custom Logits Processor Implementation", "level": 3, "text": "### Example Custom Logits Processor Implementation  \nThe contrived example below implements a custom logits processor which consumes a `(num\\_requests) \\times (vocab\\_size)` logits tensor and masks out all tokens except for one (`target_token`) with `float(-inf)`. The logits processor is disabled for any request that does not specify `target_token`. To determine whether the logits processor is enabled and which token to leave unmasked, the logits processor checks `SamplingParams.extra_args` for a `target_token` custom argument associated with each request:  \n??? code \"Example custom logits processor definition\"  \n``` python\nimport torch\nfrom vllm.config import VllmConfig\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.v1.sample.logits_processor import (BatchUpdate,\nLogitsProcessor,\nMoveDirectionality)\n\nclass DummyLogitsProcessor(LogitsProcessor):\n\"\"\"Fake logit processor to support unit testing and examples\"\"\"", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Creating a Custom Logits Processor/Example Custom Logits Processor Implementation", "level": 3, "text": "def __init__(self, vllm_config: \"VllmConfig\", device: torch.device,\nis_pin_memory: bool):\nself.req_info: dict[int, int] = {}\n\ndef is_argmax_invariant(self) -> bool:\n\"\"\"Never impacts greedy sampling\"\"\"\nreturn False\n\ndef update_state(self, batch_update: BatchUpdate | None):\nif not batch_update:\nreturn\n\n# Process added requests.\nfor index, params, _, _ in batch_update.added:\nassert params is not None\nif params.extra_args and (target_token :=\nparams.extra_args.get(\"target_token\")):\nself.req_info[index] = target_token\nelse:\nself.req_info.pop(index, None)\n\nif self.req_info:\n# Process removed requests.\nfor index in batch_update.removed:\nself.req_info.pop(index, None)\n\n# Process moved requests, unidirectional move (a->b) and swap\n# (a<->b)\nfor adx, bdx, direct in batch_update.moved:\na_val = self.req_info.pop(adx, None)\nb_val = self.req_info.pop(bdx, None)\nif a_val is not None:\nself.req_info[bdx] = a_val\nif direct == MoveDirectionality.SWAP and b_val is not None:\nself.req_info[adx] = b_val", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Creating a Custom Logits Processor/Example Custom Logits Processor Implementation", "level": 3, "text": "def apply(self, logits: torch.Tensor) -> torch.Tensor:\nif not self.req_info:\nreturn logits\n\n# Save target values before modification\ncols = torch.tensor(\nlist(self.req_info.values()), dtype=torch.long, device=logits.device\n)\nrows = torch.tensor(\nlist(self.req_info.keys()), dtype=torch.long, device=logits.device\n)\nvalues_to_keep = logits[rows, cols].clone()\n\n# Mask all but target tokens\nlogits[rows] = float('-inf')\nlogits[rows, cols] = values_to_keep", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Creating a Custom Logits Processor/Example Custom Logits Processor Implementation", "level": 3, "text": "return logits\n```  \nIn the rest of this document, we will use `DummyLogitsProcessor` as an example of a custom logits processor.  \nThe `DummyLogitsProcessor.update_state()` implementation maintains a \"sparse\" representation of the batched requests in the `self.req_info` dictionary: only those requests which specify a `target_token` value have a key in the dictionary. `update_state()` adjusts the stored request indices and `target_token` values (keys and values respectively in `self.req_info`) in response to Add, Remove and Move operations against the persistent batch.", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Creating a Custom Logits Processor/Wrapping an Existing Request-Level Logits Processor", "level": 3, "text": "### Wrapping an Existing Request-Level Logits Processor  \nAlthough the vLLM engine applies logits processors at batch granularity, some users may want to use vLLM with a \"request-level\" logits processor implementation - an implementation which operates on individual requests. This will be especially true if your logits processor was developed for vLLM version 0, which required it to be a `Callable` (as described [here](https://docs.vllm.ai/en/v0.10.1.1/api/vllm/logits_process.html)) conforming to the following type annotation:  \n``` python\nRequestLogitsProcessor = Union[\n\n# (output token ids, logits tensor) -> logits tensor\nCallable[[list[int], Tensor], Tensor],", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Creating a Custom Logits Processor/Wrapping an Existing Request-Level Logits Processor", "level": 3, "text": "# (prompt token ids, output token ids, logits tensor) -> logits tensor\nCallable[[list[int], list[int], Tensor], Tensor],\n]\n```  \nWhile request-level logits processors are explicitly *not* supported in the vLLM engine, vLLM *does* provide a convenient process to wrap an existing `Callable` request-level logits processor and create a batch-level logits processor that is compatible with vLLM. The `Callable` must conform to the type annotation above; if your request-level logits processor has a different interface, then in order to wrap it, you may need to modify it or implement an additional wrapper layer to comply with the interface specification above.", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Creating a Custom Logits Processor/Wrapping an Existing Request-Level Logits Processor", "level": 3, "text": "You can wrap the request-level logits processor by subclassing `AdapterLogitsProcessor` as shown in the example below (in this example, `DummyPerReqLogitsProcessor` is a stand-in for your request-level logits processor which needs to be wrapped.) Override `AdapterLogitsProcessor.is_argmax_invariant(self)` to accurately reflect whether your request-level logits processor may impact which token has the highest-value logit. Override `AdapterLogitsProcessor.new_req_logits_processor(self,params)` to create a new request-level logits processor instance from a `SamplingParams` instance:  \n??? code \"Example of Wrapping a Request-Level Logits Processor\"  \n``` python\n...", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Creating a Custom Logits Processor/Wrapping an Existing Request-Level Logits Processor", "level": 3, "text": "from vllm.v1.sample.logits_processor import (\nAdapterLogitsProcessor, # Wrapper base-class\nRequestLogitsProcessor, # Request-level logitsproc type annotation\n)\n\n...\n\n# Stand-in for your request-level logits processor:\nclass DummyPerReqLogitsProcessor:\n\"\"\"The request-level logits processor masks out all logits except the\ntoken id identified by `target_token`\"\"\"\n\ndef __init__(self, target_token: int) -> None:\n\"\"\"Specify `target_token`\"\"\"\nself.target_token = target_token\n\ndef __call__(\nself,\noutput_ids: list[int],\nlogits: torch.Tensor,\n) -> torch.Tensor:\nval_to_keep = logits[self.target_token].item()\nlogits[:] = float(\"-inf\")\nlogits[self.target_token] = val_to_keep\nreturn logits\n\n...\n\n# Example of wrapping the request-level logits processor:\nclass WrappedPerReqLogitsProcessor(AdapterLogitsProcessor):\n\"\"\"Example of wrapping a fake request-level logit processor to create a\nbatch-level logits processor\"\"\"\n\ndef is_argmax_invariant(self) -> bool:\nreturn False", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Creating a Custom Logits Processor/Wrapping an Existing Request-Level Logits Processor", "level": 3, "text": "def is_argmax_invariant(self) -> bool:\nreturn False\n\ndef new_req_logits_processor(\nself,\nparams: SamplingParams,\n) -> Optional[RequestLogitsProcessor]:\n\"\"\"This method returns a new request-level logits processor, customized\nto the `target_token` value associated with a particular request.\n\nReturns None if the logits processor should not be applied to the\nparticular request. To use the logits processor the request must have\na \"target_token\" custom argument with an integer value.\n\nArgs:\nparams: per-request sampling params", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Creating a Custom Logits Processor/Wrapping an Existing Request-Level Logits Processor", "level": 3, "text": "Args:\nparams: per-request sampling params\n\nReturns:\n`Callable` request logits processor, or None\n\"\"\"\ntarget_token: Optional[Any] = params.extra_args and params.extra_args.get(\n\"target_token\"\n)\nif target_token is None:\nreturn None\nif not isinstance(target_token, int):\nlogger.warning(\n\"target_token value %s is not int; not applying logits\"\n\" processor to request.\",\ntarget_token,\n)\nreturn None\nreturn DummyPerReqLogitsProcessor(target_token)\n```  \n!!! note\nYour `new_req_logits_processor()` override can return `None` to signal that the wrapped logits processor should not be applied to the request in question.  \nOnce you have created a custom subclass (like `WrappedPerReqLogitsProcessor`) which wraps your request level logits processor, you can pass the custom subclass to vLLM via any of the methods described in the following section.", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Ways to Load Your Custom Logits Processor in vLLM", "level": 2, "text": "## Ways to Load Your Custom Logits Processor in vLLM  \nLogits processors are loaded at initialization. Critically, the set of loaded logits processors cannot be modified after the vLLM engine finishes loading, and new logits logits processors cannot be loaded on-demand for individual requests.  \nThis section details different ways of making your logits processor visible to vLLM and triggering vLLM to load your logits processor.", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Ways to Load Your Custom Logits Processor in vLLM/Method 1: Pass the Custom Logits Processor Fully-Qualified Class Name (FQCN) to vLLM at Initialization Time", "level": 3, "text": "### Method 1: Pass the Custom Logits Processor Fully-Qualified Class Name (FQCN) to vLLM at Initialization Time  \nThis method is supported in both offline and online vLLM usage scenarios. The custom logits processor's FQCN (in the form of `dotted.path.to.module:ClassName`) can be passed as an argument to the `LLM` and `AsyncLLM` Python constructors, or as a CLI argument to `vllm serve` with the following syntax  \n``` bash\nvllm serve ... --logits_processors <logits processor 1> <logits processor 2> ...\n```  \nThe only requirements on the FQCN are  \n1. Python's `importlib.import_module()` must be able to resolve the dotted path portion of the FQCN and load it as a module  \n2. The class-name portion of the FQCN must be possible to import from the loaded module  \n3. The object pointed to by the FQCN must be a subclass of `LogitsProcessor`  \nSee examples below:  \n??? code \"Passing custom logits processor FQCN to `LLM` in Python\"  \n``` python\n# Pass in FQCN\nllm = LLM(\nmodel=\"facebook/opt-125m\",", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Ways to Load Your Custom Logits Processor in vLLM/Method 1: Pass the Custom Logits Processor Fully-Qualified Class Name (FQCN) to vLLM at Initialization Time", "level": 3, "text": "``` python\n# Pass in FQCN\nllm = LLM(\nmodel=\"facebook/opt-125m\",\nlogits_processors=[\"your.module.path:DummyLogitsProcessor\"],\n)\n```  \n??? code \"Passing custom logits processor FQCN to `AsyncLLM` in Python\"  \n``` python\n# Pass in FQCN\nengine_args = AsyncEngineArgs(model=\"facebook/opt-125m\",\nlogits_processors=[\"your.module.path:DummyLogitsProcessor\"])\nasync_llm = AsyncLLM.from_engine_args(engine_args)\n```  \n??? code \"Passing custom logits processor FQCN to vLLM server via CLI\"  \n```bash\nvllm serve facebook/opt-125m --logits_processors your.module.path:DummyLogitsProcessor\n```", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Ways to Load Your Custom Logits Processor in vLLM/Method 2: Automatically Detect Custom Logits Processors Installed in Your Python Environment As Entry Points", "level": 3, "text": "### Method 2: Automatically Detect Custom Logits Processors Installed in Your Python Environment As Entry Points  \n[`setuptools`](https://setuptools.pypa.io/en/latest/userguide/entry_point.html) can enable installed packages to make themselves available as plugins to other Python programs, via pieces of metadata known as \"entry points\".  \nDuring initialization, vLLM automatically scans the `vllm.logits_processors` entry point group and loads any installed logits processors which it finds.  \nSuppose that you have developed a Python package that holds your custom logits processors. You can expose each logits processor to vLLM by adding a unique entrypoint for each logits processor to your logits processor Python package. The example below shows how to add an entrypoint to your project's `pyproject.toml` file:  \n??? code \"Exposing a custom logits processor as a Python entrypoint\"  \n``` toml\n[project.entry-points.\"vllm.logits_processors\"]\ndummy_logits_processor = \"your.module.path:DummyLogitsProcessor\"\n```", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Ways to Load Your Custom Logits Processor in vLLM/Method 2: Automatically Detect Custom Logits Processors Installed in Your Python Environment As Entry Points", "level": 3, "text": "```  \nOnce your package is installed, your custom logits processor will be loaded automatically whenever vLLM is initialized. You do *not* need to pass the custom logits processor to the `LLM` or `AsyncLLM` constructors or to the vLLM server explicitly at initialization time if your logits processor is exposed as an entry point.  \n!!! note\nvLLM will *always* load *all* logits processors which are exposed via entrypoints under the `vllm.logits_processors` grouping.", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Ways to Load Your Custom Logits Processor in vLLM/Method 3 (Offline-only): Pass a Python Class Object to the vLLM Constructor", "level": 3, "text": "### Method 3 (Offline-only): Pass a Python Class Object to the vLLM Constructor  \nYou can pass one or more custom logits processor class objects to the `LLM` and `AsyncLLM` constructors. This option is very flexible, as the logits processor classes may either be (1) defined locally within the same Python source file where `LLM` or `AsyncLLM` is instantiated, or (2) imported from a Python package.  \n??? code \"Passing custom logits processor class object to `LLM` or `AsyncLLM` in Python\"  \n``` python\n# Import custom logits processor\nfrom some.module import DummyLogitsProcessor\n\n# ...or...\n\n# Define custom logits processor locally\nfrom vllm.v1.sample.logits_processor import LogitsProcessor\n\nclass DummyLogitsProcessor(LogitsProcessor):\n# See DummyLogitsProcessor implementation above\n...\n\n# Pass class object to LLM constructor\nllm = LLM(\nmodel=\"facebook/opt-125m\",\nlogits_processors=[DummyLogitsProcessor],\n)", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Ways to Load Your Custom Logits Processor in vLLM/Method 3 (Offline-only): Pass a Python Class Object to the vLLM Constructor", "level": 3, "text": "# Pass class object to AsyncLLM constructor\nengine_args = AsyncEngineArgs(model=\"facebook/opt-125m\",\nlogits_processors=[DummyLogitsProcessor])\nasync_llm = AsyncLLM.from_engine_args(engine_args)\n```", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Invoking a Custom Logits Processor Against a Request", "level": 2, "text": "## Invoking a Custom Logits Processor Against a Request  \nThe design of the custom logits processor determines whether the logits processor must be enabled/disabled for a given request, and what arguments must be provided to configure the logits processor.  \nThe examples below show how a user would pass a custom argument (`target_token`) to `DummyLogitsProcessor` in order to (1) enable the logits processor for that particular request and (2) control the logits processor's behavior.  \n??? code \"vLLM REST API: configure custom logits processor for a request\"  \n``` bash\ncurl http://localhost:8000/v1/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n...\n\"vllm_xargs\": {\"target_token\": 67}\n}'\n```  \n??? code \"OpenAI SDK: configure custom logits processor for a request\"  \n``` python\nbatch = await client.completions.create(\nmodel=\"Qwen/Qwen2.5-1.5B-Instruct\",\n...,\nextra_body={\n\"vllm_xargs\": {\n\"target_token\": 67\n}\n}\n)\n```", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Invoking a Custom Logits Processor Against a Request", "level": 2, "text": "extra_body={\n\"vllm_xargs\": {\n\"target_token\": 67\n}\n}\n)\n```  \n??? code \"Offline: configure custom logits processor for an `LLM` request\"  \n``` python\noutputs_logitproc = llm.generate(\"your prompt\",\nSamplingParams(...,\nextra_args={\"target_token\": 67}))\n```  \n??? code \"Offline: configure custom logits processor for an `AsyncLLM` request\"  \n``` python\nasync for out in engine.generate(request_id=\"your request id\",\nprompt=\"your prompt\",\nsampling_params=SamplingParams(...,\nextra_args={\"target_token\": 67})):", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Invoking a Custom Logits Processor Against a Request", "level": 2, "text": "# Process async request outputs\n...\n```", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Best Practices for Writing Custom Logits Processors", "level": 2, "text": "## Best Practices for Writing Custom Logits Processors  \nOnce vLLM loads a logits processor during initialization, then vLLM will invoke `update_state()` and `apply()` against that logits processor in every engine step. Both methods operate on all requests which currently reside in the vLLM persistent batch. Thus it is important to implement these methods efficiently.  \n* Write efficient `apply()` and `update_state()` implementations in light of the fact that logits processors operate at batch granularity\n* For example, you may be able to use efficient vectorized operations to implement `apply()` or update internal state vectors in `update_state()`\n* However, if you think that a logits processor may be used infrequently, it may be appropriate to use a \"sparse\" representation of request state i.e. the class can represent request configuration using a dictionary which only stores metadata about requests that enable the logits processor", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Best Practices for Writing Custom Logits Processors", "level": 2, "text": "* **Note:** wrapped request-level logits processors do not need to implement `apply()` and `update_state()`; the default `AdapterLogitsProcessor.update_state()` implementation maintains a sparse representation of request state, wherein requests for which `new_req_logits_processor()` returns `None` are not represented in the base-class state dictionary. The default implementation of `AdapterLogitsProcessor.apply()` applies the request-level logits processor to each row of input logits sequentially and assembles the output logits tensor. If the performance of this `AdapterLogitsProcessor` default implementation is insufficient, then avoid wrapping your request-level logits processor and instead re-implement it as a `LogitsProcessor` subclass with optimized `apply()` and `update_state()` implementations that operate at batch granularity  \n* It is up to the logits processor author to determine:", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Best Practices for Writing Custom Logits Processors", "level": 2, "text": "* It is up to the logits processor author to determine:  \n1. **The per-request attributes which configure the logits processor's behavior against that request.** Your custom logits processor's `update_state()` override determines how `SamplingParams` fields are mapped into logits processor state  \n* **Note:** for wrapped request-level logits processors, `new_req_logits_processor()` determines how `SamplingParams` fields are used to initialize a request-level logits processor instance.  \n2. **The conditions under which the logits processor is or is not enabled on a per-request basis.** Unless your intention is for the custom logits processor to act on all requests all the time, you should write your logits processor in such a way that it is possible to disable the logits processor for a given request, i.e. by defaulting an argument to `None` or by passing in a specific do-nothing argument value i.e. `0.0`. Try to save compute and memory for requests which disable the logits processor", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Best Practices for Writing Custom Logits Processors", "level": 2, "text": "* **Note:** for wrapped per-request logits processors, the default `AdapterLogitsProcessor.update_state()` implementation ensures that the request-level logits processor is disabled when `new_req_logits_processor()` returns `None` for that request", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Best Practices for Writing Custom Logits Processors", "level": 2, "text": "3. **The conditions under which the logits processor is short-circuited at the batch level.** Even if you have defined a way to disable the custom logits processor at the request level, it may be difficult to translate this into compute savings i.e. if your `update_state()` and `apply()` implementations use efficient vectorized implementations that operate on the whole persistent batch in a single command. For example, you cannot skip an entire vectorized operation in `apply()` just because one request disabled the logits processor. To save compute in the edge-case where no running requests utilize the custom logits processor, we recommend designing `apply()` to return the unmodified input tensor if all requests have the logits processor disabled. Similarly, consider whether steps can be skipped in `update_state()` if no requests enable the logits processor  \n* Additionally, an easy way to save compute in `update_state()` is to exit early when the `batch_update` is `None`", "file_path": "features/custom_logitsprocs.md"}
{"id": "7bb77580a61396749c399fdfe66b504382e26312bf46cd9becc56f515117b114", "heading": "Custom Logits Processors/Best Practices for Writing Custom Logits Processors", "level": 2, "text": "* **Note:** for wrapped per-request logits processors, the `AdapterLogitsProcessor` base-class implements the above optimizations by default  \n* Ensure that the logits processor `update_state` method discards information about finished requests (i.e. requests which are replaced by an Add or which are subject to a Remove)  \n* **Note:** for wrapped per-request logits processors, the `AdapterLogitsProcessor` base-class handles this by default  \n* `is_argmax_invariant()` can be hard-coded to `True` or `False` if the logits processor has consistent behavior. However the argmax invariance may also be determined programmatically (i.e. if your logits processor is user-customizable in some way that impacts whether the logits processor is argmax invariant). For this reason, `is_argmax_invariant()` is not a class method", "file_path": "features/custom_logitsprocs.md"}
{"id": "c7ebb5613bce2739cadf7833b49d0f85b570375d8dd5a7c61c9b518c03e4122e", "heading": "Disaggregated Prefilling (experimental)", "level": 1, "text": "# Disaggregated Prefilling (experimental)  \nThis page introduces you the disaggregated prefilling feature in vLLM.  \n!!! note\nThis feature is experimental and subject to change.", "file_path": "features/disagg_prefill.md"}
{"id": "c7ebb5613bce2739cadf7833b49d0f85b570375d8dd5a7c61c9b518c03e4122e", "heading": "Disaggregated Prefilling (experimental)/Why disaggregated prefilling?", "level": 2, "text": "## Why disaggregated prefilling?  \nTwo main reasons:  \n- **Tuning time-to-first-token (TTFT) and inter-token-latency (ITL) separately**. Disaggregated prefilling put prefill and decode phase of LLM inference inside different vLLM instances. This gives you the flexibility to assign different parallel strategies (e.g. `tp` and `pp`) to tune TTFT without affecting ITL, or to tune ITL without affecting TTFT.\n- **Controlling tail ITL**. Without disaggregated prefilling, vLLM may insert some prefill jobs during the decoding of one request. This results in higher tail latency. Disaggregated prefilling helps you solve this issue and control tail ITL. Chunked prefill with a proper chunk size also can achieve the same goal, but in practice it's hard to figure out the correct chunk size value. So disaggregated prefilling is a much more reliable way to control tail ITL.  \n!!! note\nDisaggregated prefill DOES NOT improve throughput.", "file_path": "features/disagg_prefill.md"}
{"id": "c7ebb5613bce2739cadf7833b49d0f85b570375d8dd5a7c61c9b518c03e4122e", "heading": "Disaggregated Prefilling (experimental)/Usage example", "level": 2, "text": "## Usage example  \nPlease refer to <gh-file:examples/online_serving/disaggregated_prefill.sh> for the example usage of disaggregated prefilling.  \nNow supports 5 types of connectors:  \n- **SharedStorageConnector**: refer to <gh-file:examples/offline_inference/disaggregated-prefill-v1/run.sh> for the example usage of SharedStorageConnector disaggregated prefilling.\n- **LMCacheConnectorV1**: refer to <gh-file:examples/others/lmcache/disagg_prefill_lmcache_v1/disagg_example_nixl.sh> for the example usage of LMCacheConnectorV1 disaggregated prefilling which uses NIXL as the underlying KV transmission.\n- **NixlConnector**: refer to <gh-file:tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh> for the example usage of NixlConnector disaggregated prefilling which support fully async send/recv. For detailed usage guide, see [NixlConnector Usage Guide](nixl_connector_usage.md).", "file_path": "features/disagg_prefill.md"}
{"id": "c7ebb5613bce2739cadf7833b49d0f85b570375d8dd5a7c61c9b518c03e4122e", "heading": "Disaggregated Prefilling (experimental)/Usage example", "level": 2, "text": "- **P2pNcclConnector**: refer to <gh-file:examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_example_p2p_nccl_xpyd.sh> for the example usage of P2pNcclConnector disaggregated prefilling.\n- **MultiConnector**: take advantage of the kv_connector_extra_config: dict[str, Any] already present in KVTransferConfig to stash all the connectors we want in an ordered list of kwargs.such as:  \n```bash\n--kv-transfer-config '{\"kv_connector\":\"MultiConnector\",\"kv_role\":\"kv_both\",\"kv_connector_extra_config\":{\"connectors\":[{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\"},{\"kv_connector\":\"SharedStorageConnector\",\"kv_role\":\"kv_both\",\"kv_connector_extra_config\":{\"shared_storage_path\":\"local_storage\"}}]}}'\n```  \nFor NixlConnector, you may also specify one or multiple NIXL_Backend. Such as:  \n```bash\n--kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\", \"kv_buffer_device\":\"cuda\", \"kv_connector_extra_config\":{\"backends\":[\"UCX\", \"GDS\"]}}'\n```", "file_path": "features/disagg_prefill.md"}
{"id": "c7ebb5613bce2739cadf7833b49d0f85b570375d8dd5a7c61c9b518c03e4122e", "heading": "Disaggregated Prefilling (experimental)/Usage example", "level": 2, "text": "```  \n- **OffloadingConnector**: enable offloading of KV data to CPU memory, customizing the CPU block size (in tokens) and number of blocks to allocate (per worker):  \n```bash\n--kv-transfer-config '{\"kv_connector\":\"OffloadingConnector\",\"kv_role\":\"kv_both\",\"kv_connector_extra_config\":{\"block_size\": 64, \"num_cpu_blocks\": 1000}}'\n```", "file_path": "features/disagg_prefill.md"}
{"id": "c7ebb5613bce2739cadf7833b49d0f85b570375d8dd5a7c61c9b518c03e4122e", "heading": "Disaggregated Prefilling (experimental)/Benchmarks", "level": 2, "text": "## Benchmarks  \nPlease refer to <gh-file:benchmarks/disagg_benchmarks> for disaggregated prefilling benchmarks.", "file_path": "features/disagg_prefill.md"}
{"id": "c7ebb5613bce2739cadf7833b49d0f85b570375d8dd5a7c61c9b518c03e4122e", "heading": "Disaggregated Prefilling (experimental)/Development", "level": 2, "text": "## Development  \nWe implement disaggregated prefilling by running 2 vLLM instances. One for prefill (we call it prefill instance) and one for decode (we call it decode instance), and then use a connector to transfer the prefill KV caches and results from prefill instance to decode instance.  \nAll disaggregated prefilling implementation is under `vllm/distributed/kv_transfer`.  \nKey abstractions for disaggregated prefilling:  \n- **Connector**: Connector allows **kv consumer** to retrieve the KV caches of a batch of request from **kv producer**.\n- **LookupBuffer**: LookupBuffer provides two API: `insert` KV cache and `drop_select` KV cache. The semantics of `insert` and `drop_select` are similar to SQL, where `insert` inserts a KV cache into the buffer, and `drop_select` returns the KV cache that matches the given condition and drop it from the buffer.\n- **Pipe**: A single-direction FIFO pipe for tensor transmission. It supports `send_tensor` and `recv_tensor`.  \n!!! note", "file_path": "features/disagg_prefill.md"}
{"id": "c7ebb5613bce2739cadf7833b49d0f85b570375d8dd5a7c61c9b518c03e4122e", "heading": "Disaggregated Prefilling (experimental)/Development", "level": 2, "text": "!!! note\n`insert` is non-blocking operation but `drop_select` is blocking operation.  \nHere is a figure illustrating how the above 3 abstractions are organized:  \n![Disaggregated prefilling abstractions](../assets/features/disagg_prefill/abstraction.jpg)  \nThe workflow of disaggregated prefilling is as follows:  \n![Disaggregated prefilling workflow](../assets/features/disagg_prefill/overview.jpg)  \nThe `buffer` corresponds to `insert` API in LookupBuffer, and the `drop_select` corresponds to `drop_select` API in LookupBuffer.  \nNow every process in vLLM will have a corresponding connector. Specifically, we have:  \n- Scheduler connector: the connector that locates in the same process as the scheduler process. It schedules the KV cache transfer ops.\n- Worker connectors: the connectors that locate in the worker processes. They execute KV cache transfer ops.  \nHere is a figure illustrating how the above 2 connectors are organized:", "file_path": "features/disagg_prefill.md"}
{"id": "c7ebb5613bce2739cadf7833b49d0f85b570375d8dd5a7c61c9b518c03e4122e", "heading": "Disaggregated Prefilling (experimental)/Development", "level": 2, "text": "![Disaggregated prefilling high level design](../assets/features/disagg_prefill/high_level_design.png)  \nThe figure below shows how the worker connector works with the attention module to achieve layer-by-layer KV cache store and load:  \n![Disaggregated prefilling workflow](../assets/features/disagg_prefill/workflow.png)", "file_path": "features/disagg_prefill.md"}
{"id": "c7ebb5613bce2739cadf7833b49d0f85b570375d8dd5a7c61c9b518c03e4122e", "heading": "Disaggregated Prefilling (experimental)/Third-party contributions", "level": 2, "text": "## Third-party contributions  \nDisaggregated prefilling is highly related to infrastructure, so vLLM relies on third-party connectors for production-level disaggregated prefilling (and vLLM team will actively review and merge new PRs for third-party connectors).  \nWe recommend three ways of implementations:  \n- **Fully-customized connector**: Implement your own `Connector`, and call third-party libraries to send and receive KV caches, and many many more (like editing vLLM's model input to perform customized prefilling, etc). This approach gives you the most control, but at the risk of being incompatible with future vLLM versions.\n- **Database-like connector**: Implement your own `LookupBuffer` and support the `insert` and `drop_select` APIs just like SQL.\n- **Distributed P2P connector**: Implement your own `Pipe` and support the `send_tensor` and `recv_tensor` APIs, just like `torch.distributed`.", "file_path": "features/disagg_prefill.md"}
{"id": "f8df435b37b96a6c100ef60e6f209b5652467ede533de75b9944c9b57a6b24c8", "heading": "LoRA Adapters", "level": 1, "text": "# LoRA Adapters  \nThis document shows you how to use [LoRA adapters](https://arxiv.org/abs/2106.09685) with vLLM on top of a base model.  \nLoRA adapters can be used with any vLLM model that implements [SupportsLoRA][vllm.model_executor.models.interfaces.SupportsLoRA].  \nAdapters can be efficiently served on a per request basis with minimal overhead. First we download the adapter(s) and save\nthem locally with  \n```python\nfrom huggingface_hub import snapshot_download\n\nsql_lora_path = snapshot_download(repo_id=\"yard1/llama-2-7b-sql-lora-test\")\n```  \nThen we instantiate the base model and pass in the `enable_lora=True` flag:  \n```python\nfrom vllm import LLM, SamplingParams\nfrom vllm.lora.request import LoRARequest", "file_path": "features/lora.md"}
{"id": "f8df435b37b96a6c100ef60e6f209b5652467ede533de75b9944c9b57a6b24c8", "heading": "LoRA Adapters", "level": 1, "text": "llm = LLM(model=\"meta-llama/Llama-2-7b-hf\", enable_lora=True)\n```  \nWe can now submit the prompts and call `llm.generate` with the `lora_request` parameter. The first parameter\nof `LoRARequest` is a human identifiable name, the second parameter is a globally unique ID for the adapter and\nthe third parameter is the path to the LoRA adapter.  \n??? code  \n```python\nsampling_params = SamplingParams(\ntemperature=0,\nmax_tokens=256,\nstop=[\"[/assistant]\"],\n)\n\nprompts = [\n\"[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_74 (icao VARCHAR, airport VARCHAR)\\n\\n question: Name the ICAO for lilongwe international airport [/user] [assistant]\",\n\"[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_11 (nationality VARCHAR, elector VARCHAR)\\n\\n question: When Anchero Pantaleone was the elector what is under nationality? [/user] [assistant]\",\n]", "file_path": "features/lora.md"}
{"id": "f8df435b37b96a6c100ef60e6f209b5652467ede533de75b9944c9b57a6b24c8", "heading": "LoRA Adapters", "level": 1, "text": "outputs = llm.generate(\nprompts,\nsampling_params,\nlora_request=LoRARequest(\"sql_adapter\", 1, sql_lora_path),\n)\n```  \nCheck out <gh-file:examples/offline_inference/multilora_inference.py> for an example of how to use LoRA adapters with the async engine and how to use more advanced configuration options.", "file_path": "features/lora.md"}
{"id": "f8df435b37b96a6c100ef60e6f209b5652467ede533de75b9944c9b57a6b24c8", "heading": "LoRA Adapters/Serving LoRA Adapters", "level": 2, "text": "## Serving LoRA Adapters  \nLoRA adapted models can also be served with the Open-AI compatible vLLM server. To do so, we use\n`--lora-modules {name}={path} {name}={path}` to specify each LoRA module when we kick off the server:  \n```bash\nvllm serve meta-llama/Llama-2-7b-hf \\\n--enable-lora \\\n--lora-modules sql-lora=$HOME/.cache/huggingface/hub/models--yard1--llama-2-7b-sql-lora-test/snapshots/0dfa347e8877a4d4ed19ee56c140fa518470028c/\n```  \n!!! note\nThe commit ID `0dfa347e8877a4d4ed19ee56c140fa518470028c` may change over time. Please check the latest commit ID in your environment to ensure you are using the correct one.  \nThe server entrypoint accepts all other LoRA configuration parameters (`max_loras`, `max_lora_rank`, `max_cpu_loras`,\netc.), which will apply to all forthcoming requests. Upon querying the `/models` endpoint, we should see our LoRA along\nwith its base model (if `jq` is not installed, you can follow [this guide](https://jqlang.org/download/) to install it.):  \n??? console \"Command\"  \n```bash", "file_path": "features/lora.md"}
{"id": "f8df435b37b96a6c100ef60e6f209b5652467ede533de75b9944c9b57a6b24c8", "heading": "LoRA Adapters/Serving LoRA Adapters", "level": 2, "text": "??? console \"Command\"  \n```bash\ncurl localhost:8000/v1/models | jq .\n{\n\"object\": \"list\",\n\"data\": [\n{\n\"id\": \"meta-llama/Llama-2-7b-hf\",\n\"object\": \"model\",\n...\n},\n{\n\"id\": \"sql-lora\",\n\"object\": \"model\",\n...\n}\n]\n}\n```  \nRequests can specify the LoRA adapter as if it were any other model via the `model` request parameter. The requests will be\nprocessed according to the server-wide LoRA configuration (i.e. in parallel with base model requests, and potentially other\nLoRA adapter requests if they were provided and `max_loras` is set high enough).  \nThe following is an example request  \n```bash\ncurl http://localhost:8000/v1/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"sql-lora\",\n\"prompt\": \"San Francisco is a\",\n\"max_tokens\": 7,\n\"temperature\": 0\n}' | jq\n```", "file_path": "features/lora.md"}
{"id": "f8df435b37b96a6c100ef60e6f209b5652467ede533de75b9944c9b57a6b24c8", "heading": "LoRA Adapters/Dynamically serving LoRA Adapters", "level": 2, "text": "## Dynamically serving LoRA Adapters  \nIn addition to serving LoRA adapters at server startup, the vLLM server supports dynamically configuring LoRA adapters at runtime through dedicated API endpoints and plugins. This feature can be particularly useful when the flexibility to change models on-the-fly is needed.  \nNote: Enabling this feature in production environments is risky as users may participate in model adapter management.  \nTo enable dynamic LoRA configuration, ensure that the environment variable `VLLM_ALLOW_RUNTIME_LORA_UPDATING`\nis set to `True`.  \n```bash\nexport VLLM_ALLOW_RUNTIME_LORA_UPDATING=True\n```", "file_path": "features/lora.md"}
{"id": "f8df435b37b96a6c100ef60e6f209b5652467ede533de75b9944c9b57a6b24c8", "heading": "LoRA Adapters/Dynamically serving LoRA Adapters/Using API Endpoints", "level": 3, "text": "### Using API Endpoints  \nLoading a LoRA Adapter:  \nTo dynamically load a LoRA adapter, send a POST request to the `/v1/load_lora_adapter` endpoint with the necessary\ndetails of the adapter to be loaded. The request payload should include the name and path to the LoRA adapter.  \nExample request to load a LoRA adapter:  \n```bash\ncurl -X POST http://localhost:8000/v1/load_lora_adapter \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"lora_name\": \"sql_adapter\",\n\"lora_path\": \"/path/to/sql-lora-adapter\"\n}'\n```  \nUpon a successful request, the API will respond with a `200 OK` status code from `vllm serve`, and `curl` returns the response body: `Success: LoRA adapter 'sql_adapter' added successfully`. If an error occurs, such as if the adapter\ncannot be found or loaded, an appropriate error message will be returned.  \nUnloading a LoRA Adapter:  \nTo unload a LoRA adapter that has been previously loaded, send a POST request to the `/v1/unload_lora_adapter` endpoint\nwith the name or ID of the adapter to be unloaded.", "file_path": "features/lora.md"}
{"id": "f8df435b37b96a6c100ef60e6f209b5652467ede533de75b9944c9b57a6b24c8", "heading": "LoRA Adapters/Dynamically serving LoRA Adapters/Using API Endpoints", "level": 3, "text": "with the name or ID of the adapter to be unloaded.  \nUpon a successful request, the API responds with a `200 OK` status code from `vllm serve`, and `curl` returns the response body: `Success: LoRA adapter 'sql_adapter' removed successfully`.  \nExample request to unload a LoRA adapter:  \n```bash\ncurl -X POST http://localhost:8000/v1/unload_lora_adapter \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"lora_name\": \"sql_adapter\"\n}'\n```", "file_path": "features/lora.md"}
{"id": "f8df435b37b96a6c100ef60e6f209b5652467ede533de75b9944c9b57a6b24c8", "heading": "LoRA Adapters/Dynamically serving LoRA Adapters/Using Plugins", "level": 3, "text": "### Using Plugins  \nAlternatively, you can use the LoRAResolver plugin to dynamically load LoRA adapters. LoRAResolver plugins enable you to load LoRA adapters from both local and remote sources such as local file system and S3. On every request, when there's a new model name that hasn't been loaded yet, the LoRAResolver will try to resolve and load the corresponding LoRA adapter.  \nYou can set up multiple LoRAResolver plugins if you want to load LoRA adapters from different sources. For example, you might have one resolver for local files and another for S3 storage. vLLM will load the first LoRA adapter that it finds.  \nYou can either install existing plugins or implement your own. By default, vLLM comes with a [resolver plugin to load LoRA adapters from a local directory.](https://github.com/vllm-project/vllm/tree/main/vllm/plugins/lora_resolvers)", "file_path": "features/lora.md"}
{"id": "f8df435b37b96a6c100ef60e6f209b5652467ede533de75b9944c9b57a6b24c8", "heading": "LoRA Adapters/Dynamically serving LoRA Adapters/Using Plugins", "level": 3, "text": "To enable this resolver, set `VLLM_ALLOW_RUNTIME_LORA_UPDATING` to True, set `VLLM_PLUGINS` to include `lora_filesystem_resolver`, and then set `VLLM_LORA_RESOLVER_CACHE_DIR` to a local directory. When vLLM receives a request using a LoRA adapter `foobar`,\nit will first look in the local directory for a directory `foobar`, and attempt to load the contents of that directory as a LoRA adapter. If successful, the request will complete as normal and\nthat adapter will then be available for normal use on the server.  \nAlternatively, follow these example steps to implement your own plugin:  \n1. Implement the LoRAResolver interface.  \n??? code \"Example of a simple S3 LoRAResolver implementation\"  \n```python\nimport os\nimport s3fs\nfrom vllm.lora.request import LoRARequest\nfrom vllm.lora.resolver import LoRAResolver", "file_path": "features/lora.md"}
{"id": "f8df435b37b96a6c100ef60e6f209b5652467ede533de75b9944c9b57a6b24c8", "heading": "LoRA Adapters/Dynamically serving LoRA Adapters/Using Plugins", "level": 3, "text": "class S3LoRAResolver(LoRAResolver):\ndef __init__(self):\nself.s3 = s3fs.S3FileSystem()\nself.s3_path_format = os.getenv(\"S3_PATH_TEMPLATE\")\nself.local_path_format = os.getenv(\"LOCAL_PATH_TEMPLATE\")\n\nasync def resolve_lora(self, base_model_name, lora_name):\ns3_path = self.s3_path_format.format(base_model_name=base_model_name, lora_name=lora_name)\nlocal_path = self.local_path_format.format(base_model_name=base_model_name, lora_name=lora_name)\n\n# Download the LoRA from S3 to the local path\nawait self.s3._get(\ns3_path, local_path, recursive=True, maxdepth=1\n)\n\nlora_request = LoRARequest(\nlora_name=lora_name,\nlora_path=local_path,\nlora_int_id=abs(hash(lora_name)),\n)\nreturn lora_request\n```  \n2. Register `LoRAResolver` plugin.  \n```python\nfrom vllm.lora.resolver import LoRAResolverRegistry\n\ns3_resolver = S3LoRAResolver()\nLoRAResolverRegistry.register_resolver(\"s3_resolver\", s3_resolver)\n```  \nFor more details, refer to the [vLLM's Plugins System](../design/plugin_system.md).", "file_path": "features/lora.md"}
{"id": "f8df435b37b96a6c100ef60e6f209b5652467ede533de75b9944c9b57a6b24c8", "heading": "LoRA Adapters/New format for `--lora-modules`", "level": 2, "text": "## New format for `--lora-modules`  \nIn the previous version, users would provide LoRA modules via the following format, either as a key-value pair or in JSON format. For example:  \n```bash\n--lora-modules sql-lora=$HOME/.cache/huggingface/hub/models--yard1--llama-2-7b-sql-lora-test/snapshots/0dfa347e8877a4d4ed19ee56c140fa518470028c/\n```  \nThis would only include the `name` and `path` for each LoRA module, but did not provide a way to specify a `base_model_name`.\nNow, you can specify a base_model_name alongside the name and path using JSON format. For example:  \n```bash\n--lora-modules '{\"name\": \"sql-lora\", \"path\": \"/path/to/lora\", \"base_model_name\": \"meta-llama/Llama-2-7b\"}'\n```  \nTo provide the backward compatibility support, you can still use the old key-value format (name=path), but the `base_model_name` will remain unspecified in that case.", "file_path": "features/lora.md"}
{"id": "f8df435b37b96a6c100ef60e6f209b5652467ede533de75b9944c9b57a6b24c8", "heading": "LoRA Adapters/LoRA model lineage in model card", "level": 2, "text": "## LoRA model lineage in model card  \nThe new format of `--lora-modules` is mainly to support the display of parent model information in the model card. Here's an explanation of how your current response supports this:  \n- The `parent` field of LoRA model `sql-lora` now links to its base model `meta-llama/Llama-2-7b-hf`. This correctly reflects the hierarchical relationship between the base model and the LoRA adapter.\n- The `root` field points to the artifact location of the lora adapter.  \n??? console \"Command output\"  \n```bash\n$ curl http://localhost:8000/v1/models", "file_path": "features/lora.md"}
{"id": "f8df435b37b96a6c100ef60e6f209b5652467ede533de75b9944c9b57a6b24c8", "heading": "LoRA Adapters/LoRA model lineage in model card", "level": 2, "text": "{\n\"object\": \"list\",\n\"data\": [\n{\n\"id\": \"meta-llama/Llama-2-7b-hf\",\n\"object\": \"model\",\n\"created\": 1715644056,\n\"owned_by\": \"vllm\",\n\"root\": \"~/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/\",\n\"parent\": null,\n\"permission\": [\n{\n.....\n}\n]\n},\n{\n\"id\": \"sql-lora\",\n\"object\": \"model\",\n\"created\": 1715644056,\n\"owned_by\": \"vllm\",\n\"root\": \"~/.cache/huggingface/hub/models--yard1--llama-2-7b-sql-lora-test/snapshots/0dfa347e8877a4d4ed19ee56c140fa518470028c/\",\n\"parent\": meta-llama/Llama-2-7b-hf,\n\"permission\": [\n{\n....\n}\n]\n}\n]\n}\n```", "file_path": "features/lora.md"}
{"id": "f8df435b37b96a6c100ef60e6f209b5652467ede533de75b9944c9b57a6b24c8", "heading": "LoRA Adapters/Default LoRA Models For Multimodal Models", "level": 2, "text": "## Default LoRA Models For Multimodal Models  \nSome models, e.g., [Granite Speech](https://huggingface.co/ibm-granite/granite-speech-3.3-8b) and [Phi-4-multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct) multimodal, contain LoRA adapter(s) that are expected to always be applied when a given modality is present. This can be a bit tedious to manage with the above approaches, as it requires the user to send the `LoRARequest` (offline) or to filter requests between the base model and LoRA model (server) depending on the content of the request's multimodal data.  \nTo this end, we allow registration of default multimodal LoRAs to handle this automatically, where users can map each modality to a LoRA adapter to automatically apply it when the corresponding inputs are present. Note that currently, we only allow one LoRA per prompt; if several modalities are provided, each of which are registered to a given modality, none of them will be applied.", "file_path": "features/lora.md"}
{"id": "f8df435b37b96a6c100ef60e6f209b5652467ede533de75b9944c9b57a6b24c8", "heading": "LoRA Adapters/Default LoRA Models For Multimodal Models", "level": 2, "text": "??? code \"Example usage for offline inference\"  \n```python\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\nfrom vllm.assets.audio import AudioAsset", "file_path": "features/lora.md"}
{"id": "f8df435b37b96a6c100ef60e6f209b5652467ede533de75b9944c9b57a6b24c8", "heading": "LoRA Adapters/Default LoRA Models For Multimodal Models", "level": 2, "text": "model_id = \"ibm-granite/granite-speech-3.3-2b\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\ndef get_prompt(question: str, has_audio: bool):\n\"\"\"Build the input prompt to send to vLLM.\"\"\"\nif has_audio:\nquestion = f\"<|audio|>{question}\"\nchat = [\n{\"role\": \"user\", \"content\": question},\n]\nreturn tokenizer.apply_chat_template(chat, tokenize=False)\n\n\nllm = LLM(\nmodel=model_id,\nenable_lora=True,\nmax_lora_rank=64,\nmax_model_len=2048,\nlimit_mm_per_prompt={\"audio\": 1},\n# Will always pass a `LoRARequest` with the `model_id`\n# whenever audio is contained in the request data.\ndefault_mm_loras = {\"audio\": model_id},\nenforce_eager=True,\n)\n\nquestion = \"can you transcribe the speech into a written format?\"\nprompt_with_audio = get_prompt(\nquestion=question,\nhas_audio=True,\n)\naudio = AudioAsset(\"mary_had_lamb\").audio_and_sample_rate\n\ninputs = {\n\"prompt\": prompt_with_audio,\n\"multi_modal_data\": {\n\"audio\": audio,\n}\n}", "file_path": "features/lora.md"}
{"id": "f8df435b37b96a6c100ef60e6f209b5652467ede533de75b9944c9b57a6b24c8", "heading": "LoRA Adapters/Default LoRA Models For Multimodal Models", "level": 2, "text": "outputs = llm.generate(\ninputs,\nsampling_params=SamplingParams(\ntemperature=0.2,\nmax_tokens=64,\n),\n)\n```  \nYou can also pass a json dictionary of `--default-mm-loras` mapping modalities to LoRA model IDs. For example, when starting the server:  \n```bash\nvllm serve ibm-granite/granite-speech-3.3-2b \\\n--max-model-len 2048 \\\n--enable-lora \\\n--default-mm-loras '{\"audio\":\"ibm-granite/granite-speech-3.3-2b\"}' \\\n--max-lora-rank 64\n```  \nNote: Default multimodal LoRAs are currently only available for `.generate` and chat completions.", "file_path": "features/lora.md"}
{"id": "f8df435b37b96a6c100ef60e6f209b5652467ede533de75b9944c9b57a6b24c8", "heading": "LoRA Adapters/Using Tips/Configuring `max_lora_rank`", "level": 3, "text": "## Using Tips  \n### Configuring `max_lora_rank`  \nThe `--max-lora-rank` parameter controls the maximum rank allowed for LoRA adapters. This setting affects memory allocation and performance:  \n- **Set it to the maximum rank** among all LoRA adapters you plan to use\n- **Avoid setting it too high** - using a value much larger than needed wastes memory and can cause performance issues  \nFor example, if your LoRA adapters have ranks [16, 32, 64], use `--max-lora-rank 64` rather than 256  \n```bash\n# Good: matches actual maximum rank\nvllm serve model --enable-lora --max-lora-rank 64\n\n# Bad: unnecessarily high, wastes memory\nvllm serve model --enable-lora --max-lora-rank 256\n```", "file_path": "features/lora.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs", "level": 1, "text": "# Multimodal Inputs  \nThis page teaches you how to pass multi-modal inputs to [multi-modal models][supported-mm-models] in vLLM.  \n!!! note\nWe are actively iterating on multi-modal support. See [this RFC](gh-issue:4194) for upcoming changes,\nand [open an issue on GitHub](https://github.com/vllm-project/vllm/issues/new/choose) if you have any feedback or feature requests.  \n!!! tip\nWhen serving multi-modal models, consider setting `--allowed-media-domains` to restrict domain that vLLM can access to prevent it from accessing arbitrary endpoints that can potentially be vulnerable to Server-Side Request Forgery (SSRF) attacks. You can provide a list of domains for this arg. For example: `--allowed-media-domains upload.wikimedia.org github.com www.bogotobogo.com`  \nAlso, consider setting `VLLM_MEDIA_URL_ALLOW_REDIRECTS=0` to prevent HTTP redirects from being followed to bypass domain restrictions.", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs", "level": 1, "text": "This restriction is especially important if you run vLLM in a containerized environment where the vLLM pods may have unrestricted access to internal networks.", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Offline Inference", "level": 2, "text": "## Offline Inference  \nTo input multi-modal data, follow this schema in [vllm.inputs.PromptType][]:  \n- `prompt`: The prompt should follow the format that is documented on HuggingFace.\n- `multi_modal_data`: This is a dictionary that follows the schema defined in [vllm.multimodal.inputs.MultiModalDataDict][].", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Offline Inference/Stable UUIDs for Caching (multi_modal_uuids)", "level": 3, "text": "### Stable UUIDs for Caching (multi_modal_uuids)  \nWhen using multi-modal inputs, vLLM normally hashes each media item by content to enable caching across requests. You can optionally pass `multi_modal_uuids` to provide your own stable IDs for each item so caching can reuse work across requests without rehashing the raw content.  \n??? code  \n```python\nfrom vllm import LLM\nfrom PIL import Image\n\n# Qwen2.5-VL example with two images\nllm = LLM(model=\"Qwen/Qwen2.5-VL-3B-Instruct\")\n\nprompt = \"USER: <image><image>\\nDescribe the differences.\\nASSISTANT:\"\nimg_a = Image.open(\"/path/to/a.jpg\")\nimg_b = Image.open(\"/path/to/b.jpg\")\n\noutputs = llm.generate({\n\"prompt\": prompt,\n\"multi_modal_data\": {\"image\": [img_a, img_b]},\n# Provide stable IDs for caching.\n# Requirements (matched by this example):\n#  - Include every modality present in multi_modal_data.\n#  - For lists, provide the same number of entries.\n#  - Use None to fall back to content hashing for that item.\n\"multi_modal_uuids\": {\"image\": [\"sku-1234-a\", None]},\n})", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Offline Inference/Stable UUIDs for Caching (multi_modal_uuids)", "level": 3, "text": "for o in outputs:\nprint(o.outputs[0].text)\n```  \nUsing UUIDs, you can also skip sending media data entirely if you expect cache hits for respective items. Note that the request will fail if the skipped media doesn't have a corresponding UUID, or if the UUID fails to hit the cache.  \n??? code  \n```python\nfrom vllm import LLM\nfrom PIL import Image\n\n# Qwen2.5-VL example with two images\nllm = LLM(model=\"Qwen/Qwen2.5-VL-3B-Instruct\")\n\nprompt = \"USER: <image><image>\\nDescribe the differences.\\nASSISTANT:\"\nimg_b = Image.open(\"/path/to/b.jpg\")\n\noutputs = llm.generate({\n\"prompt\": prompt,\n\"multi_modal_data\": {\"image\": [None, img_b]},\n# Since img_a is expected to be cached, we can skip sending the actual\n# image entirely.\n\"multi_modal_uuids\": {\"image\": [\"sku-1234-a\", None]},\n})\n\nfor o in outputs:\nprint(o.outputs[0].text)\n```  \n!!! warning\nIf both multimodal processor caching and prefix caching are disabled, user-provided `multi_modal_uuids` are ignored.", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Offline Inference/Image Inputs", "level": 3, "text": "### Image Inputs  \nYou can pass a single image to the `'image'` field of the multi-modal dictionary, as shown in the following examples:  \n??? code  \n```python\nfrom vllm import LLM\n\nllm = LLM(model=\"llava-hf/llava-1.5-7b-hf\")\n\n# Refer to the HuggingFace repo for the correct format to use\nprompt = \"USER: <image>\\nWhat is the content of this image?\\nASSISTANT:\"\n\n# Load the image using PIL.Image\nimage = PIL.Image.open(...)\n\n# Single prompt inference\noutputs = llm.generate({\n\"prompt\": prompt,\n\"multi_modal_data\": {\"image\": image},\n})\n\nfor o in outputs:\ngenerated_text = o.outputs[0].text\nprint(generated_text)\n\n# Batch inference\nimage_1 = PIL.Image.open(...)\nimage_2 = PIL.Image.open(...)\noutputs = llm.generate(\n[\n{\n\"prompt\": \"USER: <image>\\nWhat is the content of this image?\\nASSISTANT:\",\n\"multi_modal_data\": {\"image\": image_1},\n},\n{\n\"prompt\": \"USER: <image>\\nWhat's the color of this image?\\nASSISTANT:\",\n\"multi_modal_data\": {\"image\": image_2},\n}\n]\n)", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Offline Inference/Image Inputs", "level": 3, "text": "for o in outputs:\ngenerated_text = o.outputs[0].text\nprint(generated_text)\n```  \nFull example: <gh-file:examples/offline_inference/vision_language.py>  \nTo substitute multiple images inside the same text prompt, you can pass in a list of images instead:  \n??? code  \n```python\nfrom vllm import LLM\n\nllm = LLM(\nmodel=\"microsoft/Phi-3.5-vision-instruct\",\ntrust_remote_code=True,  # Required to load Phi-3.5-vision\nmax_model_len=4096,  # Otherwise, it may not fit in smaller GPUs\nlimit_mm_per_prompt={\"image\": 2},  # The maximum number to accept\n)\n\n# Refer to the HuggingFace repo for the correct format to use\nprompt = \"<|user|>\\n<|image_1|>\\n<|image_2|>\\nWhat is the content of each image?<|end|>\\n<|assistant|>\\n\"\n\n# Load the images using PIL.Image\nimage1 = PIL.Image.open(...)\nimage2 = PIL.Image.open(...)\n\noutputs = llm.generate({\n\"prompt\": prompt,\n\"multi_modal_data\": {\"image\": [image1, image2]},\n})", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Offline Inference/Image Inputs", "level": 3, "text": "for o in outputs:\ngenerated_text = o.outputs[0].text\nprint(generated_text)\n```  \nFull example: <gh-file:examples/offline_inference/vision_language_multi_image.py>  \nIf using the [LLM.chat](../models/generative_models.md#llmchat) method, you can pass images directly in the message content using various formats: image URLs, PIL Image objects, or pre-computed embeddings:  \n```python\nfrom vllm import LLM\nfrom vllm.assets.image import ImageAsset\n\nllm = LLM(model=\"llava-hf/llava-1.5-7b-hf\")\nimage_url = \"https://picsum.photos/id/32/512/512\"\nimage_pil = ImageAsset('cherry_blossom').pil_image\nimage_embeds = torch.load(...)", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Offline Inference/Image Inputs", "level": 3, "text": "conversation = [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n{\"role\": \"user\", \"content\": \"Hello\"},\n{\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"},\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image_url\",\n\"image_url\": {\"url\": image_url},\n},\n{\n\"type\": \"image_pil\",\n\"image_pil\": image_pil,\n},\n{\n\"type\": \"image_embeds\",\n\"image_embeds\": image_embeds,\n},\n{\n\"type\": \"text\",\n\"text\": \"What's in these images?\",\n},\n],\n},\n]\n\n# Perform inference and log output.\noutputs = llm.chat(conversation)\n\nfor o in outputs:\ngenerated_text = o.outputs[0].text\nprint(generated_text)\n```  \nMulti-image input can be extended to perform video captioning. We show this with [Qwen2-VL](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct) as it supports videos:  \n??? code  \n```python\nfrom vllm import LLM\n\n# Specify the maximum number of frames per video to be 4. This can be changed.\nllm = LLM(\"Qwen/Qwen2-VL-2B-Instruct\", limit_mm_per_prompt={\"image\": 4})", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Offline Inference/Image Inputs", "level": 3, "text": "# Create the request payload.\nvideo_frames = ... # load your video making sure it only has the number of frames specified earlier.\nmessage = {\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"Describe this set of frames. Consider the frames to be a part of the same video.\",\n},\n],\n}\nfor i in range(len(video_frames)):\nbase64_image = encode_image(video_frames[i]) # base64 encoding.\nnew_image = {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\nmessage[\"content\"].append(new_image)\n\n# Perform inference and log output.\noutputs = llm.chat([message])\n\nfor o in outputs:\ngenerated_text = o.outputs[0].text\nprint(generated_text)\n```  \n#### Custom RGBA Background Color  \nWhen loading RGBA images (images with transparency), vLLM converts them to RGB format. By default, transparent pixels are replaced with white background. You can customize this background color using the `rgba_background_color` parameter in `media_io_kwargs`.  \n??? code  \n```python\nfrom vllm import LLM", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Offline Inference/Image Inputs", "level": 3, "text": "# Default white background (no configuration needed)\nllm = LLM(model=\"llava-hf/llava-1.5-7b-hf\")\n\n# Custom black background for dark theme\nllm = LLM(\nmodel=\"llava-hf/llava-1.5-7b-hf\",\nmedia_io_kwargs={\"image\": {\"rgba_background_color\": [0, 0, 0]}},\n)\n\n# Custom brand color background (e.g., blue)\nllm = LLM(\nmodel=\"llava-hf/llava-1.5-7b-hf\",\nmedia_io_kwargs={\"image\": {\"rgba_background_color\": [0, 0, 255]}},\n)\n```  \n!!! note\n- The `rgba_background_color` accepts RGB values as a list `[R, G, B]` or tuple `(R, G, B)` where each value is 0-255\n- This setting only affects RGBA images with transparency; RGB images are unchanged\n- If not specified, the default white background `(255, 255, 255)` is used for backward compatibility", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Offline Inference/Video Inputs", "level": 3, "text": "### Video Inputs  \nYou can pass a list of NumPy arrays directly to the `'video'` field of the multi-modal dictionary\ninstead of using multi-image input.  \nInstead of NumPy arrays, you can also pass `'torch.Tensor'` instances, as shown in this example using Qwen2.5-VL:  \n??? code  \n```python\nfrom transformers import AutoProcessor\nfrom vllm import LLM, SamplingParams\nfrom qwen_vl_utils import process_vision_info\n\nmodel_path = \"Qwen/Qwen2.5-VL-3B-Instruct\"\nvideo_path = \"https://content.pexels.com/videos/free-videos.mp4\"\n\nllm = LLM(\nmodel=model_path,\ngpu_memory_utilization=0.8,\nenforce_eager=True,\nlimit_mm_per_prompt={\"video\": 1},\n)\n\nsampling_params = SamplingParams(max_tokens=1024)\n\nvideo_messages = [\n{\n\"role\": \"system\",\n\"content\": \"You are a helpful assistant.\",\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"describe this video.\"},\n{\n\"type\": \"video\",\n\"video\": video_path,\n\"total_pixels\": 20480 * 28 * 28,\n\"min_pixels\": 16 * 28 * 28,\n},\n]\n},\n]", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Offline Inference/Video Inputs", "level": 3, "text": "messages = video_messages\nprocessor = AutoProcessor.from_pretrained(model_path)\nprompt = processor.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\n\nimage_inputs, video_inputs = process_vision_info(messages)\nmm_data = {}\nif video_inputs is not None:\nmm_data[\"video\"] = video_inputs\n\nllm_inputs = {\n\"prompt\": prompt,\n\"multi_modal_data\": mm_data,\n}\n\noutputs = llm.generate([llm_inputs], sampling_params=sampling_params)\nfor o in outputs:\ngenerated_text = o.outputs[0].text\nprint(generated_text)\n```  \n!!! note\n'process_vision_info' is only applicable to Qwen2.5-VL and similar models.  \nFull example: <gh-file:examples/offline_inference/vision_language.py>", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Offline Inference/Audio Inputs", "level": 3, "text": "### Audio Inputs  \nYou can pass a tuple `(array, sampling_rate)` to the `'audio'` field of the multi-modal dictionary.  \nFull example: <gh-file:examples/offline_inference/audio_language.py>", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Offline Inference/Embedding Inputs", "level": 3, "text": "### Embedding Inputs  \nTo input pre-computed embeddings belonging to a data type (i.e. image, video, or audio) directly to the language model,\npass a tensor of shape `(num_items, feature_size, hidden_size of LM)` to the corresponding field of the multi-modal dictionary.  \n??? code  \n```python\nfrom vllm import LLM\n\n# Inference with image embeddings as input\nllm = LLM(model=\"llava-hf/llava-1.5-7b-hf\")\n\n# Refer to the HuggingFace repo for the correct format to use\nprompt = \"USER: <image>\\nWhat is the content of this image?\\nASSISTANT:\"\n\n# Embeddings for single image\n# torch.Tensor of shape (1, image_feature_size, hidden_size of LM)\nimage_embeds = torch.load(...)\n\noutputs = llm.generate({\n\"prompt\": prompt,\n\"multi_modal_data\": {\"image\": image_embeds},\n})\n\nfor o in outputs:\ngenerated_text = o.outputs[0].text\nprint(generated_text)\n```  \nFor Qwen2-VL and MiniCPM-V, we accept additional parameters alongside the embeddings:  \n??? code  \n```python\n# Construct the prompt based on your model\nprompt = ...", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Offline Inference/Embedding Inputs", "level": 3, "text": "# Embeddings for multiple images\n# torch.Tensor of shape (num_images, image_feature_size, hidden_size of LM)\nimage_embeds = torch.load(...)\n\n# Qwen2-VL\nllm = LLM(\"Qwen/Qwen2-VL-2B-Instruct\", limit_mm_per_prompt={\"image\": 4})\nmm_data = {\n\"image\": {\n\"image_embeds\": image_embeds,\n# image_grid_thw is needed to calculate positional encoding.\n\"image_grid_thw\": torch.load(...),  # torch.Tensor of shape (1, 3),\n}\n}\n\n# MiniCPM-V\nllm = LLM(\"openbmb/MiniCPM-V-2_6\", trust_remote_code=True, limit_mm_per_prompt={\"image\": 4})\nmm_data = {\n\"image\": {\n\"image_embeds\": image_embeds,\n# image_sizes is needed to calculate details of the sliced image.\n\"image_sizes\": [image.size for image in images],  # list of image sizes\n}\n}\n\noutputs = llm.generate({\n\"prompt\": prompt,\n\"multi_modal_data\": mm_data,\n})\n\nfor o in outputs:\ngenerated_text = o.outputs[0].text\nprint(generated_text)\n```", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Online Serving", "level": 2, "text": "## Online Serving  \nOur OpenAI-compatible server accepts multi-modal data via the [Chat Completions API](https://platform.openai.com/docs/api-reference/chat). Media inputs also support optional UUIDs users can provide to uniquely identify each media, which is used to cache the media results across requests.  \n!!! important\nA chat template is **required** to use Chat Completions API.\nFor HF format models, the default chat template is defined inside `chat_template.json` or `tokenizer_config.json`.  \nIf no default chat template is available, we will first look for a built-in fallback in <gh-file:vllm/transformers_utils/chat_templates/registry.py>.\nIf no fallback is available, an error is raised and you have to provide the chat template manually via the `--chat-template` argument.  \nFor certain models, we provide alternative chat templates inside <gh-dir:examples>.\nFor example, VLM2Vec uses <gh-file:examples/template_vlm2vec_phi3v.jinja> which is different from the default one for Phi-3-Vision.", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Online Serving/Image Inputs", "level": 3, "text": "### Image Inputs  \nImage input is supported according to [OpenAI Vision API](https://platform.openai.com/docs/guides/vision).\nHere is a simple example using Phi-3.5-Vision.  \nFirst, launch the OpenAI-compatible server:  \n```bash\nvllm serve microsoft/Phi-3.5-vision-instruct --runner generate \\\n--trust-remote-code --max-model-len 4096 --limit-mm-per-prompt '{\"image\":2}'\n```  \nThen, you can use the OpenAI client as follows:  \n??? code  \n```python\nfrom openai import OpenAI\n\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\n\n# Single-image input inference\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Online Serving/Image Inputs", "level": 3, "text": "chat_response = client.chat.completions.create(\nmodel=\"microsoft/Phi-3.5-vision-instruct\",\nmessages=[\n{\n\"role\": \"user\",\n\"content\": [\n# NOTE: The prompt formatting with the image token `<image>` is not needed\n# since the prompt will be processed automatically by the API server.\n{\n\"type\": \"text\",\n\"text\": \"Whatâ€™s in this image?\",\n},\n{\n\"type\": \"image_url\",\n\"image_url\": {\"url\": image_url},\n\"uuid\": image_url,  # Optional\n},\n],\n}\n],\n)\nprint(\"Chat completion output:\", chat_response.choices[0].message.content)\n\n# Multi-image input inference\nimage_url_duck = \"https://upload.wikimedia.org/wikipedia/commons/d/da/2015_Kaczka_krzy%C5%BCowka_w_wodzie_%28samiec%29.jpg\"\nimage_url_lion = \"https://upload.wikimedia.org/wikipedia/commons/7/77/002_The_lion_king_Snyggve_in_the_Serengeti_National_Park_Photo_by_Giles_Laurent.jpg\"", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Online Serving/Image Inputs", "level": 3, "text": "chat_response = client.chat.completions.create(\nmodel=\"microsoft/Phi-3.5-vision-instruct\",\nmessages=[\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"What are the animals in these images?\",\n},\n{\n\"type\": \"image_url\",\n\"image_url\": {\"url\": image_url_duck},\n\"uuid\": image_url_duck,  # Optional\n},\n{\n\"type\": \"image_url\",\n\"image_url\": {\"url\": image_url_lion},\n\"uuid\": image_url_lion,  # Optional\n},\n],\n}\n],\n)\nprint(\"Chat completion output:\", chat_response.choices[0].message.content)\n```  \nFull example: <gh-file:examples/online_serving/openai_chat_completion_client_for_multimodal.py>  \n!!! tip\nLoading from local file paths is also supported on vLLM: You can specify the allowed local media path via `--allowed-local-media-path` when launching the API server/engine,\nand pass the file path as `url` in the API request.  \n!!! tip\nThere is no need to place image placeholders in the text content of the API request - they are already represented by the image content.", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Online Serving/Image Inputs", "level": 3, "text": "In fact, you can place image placeholders in the middle of the text by interleaving text and image content.  \n!!! note\nBy default, the timeout for fetching images through HTTP URL is `5` seconds.\nYou can override this by setting the environment variable:  \n```bash\nexport VLLM_IMAGE_FETCH_TIMEOUT=<timeout>\n```", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Online Serving/Video Inputs", "level": 3, "text": "### Video Inputs  \nInstead of `image_url`, you can pass a video file via `video_url`. Here is a simple example using [LLaVA-OneVision](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf).  \nFirst, launch the OpenAI-compatible server:  \n```bash\nvllm serve llava-hf/llava-onevision-qwen2-0.5b-ov-hf --runner generate --max-model-len 8192\n```  \nThen, you can use the OpenAI client as follows:  \n??? code  \n```python\nfrom openai import OpenAI\n\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\n\nvideo_url = \"http://commondatastorage.googleapis.com/gtv-videos-bucket/sample/ForBiggerFun.mp4\"\n\n## Use video url in the payload\nchat_completion_from_url = client.chat.completions.create(\nmessages=[\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"What's in this video?\",\n},\n{\n\"type\": \"video_url\",\n\"video_url\": {\"url\": video_url},\n\"uuid\": video_url,  # Optional\n},\n],\n}\n],\nmodel=model,\nmax_completion_tokens=64,\n)", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Online Serving/Video Inputs", "level": 3, "text": "result = chat_completion_from_url.choices[0].message.content\nprint(\"Chat completion output from image url:\", result)\n```  \nFull example: <gh-file:examples/online_serving/openai_chat_completion_client_for_multimodal.py>  \n!!! note\nBy default, the timeout for fetching videos through HTTP URL is `30` seconds.\nYou can override this by setting the environment variable:  \n```bash\nexport VLLM_VIDEO_FETCH_TIMEOUT=<timeout>\n```  \n#### Custom RGBA Background Color  \nTo use a custom background color for RGBA images, pass the `rgba_background_color` parameter via `--media-io-kwargs`:  \n```bash\n# Example: Black background for dark theme\nvllm serve llava-hf/llava-1.5-7b-hf \\\n--media-io-kwargs '{\"image\": {\"rgba_background_color\": [0, 0, 0]}}'\n\n# Example: Custom gray background\nvllm serve llava-hf/llava-1.5-7b-hf \\\n--media-io-kwargs '{\"image\": {\"rgba_background_color\": [128, 128, 128]}}'\n```", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Online Serving/Audio Inputs", "level": 3, "text": "### Audio Inputs  \nAudio input is supported according to [OpenAI Audio API](https://platform.openai.com/docs/guides/audio?audio-generation-quickstart-example=audio-in).\nHere is a simple example using Ultravox-v0.5-1B.  \nFirst, launch the OpenAI-compatible server:  \n```bash\nvllm serve fixie-ai/ultravox-v0_5-llama-3_2-1b\n```  \nThen, you can use the OpenAI client as follows:  \n??? code  \n```python\nimport base64\nimport requests\nfrom openai import OpenAI\nfrom vllm.assets.audio import AudioAsset\n\ndef encode_base64_content_from_url(content_url: str) -> str:\n\"\"\"Encode a content retrieved from a remote url to base64 format.\"\"\"\n\nwith requests.get(content_url) as response:\nresponse.raise_for_status()\nresult = base64.b64encode(response.content).decode('utf-8')\n\nreturn result\n\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Online Serving/Audio Inputs", "level": 3, "text": "# Any format supported by librosa is supported\naudio_url = AudioAsset(\"winning_call\").url\naudio_base64 = encode_base64_content_from_url(audio_url)\n\nchat_completion_from_base64 = client.chat.completions.create(\nmessages=[\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"What's in this audio?\",\n},\n{\n\"type\": \"input_audio\",\n\"input_audio\": {\n\"data\": audio_base64,\n\"format\": \"wav\",\n},\n\"uuid\": audio_url,  # Optional\n},\n],\n},\n],\nmodel=model,\nmax_completion_tokens=64,\n)", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Online Serving/Audio Inputs", "level": 3, "text": "result = chat_completion_from_base64.choices[0].message.content\nprint(\"Chat completion output from input audio:\", result)\n```  \nAlternatively, you can pass `audio_url`, which is the audio counterpart of `image_url` for image input:  \n??? code  \n```python\nchat_completion_from_url = client.chat.completions.create(\nmessages=[\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"What's in this audio?\",\n},\n{\n\"type\": \"audio_url\",\n\"audio_url\": {\"url\": audio_url},\n\"uuid\": audio_url,  # Optional\n},\n],\n}\n],\nmodel=model,\nmax_completion_tokens=64,\n)\n\nresult = chat_completion_from_url.choices[0].message.content\nprint(\"Chat completion output from audio url:\", result)\n```  \nFull example: <gh-file:examples/online_serving/openai_chat_completion_client_for_multimodal.py>  \n!!! note\nBy default, the timeout for fetching audios through HTTP URL is `10` seconds.\nYou can override this by setting the environment variable:  \n```bash\nexport VLLM_AUDIO_FETCH_TIMEOUT=<timeout>\n```", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Online Serving/Embedding Inputs", "level": 3, "text": "### Embedding Inputs  \nTo input pre-computed embeddings belonging to a data type (i.e. image, video, or audio) directly to the language model,\npass a tensor of shape to the corresponding field of the multi-modal dictionary.  \n#### Image Embedding Inputs  \nFor image embeddings, you can pass the base64-encoded tensor to the `image_embeds` field.\nThe following example demonstrates how to pass image embeddings to the OpenAI server:  \n??? code  \n```python\nimage_embedding = torch.load(...)\ngrid_thw = torch.load(...) # Required by Qwen/Qwen2-VL-2B-Instruct\n\nbuffer = io.BytesIO()\ntorch.save(image_embedding, buffer)\nbuffer.seek(0)\nbinary_data = buffer.read()\nbase64_image_embedding = base64.b64encode(binary_data).decode('utf-8')\n\nclient = OpenAI(\n# defaults to os.environ.get(\"OPENAI_API_KEY\")\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Online Serving/Embedding Inputs", "level": 3, "text": "# Basic usage - this is equivalent to the LLaVA example for offline inference\nmodel = \"llava-hf/llava-1.5-7b-hf\"\nembeds = {\n\"type\": \"image_embeds\",\n\"image_embeds\": f\"{base64_image_embedding}\",\n\"uuid\": image_url,  # Optional\n}", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Online Serving/Embedding Inputs", "level": 3, "text": "# Pass additional parameters (available to Qwen2-VL and MiniCPM-V)\nmodel = \"Qwen/Qwen2-VL-2B-Instruct\"\nembeds = {\n\"type\": \"image_embeds\",\n\"image_embeds\": {\n\"image_embeds\": f\"{base64_image_embedding}\",  # Required\n\"image_grid_thw\": f\"{base64_image_grid_thw}\",  # Required by Qwen/Qwen2-VL-2B-Instruct\n},\n\"uuid\": image_url,  # Optional\n}\nmodel = \"openbmb/MiniCPM-V-2_6\"\nembeds = {\n\"type\": \"image_embeds\",\n\"image_embeds\": {\n\"image_embeds\": f\"{base64_image_embedding}\",  # Required\n\"image_sizes\": f\"{base64_image_sizes}\",  # Required by openbmb/MiniCPM-V-2_6\n},\n\"uuid\": image_url,  # Optional\n}\nchat_completion = client.chat.completions.create(\nmessages=[\n{\n\"role\": \"system\",\n\"content\": \"You are a helpful assistant.\",\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"What's in this image?\",\n},\nembeds,\n],\n},\n],\nmodel=model,\n)\n```  \nFor Online Serving, you can also skip sending media if you expect cache hits with provided UUIDs. You can do so by sending media like this:  \n```python\n# Image/video/audio URL:\n{", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Online Serving/Embedding Inputs", "level": 3, "text": "```python\n# Image/video/audio URL:\n{\n\"type\": \"image_url\",\n\"image_url\": None,\n\"uuid\": image_uuid,\n},", "file_path": "features/multimodal_inputs.md"}
{"id": "a83b9ff6f972ff7ecb52f79986cd061e8a7f0f86ba39bfa26842bc9aa7828c48", "heading": "Multimodal Inputs/Online Serving/Embedding Inputs", "level": 3, "text": "# image_embeds\n{\n\"type\": \"image_embeds\",\n\"image_embeds\": None,\n\"uuid\": image_uuid,\n},\n\n# input_audio:\n{\n\"type\": \"input_audio\",\n\"input_audio\": None,\n\"uuid\": audio_uuid,\n},\n\n# PIL Image:\n{\n\"type\": \"image_pil\",\n\"image_pil\": None,\n\"uuid\": image_uuid,\n},\n\n```  \n!!! note\nOnly one message can contain `{\"type\": \"image_embeds\"}`.\nIf used with a model that requires additional parameters, you must also provide a tensor for each of them, e.g. `image_grid_thw`, `image_sizes`, etc.", "file_path": "features/multimodal_inputs.md"}
{"id": "fe3ded151216ac6d7491633aee4b2ffbbfd95e71ba4f72de72d4a46403791974", "heading": "NixlConnector Usage Guide", "level": 1, "text": "# NixlConnector Usage Guide  \nNixlConnector is a high-performance KV cache transfer connector for vLLM's disaggregated prefilling feature. It provides fully asynchronous send/receive operations using the NIXL library for efficient cross-process KV cache transfer.", "file_path": "features/nixl_connector_usage.md"}
{"id": "fe3ded151216ac6d7491633aee4b2ffbbfd95e71ba4f72de72d4a46403791974", "heading": "NixlConnector Usage Guide/Prerequisites/Installation", "level": 3, "text": "## Prerequisites  \n### Installation  \nInstall the NIXL library: `uv pip install nixl`, as a quick start.  \n- Refer to [NIXL official repository](https://github.com/ai-dynamo/nixl) for more installation instructions\n- The specified required NIXL version can be found in [requirements/kv_connectors.txt](gh-file:requirements/kv_connectors.txt) and other relevant config files  \nFor non-cuda platform, please install nixl with ucx build from source, instructed as below.  \n```bash\npython tools/install_nixl_from_source_ubuntu.py\n```", "file_path": "features/nixl_connector_usage.md"}
{"id": "fe3ded151216ac6d7491633aee4b2ffbbfd95e71ba4f72de72d4a46403791974", "heading": "NixlConnector Usage Guide/Prerequisites/Transport Configuration", "level": 3, "text": "### Transport Configuration  \nNixlConnector uses NIXL library for underlying communication, which supports multiple transport backends. UCX (Unified Communication X) is the primary default transport library used by NIXL. Configure transport environment variables:  \n```bash\n# Example UCX configuration, adjust according to your enviroment\nexport UCX_TLS=all  # or specify specific transports like \"rc,ud,sm,^cuda_ipc\" ..etc\nexport UCX_NET_DEVICES=all  # or specify network devices like \"mlx5_0:1,mlx5_1:1\"\n```  \n!!! tip\nWhen using UCX as the transport backend, NCCL environment variables (like `NCCL_IB_HCA`, `NCCL_SOCKET_IFNAME`) are not applicable to NixlConnector, so configure UCX-specific environment variables instead of NCCL variables.", "file_path": "features/nixl_connector_usage.md"}
{"id": "fe3ded151216ac6d7491633aee4b2ffbbfd95e71ba4f72de72d4a46403791974", "heading": "NixlConnector Usage Guide/Basic Usage (on the same host)/Producer (Prefiller) Configuration", "level": 3, "text": "## Basic Usage (on the same host)  \n### Producer (Prefiller) Configuration  \nStart a prefiller instance that produces KV caches  \n```bash\n# 1st GPU as prefiller\nCUDA_VISIBLE_DEVICES=0 \\\nUCX_NET_DEVICES=all \\\nVLLM_NIXL_SIDE_CHANNEL_PORT=5600 \\\nvllm serve Qwen/Qwen3-0.6B \\\n--port 8100 \\\n--enforce-eager \\\n--kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\"}'\n```", "file_path": "features/nixl_connector_usage.md"}
{"id": "fe3ded151216ac6d7491633aee4b2ffbbfd95e71ba4f72de72d4a46403791974", "heading": "NixlConnector Usage Guide/Basic Usage (on the same host)/Consumer (Decoder) Configuration", "level": 3, "text": "### Consumer (Decoder) Configuration  \nStart a decoder instance that consumes KV caches:  \n```bash\n# 2nd GPU as decoder\nCUDA_VISIBLE_DEVICES=1 \\\nUCX_NET_DEVICES=all \\\nVLLM_NIXL_SIDE_CHANNEL_PORT=5601 \\\nvllm serve Qwen/Qwen3-0.6B \\\n--port 8200 \\\n--enforce-eager \\\n--kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\"}'\n```", "file_path": "features/nixl_connector_usage.md"}
{"id": "fe3ded151216ac6d7491633aee4b2ffbbfd95e71ba4f72de72d4a46403791974", "heading": "NixlConnector Usage Guide/Basic Usage (on the same host)/Proxy Server", "level": 3, "text": "### Proxy Server  \nUse a proxy server to route requests between prefiller and decoder:  \n```bash\npython tests/v1/kv_connector/nixl_integration/toy_proxy_server.py \\\n--port 8192 \\\n--prefiller-hosts localhost \\\n--prefiller-ports 8100 \\\n--decoder-hosts localhost \\\n--decoder-ports 8200\n```", "file_path": "features/nixl_connector_usage.md"}
{"id": "fe3ded151216ac6d7491633aee4b2ffbbfd95e71ba4f72de72d4a46403791974", "heading": "NixlConnector Usage Guide/Environment Variables", "level": 2, "text": "## Environment Variables  \n- `VLLM_NIXL_SIDE_CHANNEL_PORT`: Port for NIXL handshake communication\n- Default: 5600\n- **Required for both prefiller and decoder instances**\n- Each vLLM worker needs a unique port on its host; using the same port number across different hosts is fine\n- For TP/DP deployments, each worker's port on a node is computed as: base_port + dp_rank * tp_size + tp_rank (e.g., with `--tensor-parallel-size=4` and base_port=5600, tp_rank 0..3 use ports 5600, 5601, 5602, 5603 on that node).\n- Used for the initial NIXL handshake between the prefiller and the decoder  \n- `VLLM_NIXL_SIDE_CHANNEL_HOST`: Host for side channel communication\n- Default: \"localhost\"\n- Set when prefiller and decoder are on different machines\n- Connection info is passed via KVTransferParams from prefiller to decoder for handshake  \n- `VLLM_NIXL_ABORT_REQUEST_TIMEOUT`: Timeout (in seconds) for automatically releasing the prefillerâ€™s KV cache for a particular request. (Optional)\n- Default: 480", "file_path": "features/nixl_connector_usage.md"}
{"id": "fe3ded151216ac6d7491633aee4b2ffbbfd95e71ba4f72de72d4a46403791974", "heading": "NixlConnector Usage Guide/Environment Variables", "level": 2, "text": "- Default: 480\n- If a request is aborted and the decoder has not yet read the KV-cache blocks through the nixl channel, the prefill instance will release its KV-cache blocks after this timeout to avoid holding them indefinitely.", "file_path": "features/nixl_connector_usage.md"}
{"id": "fe3ded151216ac6d7491633aee4b2ffbbfd95e71ba4f72de72d4a46403791974", "heading": "NixlConnector Usage Guide/Multi-Instance Setup/Multiple Prefiller Instances on Different Machines", "level": 3, "text": "## Multi-Instance Setup  \n### Multiple Prefiller Instances on Different Machines  \n```bash\n# Prefiller 1 on Machine A (example IP: ${IP1})\nVLLM_NIXL_SIDE_CHANNEL_HOST=${IP1} \\\nVLLM_NIXL_SIDE_CHANNEL_PORT=5600 \\\nUCX_NET_DEVICES=all \\\nvllm serve Qwen/Qwen3-0.6B --port 8000 \\\n--tensor-parallel-size 8 \\\n--kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_producer\"}'\n\n# Prefiller 2 on Machine B (example IP: ${IP2})\nVLLM_NIXL_SIDE_CHANNEL_HOST=${IP2} \\\nVLLM_NIXL_SIDE_CHANNEL_PORT=5600 \\\nUCX_NET_DEVICES=all \\\nvllm serve Qwen/Qwen3-0.6B --port 8000 \\\n--tensor-parallel-size 8 \\\n--kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_producer\"}'\n```", "file_path": "features/nixl_connector_usage.md"}
{"id": "fe3ded151216ac6d7491633aee4b2ffbbfd95e71ba4f72de72d4a46403791974", "heading": "NixlConnector Usage Guide/Multi-Instance Setup/Multiple Decoder Instances on Different Machines", "level": 3, "text": "### Multiple Decoder Instances on Different Machines  \n```bash\n# Decoder 1 on Machine C (example IP: ${IP3})\nVLLM_NIXL_SIDE_CHANNEL_HOST=${IP3} \\\nVLLM_NIXL_SIDE_CHANNEL_PORT=5600 \\\nUCX_NET_DEVICES=all \\\nvllm serve Qwen/Qwen3-0.6B --port 8000 \\\n--tensor-parallel-size 8 \\\n--kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_consumer\"}'\n\n# Decoder 2 on Machine D (example IP: ${IP4})\nVLLM_NIXL_SIDE_CHANNEL_HOST=${IP4} \\\nVLLM_NIXL_SIDE_CHANNEL_PORT=5600 \\\nUCX_NET_DEVICES=all \\\nvllm serve Qwen/Qwen3-0.6B --port 8000 \\\n--tensor-parallel-size 8 \\\n--kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_consumer\"}'\n```", "file_path": "features/nixl_connector_usage.md"}
{"id": "fe3ded151216ac6d7491633aee4b2ffbbfd95e71ba4f72de72d4a46403791974", "heading": "NixlConnector Usage Guide/Multi-Instance Setup/Proxy for Multiple Instances", "level": 3, "text": "### Proxy for Multiple Instances  \n```bash\npython tests/v1/kv_connector/nixl_integration/toy_proxy_server.py \\\n--port 8192 \\\n--prefiller-hosts ${IP1} ${IP2} \\\n--prefiller-ports 8000 8000 \\\n--decoder-hosts ${IP3} ${IP4} \\\n--decoder-ports 8000 8000\n```", "file_path": "features/nixl_connector_usage.md"}
{"id": "fe3ded151216ac6d7491633aee4b2ffbbfd95e71ba4f72de72d4a46403791974", "heading": "NixlConnector Usage Guide/Multi-Instance Setup/KV Role Options", "level": 3, "text": "### KV Role Options  \n- **kv_producer**: For prefiller instances that generate KV caches\n- **kv_consumer**: For decoder instances that consume KV caches from prefiller\n- **kv_both**: Enables symmetric functionality where the connector can act as both producer and consumer. This provides flexibility for experimental setups and scenarios where the role distinction is not predetermined.  \n!!! tip\nNixlConnector currently does not distinguish `kv_role`; the actual prefiller/decoder roles are determined by the upper-level proxy (e.g., `toy_proxy_server.py` using `--prefiller-hosts` and `--decoder-hosts`).\nTherefore, `kv_role` in `--kv-transfer-config` is effectively a placeholder and does not affect NixlConnector's behavior.", "file_path": "features/nixl_connector_usage.md"}
{"id": "fe3ded151216ac6d7491633aee4b2ffbbfd95e71ba4f72de72d4a46403791974", "heading": "NixlConnector Usage Guide/Experimental Feature/Heterogenuous KV Layout support", "level": 3, "text": "## Experimental Feature  \n### Heterogenuous KV Layout support  \nSupport use case: Prefill with 'HND' and decode with 'NHD' with experimental configuration  \n```bash\n--kv-transfer-config '{..., \"enable_permute_local_kv\":\"True\"}'\n```", "file_path": "features/nixl_connector_usage.md"}
{"id": "fe3ded151216ac6d7491633aee4b2ffbbfd95e71ba4f72de72d4a46403791974", "heading": "NixlConnector Usage Guide/Example Scripts/Code", "level": 2, "text": "## Example Scripts/Code  \nRefer to these example scripts in the vLLM repository:  \n- [run_accuracy_test.sh](gh-file:tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh)\n- [toy_proxy_server.py](gh-file:tests/v1/kv_connector/nixl_integration/toy_proxy_server.py)\n- [test_accuracy.py](gh-file:tests/v1/kv_connector/nixl_integration/test_accuracy.py)", "file_path": "features/nixl_connector_usage.md"}
{"id": "2fc317f7ac4880bc647a433f751b1747e83023829c6f1bd6cb5030cbd313ed30", "heading": "Prompt Embedding Inputs", "level": 1, "text": "# Prompt Embedding Inputs  \nThis page teaches you how to pass prompt embedding inputs to vLLM.", "file_path": "features/prompt_embeds.md"}
{"id": "2fc317f7ac4880bc647a433f751b1747e83023829c6f1bd6cb5030cbd313ed30", "heading": "Prompt Embedding Inputs/What are prompt embeddings?", "level": 2, "text": "## What are prompt embeddings?  \nThe traditional flow of text data for a Large Language Model goes from text to token ids (via a tokenizer) then from token ids to prompt embeddings. For a traditional decoder-only model (such as meta-llama/Llama-3.1-8B-Instruct), this step of converting token ids to prompt embeddings happens via a look-up from a learned embedding matrix, but the model is not limited to processing only the embeddings corresponding to its token vocabulary.", "file_path": "features/prompt_embeds.md"}
{"id": "2fc317f7ac4880bc647a433f751b1747e83023829c6f1bd6cb5030cbd313ed30", "heading": "Prompt Embedding Inputs/Offline Inference", "level": 2, "text": "## Offline Inference  \nTo input multi-modal data, follow this schema in [vllm.inputs.EmbedsPrompt][]:  \n- `prompt_embeds`: A torch tensor representing a sequence of prompt/token embeddings. This has the shape (sequence_length, hidden_size), where sequence length is the number of tokens embeddings and hidden_size is the hidden size (embedding size) of the model.", "file_path": "features/prompt_embeds.md"}
{"id": "2fc317f7ac4880bc647a433f751b1747e83023829c6f1bd6cb5030cbd313ed30", "heading": "Prompt Embedding Inputs/Offline Inference/Hugging Face Transformers Inputs", "level": 3, "text": "### Hugging Face Transformers Inputs  \nYou can pass prompt embeddings from Hugging Face Transformers models to the  `'prompt_embeds'` field of the prompt embedding dictionary, as shown in the following examples:  \n<gh-file:examples/offline_inference/prompt_embed_inference.py>", "file_path": "features/prompt_embeds.md"}
{"id": "2fc317f7ac4880bc647a433f751b1747e83023829c6f1bd6cb5030cbd313ed30", "heading": "Prompt Embedding Inputs/Online Serving", "level": 2, "text": "## Online Serving  \nOur OpenAI-compatible server accepts prompt embeddings inputs via the [Completions API](https://platform.openai.com/docs/api-reference/completions). Prompt embeddings inputs are added via a new `'prompt_embeds'` key in the JSON package.  \nWhen a mixture of `'prompt_embeds'` and `'prompt'` inputs are provided in a single request, the prompt embeds are always returned first.  \nPrompt embeddings are passed in as base64 encoded torch tensors.", "file_path": "features/prompt_embeds.md"}
{"id": "2fc317f7ac4880bc647a433f751b1747e83023829c6f1bd6cb5030cbd313ed30", "heading": "Prompt Embedding Inputs/Online Serving/Transformers Inputs via OpenAI Client", "level": 3, "text": "### Transformers Inputs via OpenAI Client  \nFirst, launch the OpenAI-compatible server:  \n```bash\nvllm serve meta-llama/Llama-3.2-1B-Instruct --runner generate \\\n--max-model-len 4096 --enable-prompt-embeds\n```  \nThen, you can use the OpenAI client as follows:  \n<gh-file:examples/online_serving/prompt_embed_inference_with_openai_client.py>", "file_path": "features/prompt_embeds.md"}
{"id": "1c5c25041eb57257570caafecb0d39281368ef140e90e8ffa1100f0cf1600d34", "heading": "Quantization", "level": 1, "text": "# Quantization  \nQuantization trades off model precision for smaller memory footprint, allowing large models to be run on a wider range of devices.  \nContents:  \n- [AutoAWQ](auto_awq.md)\n- [AutoRound](auto_round.md)\n- [BitsAndBytes](bnb.md)\n- [BitBLAS](bitblas.md)\n- [GGUF](gguf.md)\n- [GPTQModel](gptqmodel.md)\n- [INC](inc.md)\n- [INT4 W4A16](int4.md)\n- [INT8 W8A8](int8.md)\n- [FP8 W8A8](fp8.md)\n- [NVIDIA TensorRT Model Optimizer](modelopt.md)\n- [AMD Quark](quark.md)\n- [Quantized KV Cache](quantized_kvcache.md)\n- [TorchAO](torchao.md)", "file_path": "features/quantization/README.md"}
{"id": "1c5c25041eb57257570caafecb0d39281368ef140e90e8ffa1100f0cf1600d34", "heading": "Quantization/Supported Hardware", "level": 2, "text": "## Supported Hardware  \nThe table below shows the compatibility of various quantization implementations with different hardware platforms in vLLM:  \n<style>\ntd:not(:first-child) {\ntext-align: center !important;\n}\ntd {\npadding: 0.5rem !important;\nwhite-space: nowrap;\n}  \nth {\npadding: 0.5rem !important;\nmin-width: 0 !important;\n}  \nth:not(:first-child) {\nwriting-mode: vertical-lr;\ntransform: rotate(180deg)\n}\n</style>  \n| Implementation        | Volta   | Turing   | Ampere   | Ada   | Hopper   | AMD GPU   | Intel GPU   | Intel Gaudi | x86 CPU   | Google TPU   |\n|-----------------------|---------|----------|----------|-------|----------|-----------|-------------|-------------|-----------|--------------|\n| AWQ                   | âŒ      | âœ…ï¸Ž       | âœ…ï¸Ž       | âœ…ï¸Ž    | âœ…ï¸Ž       | âŒ         | âœ…ï¸Ž          | âŒ         | âœ…ï¸Ž        | âŒ           |\n| GPTQ                  | âœ…ï¸Ž      | âœ…ï¸Ž       | âœ…ï¸Ž       | âœ…ï¸Ž    | âœ…ï¸Ž       | âŒ         | âœ…ï¸Ž          | âŒ         | âœ…ï¸Ž        | âŒ           |", "file_path": "features/quantization/README.md"}
{"id": "1c5c25041eb57257570caafecb0d39281368ef140e90e8ffa1100f0cf1600d34", "heading": "Quantization/Supported Hardware", "level": 2, "text": "| Marlin (GPTQ/AWQ/FP8) | âŒ      | âŒ       | âœ…ï¸Ž       | âœ…ï¸Ž    | âœ…ï¸Ž       | âŒ         | âŒ          | âŒ         | âŒ        | âŒ           |\n| INT8 (W8A8)           | âŒ      | âœ…ï¸Ž       | âœ…ï¸Ž       | âœ…ï¸Ž    | âœ…ï¸Ž       | âŒ         | âŒ          | âŒ         | âœ…ï¸Ž        | âœ…ï¸Ž           |\n| FP8 (W8A8)            | âŒ      | âŒ       | âŒ       | âœ…ï¸Ž    | âœ…ï¸Ž       | âœ…ï¸Ž         | âŒ          | âŒ         | âŒ        | âŒ           |\n| BitBLAS               | âœ…ï¸Ž      | âœ…       | âœ…ï¸Ž       | âœ…ï¸Ž    | âœ…ï¸Ž       | âŒ         | âŒ          | âŒ         | âŒ        | âŒ           |\n| BitBLAS (GPTQ)        | âŒ      | âŒ       | âœ…ï¸Ž       | âœ…ï¸Ž    | âœ…ï¸Ž       | âŒ         | âŒ          | âŒ         | âŒ        | âŒ           |\n| bitsandbytes          | âœ…ï¸Ž      | âœ…ï¸Ž       | âœ…ï¸Ž       | âœ…ï¸Ž    | âœ…ï¸Ž       | âŒ         | âŒ          | âŒ         | âŒ        | âŒ           |\n| DeepSpeedFP           | âœ…ï¸Ž      | âœ…ï¸Ž       | âœ…ï¸Ž       | âœ…ï¸Ž    | âœ…ï¸Ž       | âŒ         | âŒ          | âŒ         | âŒ        | âŒ           |", "file_path": "features/quantization/README.md"}
{"id": "1c5c25041eb57257570caafecb0d39281368ef140e90e8ffa1100f0cf1600d34", "heading": "Quantization/Supported Hardware", "level": 2, "text": "| GGUF                  | âœ…ï¸Ž      | âœ…ï¸Ž       | âœ…ï¸Ž       | âœ…ï¸Ž    | âœ…ï¸Ž       | âœ…ï¸Ž         | âŒ          | âŒ         | âŒ        | âŒ           |\n| INC (W8A8)            | âŒ      | âŒ       | âŒ       | âŒ    | âŒ       | âŒ         | âŒ          | âœ…ï¸Ž         | âŒ        | âŒ           |  \n- Volta refers to SM 7.0, Turing to SM 7.5, Ampere to SM 8.0/8.6, Ada to SM 8.9, and Hopper to SM 9.0.\n- âœ…ï¸Ž indicates that the quantization method is supported on the specified hardware.\n- âŒ indicates that the quantization method is not supported on the specified hardware.  \n!!! note\nThis compatibility chart is subject to change as vLLM continues to evolve and expand its support for different hardware platforms and quantization methods.  \nFor the most up-to-date information on hardware support and quantization methods, please refer to <gh-dir:vllm/model_executor/layers/quantization> or consult with the vLLM development team.", "file_path": "features/quantization/README.md"}
{"id": "1c4445f21ac6f2d960baf420a7ab84bf407e170640d8230c802f958b0c76827e", "heading": "AutoAWQ", "level": 1, "text": "# AutoAWQ  \n> âš ï¸ **Warning:**\nThe `AutoAWQ` library is deprecated. This functionality has been adopted by the vLLM project in [`llm-compressor`](https://github.com/vllm-project/llm-compressor/tree/main/examples/awq).\nFor the recommended quantization workflow, please see the AWQ examples in [`llm-compressor`](https://github.com/vllm-project/llm-compressor/tree/main/examples/awq). For more details on the deprecation, refer to the original [AutoAWQ repository](https://github.com/casper-hansen/AutoAWQ).  \nTo create a new 4-bit quantized model, you can leverage [AutoAWQ](https://github.com/casper-hansen/AutoAWQ).\nQuantization reduces the model's precision from BF16/FP16 to INT4 which effectively reduces the total model memory footprint.\nThe main benefits are lower latency and memory usage.  \nYou can quantize your own models by installing AutoAWQ or picking one of the [6500+ models on Huggingface](https://huggingface.co/models?search=awq).  \n```bash\npip install autoawq\n```", "file_path": "features/quantization/auto_awq.md"}
{"id": "1c4445f21ac6f2d960baf420a7ab84bf407e170640d8230c802f958b0c76827e", "heading": "AutoAWQ", "level": 1, "text": "```bash\npip install autoawq\n```  \nAfter installing AutoAWQ, you are ready to quantize a model. Please refer to the [AutoAWQ documentation](https://casper-hansen.github.io/AutoAWQ/examples/#basic-quantization) for further details. Here is an example of how to quantize `mistralai/Mistral-7B-Instruct-v0.2`:  \n??? code  \n```python\nfrom awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer", "file_path": "features/quantization/auto_awq.md"}
{"id": "1c4445f21ac6f2d960baf420a7ab84bf407e170640d8230c802f958b0c76827e", "heading": "AutoAWQ", "level": 1, "text": "model_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\nquant_path = \"mistral-instruct-v0.2-awq\"\nquant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\"}\n\n# Load model\nmodel = AutoAWQForCausalLM.from_pretrained(\nmodel_path,\nlow_cpu_mem_usage=True,\nuse_cache=False,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n# Quantize\nmodel.quantize(tokenizer, quant_config=quant_config)\n\n# Save quantized model\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)\n\nprint(f'Model is quantized and saved at \"{quant_path}\"')\n```  \nTo run an AWQ model with vLLM, you can use [TheBloke/Llama-2-7b-Chat-AWQ](https://huggingface.co/TheBloke/Llama-2-7b-Chat-AWQ) with the following command:  \n```bash\npython examples/offline_inference/llm_engine_example.py \\\n--model TheBloke/Llama-2-7b-Chat-AWQ \\\n--quantization awq\n```  \nAWQ models are also supported directly through the LLM entrypoint:  \n??? code  \n```python\nfrom vllm import LLM, SamplingParams", "file_path": "features/quantization/auto_awq.md"}
{"id": "1c4445f21ac6f2d960baf420a7ab84bf407e170640d8230c802f958b0c76827e", "heading": "AutoAWQ", "level": 1, "text": "# Sample prompts.\nprompts = [\n\"Hello, my name is\",\n\"The president of the United States is\",\n\"The capital of France is\",\n\"The future of AI is\",\n]\n# Create a sampling params object.\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\n# Create an LLM.\nllm = LLM(model=\"TheBloke/Llama-2-7b-Chat-AWQ\", quantization=\"AWQ\")\n# Generate texts from the prompts. The output is a list of RequestOutput objects\n# that contain the prompt, generated text, and other information.\noutputs = llm.generate(prompts, sampling_params)\n# Print the outputs.\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```", "file_path": "features/quantization/auto_awq.md"}
{"id": "e6a3daacde6bdeb35a4c8af080049fcaaad819479977cd49514ded44cc8a8607", "heading": "AutoRound", "level": 1, "text": "# AutoRound  \n[AutoRound](https://github.com/intel/auto-round) is Intelâ€™s advanced quantization algorithm designed to produce highly efficient **INT2, INT3, INT4, and INT8**\nquantized large language modelsâ€”striking an optimal balance between accuracy and deployment performance.  \nAutoRound applies weight-only quantization to transformer-based models, enabling significant memory savings and faster\ninference while maintaining near-original accuracy. It supports a wide range of hardware platforms, including **CPUs,\nIntel GPUs, HPUs, and CUDA-enabled devices**.  \nPlease refer to the [AutoRound guide](https://github.com/intel/auto-round/blob/main/docs/step_by_step.md) for more details.  \nKey Features:  \nâœ… **AutoRound, AutoAWQ, AutoGPTQ, and GGUF** are supported  \nâœ… **10+ vision-language models (VLMs)** are supported  \nâœ… **Per-layer mixed-bit quantization** for fine-grained control  \nâœ… **RTN (Round-To-Nearest) mode** for quick quantization with slight accuracy loss", "file_path": "features/quantization/auto_round.md"}
{"id": "e6a3daacde6bdeb35a4c8af080049fcaaad819479977cd49514ded44cc8a8607", "heading": "AutoRound", "level": 1, "text": "âœ… **Multiple quantization recipes**: best, base, and light  \nâœ… Advanced utilities such as immediate packing and support for **10+ backends**", "file_path": "features/quantization/auto_round.md"}
{"id": "e6a3daacde6bdeb35a4c8af080049fcaaad819479977cd49514ded44cc8a8607", "heading": "AutoRound/Installation", "level": 2, "text": "## Installation  \n```bash\nuv pip install auto-round\n```", "file_path": "features/quantization/auto_round.md"}
{"id": "e6a3daacde6bdeb35a4c8af080049fcaaad819479977cd49514ded44cc8a8607", "heading": "AutoRound/Quantizing a model", "level": 2, "text": "## Quantizing a model  \nFor VLMs, please change to `auto-round-mllm` in CLI usage and `AutoRoundMLLM` in API usage.", "file_path": "features/quantization/auto_round.md"}
{"id": "e6a3daacde6bdeb35a4c8af080049fcaaad819479977cd49514ded44cc8a8607", "heading": "AutoRound/Quantizing a model/CLI usage", "level": 3, "text": "### CLI usage  \n```bash\nauto-round \\\n--model Qwen/Qwen3-0.6B \\\n--bits 4 \\\n--group_size 128 \\\n--format \"auto_round\" \\\n--output_dir ./tmp_autoround\n```  \n```bash\nauto-round \\\n--model Qwen/Qwen3-0.6B \\\n--format \"gguf:q4_k_m\" \\\n--output_dir ./tmp_autoround\n```", "file_path": "features/quantization/auto_round.md"}
{"id": "e6a3daacde6bdeb35a4c8af080049fcaaad819479977cd49514ded44cc8a8607", "heading": "AutoRound/Quantizing a model/API usage", "level": 3, "text": "### API usage  \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom auto_round import AutoRound\n\nmodel_name = \"Qwen/Qwen3-0.6B\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nbits, group_size, sym = 4, 128, True\nautoround = AutoRound(model, tokenizer, bits=bits, group_size=group_size, sym=sym)\n\n# the best accuracy, 4-5X slower, low_gpu_mem_usage could save ~20G but ~30% slower\n# autoround = AutoRound(model, tokenizer, nsamples=512, iters=1000, low_gpu_mem_usage=True, bits=bits, group_size=group_size, sym=sym)\n\n# 2-3X speedup, slight accuracy drop at W4G128\n# autoround = AutoRound(model, tokenizer, nsamples=128, iters=50, lr=5e-3, bits=bits, group_size=group_size, sym=sym )\n\noutput_dir = \"./tmp_autoround\"\n# format= 'auto_round'(default), 'auto_gptq', 'auto_awq'\nautoround.quantize_and_save(output_dir, format=\"auto_round\")\n```", "file_path": "features/quantization/auto_round.md"}
{"id": "e6a3daacde6bdeb35a4c8af080049fcaaad819479977cd49514ded44cc8a8607", "heading": "AutoRound/Running a quantized model with vLLM", "level": 2, "text": "## Running a quantized model with vLLM  \nHere is some example code to run auto-round format in vLLM:  \n```python\nfrom vllm import LLM, SamplingParams\n\nprompts = [\n\"Hello, my name is\",\n]\nsampling_params = SamplingParams(temperature=0.6, top_p=0.95)\nmodel_name = \"Intel/DeepSeek-R1-0528-Qwen3-8B-int4-AutoRound\"\nllm = LLM(model=model_name)\n\noutputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```", "file_path": "features/quantization/auto_round.md"}
{"id": "e6a3daacde6bdeb35a4c8af080049fcaaad819479977cd49514ded44cc8a8607", "heading": "AutoRound/Acknowledgement", "level": 2, "text": "## Acknowledgement  \nSpecial thanks to open-source low precision libraries such as AutoGPTQ, AutoAWQ, GPTQModel, Triton, Marlin, and\nExLLaMAV2 for providing low-precision CUDA kernels, which are leveraged in AutoRound.", "file_path": "features/quantization/auto_round.md"}
{"id": "5365b8768ef998388cbc640eba3de962e012356551e19d9c654722b2065fdbbf", "heading": "BitBLAS", "level": 1, "text": "# BitBLAS  \nvLLM now supports [BitBLAS](https://github.com/microsoft/BitBLAS) for more efficient and flexible model inference. Compared to other quantization frameworks, BitBLAS provides more precision combinations.  \n!!! note\nEnsure your hardware supports the selected `dtype` (`torch.bfloat16` or `torch.float16`).\nMost recent NVIDIA GPUs support `float16`, while `bfloat16` is more common on newer architectures like Ampere or Hopper.\nFor details see [supported hardware](README.md#supported-hardware).  \nBelow are the steps to utilize BitBLAS with vLLM.  \n```bash\npip install bitblas>=0.1.0\n```  \nvLLM reads the model's config file and supports pre-quantized checkpoints.  \nYou can find pre-quantized models on:  \n- [Hugging Face (BitBLAS)](https://huggingface.co/models?search=bitblas)\n- [Hugging Face (GPTQ)](https://huggingface.co/models?search=gptq)  \nUsually, these repositories have a `quantize_config.json` file that includes a `quantization_config` section.", "file_path": "features/quantization/bitblas.md"}
{"id": "5365b8768ef998388cbc640eba3de962e012356551e19d9c654722b2065fdbbf", "heading": "BitBLAS/Read bitblas format checkpoint", "level": 2, "text": "## Read bitblas format checkpoint  \n```python\nfrom vllm import LLM\nimport torch\n\n# \"hxbgsyxh/llama-13b-4bit-g-1-bitblas\" is a pre-quantized checkpoint.\nmodel_id = \"hxbgsyxh/llama-13b-4bit-g-1-bitblas\"\nllm = LLM(\nmodel=model_id,\ndtype=torch.bfloat16,\ntrust_remote_code=True,\nquantization=\"bitblas\",\n)\n```", "file_path": "features/quantization/bitblas.md"}
{"id": "5365b8768ef998388cbc640eba3de962e012356551e19d9c654722b2065fdbbf", "heading": "BitBLAS/Read gptq format checkpoint", "level": 2, "text": "## Read gptq format checkpoint  \n??? code  \n```python\nfrom vllm import LLM\nimport torch\n\n# \"hxbgsyxh/llama-13b-4bit-g-1\" is a pre-quantized checkpoint.\nmodel_id = \"hxbgsyxh/llama-13b-4bit-g-1\"\nllm = LLM(\nmodel=model_id,\ndtype=torch.float16,\ntrust_remote_code=True,\nquantization=\"bitblas\",\nmax_model_len=1024,\n)\n```", "file_path": "features/quantization/bitblas.md"}
{"id": "8dbc134c974ff0f8f93ddec6e8d031d6141013d836cd99e70a78a7d084c5763c", "heading": "BitsAndBytes", "level": 1, "text": "# BitsAndBytes  \nvLLM now supports [BitsAndBytes](https://github.com/TimDettmers/bitsandbytes) for more efficient model inference.\nBitsAndBytes quantizes models to reduce memory usage and enhance performance without significantly sacrificing accuracy.\nCompared to other quantization methods, BitsAndBytes eliminates the need for calibrating the quantized model with input data.  \nBelow are the steps to utilize BitsAndBytes with vLLM.  \n```bash\npip install bitsandbytes>=0.46.1\n```  \nvLLM reads the model's config file and supports both in-flight quantization and pre-quantized checkpoint.  \nYou can find bitsandbytes quantized models on [Hugging Face](https://huggingface.co/models?search=bitsandbytes).\nAnd usually, these repositories have a config.json file that includes a quantization_config section.", "file_path": "features/quantization/bnb.md"}
{"id": "8dbc134c974ff0f8f93ddec6e8d031d6141013d836cd99e70a78a7d084c5763c", "heading": "BitsAndBytes/Read quantized checkpoint", "level": 2, "text": "## Read quantized checkpoint  \nFor pre-quantized checkpoints, vLLM will try to infer the quantization method from the config file, so you don't need to explicitly specify the quantization argument.  \n```python\nfrom vllm import LLM\nimport torch\n# unsloth/tinyllama-bnb-4bit is a pre-quantized checkpoint.\nmodel_id = \"unsloth/tinyllama-bnb-4bit\"\nllm = LLM(\nmodel=model_id,\ndtype=torch.bfloat16,\ntrust_remote_code=True,\n)\n```", "file_path": "features/quantization/bnb.md"}
{"id": "8dbc134c974ff0f8f93ddec6e8d031d6141013d836cd99e70a78a7d084c5763c", "heading": "BitsAndBytes/Inflight quantization: load as 4bit quantization", "level": 2, "text": "## Inflight quantization: load as 4bit quantization  \nFor inflight 4bit quantization with BitsAndBytes, you need to explicitly specify the quantization argument.  \n```python\nfrom vllm import LLM\nimport torch\nmodel_id = \"huggyllama/llama-7b\"\nllm = LLM(\nmodel=model_id,\ndtype=torch.bfloat16,\ntrust_remote_code=True,\nquantization=\"bitsandbytes\",\n)\n```", "file_path": "features/quantization/bnb.md"}
{"id": "8dbc134c974ff0f8f93ddec6e8d031d6141013d836cd99e70a78a7d084c5763c", "heading": "BitsAndBytes/OpenAI Compatible Server", "level": 2, "text": "## OpenAI Compatible Server  \nAppend the following to your model arguments for 4bit inflight quantization:  \n```bash\n--quantization bitsandbytes\n```", "file_path": "features/quantization/bnb.md"}
{"id": "963c034e3ec25adb7cc83555bb5839cf5d85410789d981340dbe4e9bece8f316", "heading": "FP8 W8A8", "level": 1, "text": "# FP8 W8A8  \nvLLM supports FP8 (8-bit floating point) weight and activation quantization using hardware acceleration on GPUs such as Nvidia H100 and AMD MI300x.\nCurrently, only Hopper and Ada Lovelace GPUs are officially supported for W8A8.\nAmpere GPUs are supported for W8A16 (weight-only FP8) utilizing Marlin kernels.\nQuantization of models with FP8 allows for a 2x reduction in model memory requirements and up to a 1.6x improvement in throughput with minimal impact on accuracy.  \nPlease visit the HF collection of [quantized FP8 checkpoints of popular LLMs ready to use with vLLM](https://huggingface.co/collections/neuralmagic/fp8-llms-for-vllm-666742ed2b78b7ac8df13127).  \nThe FP8 types typically supported in hardware have two distinct representations, each useful in different scenarios:  \n- **E4M3**: Consists of 1 sign bit, 4 exponent bits, and 3 bits of mantissa. It can store values up to +/-448 and `nan`.", "file_path": "features/quantization/fp8.md"}
{"id": "963c034e3ec25adb7cc83555bb5839cf5d85410789d981340dbe4e9bece8f316", "heading": "FP8 W8A8", "level": 1, "text": "- **E5M2**: Consists of 1 sign bit, 5 exponent bits, and 2 bits of mantissa. It can store values up to +/-57344, +/- `inf`, and `nan`. The tradeoff for the increased dynamic range is lower precision of the stored values.  \n!!! note\nFP8 computation is supported on NVIDIA GPUs with compute capability > 8.9 (Ada Lovelace, Hopper).\nFP8 models will run on compute capability > 8.0 (Ampere) as weight-only W8A16, utilizing FP8 Marlin.", "file_path": "features/quantization/fp8.md"}
{"id": "963c034e3ec25adb7cc83555bb5839cf5d85410789d981340dbe4e9bece8f316", "heading": "FP8 W8A8/Installation", "level": 2, "text": "## Installation  \nTo produce performant FP8 quantized models with vLLM, you'll need to install the [llm-compressor](https://github.com/vllm-project/llm-compressor/) library:  \n```bash\npip install llmcompressor\n```", "file_path": "features/quantization/fp8.md"}
{"id": "963c034e3ec25adb7cc83555bb5839cf5d85410789d981340dbe4e9bece8f316", "heading": "FP8 W8A8/Quantization Process", "level": 2, "text": "## Quantization Process  \nThe quantization process involves three main steps:  \n1. Loading the model\n2. Applying quantization\n3. Evaluating accuracy in vLLM", "file_path": "features/quantization/fp8.md"}
{"id": "963c034e3ec25adb7cc83555bb5839cf5d85410789d981340dbe4e9bece8f316", "heading": "FP8 W8A8/Quantization Process/1. Loading the Model", "level": 3, "text": "### 1. Loading the Model  \nLoad your model and tokenizer using the standard `transformers` AutoModel classes:  \n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\nMODEL_ID,\ndevice_map=\"auto\",\ndtype=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n```", "file_path": "features/quantization/fp8.md"}
{"id": "963c034e3ec25adb7cc83555bb5839cf5d85410789d981340dbe4e9bece8f316", "heading": "FP8 W8A8/Quantization Process/2. Applying Quantization", "level": 3, "text": "### 2. Applying Quantization  \nFor FP8 quantization, we can recover accuracy with simple RTN quantization. We recommend targeting all `Linear` layers using the `FP8_DYNAMIC` scheme, which uses:  \n- Static, per-channel quantization on the weights\n- Dynamic, per-token quantization on the activations  \nSince simple RTN does not require data for weight quantization and the activations are quantized dynamically, we do not need any calibration data for this quantization flow.  \n??? code  \n```python\nfrom llmcompressor.transformers import oneshot\nfrom llmcompressor.modifiers.quantization import QuantizationModifier\n\n# Configure the simple PTQ quantization\nrecipe = QuantizationModifier(\ntargets=\"Linear\",\nscheme=\"FP8_DYNAMIC\",\nignore=[\"lm_head\"],\n)\n\n# Apply the quantization algorithm.\noneshot(model=model, recipe=recipe)\n\n# Save the model: Meta-Llama-3-8B-Instruct-FP8-Dynamic\nSAVE_DIR = MODEL_ID.split(\"/\")[1] + \"-FP8-Dynamic\"\nmodel.save_pretrained(SAVE_DIR)\ntokenizer.save_pretrained(SAVE_DIR)\n```", "file_path": "features/quantization/fp8.md"}
{"id": "963c034e3ec25adb7cc83555bb5839cf5d85410789d981340dbe4e9bece8f316", "heading": "FP8 W8A8/Quantization Process/3. Evaluating Accuracy", "level": 3, "text": "### 3. Evaluating Accuracy  \nInstall `vllm` and `lm-evaluation-harness` for evaluation:  \n```bash\npip install vllm git+https://github.com/EleutherAI/lm-evaluation-harness.git@206b7722158f58c35b7ffcd53b035fdbdda5126d#egg=lm-eval[api]\n```  \nLoad and run the model in `vllm`:  \n```python\nfrom vllm import LLM", "file_path": "features/quantization/fp8.md"}
{"id": "963c034e3ec25adb7cc83555bb5839cf5d85410789d981340dbe4e9bece8f316", "heading": "FP8 W8A8/Quantization Process/3. Evaluating Accuracy", "level": 3, "text": "llm = LLM(\"./Meta-Llama-3-8B-Instruct-FP8-Dynamic\")\nresult = llm.generate(\"Hello my name is\")\nprint(result[0].outputs[0].text)\n```  \nEvaluate accuracy with `lm_eval` (for example on 250 samples of `gsm8k`):  \n!!! note\nQuantized models can be sensitive to the presence of the `bos` token. `lm_eval` does not add a `bos` token by default, so make sure to include the `add_bos_token=True` argument when running your evaluations.  \n```bash\nMODEL=$PWD/Meta-Llama-3-8B-Instruct-FP8-Dynamic\nlm_eval \\\n--model vllm \\\n--model_args pretrained=$MODEL,add_bos_token=True \\\n--tasks gsm8k  --num_fewshot 5 --batch_size auto --limit 250\n```  \nHere's an example of the resulting scores:  \n```text\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|â†‘  |0.768|Â±  |0.0268|\n|     |       |strict-match    |     5|exact_match|â†‘  |0.768|Â±  |0.0268|\n```", "file_path": "features/quantization/fp8.md"}
{"id": "963c034e3ec25adb7cc83555bb5839cf5d85410789d981340dbe4e9bece8f316", "heading": "FP8 W8A8/Troubleshooting and Support", "level": 2, "text": "## Troubleshooting and Support  \nIf you encounter any issues or have feature requests, please open an issue on the [vllm-project/llm-compressor](https://github.com/vllm-project/llm-compressor/issues) GitHub repository.", "file_path": "features/quantization/fp8.md"}
{"id": "963c034e3ec25adb7cc83555bb5839cf5d85410789d981340dbe4e9bece8f316", "heading": "FP8 W8A8/Online Dynamic Quantization", "level": 2, "text": "## Online Dynamic Quantization  \nDynamic quantization of an original precision BF16/FP16 model to FP8 can be achieved with vLLM without any calibration data required. You can enable the feature by specifying `--quantization=\"fp8\"` in the command line or setting `quantization=\"fp8\"` in the LLM constructor.  \nIn this mode, all Linear modules (except for the final `lm_head`) have their weights quantized down to FP8_E4M3 precision with a per-tensor scale. Activations have their minimum and maximum values calculated during each forward pass to provide a dynamic per-tensor scale for high accuracy. As a result, latency improvements are limited in this mode.  \n```python\nfrom vllm import LLM", "file_path": "features/quantization/fp8.md"}
{"id": "963c034e3ec25adb7cc83555bb5839cf5d85410789d981340dbe4e9bece8f316", "heading": "FP8 W8A8/Online Dynamic Quantization", "level": 2, "text": "llm = LLM(\"facebook/opt-125m\", quantization=\"fp8\")\n# INFO 06-10 17:55:42 model_runner.py:157] Loading model weights took 0.1550 GB\nresult = llm.generate(\"Hello, my name is\")\nprint(result[0].outputs[0].text)\n```  \n!!! warning\nCurrently, we load the model at original precision before quantizing down to 8-bits, so you need enough memory to load the whole model.", "file_path": "features/quantization/fp8.md"}
{"id": "9f818527b16a1ff4fe3a3db9ea34b1367829e8e900b92afb0ca5508d9dfd3342", "heading": "GGUF", "level": 1, "text": "# GGUF  \n!!! warning\nPlease note that GGUF support in vLLM is highly experimental and under-optimized at the moment, it might be incompatible with other features. Currently, you can use GGUF as a way to reduce memory footprint. If you encounter any issues, please report them to the vLLM team.  \n!!! warning\nCurrently, vllm only supports loading single-file GGUF models. If you have a multi-files GGUF model, you can use [gguf-split](https://github.com/ggerganov/llama.cpp/pull/6135) tool to merge them to a single-file model.  \nTo run a GGUF model with vLLM, you can download and use the local GGUF model from [TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF) with the following command:  \n```bash\nwget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n# We recommend using the tokenizer from base model to avoid long-time and buggy tokenizer conversion.\nvllm serve ./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \\", "file_path": "features/quantization/gguf.md"}
{"id": "9f818527b16a1ff4fe3a3db9ea34b1367829e8e900b92afb0ca5508d9dfd3342", "heading": "GGUF", "level": 1, "text": "vllm serve ./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \\\n--tokenizer TinyLlama/TinyLlama-1.1B-Chat-v1.0\n```  \nYou can also add `--tensor-parallel-size 2` to enable tensor parallelism inference with 2 GPUs:  \n```bash\n# We recommend using the tokenizer from base model to avoid long-time and buggy tokenizer conversion.\nvllm serve ./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \\\n--tokenizer TinyLlama/TinyLlama-1.1B-Chat-v1.0 \\\n--tensor-parallel-size 2\n```  \n!!! warning\nWe recommend using the tokenizer from base model instead of GGUF model. Because the tokenizer conversion from GGUF is time-consuming and unstable, especially for some models with large vocab size.  \nGGUF assumes that huggingface can convert the metadata to a config file. In case huggingface doesn't support your model you can manually create a config and pass it as hf-config-path  \n```bash\n# If you model is not supported by huggingface you can manually provide a huggingface compatible config path\nvllm serve ./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \\", "file_path": "features/quantization/gguf.md"}
{"id": "9f818527b16a1ff4fe3a3db9ea34b1367829e8e900b92afb0ca5508d9dfd3342", "heading": "GGUF", "level": 1, "text": "vllm serve ./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \\\n--tokenizer TinyLlama/TinyLlama-1.1B-Chat-v1.0 \\\n--hf-config-path Tinyllama/TInyLlama-1.1B-Chat-v1.0\n```  \nYou can also use the GGUF model directly through the LLM entrypoint:  \n??? code  \n```python\nfrom vllm import LLM, SamplingParams", "file_path": "features/quantization/gguf.md"}
{"id": "9f818527b16a1ff4fe3a3db9ea34b1367829e8e900b92afb0ca5508d9dfd3342", "heading": "GGUF", "level": 1, "text": "# In this script, we demonstrate how to pass input to the chat method:\nconversation = [\n{\n\"role\": \"system\",\n\"content\": \"You are a helpful assistant\",\n},\n{\n\"role\": \"user\",\n\"content\": \"Hello\",\n},\n{\n\"role\": \"assistant\",\n\"content\": \"Hello! How can I assist you today?\",\n},\n{\n\"role\": \"user\",\n\"content\": \"Write an essay about the importance of higher education.\",\n},\n]\n\n# Create a sampling params object.\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\n# Create an LLM.\nllm = LLM(\nmodel=\"./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\",\ntokenizer=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n)\n# Generate texts from the prompts. The output is a list of RequestOutput objects\n# that contain the prompt, generated text, and other information.\noutputs = llm.chat(conversation, sampling_params)\n\n# Print the outputs.\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```", "file_path": "features/quantization/gguf.md"}
{"id": "a4ff03e8071387fc94e4838379c0bc5659906e14dfe422d3bf528a9e71371d61", "heading": "GPTQModel", "level": 1, "text": "# GPTQModel  \nTo create a new 4-bit or 8-bit GPTQ quantized model, you can leverage [GPTQModel](https://github.com/ModelCloud/GPTQModel) from ModelCloud.AI.  \nQuantization reduces the model's precision from BF16/FP16 (16-bits) to INT4 (4-bits) or INT8 (8-bits) which significantly reduces the\ntotal model memory footprint while at-the-same-time increasing inference performance.  \nCompatible GPTQModel quantized models can leverage the `Marlin` and `Machete` vLLM custom kernels to maximize batching\ntransactions-per-second `tps` and token-latency performance for both Ampere (A100+) and Hopper (H100+) Nvidia GPUs.\nThese two kernels are highly optimized by vLLM and NeuralMagic (now part of Redhat) to allow world-class inference performance of quantized GPTQ\nmodels.  \nGPTQModel is one of the few quantization toolkits in the world that allows `Dynamic` per-module quantization where different layers and/or modules within a llm model can be further optimized with custom quantization parameters. `Dynamic` quantization", "file_path": "features/quantization/gptqmodel.md"}
{"id": "a4ff03e8071387fc94e4838379c0bc5659906e14dfe422d3bf528a9e71371d61", "heading": "GPTQModel", "level": 1, "text": "is fully integrated into vLLM and backed up by support from the ModelCloud.AI team. Please refer to [GPTQModel readme](https://github.com/ModelCloud/GPTQModel?tab=readme-ov-file#dynamic-quantization-per-module-quantizeconfig-override)\nfor more details on this and other advanced features.", "file_path": "features/quantization/gptqmodel.md"}
{"id": "a4ff03e8071387fc94e4838379c0bc5659906e14dfe422d3bf528a9e71371d61", "heading": "GPTQModel/Installation", "level": 2, "text": "## Installation  \nYou can quantize your own models by installing [GPTQModel](https://github.com/ModelCloud/GPTQModel) or picking one of the [5000+ models on Huggingface](https://huggingface.co/models?search=gptq).  \n```bash\npip install -U gptqmodel --no-build-isolation -v\n```", "file_path": "features/quantization/gptqmodel.md"}
{"id": "a4ff03e8071387fc94e4838379c0bc5659906e14dfe422d3bf528a9e71371d61", "heading": "GPTQModel/Quantizing a model", "level": 2, "text": "## Quantizing a model  \nAfter installing GPTQModel, you are ready to quantize a model. Please refer to the [GPTQModel readme](https://github.com/ModelCloud/GPTQModel/?tab=readme-ov-file#quantization) for further details.  \nHere is an example of how to quantize `meta-llama/Llama-3.2-1B-Instruct`:  \n??? code  \n```python\nfrom datasets import load_dataset\nfrom gptqmodel import GPTQModel, QuantizeConfig\n\nmodel_id = \"meta-llama/Llama-3.2-1B-Instruct\"\nquant_path = \"Llama-3.2-1B-Instruct-gptqmodel-4bit\"\n\ncalibration_dataset = load_dataset(\n\"allenai/c4\",\ndata_files=\"en/c4-train.00001-of-01024.json.gz\",\nsplit=\"train\",\n).select(range(1024))[\"text\"]\n\nquant_config = QuantizeConfig(bits=4, group_size=128)\n\nmodel = GPTQModel.load(model_id, quant_config)\n\n# increase `batch_size` to match gpu/vram specs to speed up quantization\nmodel.quantize(calibration_dataset, batch_size=2)\n\nmodel.save(quant_path)\n```", "file_path": "features/quantization/gptqmodel.md"}
{"id": "a4ff03e8071387fc94e4838379c0bc5659906e14dfe422d3bf528a9e71371d61", "heading": "GPTQModel/Running a quantized model with vLLM", "level": 2, "text": "## Running a quantized model with vLLM  \nTo run an GPTQModel quantized model with vLLM, you can use [DeepSeek-R1-Distill-Qwen-7B-gptqmodel-4bit-vortex-v2](https://huggingface.co/ModelCloud/DeepSeek-R1-Distill-Qwen-7B-gptqmodel-4bit-vortex-v2) with the following command:  \n```bash\npython examples/offline_inference/llm_engine_example.py \\\n--model ModelCloud/DeepSeek-R1-Distill-Qwen-7B-gptqmodel-4bit-vortex-v2\n```", "file_path": "features/quantization/gptqmodel.md"}
{"id": "a4ff03e8071387fc94e4838379c0bc5659906e14dfe422d3bf528a9e71371d61", "heading": "GPTQModel/Using GPTQModel with vLLM's Python API", "level": 2, "text": "## Using GPTQModel with vLLM's Python API  \nGPTQModel quantized models are also supported directly through the LLM entrypoint:  \n??? code  \n```python\nfrom vllm import LLM, SamplingParams\n\n# Sample prompts.\nprompts = [\n\"Hello, my name is\",\n\"The president of the United States is\",\n\"The capital of France is\",\n\"The future of AI is\",\n]\n\n# Create a sampling params object.\nsampling_params = SamplingParams(temperature=0.6, top_p=0.9)\n\n# Create an LLM.\nllm = LLM(model=\"ModelCloud/DeepSeek-R1-Distill-Qwen-7B-gptqmodel-4bit-vortex-v2\")\n\n# Generate texts from the prompts. The output is a list of RequestOutput objects\n# that contain the prompt, generated text, and other information.\noutputs = llm.generate(prompts, sampling_params)\n\n# Print the outputs.\nprint(\"-\"*50)\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}\\nGenerated text: {generated_text!r}\")\nprint(\"-\"*50)\n```", "file_path": "features/quantization/gptqmodel.md"}
{"id": "7318a9b30ef9934550866dde1cf17eca15aadc6e2395c685c5b06338402a95c4", "heading": "FP8 INC", "level": 1, "text": "# FP8 INC  \nvLLM supports FP8 (8-bit floating point) weight and activation quantization using IntelÂ® Neural Compressor (INC) on IntelÂ® GaudiÂ® 2 and IntelÂ® GaudiÂ® 3 AI accelerators.\nCurrently, quantization is validated only in Llama models.  \nIntel Gaudi supports quantization of various modules and functions, including, but not limited to `Linear`, `KVCache`, `Matmul` and `Softmax`. For more information, please refer to:\n[Supported Modules\\\\Supported Functions\\\\Custom Patched Modules](https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/Quantization/Inference_Using_FP8.html#supported-modules).  \n!!! note\nMeasurement files are required to run quantized models with vLLM on Gaudi accelerators. The FP8 model calibration procedure is described in the [vLLM HPU extension](https://github.com/HabanaAI/vllm-hpu-extension/tree/main/calibration/README.md) package.  \n!!! note", "file_path": "features/quantization/inc.md"}
{"id": "7318a9b30ef9934550866dde1cf17eca15aadc6e2395c685c5b06338402a95c4", "heading": "FP8 INC", "level": 1, "text": "!!! note\n`QUANT_CONFIG` is an environment variable that points to the measurement or quantization [JSON config file](https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/Quantization/Inference_Using_FP8.html#supported-json-config-file-options).\nThe measurement configuration file is used during the calibration procedure to collect measurements for a given model. The quantization configuration is used during inference.", "file_path": "features/quantization/inc.md"}
{"id": "7318a9b30ef9934550866dde1cf17eca15aadc6e2395c685c5b06338402a95c4", "heading": "FP8 INC/Run Online Inference Using FP8", "level": 2, "text": "## Run Online Inference Using FP8  \nOnce you've completed the model calibration process and collected the measurements, you can run FP8 inference with vLLM using the following command:  \n```bash\nexport QUANT_CONFIG=/path/to/quant/config/inc/meta-llama-3.1-405b-instruct/maxabs_measure_g3.json\nvllm serve meta-llama/Llama-3.1-405B-Instruct --quantization inc --kv-cache-dtype fp8_inc --tensor_paralel_size 8\n```  \n!!! tip\nIf you are just prototyping or testing your model with FP8, you can use the `VLLM_SKIP_WARMUP=true` environment variable to disable the warmup stage, which can take a long time. However, we do not recommend disabling this feature in production environments as it causes a significant performance drop.  \n!!! tip\nWhen using FP8 models, you may experience timeouts caused by the long compilation time of FP8 operations. To mitigate this problem, you can use the below environment variables:", "file_path": "features/quantization/inc.md"}
{"id": "7318a9b30ef9934550866dde1cf17eca15aadc6e2395c685c5b06338402a95c4", "heading": "FP8 INC/Run Online Inference Using FP8", "level": 2, "text": "`VLLM_ENGINE_ITERATION_TIMEOUT_S` - to adjust the vLLM server timeout. You can set the value in seconds, e.g., 600 equals 10 minutes.\n`VLLM_RPC_TIMEOUT` - to adjust the RPC protocol timeout used by the OpenAI-compatible API. This value is in microseconds, e.g., 600000 equals 10 minutes.", "file_path": "features/quantization/inc.md"}
{"id": "7318a9b30ef9934550866dde1cf17eca15aadc6e2395c685c5b06338402a95c4", "heading": "FP8 INC/Run Offline Inference Using FP8", "level": 2, "text": "## Run Offline Inference Using FP8  \nTo run offline inference (after completing the model calibration process):  \n* Set the \"QUANT_CONFIG\" environment variable to point to a JSON configuration file with QUANTIZE mode.\n* Pass `quantization=inc` and `kv_cache_dtype=fp8_inc` as parameters to the `LLM` object.\n* Call shutdown method of the model_executor at the end of the run.  \n```python\nfrom vllm import LLM\nllm = LLM(\"llama3.1/Meta-Llama-3.1-8B-Instruct\", quantization=\"inc\", kv_cache_dtype=\"fp8_inc\")\n...\n# Call llm.generate on the required prompts and sampling params.\n...\nllm.llm_engine.model_executor.shutdown()\n```", "file_path": "features/quantization/inc.md"}
{"id": "7318a9b30ef9934550866dde1cf17eca15aadc6e2395c685c5b06338402a95c4", "heading": "FP8 INC/Device for the Model's Weights Uploading", "level": 2, "text": "## Device for the Model's Weights Uploading  \nThe unquantized weights are first loaded onto the CPU, then quantized and transferred to the target device (HPU) for model execution.\nThis reduces the device memory footprint of model weights, as only quantized weights are stored in the device memory.", "file_path": "features/quantization/inc.md"}
{"id": "34faf6cc9417ac8ee4f31193d38728c7c300c8e0103bca917c7355ec6da8be5f", "heading": "INT4 W4A16", "level": 1, "text": "# INT4 W4A16  \nvLLM supports quantizing weights to INT4 for memory savings and inference acceleration. This quantization method is particularly useful for reducing model size and maintaining low latency in workloads with low queries per second (QPS).  \nPlease visit the HF collection of [quantized INT4 checkpoints of popular LLMs ready to use with vLLM](https://huggingface.co/collections/neuralmagic/int4-llms-for-vllm-668ec34bf3c9fa45f857df2c).  \n!!! note\nINT4 computation is supported on NVIDIA GPUs with compute capability > 8.0 (Ampere, Ada Lovelace, Hopper, Blackwell).", "file_path": "features/quantization/int4.md"}
{"id": "34faf6cc9417ac8ee4f31193d38728c7c300c8e0103bca917c7355ec6da8be5f", "heading": "INT4 W4A16/Prerequisites", "level": 2, "text": "## Prerequisites  \nTo use INT4 quantization with vLLM, you'll need to install the [llm-compressor](https://github.com/vllm-project/llm-compressor/) library:  \n```bash\npip install llmcompressor\n```  \nAdditionally, install `vllm` and `lm-evaluation-harness` for evaluation:  \n```bash\npip install vllm git+https://github.com/EleutherAI/lm-evaluation-harness.git@206b7722158f58c35b7ffcd53b035fdbdda5126d#egg=lm-eval[api]\n```", "file_path": "features/quantization/int4.md"}
{"id": "34faf6cc9417ac8ee4f31193d38728c7c300c8e0103bca917c7355ec6da8be5f", "heading": "INT4 W4A16/Quantization Process", "level": 2, "text": "## Quantization Process  \nThe quantization process involves four main steps:  \n1. Loading the model\n2. Preparing calibration data\n3. Applying quantization\n4. Evaluating accuracy in vLLM", "file_path": "features/quantization/int4.md"}
{"id": "34faf6cc9417ac8ee4f31193d38728c7c300c8e0103bca917c7355ec6da8be5f", "heading": "INT4 W4A16/Quantization Process/1. Loading the Model", "level": 3, "text": "### 1. Loading the Model  \nLoad your model and tokenizer using the standard `transformers` AutoModel classes:  \n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\nMODEL_ID,\ndevice_map=\"auto\",\ndtype=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n```", "file_path": "features/quantization/int4.md"}
{"id": "34faf6cc9417ac8ee4f31193d38728c7c300c8e0103bca917c7355ec6da8be5f", "heading": "INT4 W4A16/Quantization Process/2. Preparing Calibration Data", "level": 3, "text": "### 2. Preparing Calibration Data  \nWhen quantizing weights to INT4, you need sample data to estimate the weight updates and calibrated scales.\nIt's best to use calibration data that closely matches your deployment data.\nFor a general-purpose instruction-tuned model, you can use a dataset like `ultrachat`:  \n??? code  \n```python\nfrom datasets import load_dataset\n\nNUM_CALIBRATION_SAMPLES = 512\nMAX_SEQUENCE_LENGTH = 2048\n\n# Load and preprocess the dataset\nds = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\")\nds = ds.shuffle(seed=42).select(range(NUM_CALIBRATION_SAMPLES))\n\ndef preprocess(example):\nreturn {\"text\": tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)}\nds = ds.map(preprocess)\n\ndef tokenize(sample):\nreturn tokenizer(sample[\"text\"], padding=False, max_length=MAX_SEQUENCE_LENGTH, truncation=True, add_special_tokens=False)\nds = ds.map(tokenize, remove_columns=ds.column_names)\n```", "file_path": "features/quantization/int4.md"}
{"id": "34faf6cc9417ac8ee4f31193d38728c7c300c8e0103bca917c7355ec6da8be5f", "heading": "INT4 W4A16/Quantization Process/3. Applying Quantization", "level": 3, "text": "### 3. Applying Quantization  \nNow, apply the quantization algorithms:  \n??? code  \n```python\nfrom llmcompressor.transformers import oneshot\nfrom llmcompressor.modifiers.quantization import GPTQModifier\nfrom llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n\n# Configure the quantization algorithms\nrecipe = GPTQModifier(targets=\"Linear\", scheme=\"W4A16\", ignore=[\"lm_head\"])\n\n# Apply quantization\noneshot(\nmodel=model,\ndataset=ds,\nrecipe=recipe,\nmax_seq_length=MAX_SEQUENCE_LENGTH,\nnum_calibration_samples=NUM_CALIBRATION_SAMPLES,\n)\n\n# Save the compressed model: Meta-Llama-3-8B-Instruct-W4A16-G128\nSAVE_DIR = MODEL_ID.split(\"/\")[1] + \"-W4A16-G128\"\nmodel.save_pretrained(SAVE_DIR, save_compressed=True)\ntokenizer.save_pretrained(SAVE_DIR)\n```  \nThis process creates a W4A16 model with weights quantized to 4-bit integers.", "file_path": "features/quantization/int4.md"}
{"id": "34faf6cc9417ac8ee4f31193d38728c7c300c8e0103bca917c7355ec6da8be5f", "heading": "INT4 W4A16/Quantization Process/4. Evaluating Accuracy", "level": 3, "text": "### 4. Evaluating Accuracy  \nAfter quantization, you can load and run the model in vLLM:  \n```python\nfrom vllm import LLM\n\nllm = LLM(\"./Meta-Llama-3-8B-Instruct-W4A16-G128\")\n```  \nTo evaluate accuracy, you can use `lm_eval`:  \n```bash\nlm_eval --model vllm \\\n--model_args pretrained=\"./Meta-Llama-3-8B-Instruct-W4A16-G128\",add_bos_token=true \\\n--tasks gsm8k \\\n--num_fewshot 5 \\\n--limit 250 \\\n--batch_size 'auto'\n```  \n!!! note\nQuantized models can be sensitive to the presence of the `bos` token. Make sure to include the `add_bos_token=True` argument when running evaluations.", "file_path": "features/quantization/int4.md"}
{"id": "34faf6cc9417ac8ee4f31193d38728c7c300c8e0103bca917c7355ec6da8be5f", "heading": "INT4 W4A16/Best Practices", "level": 2, "text": "## Best Practices  \n- Start with 512 samples for calibration data, and increase if accuracy drops\n- Ensure the calibration data contains a high variety of samples to prevent overfitting towards a specific use case\n- Use a sequence length of 2048 as a starting point\n- Employ the chat template or instruction template that the model was trained with\n- If you've fine-tuned a model, consider using a sample of your training data for calibration\n- Tune key hyperparameters to the quantization algorithm:\n- `dampening_frac` sets how much influence the GPTQ algorithm has. Lower values can improve accuracy, but can lead to numerical instabilities that cause the algorithm to fail.\n- `actorder` sets the activation ordering. When compressing the weights of a layer weight, the order in which channels are quantized matters. Setting `actorder=\"weight\"` can improve accuracy without added latency.  \nThe following is an example of an expanded quantization recipe you can tune to your own use case:  \n??? code  \n```python", "file_path": "features/quantization/int4.md"}
{"id": "34faf6cc9417ac8ee4f31193d38728c7c300c8e0103bca917c7355ec6da8be5f", "heading": "INT4 W4A16/Best Practices", "level": 2, "text": "??? code  \n```python\nfrom compressed_tensors.quantization import (\nQuantizationArgs,\nQuantizationScheme,\nQuantizationStrategy,\nQuantizationType,\n)\nrecipe = GPTQModifier(\ntargets=\"Linear\",\nconfig_groups={\n\"config_group\": QuantizationScheme(\ntargets=[\"Linear\"],\nweights=QuantizationArgs(\nnum_bits=4,\ntype=QuantizationType.INT,\nstrategy=QuantizationStrategy.GROUP,\ngroup_size=128,\nsymmetric=True,\ndynamic=False,\nactorder=\"weight\",\n),\n),\n},\nignore=[\"lm_head\"],\nupdate_size=NUM_CALIBRATION_SAMPLES,\ndampening_frac=0.01,\n)\n```", "file_path": "features/quantization/int4.md"}
{"id": "34faf6cc9417ac8ee4f31193d38728c7c300c8e0103bca917c7355ec6da8be5f", "heading": "INT4 W4A16/Troubleshooting and Support", "level": 2, "text": "## Troubleshooting and Support  \nIf you encounter any issues or have feature requests, please open an issue on the [vllm-project/llm-compressor](https://github.com/vllm-project/llm-compressor/issues) GitHub repository. The full INT4 quantization example in `llm-compressor` is available [here](https://github.com/vllm-project/llm-compressor/blob/main/examples/quantization_w4a16/llama3_example.py).", "file_path": "features/quantization/int4.md"}
{"id": "4ecb8b27b3ead5c2784da0ada3f151368548cdcec3aa6fe74ba9b92e19d7f177", "heading": "INT8 W8A8", "level": 1, "text": "# INT8 W8A8  \nvLLM supports quantizing weights and activations to INT8 for memory savings and inference acceleration.\nThis quantization method is particularly useful for reducing model size while maintaining good performance.  \nPlease visit the HF collection of [quantized INT8 checkpoints of popular LLMs ready to use with vLLM](https://huggingface.co/collections/neuralmagic/int8-llms-for-vllm-668ec32c049dca0369816415).  \n!!! note\nINT8 computation is supported on NVIDIA GPUs with compute capability > 7.5 (Turing, Ampere, Ada Lovelace, Hopper).  \n!!! warning\n**Blackwell GPU Limitation**: INT8 is not supported on compute capability >= 100 (e.g., RTX 6000 Blackwell).\nUse [FP8 quantization](fp8.md) instead, or run on Hopper/Ada/Ampere architectures.", "file_path": "features/quantization/int8.md"}
{"id": "4ecb8b27b3ead5c2784da0ada3f151368548cdcec3aa6fe74ba9b92e19d7f177", "heading": "INT8 W8A8/Prerequisites", "level": 2, "text": "## Prerequisites  \nTo use INT8 quantization with vLLM, you'll need to install the [llm-compressor](https://github.com/vllm-project/llm-compressor/) library:  \n```bash\npip install llmcompressor\n```  \nAdditionally, install `vllm` and `lm-evaluation-harness` for evaluation:  \n```bash\npip install vllm git+https://github.com/EleutherAI/lm-evaluation-harness.git@206b7722158f58c35b7ffcd53b035fdbdda5126d#egg=lm-eval[api]\n```", "file_path": "features/quantization/int8.md"}
{"id": "4ecb8b27b3ead5c2784da0ada3f151368548cdcec3aa6fe74ba9b92e19d7f177", "heading": "INT8 W8A8/Quantization Process", "level": 2, "text": "## Quantization Process  \nThe quantization process involves four main steps:  \n1. Loading the model\n2. Preparing calibration data\n3. Applying quantization\n4. Evaluating accuracy in vLLM", "file_path": "features/quantization/int8.md"}
{"id": "4ecb8b27b3ead5c2784da0ada3f151368548cdcec3aa6fe74ba9b92e19d7f177", "heading": "INT8 W8A8/Quantization Process/1. Loading the Model", "level": 3, "text": "### 1. Loading the Model  \nLoad your model and tokenizer using the standard `transformers` AutoModel classes:  \n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\nMODEL_ID,\ndevice_map=\"auto\",\ndtype=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n```", "file_path": "features/quantization/int8.md"}
{"id": "4ecb8b27b3ead5c2784da0ada3f151368548cdcec3aa6fe74ba9b92e19d7f177", "heading": "INT8 W8A8/Quantization Process/2. Preparing Calibration Data", "level": 3, "text": "### 2. Preparing Calibration Data  \nWhen quantizing activations to INT8, you need sample data to estimate the activation scales.\nIt's best to use calibration data that closely matches your deployment data.\nFor a general-purpose instruction-tuned model, you can use a dataset like `ultrachat`:  \n??? code  \n```python\nfrom datasets import load_dataset\n\nNUM_CALIBRATION_SAMPLES = 512\nMAX_SEQUENCE_LENGTH = 2048\n\n# Load and preprocess the dataset\nds = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\")\nds = ds.shuffle(seed=42).select(range(NUM_CALIBRATION_SAMPLES))\n\ndef preprocess(example):\nreturn {\"text\": tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)}\nds = ds.map(preprocess)\n\ndef tokenize(sample):\nreturn tokenizer(sample[\"text\"], padding=False, max_length=MAX_SEQUENCE_LENGTH, truncation=True, add_special_tokens=False)\nds = ds.map(tokenize, remove_columns=ds.column_names)\n```  \n</details>", "file_path": "features/quantization/int8.md"}
{"id": "4ecb8b27b3ead5c2784da0ada3f151368548cdcec3aa6fe74ba9b92e19d7f177", "heading": "INT8 W8A8/Quantization Process/3. Applying Quantization", "level": 3, "text": "### 3. Applying Quantization  \nNow, apply the quantization algorithms:  \n??? code  \n```python\nfrom llmcompressor.transformers import oneshot\nfrom llmcompressor.modifiers.quantization import GPTQModifier\nfrom llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n\n# Configure the quantization algorithms\nrecipe = [\nSmoothQuantModifier(smoothing_strength=0.8),\nGPTQModifier(targets=\"Linear\", scheme=\"W8A8\", ignore=[\"lm_head\"]),\n]\n\n# Apply quantization\noneshot(\nmodel=model,\ndataset=ds,\nrecipe=recipe,\nmax_seq_length=MAX_SEQUENCE_LENGTH,\nnum_calibration_samples=NUM_CALIBRATION_SAMPLES,\n)\n\n# Save the compressed model: Meta-Llama-3-8B-Instruct-W8A8-Dynamic-Per-Token\nSAVE_DIR = MODEL_ID.split(\"/\")[1] + \"-W8A8-Dynamic-Per-Token\"\nmodel.save_pretrained(SAVE_DIR, save_compressed=True)\ntokenizer.save_pretrained(SAVE_DIR)\n```  \nThis process creates a W8A8 model with weights and activations quantized to 8-bit integers.", "file_path": "features/quantization/int8.md"}
{"id": "4ecb8b27b3ead5c2784da0ada3f151368548cdcec3aa6fe74ba9b92e19d7f177", "heading": "INT8 W8A8/Quantization Process/4. Evaluating Accuracy", "level": 3, "text": "### 4. Evaluating Accuracy  \nAfter quantization, you can load and run the model in vLLM:  \n```python\nfrom vllm import LLM\n\nllm = LLM(\"./Meta-Llama-3-8B-Instruct-W8A8-Dynamic-Per-Token\")\n```  \nTo evaluate accuracy, you can use `lm_eval`:  \n```bash\nlm_eval --model vllm \\\n--model_args pretrained=\"./Meta-Llama-3-8B-Instruct-W8A8-Dynamic-Per-Token\",add_bos_token=true \\\n--tasks gsm8k \\\n--num_fewshot 5 \\\n--limit 250 \\\n--batch_size 'auto'\n```  \n!!! note\nQuantized models can be sensitive to the presence of the `bos` token. Make sure to include the `add_bos_token=True` argument when running evaluations.", "file_path": "features/quantization/int8.md"}
{"id": "4ecb8b27b3ead5c2784da0ada3f151368548cdcec3aa6fe74ba9b92e19d7f177", "heading": "INT8 W8A8/Best Practices", "level": 2, "text": "## Best Practices  \n- Start with 512 samples for calibration data (increase if accuracy drops)\n- Use a sequence length of 2048 as a starting point\n- Employ the chat template or instruction template that the model was trained with\n- If you've fine-tuned a model, consider using a sample of your training data for calibration", "file_path": "features/quantization/int8.md"}
{"id": "4ecb8b27b3ead5c2784da0ada3f151368548cdcec3aa6fe74ba9b92e19d7f177", "heading": "INT8 W8A8/Troubleshooting and Support", "level": 2, "text": "## Troubleshooting and Support  \nIf you encounter any issues or have feature requests, please open an issue on the [vllm-project/llm-compressor](https://github.com/vllm-project/llm-compressor/issues) GitHub repository.", "file_path": "features/quantization/int8.md"}
{"id": "0cad6c28ff6f2c1d709a985e609320d437351e3e9cf1706f87ebe478db8727ac", "heading": "NVIDIA TensorRT Model Optimizer", "level": 1, "text": "# NVIDIA TensorRT Model Optimizer  \nThe [NVIDIA TensorRT Model Optimizer](https://github.com/NVIDIA/TensorRT-Model-Optimizer) is a library designed to optimize models for inference with NVIDIA GPUs. It includes tools for Post-Training Quantization (PTQ) and Quantization Aware Training (QAT) of Large Language Models (LLMs), Vision Language Models (VLMs), and diffusion models.  \nWe recommend installing the library with:  \n```bash\npip install nvidia-modelopt\n```", "file_path": "features/quantization/modelopt.md"}
{"id": "0cad6c28ff6f2c1d709a985e609320d437351e3e9cf1706f87ebe478db8727ac", "heading": "NVIDIA TensorRT Model Optimizer/Quantizing HuggingFace Models with PTQ", "level": 2, "text": "## Quantizing HuggingFace Models with PTQ  \nYou can quantize HuggingFace models using the example scripts provided in the TensorRT Model Optimizer repository. The primary script for LLM PTQ is typically found within the `examples/llm_ptq` directory.  \nBelow is an example showing how to quantize a model using modelopt's PTQ API:  \n??? code  \n```python\nimport modelopt.torch.quantization as mtq\nfrom transformers import AutoModelForCausalLM\n\n# Load the model from HuggingFace\nmodel = AutoModelForCausalLM.from_pretrained(\"<path_or_model_id>\")\n\n# Select the quantization config, for example, FP8\nconfig = mtq.FP8_DEFAULT_CFG\n\n# Define a forward loop function for calibration\ndef forward_loop(model):\nfor data in calib_set:\nmodel(data)\n\n# PTQ with in-place replacement of quantized modules\nmodel = mtq.quantize(model, config, forward_loop)\n```  \nAfter the model is quantized, you can export it to a quantized checkpoint using the export API:  \n```python\nimport torch\nfrom modelopt.torch.export import export_hf_checkpoint", "file_path": "features/quantization/modelopt.md"}
{"id": "0cad6c28ff6f2c1d709a985e609320d437351e3e9cf1706f87ebe478db8727ac", "heading": "NVIDIA TensorRT Model Optimizer/Quantizing HuggingFace Models with PTQ", "level": 2, "text": "with torch.inference_mode():\nexport_hf_checkpoint(\nmodel,  # The quantized model.\nexport_dir,  # The directory where the exported files will be stored.\n)\n```  \nThe quantized checkpoint can then be deployed with vLLM. As an example, the following code shows how to deploy `nvidia/Llama-3.1-8B-Instruct-FP8`, which is the FP8 quantized checkpoint derived from `meta-llama/Llama-3.1-8B-Instruct`, using vLLM:  \n??? code  \n```python\nfrom vllm import LLM, SamplingParams\n\ndef main():\nmodel_id = \"nvidia/Llama-3.1-8B-Instruct-FP8\"\n\n# Ensure you specify quantization=\"modelopt\" when loading the modelopt checkpoint\nllm = LLM(model=model_id, quantization=\"modelopt\", trust_remote_code=True)\n\nsampling_params = SamplingParams(temperature=0.8, top_p=0.9)\n\nprompts = [\n\"Hello, my name is\",\n\"The president of the United States is\",\n\"The capital of France is\",\n\"The future of AI is\",\n]\n\noutputs = llm.generate(prompts, sampling_params)", "file_path": "features/quantization/modelopt.md"}
{"id": "0cad6c28ff6f2c1d709a985e609320d437351e3e9cf1706f87ebe478db8727ac", "heading": "NVIDIA TensorRT Model Optimizer/Quantizing HuggingFace Models with PTQ", "level": 2, "text": "outputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n\nif __name__ == \"__main__\":\nmain()\n```", "file_path": "features/quantization/modelopt.md"}
{"id": "21ced59d1d67c36734ca4b69443ba53250fd3269fbb47564425e0dbe95602433", "heading": "Quantized KV Cache/FP8 KV Cache", "level": 2, "text": "# Quantized KV Cache  \n## FP8 KV Cache  \nQuantizing the KV cache to FP8 reduces its memory footprint. This increases the number of tokens that can be stored in the cache, improving throughput.", "file_path": "features/quantization/quantized_kvcache.md"}
{"id": "21ced59d1d67c36734ca4b69443ba53250fd3269fbb47564425e0dbe95602433", "heading": "Quantized KV Cache/FP8 KV Cache/FP8 Formats", "level": 3, "text": "### FP8 Formats  \n[OCP (Open Compute Project)](https://www.opencompute.org) specifies two common 8-bit floating point data formats:  \n- E5M2 (5 exponent bits and 2 mantissa bits)\n- E4M3FN (4 exponent bits and 3 mantissa bits, often shortened as E4M3)  \nThe E4M3 format offers higher precision compared to E5M2. However, due to its small dynamic range (Â±240.0), E4M3 typically requires a higher-precision (FP32) scaling factor alongside each quantized tensor.", "file_path": "features/quantization/quantized_kvcache.md"}
{"id": "21ced59d1d67c36734ca4b69443ba53250fd3269fbb47564425e0dbe95602433", "heading": "Quantized KV Cache/FP8 KV Cache/Current Limitations", "level": 3, "text": "### Current Limitations  \nFor now, only per-tensor (scalar) scaling factors are supported. Development is ongoing to support scaling factors of a finer granularity (e.g. per-channel).", "file_path": "features/quantization/quantized_kvcache.md"}
{"id": "21ced59d1d67c36734ca4b69443ba53250fd3269fbb47564425e0dbe95602433", "heading": "Quantized KV Cache/FP8 KV Cache/Performance Impact", "level": 3, "text": "### Performance Impact  \nThe current FP8 KV cache implementation primarily benefits throughput by allowing approximately double the amount of space for KV cache allocation. This enables either:  \n- Processing longer context lengths for individual requests, or\n- Handling more concurrent request batches  \nHowever, there are currently no latency improvements as the implementation does not yet include fused dequantization and attention operations. Future releases will support quantized attention with hardware acceleration, which should provide additional performance benefits. While the most recent silicon offerings (e.g. AMD MI300, NVIDIA Hopper or later) support native hardware conversion between FP8 and other formats (fp32, fp16, bf16), this benefit is not yet fully realized.  \nStudies have shown that FP8 E4M3 quantization typically only minimally degrades inference accuracy, making it a practical choice for throughput optimization.", "file_path": "features/quantization/quantized_kvcache.md"}
{"id": "21ced59d1d67c36734ca4b69443ba53250fd3269fbb47564425e0dbe95602433", "heading": "Quantized KV Cache/Usage Example", "level": 2, "text": "## Usage Example  \nHere is an example of how to enable FP8 quantization:  \n??? code  \n```python\n# To calculate kv cache scales on the fly enable the calculate_kv_scales\n# parameter\n\nfrom vllm import LLM, SamplingParams\n\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8)\nllm = LLM(\nmodel=\"meta-llama/Llama-2-7b-chat-hf\",\nkv_cache_dtype=\"fp8\",\ncalculate_kv_scales=True,\n)\nprompt = \"London is the capital of\"\nout = llm.generate(prompt, sampling_params)[0].outputs[0].text\nprint(out)\n```  \nThe `kv_cache_dtype` argument specifies the data type for KV cache storage:  \n- `\"auto\"`: Uses the model's default \"unquantized\" data type\n- `\"fp8\"` or `\"fp8_e4m3\"`: Supported on CUDA 11.8+ and ROCm (AMD GPU)\n- `\"fp8_e5m2\"`: Supported on CUDA 11.8+", "file_path": "features/quantization/quantized_kvcache.md"}
{"id": "21ced59d1d67c36734ca4b69443ba53250fd3269fbb47564425e0dbe95602433", "heading": "Quantized KV Cache/Calibrated Scales for Better Accuracy", "level": 2, "text": "## Calibrated Scales for Better Accuracy  \nFor optimal model quality when using FP8 KV Cache, we recommend using calibrated scales tuned to representative inference data. [LLM Compressor](https://github.com/vllm-project/llm-compressor/) is the recommended tool for this process.", "file_path": "features/quantization/quantized_kvcache.md"}
{"id": "21ced59d1d67c36734ca4b69443ba53250fd3269fbb47564425e0dbe95602433", "heading": "Quantized KV Cache/Calibrated Scales for Better Accuracy/Installation", "level": 3, "text": "### Installation  \nFirst, install the required dependencies:  \n```bash\npip install llmcompressor\n```", "file_path": "features/quantization/quantized_kvcache.md"}
{"id": "21ced59d1d67c36734ca4b69443ba53250fd3269fbb47564425e0dbe95602433", "heading": "Quantized KV Cache/Calibrated Scales for Better Accuracy/Example Usage", "level": 3, "text": "### Example Usage  \nHere's a complete example using `meta-llama/Llama-3.1-8B-Instruct` (most models can use this same pattern):  \n??? code  \n```python\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom llmcompressor.transformers import oneshot\n\n# Select model and load it\nMODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\", dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\n# Select calibration dataset\nDATASET_ID = \"HuggingFaceH4/ultrachat_200k\"\nDATASET_SPLIT = \"train_sft\"\n\n# Configure calibration parameters\nNUM_CALIBRATION_SAMPLES = 512  # 512 samples is a good starting point\nMAX_SEQUENCE_LENGTH = 2048\n\n# Load and preprocess dataset\nds = load_dataset(DATASET_ID, split=DATASET_SPLIT)\nds = ds.shuffle(seed=42).select(range(NUM_CALIBRATION_SAMPLES))", "file_path": "features/quantization/quantized_kvcache.md"}
{"id": "21ced59d1d67c36734ca4b69443ba53250fd3269fbb47564425e0dbe95602433", "heading": "Quantized KV Cache/Calibrated Scales for Better Accuracy/Example Usage", "level": 3, "text": "def process_and_tokenize(example):\ntext = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)\nreturn tokenizer(\ntext,\npadding=False,\nmax_length=MAX_SEQUENCE_LENGTH,\ntruncation=True,\nadd_special_tokens=False,\n)\n\nds = ds.map(process_and_tokenize, remove_columns=ds.column_names)\n\n# Configure quantization settings\nrecipe = \"\"\"\nquant_stage:\nquant_modifiers:\nQuantizationModifier:\nkv_cache_scheme:\nnum_bits: 8\ntype: float\nstrategy: tensor\ndynamic: false\nsymmetric: true\n\"\"\"\n\n# Apply quantization\noneshot(\nmodel=model,\ndataset=ds,\nrecipe=recipe,\nmax_seq_length=MAX_SEQUENCE_LENGTH,\nnum_calibration_samples=NUM_CALIBRATION_SAMPLES,\n)", "file_path": "features/quantization/quantized_kvcache.md"}
{"id": "21ced59d1d67c36734ca4b69443ba53250fd3269fbb47564425e0dbe95602433", "heading": "Quantized KV Cache/Calibrated Scales for Better Accuracy/Example Usage", "level": 3, "text": "# Save quantized model: Llama-3.1-8B-Instruct-FP8-KV\nSAVE_DIR = MODEL_ID.split(\"/\")[1] + \"-FP8-KV\"\nmodel.save_pretrained(SAVE_DIR, save_compressed=True)\ntokenizer.save_pretrained(SAVE_DIR)\n```  \nThe above script will create a folder in your current directory containing your quantized model (e.g., `Llama-3.1-8B-Instruct-FP8-KV`) with calibrated scales.  \nWhen running the model you must specify `kv_cache_dtype=\"fp8\"` in order to enable the kv cache quantization and use the scales.  \n```python\nfrom vllm import LLM, SamplingParams\n\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8)\nllm = LLM(model=\"Llama-3.1-8B-Instruct-FP8-KV\", kv_cache_dtype=\"fp8\")\nprompt = \"London is the capital of\"\nout = llm.generate(prompt, sampling_params)[0].outputs[0].text\nprint(out)\n```", "file_path": "features/quantization/quantized_kvcache.md"}
{"id": "f90a56366ce8f79bf16437c149a4c4f1fbd8fbf36b22001b54e3b664063eb962", "heading": "AMD Quark", "level": 1, "text": "# AMD Quark  \nQuantization can effectively reduce memory and bandwidth usage, accelerate computation and improve\nthroughput while with minimal accuracy loss. vLLM can leverage [Quark](https://quark.docs.amd.com/latest/),\nthe flexible and powerful quantization toolkit, to produce performant quantized models to run on AMD GPUs. Quark has specialized support for quantizing large language models with weight,\nactivation and kv-cache quantization and cutting-edge quantization algorithms like\nAWQ, GPTQ, Rotation and SmoothQuant.", "file_path": "features/quantization/quark.md"}
{"id": "f90a56366ce8f79bf16437c149a4c4f1fbd8fbf36b22001b54e3b664063eb962", "heading": "AMD Quark/Quark Installation", "level": 2, "text": "## Quark Installation  \nBefore quantizing models, you need to install Quark. The latest release of Quark can be installed with pip:  \n```bash\npip install amd-quark\n```  \nYou can refer to [Quark installation guide](https://quark.docs.amd.com/latest/install.html)\nfor more installation details.  \nAdditionally, install `vllm` and `lm-evaluation-harness` for evaluation:  \n```bash\npip install vllm git+https://github.com/EleutherAI/lm-evaluation-harness.git@206b7722158f58c35b7ffcd53b035fdbdda5126d#egg=lm-eval[api]\n```", "file_path": "features/quantization/quark.md"}
{"id": "f90a56366ce8f79bf16437c149a4c4f1fbd8fbf36b22001b54e3b664063eb962", "heading": "AMD Quark/Quantization Process", "level": 2, "text": "## Quantization Process  \nAfter installing Quark, we will use an example to illustrate how to use Quark.\nThe Quark quantization process can be listed for 5 steps as below:  \n1. Load the model\n2. Prepare the calibration dataloader\n3. Set the quantization configuration\n4. Quantize the model and export\n5. Evaluation in vLLM", "file_path": "features/quantization/quark.md"}
{"id": "f90a56366ce8f79bf16437c149a4c4f1fbd8fbf36b22001b54e3b664063eb962", "heading": "AMD Quark/Quantization Process/1. Load the Model", "level": 3, "text": "### 1. Load the Model  \nQuark uses [Transformers](https://huggingface.co/docs/transformers/en/index)\nto fetch model and tokenizer.  \n??? code  \n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMODEL_ID = \"meta-llama/Llama-2-70b-chat-hf\"\nMAX_SEQ_LEN = 512\n\nmodel = AutoModelForCausalLM.from_pretrained(\nMODEL_ID,\ndevice_map=\"auto\",\ndtype=\"auto\",\n)\nmodel.eval()\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, model_max_length=MAX_SEQ_LEN)\ntokenizer.pad_token = tokenizer.eos_token\n```", "file_path": "features/quantization/quark.md"}
{"id": "f90a56366ce8f79bf16437c149a4c4f1fbd8fbf36b22001b54e3b664063eb962", "heading": "AMD Quark/Quantization Process/2. Prepare the Calibration Dataloader", "level": 3, "text": "### 2. Prepare the Calibration Dataloader  \nQuark uses the [PyTorch Dataloader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\nto load calibration data. For more details about how to use calibration datasets efficiently, please refer\nto [Adding Calibration Datasets](https://quark.docs.amd.com/latest/pytorch/calibration_datasets.html).  \n??? code  \n```python\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\n\nBATCH_SIZE = 1\nNUM_CALIBRATION_DATA = 512\n\n# Load the dataset and get calibration data.\ndataset = load_dataset(\"mit-han-lab/pile-val-backup\", split=\"validation\")\ntext_data = dataset[\"text\"][:NUM_CALIBRATION_DATA]\n\ntokenized_outputs = tokenizer(\ntext_data,\nreturn_tensors=\"pt\",\npadding=True,\ntruncation=True,\nmax_length=MAX_SEQ_LEN,\n)\ncalib_dataloader = DataLoader(\ntokenized_outputs['input_ids'],\nbatch_size=BATCH_SIZE,\ndrop_last=True,\n)\n```", "file_path": "features/quantization/quark.md"}
{"id": "f90a56366ce8f79bf16437c149a4c4f1fbd8fbf36b22001b54e3b664063eb962", "heading": "AMD Quark/Quantization Process/3. Set the Quantization Configuration", "level": 3, "text": "### 3. Set the Quantization Configuration  \nWe need to set the quantization configuration, you can check\n[quark config guide](https://quark.docs.amd.com/latest/pytorch/user_guide_config_description.html)\nfor further details. Here we use FP8 per-tensor quantization on weight, activation,\nkv-cache and the quantization algorithm is AutoSmoothQuant.  \n!!! note\nNote the quantization algorithm needs a JSON config file and the config file is located in\n[Quark Pytorch examples](https://quark.docs.amd.com/latest/pytorch/pytorch_examples.html),\nunder the directory `examples/torch/language_modeling/llm_ptq/models`. For example,\nAutoSmoothQuant config file for Llama is\n`examples/torch/language_modeling/llm_ptq/models/llama/autosmoothquant_config.json`.  \n??? code  \n```python\nfrom quark.torch.quantization import (Config, QuantizationConfig,\nFP8E4M3PerTensorSpec,\nload_quant_algo_config_from_file)", "file_path": "features/quantization/quark.md"}
{"id": "f90a56366ce8f79bf16437c149a4c4f1fbd8fbf36b22001b54e3b664063eb962", "heading": "AMD Quark/Quantization Process/3. Set the Quantization Configuration", "level": 3, "text": "# Define fp8/per-tensor/static spec.\nFP8_PER_TENSOR_SPEC = FP8E4M3PerTensorSpec(\nobserver_method=\"min_max\",\nis_dynamic=False,\n).to_quantization_spec()\n\n# Define global quantization config, input tensors and weight apply FP8_PER_TENSOR_SPEC.\nglobal_quant_config = QuantizationConfig(\ninput_tensors=FP8_PER_TENSOR_SPEC,\nweight=FP8_PER_TENSOR_SPEC,\n)\n\n# Define quantization config for kv-cache layers, output tensors apply FP8_PER_TENSOR_SPEC.\nKV_CACHE_SPEC = FP8_PER_TENSOR_SPEC\nkv_cache_layer_names_for_llama = [\"*k_proj\", \"*v_proj\"]\nkv_cache_quant_config = {\nname: QuantizationConfig(\ninput_tensors=global_quant_config.input_tensors,\nweight=global_quant_config.weight,\noutput_tensors=KV_CACHE_SPEC,\n)\nfor name in kv_cache_layer_names_for_llama\n}\nlayer_quant_config = kv_cache_quant_config.copy()", "file_path": "features/quantization/quark.md"}
{"id": "f90a56366ce8f79bf16437c149a4c4f1fbd8fbf36b22001b54e3b664063eb962", "heading": "AMD Quark/Quantization Process/3. Set the Quantization Configuration", "level": 3, "text": "# Define algorithm config by config file.\nLLAMA_AUTOSMOOTHQUANT_CONFIG_FILE = \"examples/torch/language_modeling/llm_ptq/models/llama/autosmoothquant_config.json\"\nalgo_config = load_quant_algo_config_from_file(LLAMA_AUTOSMOOTHQUANT_CONFIG_FILE)\n\nEXCLUDE_LAYERS = [\"lm_head\"]\nquant_config = Config(\nglobal_quant_config=global_quant_config,\nlayer_quant_config=layer_quant_config,\nkv_cache_quant_config=kv_cache_quant_config,\nexclude=EXCLUDE_LAYERS,\nalgo_config=algo_config,\n)\n```", "file_path": "features/quantization/quark.md"}
{"id": "f90a56366ce8f79bf16437c149a4c4f1fbd8fbf36b22001b54e3b664063eb962", "heading": "AMD Quark/Quantization Process/4. Quantize the Model and Export", "level": 3, "text": "### 4. Quantize the Model and Export  \nThen we can apply the quantization. After quantizing, we need to freeze the\nquantized model first before exporting. Note that we need to export model with format of\nHuggingFace `safetensors`, you can refer to\n[HuggingFace format exporting](https://quark.docs.amd.com/latest/pytorch/export/quark_export_hf.html)\nfor more exporting format details.  \n??? code  \n```python\nimport torch\nfrom quark.torch import ModelQuantizer, ModelExporter\nfrom quark.torch.export import ExporterConfig, JsonExporterConfig\n\n# Apply quantization.\nquantizer = ModelQuantizer(quant_config)\nquant_model = quantizer.quantize_model(model, calib_dataloader)\n\n# Freeze quantized model to export.\nfreezed_model = quantizer.freeze(model)\n\n# Define export config.\nLLAMA_KV_CACHE_GROUP = [\"*k_proj\", \"*v_proj\"]\nexport_config = ExporterConfig(json_export_config=JsonExporterConfig())\nexport_config.json_export_config.kv_cache_group = LLAMA_KV_CACHE_GROUP", "file_path": "features/quantization/quark.md"}
{"id": "f90a56366ce8f79bf16437c149a4c4f1fbd8fbf36b22001b54e3b664063eb962", "heading": "AMD Quark/Quantization Process/4. Quantize the Model and Export", "level": 3, "text": "# Model: Llama-2-70b-chat-hf-w-fp8-a-fp8-kvcache-fp8-pertensor-autosmoothquant\nEXPORT_DIR = MODEL_ID.split(\"/\")[1] + \"-w-fp8-a-fp8-kvcache-fp8-pertensor-autosmoothquant\"\nexporter = ModelExporter(config=export_config, export_dir=EXPORT_DIR)\nwith torch.no_grad():\nexporter.export_safetensors_model(\nfreezed_model,\nquant_config=quant_config,\ntokenizer=tokenizer,\n)\n```", "file_path": "features/quantization/quark.md"}
{"id": "f90a56366ce8f79bf16437c149a4c4f1fbd8fbf36b22001b54e3b664063eb962", "heading": "AMD Quark/Quantization Process/5. Evaluation in vLLM", "level": 3, "text": "### 5. Evaluation in vLLM  \nNow, you can load and run the Quark quantized model directly through the LLM entrypoint:  \n??? code  \n```python\nfrom vllm import LLM, SamplingParams\n\n# Sample prompts.\nprompts = [\n\"Hello, my name is\",\n\"The president of the United States is\",\n\"The capital of France is\",\n\"The future of AI is\",\n]\n# Create a sampling params object.\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)", "file_path": "features/quantization/quark.md"}
{"id": "f90a56366ce8f79bf16437c149a4c4f1fbd8fbf36b22001b54e3b664063eb962", "heading": "AMD Quark/Quantization Process/5. Evaluation in vLLM", "level": 3, "text": "# Create an LLM.\nllm = LLM(\nmodel=\"Llama-2-70b-chat-hf-w-fp8-a-fp8-kvcache-fp8-pertensor-autosmoothquant\",\nkv_cache_dtype=\"fp8\",\nquantization=\"quark\",\n)\n# Generate texts from the prompts. The output is a list of RequestOutput objects\n# that contain the prompt, generated text, and other information.\noutputs = llm.generate(prompts, sampling_params)\n# Print the outputs.\nprint(\"\\nGenerated Outputs:\\n\" + \"-\" * 60)\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt:    {prompt!r}\")\nprint(f\"Output:    {generated_text!r}\")\nprint(\"-\" * 60)\n```  \nOr, you can use `lm_eval` to evaluate accuracy:  \n```bash\nlm_eval --model vllm \\\n--model_args pretrained=Llama-2-70b-chat-hf-w-fp8-a-fp8-kvcache-fp8-pertensor-autosmoothquant,kv_cache_dtype='fp8',quantization='quark' \\\n--tasks gsm8k\n```", "file_path": "features/quantization/quark.md"}
{"id": "f90a56366ce8f79bf16437c149a4c4f1fbd8fbf36b22001b54e3b664063eb962", "heading": "AMD Quark/Quark Quantization Script", "level": 2, "text": "## Quark Quantization Script  \nIn addition to the example of Python API above, Quark also offers a\n[quantization script](https://quark.docs.amd.com/latest/pytorch/example_quark_torch_llm_ptq.html)\nto quantize large language models more conveniently. It supports quantizing models with variety\nof different quantization schemes and optimization algorithms. It can export the quantized model\nand run evaluation tasks on the fly. With the script, the example above can be:  \n```bash\npython3 quantize_quark.py --model_dir meta-llama/Llama-2-70b-chat-hf \\\n--output_dir /path/to/output \\\n--quant_scheme w_fp8_a_fp8 \\\n--kv_cache_dtype fp8 \\\n--quant_algo autosmoothquant \\\n--num_calib_data 512 \\\n--model_export hf_format \\\n--tasks gsm8k\n```", "file_path": "features/quantization/quark.md"}
{"id": "f90a56366ce8f79bf16437c149a4c4f1fbd8fbf36b22001b54e3b664063eb962", "heading": "AMD Quark/Using OCP MX (MXFP4, MXFP6) models", "level": 2, "text": "## Using OCP MX (MXFP4, MXFP6) models  \nvLLM supports loading MXFP4 and MXFP6 models quantized offline through AMD Quark, compliant with [Open Compute Project (OCP) specification](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf).  \nThe scheme currently only supports dynamic quantization for activations.  \nExample usage, after installing the latest AMD Quark release:  \n```bash\nvllm serve fxmarty/qwen_1.5-moe-a2.7b-mxfp4 --tensor-parallel-size 1\n# or, for a model using fp6 activations and fp4 weights:\nvllm serve fxmarty/qwen1.5_moe_a2.7b_chat_w_fp4_a_fp6_e2m3 --tensor-parallel-size 1\n```", "file_path": "features/quantization/quark.md"}
{"id": "f90a56366ce8f79bf16437c149a4c4f1fbd8fbf36b22001b54e3b664063eb962", "heading": "AMD Quark/Using OCP MX (MXFP4, MXFP6) models", "level": 2, "text": "```  \nA simulation of the matrix multiplication execution in MXFP4/MXFP6 can be run on devices that do not support OCP MX operations natively (e.g. AMD Instinct MI325, MI300 and MI250), dequantizing weights from FP4/FP6 to half precision on the fly, using a fused kernel. This is useful e.g. to evaluate FP4/FP6 models using vLLM, or alternatively to benefit from the ~2.5-4x memory savings (compared to float16 and bfloat16).  \nTo generate offline models quantized using MXFP4 data type, the easiest approach is to use AMD Quark's [quantization script](https://quark.docs.amd.com/latest/pytorch/example_quark_torch_llm_ptq.html), as an example:  \n```bash\npython quantize_quark.py --model_dir Qwen/Qwen1.5-MoE-A2.7B-Chat \\\n--quant_scheme w_mxfp4_a_mxfp4 \\\n--output_dir qwen_1.5-moe-a2.7b-mxfp4 \\\n--skip_evaluation \\\n--model_export hf_format \\\n--group_size 32\n```", "file_path": "features/quantization/quark.md"}
{"id": "f90a56366ce8f79bf16437c149a4c4f1fbd8fbf36b22001b54e3b664063eb962", "heading": "AMD Quark/Using OCP MX (MXFP4, MXFP6) models", "level": 2, "text": "--model_export hf_format \\\n--group_size 32\n```  \nThe current integration supports [all combination of FP4, FP6_E3M2, FP6_E2M3](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/quantization/utils/ocp_mx_utils.py) used for either weights or activations. Eventually, some target hardware support mixed precision GEMM, as AMD Instinct MI350/MI355, for example using FP6 for activations and FP4 for weights.", "file_path": "features/quantization/quark.md"}
{"id": "91c6123aa0bc7c8e630f8a9b3131403a114f7334a71615d4bb947c40edf57e36", "heading": "TorchAO", "level": 1, "text": "# TorchAO  \nTorchAO is an architecture optimization library for PyTorch, it provides high performance dtypes, optimization techniques and kernels for inference and training, featuring composability with native PyTorch features like torch.compile, FSDP etc.. Some benchmark numbers can be found [here](https://github.com/pytorch/ao/tree/main/torchao/quantization#benchmarks).  \nWe recommend installing the latest torchao nightly with  \n```bash\n# Install the latest TorchAO nightly build\n# Choose the CUDA version that matches your system (cu126, cu128, etc.)\npip install \\\n--pre torchao>=10.0.0 \\\n--index-url https://download.pytorch.org/whl/nightly/cu126\n```", "file_path": "features/quantization/torchao.md"}
{"id": "91c6123aa0bc7c8e630f8a9b3131403a114f7334a71615d4bb947c40edf57e36", "heading": "TorchAO/Quantizing HuggingFace Models", "level": 2, "text": "## Quantizing HuggingFace Models  \nYou can quantize your own huggingface model with torchao, e.g. [transformers](https://huggingface.co/docs/transformers/main/en/quantization/torchao) and [diffusers](https://huggingface.co/docs/diffusers/en/quantization/torchao), and save the checkpoint to huggingface hub like [this](https://huggingface.co/jerryzh168/llama3-8b-int8wo) with the following example code:  \n??? code  \n```Python\nimport torch\nfrom transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\nfrom torchao.quantization import Int8WeightOnlyConfig\n\nmodel_name = \"meta-llama/Meta-Llama-3-8B\"\nquantization_config = TorchAoConfig(Int8WeightOnlyConfig())\nquantized_model = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ndtype=\"auto\",\ndevice_map=\"auto\",\nquantization_config=quantization_config\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ninput_text = \"What are we having for dinner?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")", "file_path": "features/quantization/torchao.md"}
{"id": "91c6123aa0bc7c8e630f8a9b3131403a114f7334a71615d4bb947c40edf57e36", "heading": "TorchAO/Quantizing HuggingFace Models", "level": 2, "text": "hub_repo = # YOUR HUB REPO ID\ntokenizer.push_to_hub(hub_repo)\nquantized_model.push_to_hub(hub_repo, safe_serialization=False)\n```  \nAlternatively, you can use the [TorchAO Quantization space](https://huggingface.co/spaces/medmekk/TorchAO_Quantization) for quantizing models with a simple UI.", "file_path": "features/quantization/torchao.md"}
{"id": "45c29a6b2ff5f6a2292dc467fa058671f46b3ab65ef97a76ee4e326d6ae74654", "heading": "Reasoning Outputs", "level": 1, "text": "# Reasoning Outputs  \nvLLM offers support for reasoning models like [DeepSeek R1](https://huggingface.co/deepseek-ai/DeepSeek-R1), which are designed to generate outputs containing both reasoning steps and final conclusions.  \nReasoning models return an additional `reasoning_content` field in their outputs, which contains the reasoning steps that led to the final conclusion. This field is not present in the outputs of other models.", "file_path": "features/reasoning_outputs.md"}
{"id": "45c29a6b2ff5f6a2292dc467fa058671f46b3ab65ef97a76ee4e326d6ae74654", "heading": "Reasoning Outputs/Supported Models", "level": 2, "text": "## Supported Models  \nvLLM currently supports the following reasoning models:  \n| Model Series | Parser Name | Structured Output Support | Tool Calling |\n|--------------|-------------|------------------|-------------|\n| [DeepSeek R1 series](https://huggingface.co/collections/deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d) | `deepseek_r1` | `json`, `regex` | âŒ |\n| [DeepSeek-V3.1](https://huggingface.co/collections/deepseek-ai/deepseek-v31-68a491bed32bd77e7fca048f) | `deepseek_v3` | `json`, `regex` | âŒ |\n| [ERNIE-4.5-VL series](https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-PT) | `ernie45` | `json`, `regex` | âŒ |\n| [ERNIE-4.5-21B-A3B-Thinking](https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-Thinking) | `ernie45` | `json`, `regex` | âœ… |\n| [QwQ-32B](https://huggingface.co/Qwen/QwQ-32B) | `deepseek_r1` | `json`, `regex` | âœ… |\n| [IBM Granite 3.2 language models](https://huggingface.co/collections/ibm-granite/granite-32-language-models-67b3bc8c13508f6d064cff9a) | `granite` | âŒ | âŒ |", "file_path": "features/reasoning_outputs.md"}
{"id": "45c29a6b2ff5f6a2292dc467fa058671f46b3ab65ef97a76ee4e326d6ae74654", "heading": "Reasoning Outputs/Supported Models", "level": 2, "text": "| [Qwen3 series](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f) | `qwen3` | `json`, `regex` | âœ… |\n| [Hunyuan A13B series](https://huggingface.co/collections/tencent/hunyuan-a13b-685ec38e5b46321e3ea7c4be) | `hunyuan_a13b` | `json`, `regex` | âœ… |\n| [GLM-4.5 series](https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b) | `glm45` | `json`, `regex` | âœ… |  \n!!! note\nIBM Granite 3.2 and DeepSeek-V3.1 reasoning is disabled by default; to enable it, you must also pass `thinking=True` in your `chat_template_kwargs`.\nThe reasoning feature for the Qwen3 series is enabled by default. To disable it, you must pass `enable_thinking=False` in your `chat_template_kwargs`.\nDeepSeek-V3.1 tool calling is supported in non-thinking mode.", "file_path": "features/reasoning_outputs.md"}
{"id": "45c29a6b2ff5f6a2292dc467fa058671f46b3ab65ef97a76ee4e326d6ae74654", "heading": "Reasoning Outputs/Quickstart", "level": 2, "text": "## Quickstart  \nTo use reasoning models, you need to specify the `--reasoning-parser` flags when making a request to the chat completion endpoint. The `--reasoning-parser` flag specifies the reasoning parser to use for extracting reasoning content from the model output.  \n```bash\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \\\n--reasoning-parser deepseek_r1\n```  \nNext, make a request to the model that should return the reasoning content in the response.  \n??? code  \n```python\nfrom openai import OpenAI\n\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id", "file_path": "features/reasoning_outputs.md"}
{"id": "45c29a6b2ff5f6a2292dc467fa058671f46b3ab65ef97a76ee4e326d6ae74654", "heading": "Reasoning Outputs/Quickstart", "level": 2, "text": "models = client.models.list()\nmodel = models.data[0].id\n\n# Round 1\nmessages = [{\"role\": \"user\", \"content\": \"9.11 and 9.8, which is greater?\"}]\n# For granite, add: `extra_body={\"chat_template_kwargs\": {\"thinking\": True}}`\n# For Qwen3 series, if you want to disable thinking in reasoning mode, add:\n# extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}}\nresponse = client.chat.completions.create(model=model, messages=messages)\n\nreasoning_content = response.choices[0].message.reasoning_content\ncontent = response.choices[0].message.content\n\nprint(\"reasoning_content:\", reasoning_content)\nprint(\"content:\", content)\n```  \nThe `reasoning_content` field contains the reasoning steps that led to the final conclusion, while the `content` field contains the final conclusion.", "file_path": "features/reasoning_outputs.md"}
{"id": "45c29a6b2ff5f6a2292dc467fa058671f46b3ab65ef97a76ee4e326d6ae74654", "heading": "Reasoning Outputs/Streaming chat completions", "level": 2, "text": "## Streaming chat completions  \nStreaming chat completions are also supported for reasoning models. The `reasoning_content` field is available in the `delta` field in [chat completion response chunks](https://platform.openai.com/docs/api-reference/chat/streaming).  \n??? console \"Json\"  \n```json\n{\n\"id\": \"chatcmpl-123\",\n\"object\": \"chat.completion.chunk\",\n\"created\": 1694268190,\n\"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n\"system_fingerprint\": \"fp_44709d6fcb\",\n\"choices\": [\n{\n\"index\": 0,\n\"delta\": {\n\"role\": \"assistant\",\n\"reasoning_content\": \"is\",\n},\n\"logprobs\": null,\n\"finish_reason\": null\n}\n]\n}\n```  \nOpenAI Python client library does not officially support `reasoning_content` attribute for streaming output. But the client supports extra attributes in the response. You can use `hasattr` to check if the `reasoning_content` attribute is present in the response. For example:  \n??? code  \n```python\nfrom openai import OpenAI", "file_path": "features/reasoning_outputs.md"}
{"id": "45c29a6b2ff5f6a2292dc467fa058671f46b3ab65ef97a76ee4e326d6ae74654", "heading": "Reasoning Outputs/Streaming chat completions", "level": 2, "text": "# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\nmessages = [{\"role\": \"user\", \"content\": \"9.11 and 9.8, which is greater?\"}]\n# For granite, add: `extra_body={\"chat_template_kwargs\": {\"thinking\": True}}`\n# For Qwen3 series, if you want to disable thinking in reasoning mode, add:\n# extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}}\nstream = client.chat.completions.create(\nmodel=model,\nmessages=messages,\nstream=True,\n)\n\nprint(\"client: Start streaming chat completions...\")\nprinted_reasoning_content = False\nprinted_content = False", "file_path": "features/reasoning_outputs.md"}
{"id": "45c29a6b2ff5f6a2292dc467fa058671f46b3ab65ef97a76ee4e326d6ae74654", "heading": "Reasoning Outputs/Streaming chat completions", "level": 2, "text": "for chunk in stream:\n# Safely extract reasoning_content and content from delta,\n# defaulting to None if attributes don't exist or are empty strings\nreasoning_content = (\ngetattr(chunk.choices[0].delta, \"reasoning_content\", None) or None\n)\ncontent = getattr(chunk.choices[0].delta, \"content\", None) or None\n\nif reasoning_content is not None:\nif not printed_reasoning_content:\nprinted_reasoning_content = True\nprint(\"reasoning_content:\", end=\"\", flush=True)\nprint(reasoning_content, end=\"\", flush=True)\nelif content is not None:\nif not printed_content:\nprinted_content = True\nprint(\"\\ncontent:\", end=\"\", flush=True)\n# Extract and print the content\nprint(content, end=\"\", flush=True)\n```  \nRemember to check whether the `reasoning_content` exists in the response before accessing it. You could check out the [example](https://github.com/vllm-project/vllm/blob/main/examples/online_serving/openai_chat_completion_with_reasoning_streaming.py).", "file_path": "features/reasoning_outputs.md"}
{"id": "45c29a6b2ff5f6a2292dc467fa058671f46b3ab65ef97a76ee4e326d6ae74654", "heading": "Reasoning Outputs/Tool Calling", "level": 2, "text": "## Tool Calling  \nThe reasoning content is also available when both tool calling and the reasoning parser are enabled. Additionally, tool calling only parses functions from the `content` field, not from the `reasoning_content`.  \n??? code  \n```python\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"dummy\")\n\ntools = [\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_weather\",\n\"description\": \"Get the current weather in a given location\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\"type\": \"string\", \"description\": \"City and state, e.g., 'San Francisco, CA'\"},\n\"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n},\n\"required\": [\"location\", \"unit\"],\n}\n},\n}\n]\n\nresponse = client.chat.completions.create(\nmodel=client.models.list().data[0].id,\nmessages=[{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}],\ntools=tools,\ntool_choice=\"auto\",\n)\n\nprint(response)\ntool_call = response.choices[0].message.tool_calls[0].function", "file_path": "features/reasoning_outputs.md"}
{"id": "45c29a6b2ff5f6a2292dc467fa058671f46b3ab65ef97a76ee4e326d6ae74654", "heading": "Reasoning Outputs/Tool Calling", "level": 2, "text": "print(f\"reasoning_content: {response.choices[0].message.reasoning_content}\")\nprint(f\"Function called: {tool_call.name}\")\nprint(f\"Arguments: {tool_call.arguments}\")\n```  \nFor more examples, please refer to <gh-file:examples/online_serving/openai_chat_completion_tool_calls_with_reasoning.py>.", "file_path": "features/reasoning_outputs.md"}
{"id": "45c29a6b2ff5f6a2292dc467fa058671f46b3ab65ef97a76ee4e326d6ae74654", "heading": "Reasoning Outputs/Limitations", "level": 2, "text": "## Limitations  \n- The reasoning content is only available for online serving's chat completion endpoint (`/v1/chat/completions`).", "file_path": "features/reasoning_outputs.md"}
{"id": "45c29a6b2ff5f6a2292dc467fa058671f46b3ab65ef97a76ee4e326d6ae74654", "heading": "Reasoning Outputs/How to support a new reasoning model", "level": 2, "text": "## How to support a new reasoning model  \nYou can add a new `ReasoningParser` similar to <gh-file:vllm/reasoning/deepseek_r1_reasoning_parser.py>.  \n??? code  \n```python\n# import the required packages\n\nfrom vllm.reasoning import ReasoningParser, ReasoningParserManager\nfrom vllm.entrypoints.openai.protocol import (ChatCompletionRequest,\nDeltaMessage)\n\n# define a reasoning parser and register it to vllm\n# the name list in register_module can be used\n# in --reasoning-parser.\n@ReasoningParserManager.register_module([\"example\"])\nclass ExampleParser(ReasoningParser):\ndef __init__(self, tokenizer: AnyTokenizer):\nsuper().__init__(tokenizer)", "file_path": "features/reasoning_outputs.md"}
{"id": "45c29a6b2ff5f6a2292dc467fa058671f46b3ab65ef97a76ee4e326d6ae74654", "heading": "Reasoning Outputs/How to support a new reasoning model", "level": 2, "text": "def extract_reasoning_content_streaming(\nself,\nprevious_text: str,\ncurrent_text: str,\ndelta_text: str,\nprevious_token_ids: Sequence[int],\ncurrent_token_ids: Sequence[int],\ndelta_token_ids: Sequence[int],\n) -> DeltaMessage | None:\n\"\"\"\nInstance method that should be implemented for extracting reasoning\nfrom an incomplete response; for use when handling reasoning calls and\nstreaming. Has to be an instance method because  it requires state -\nthe current tokens/diffs, but also the information about what has\npreviously been parsed and extracted (see constructor)\n\"\"\"\n\ndef extract_reasoning_content(\nself,\nmodel_output: str,\nrequest: ChatCompletionRequest | ResponsesRequest,\n) -> tuple[str | None, str | None]:\n\"\"\"\nExtract reasoning content from a complete model-generated string.\n\nUsed for non-streaming responses where we have the entire model response\navailable before sending to the client.\n\nParameters:\nmodel_output: str\nThe model-generated string to extract reasoning content from.", "file_path": "features/reasoning_outputs.md"}
{"id": "45c29a6b2ff5f6a2292dc467fa058671f46b3ab65ef97a76ee4e326d6ae74654", "heading": "Reasoning Outputs/How to support a new reasoning model", "level": 2, "text": "request: ChatCompletionRequest\nThe request object that was used to generate the model_output.\n\nReturns:\ntuple[Optional[str], Optional[str]]\nA tuple containing the reasoning content and the content.\n\"\"\"\n```  \nAdditionally, to enable structured output, you'll need to create a new `Reasoner` similar to the one in <gh-file:vllm/reasoning/deepseek_r1_reasoning_parser.py>.  \n??? code  \n```python\n@dataclass\nclass DeepSeekReasoner(Reasoner):\n\"\"\"\nReasoner for DeepSeek R series models.\n\"\"\"\nstart_token_id: int\nend_token_id: int\n\nstart_token: str = \"<think>\"\nend_token: str = \"</think>\"\n\n@classmethod\ndef from_tokenizer(cls, tokenizer: PreTrainedTokenizer) -> Reasoner:\nreturn cls(\nstart_token_id=tokenizer.encode(\"<think>\", add_special_tokens=False)[0],\nend_token_id=tokenizer.encode(\"</think>\", add_special_tokens=False)[0],\n)", "file_path": "features/reasoning_outputs.md"}
{"id": "45c29a6b2ff5f6a2292dc467fa058671f46b3ab65ef97a76ee4e326d6ae74654", "heading": "Reasoning Outputs/How to support a new reasoning model", "level": 2, "text": "def is_reasoning_end(self, input_ids: list[int]) -> bool:\nreturn self.end_token_id in input_ids\n...\n```  \nThe structured output engine like [xgrammar](https://github.com/mlc-ai/xgrammar) will use `end_token_id` to check if the reasoning content is present in the model output and skip the structured output if it is the case.  \nFinally, you can enable reasoning for the model by using the `--reasoning-parser` flags.  \n```bash\nvllm serve <model_tag> --reasoning-parser example\n```", "file_path": "features/reasoning_outputs.md"}
{"id": "19caa863edaa6df7d858b5802069d6f489a52d8e3e01d39520d5b0e6861780d0", "heading": "Sleep Mode", "level": 1, "text": "# Sleep Mode  \nvLLM'sSleep Modeallows you to temporarily release most GPU memory used by a model, including model weights and KV cache, without stopping the server or unloading the Docker container. This is especially useful for RLHF, training, or cost-saving scenarios where GPU resources need to be freed between inference workloads.  \nKey benefits:  \n- **Frees GPU memory**: Offloads model weights to CPU RAM and discards KV cache, releasing up to 90%+ of GPU memory for other tasks.\n- **Fast resume**: Quickly wake up the engine and resume inference without full model reload.\n- **API endpoints**: Control sleep/wake_up state via HTTP endpoints or Python API.\n- **Supports distributed workloads**: Works with tensor parallelism, pipeline parallelism, etc.\n- **Fine-grained control**: Optionally wake up only model weights or KV cache to avoid OOM during weight updates.  \n!!! note\nThis feature is only supported on CUDA platform.", "file_path": "features/sleep_mode.md"}
{"id": "19caa863edaa6df7d858b5802069d6f489a52d8e3e01d39520d5b0e6861780d0", "heading": "Sleep Mode/Sleep levels", "level": 2, "text": "## Sleep levels  \nLevel 1 sleep will offload the model weights and discard the KV cache. The content of KV cache is forgotten. Level 1 sleep is good for sleeping and waking up the engine to run the same model again. The model weights are backed up in CPU memory. Please make sure there's enough CPU memory to store the model weights. Level 2 sleep will discard both the model weights and the KV cache (while the model's buffers are kept in CPU, like rope scaling tensors). The content of both the model weights and KV cache is forgotten. Level 2 sleep is good for sleeping and waking up the engine to run a different model or update the model, where previous model weights are not needed, e.g. RLHF weight update.", "file_path": "features/sleep_mode.md"}
{"id": "19caa863edaa6df7d858b5802069d6f489a52d8e3e01d39520d5b0e6861780d0", "heading": "Sleep Mode/Usage/Offline inference", "level": 3, "text": "## Usage  \n### Offline inference  \nEnable sleep mode by passing`enable_sleep_mode=True`to the`LLM`class.  \n```python\nfrom vllm import LLM\nllm = LLM(\"Qwen/Qwen3-0.6B\", enable_sleep_mode=True)\n```  \n#### Python API  \n```python\n# Put the engine to sleep (level=1: offload weights to CPU RAM, discard KV cache)\nllm.sleep(level=1)", "file_path": "features/sleep_mode.md"}
{"id": "19caa863edaa6df7d858b5802069d6f489a52d8e3e01d39520d5b0e6861780d0", "heading": "Sleep Mode/Usage/Offline inference", "level": 3, "text": "# Wake up the engine (restore weights)\nllm.wake_up()\n```  \n#### RLHF weight updates  \nDuring RLHF training, vLLM allows you to selectively wake up only the model weights or the KV cache using the tags argument in wake_up(). This fine-grained control is especially useful when updating model weights: by waking up just the weights (e.g., llm.wake_up(tags=[\"weights\"])), you avoid allocating memory for the KV cache until after the weight update is complete. This approach helps prevent GPU out-of-memory (OOM) errors, particularly with large models, by minimizing peak memory usage during weight synchronization and update operations.  \nUse `tags=[\"weights\"]` or `tags=[\"kv_cache\"]` to control which resources are restored, useful for RLHF and weight updates. **Note** that `is_sleeping` will report `true` until all components are awake.  \n```python\n# Put engine to deep sleep (level=2)\nllm.sleep(level=2)\n# ... Get the new weights\n# Wake up only weights to avoid OOM\nllm.wake_up(tags=[\"weights\"])\n# ... Update the weights", "file_path": "features/sleep_mode.md"}
{"id": "19caa863edaa6df7d858b5802069d6f489a52d8e3e01d39520d5b0e6861780d0", "heading": "Sleep Mode/Usage/Offline inference", "level": 3, "text": "llm.wake_up(tags=[\"weights\"])\n# ... Update the weights\n# wake up KV cache after weights are updated\nllm.wake_up(tags=[\"kv_cache\"])\n```", "file_path": "features/sleep_mode.md"}
{"id": "19caa863edaa6df7d858b5802069d6f489a52d8e3e01d39520d5b0e6861780d0", "heading": "Sleep Mode/Usage/Online Serving", "level": 3, "text": "### Online Serving  \nTo enable sleep mode in a vLLM server you need to initialize it with the flag `VLLM_SERVER_DEV_MODE=1` and pass `--enable-sleep-mode` to the vLLM server.  \n#### Server in development mode  \nWhen using the flag `VLLM_SERVER_DEV_MODE=1` you enable development endpoints, and these endpoints should not be exposed to users.  \n```bash\nVLLM_SERVER_DEV_MODE=1 vllm serve Qwen/Qwen3-0.6B \\\n--enable-sleep-mode \\\n--port 8000\n```  \n#### HTTP endpoints  \n- `POST /sleep?level=1`â€” Put the model to sleep (`level=1`).\n- `POST /wake_up` â€” Wake up the model. Supports optional `tags` query parameters for partial wake-up (e.g., `?tags=weights`).\n- `GET /is_sleeping`â€” Check if the model is sleeping.  \n!!! note\nThese endpoints are only available when passing `VLLM_SERVER_DEV_MODE=1`.", "file_path": "features/sleep_mode.md"}
{"id": "762131b89ee97a0c153f891f61c8206a4c663e204fe3d5ec06f63a407c8cfb58", "heading": "Speculative Decoding", "level": 1, "text": "# Speculative Decoding  \n!!! warning\nPlease note that speculative decoding in vLLM is not yet optimized and does\nnot usually yield inter-token latency reductions for all prompt datasets or sampling parameters.\nThe work to optimize it is ongoing and can be followed here: <gh-issue:4630>  \n!!! warning\nCurrently, speculative decoding in vLLM is not compatible with pipeline parallelism.  \nThis document shows how to use [Speculative Decoding](https://x.com/karpathy/status/1697318534555336961) with vLLM.\nSpeculative decoding is a technique which improves inter-token latency in memory-bound LLM inference.", "file_path": "features/spec_decode.md"}
{"id": "762131b89ee97a0c153f891f61c8206a4c663e204fe3d5ec06f63a407c8cfb58", "heading": "Speculative Decoding/Speculating with a draft model", "level": 2, "text": "## Speculating with a draft model  \nThe following code configures vLLM in an offline mode to use speculative decoding with a draft model, speculating 5 tokens at a time.  \n!!! warning\nIn vllm v0.10.0, speculative decoding with a draft model is not supported.\nIf you use the following code, you will get a `NotImplementedError`.  \n??? code  \n```python\nfrom vllm import LLM, SamplingParams\n\nprompts = [\n\"The future of AI is\",\n]\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\nllm = LLM(\nmodel=\"facebook/opt-6.7b\",\ntensor_parallel_size=1,\nspeculative_config={\n\"model\": \"facebook/opt-125m\",\n\"num_speculative_tokens\": 5,\n},\n)\noutputs = llm.generate(prompts, sampling_params)", "file_path": "features/spec_decode.md"}
{"id": "762131b89ee97a0c153f891f61c8206a4c663e204fe3d5ec06f63a407c8cfb58", "heading": "Speculative Decoding/Speculating with a draft model", "level": 2, "text": "for output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```  \nTo perform the same with an online mode launch the server:  \n```bash\nvllm serve facebook/opt-6.7b \\\n--host 0.0.0.0 \\\n--port 8000 \\\n--seed 42 \\\n-tp 1 \\\n--gpu_memory_utilization 0.8 \\\n--speculative_config '{\"model\": \"facebook/opt-125m\", \"num_speculative_tokens\": 5}'\n```  \n!!! warning\nNote: Please use `--speculative_config` to set all configurations related to speculative decoding. The previous method of specifying the model through `--speculative_model` and adding related parameters (e.g., `--num_speculative_tokens`) separately has been deprecated now.  \nThen use a client:  \n??? code  \n```python\nfrom openai import OpenAI\n\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"", "file_path": "features/spec_decode.md"}
{"id": "762131b89ee97a0c153f891f61c8206a4c663e204fe3d5ec06f63a407c8cfb58", "heading": "Speculative Decoding/Speculating with a draft model", "level": 2, "text": "client = OpenAI(\n# defaults to os.environ.get(\"OPENAI_API_KEY\")\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\n# Completion API\nstream = False\ncompletion = client.completions.create(\nmodel=model,\nprompt=\"The future of AI is\",\necho=False,\nn=1,\nstream=stream,\n)\n\nprint(\"Completion results:\")\nif stream:\nfor c in completion:\nprint(c)\nelse:\nprint(completion)\n```", "file_path": "features/spec_decode.md"}
{"id": "762131b89ee97a0c153f891f61c8206a4c663e204fe3d5ec06f63a407c8cfb58", "heading": "Speculative Decoding/Speculating by matching n-grams in the prompt", "level": 2, "text": "## Speculating by matching n-grams in the prompt  \nThe following code configures vLLM to use speculative decoding where proposals are generated by\nmatching n-grams in the prompt. For more information read [this thread.](https://x.com/joao_gante/status/1747322413006643259)  \n??? code  \n```python\nfrom vllm import LLM, SamplingParams\n\nprompts = [\n\"The future of AI is\",\n]\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\nllm = LLM(\nmodel=\"facebook/opt-6.7b\",\ntensor_parallel_size=1,\nspeculative_config={\n\"method\": \"ngram\",\n\"num_speculative_tokens\": 5,\n\"prompt_lookup_max\": 4,\n},\n)\noutputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```", "file_path": "features/spec_decode.md"}
{"id": "762131b89ee97a0c153f891f61c8206a4c663e204fe3d5ec06f63a407c8cfb58", "heading": "Speculative Decoding/Speculating using MLP speculators", "level": 2, "text": "## Speculating using MLP speculators  \nThe following code configures vLLM to use speculative decoding where proposals are generated by\ndraft models that conditioning draft predictions on both context vectors and sampled tokens.\nFor more information see [this blog](https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/) or\n[this technical report](https://arxiv.org/abs/2404.19124).  \n??? code  \n```python\nfrom vllm import LLM, SamplingParams\n\nprompts = [\n\"The future of AI is\",\n]\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\nllm = LLM(\nmodel=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\ntensor_parallel_size=4,\nspeculative_config={\n\"model\": \"ibm-ai-platform/llama3-70b-accelerator\",\n\"draft_tensor_parallel_size\": 1,\n},\n)\noutputs = llm.generate(prompts, sampling_params)", "file_path": "features/spec_decode.md"}
{"id": "762131b89ee97a0c153f891f61c8206a4c663e204fe3d5ec06f63a407c8cfb58", "heading": "Speculative Decoding/Speculating using MLP speculators", "level": 2, "text": "for output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```  \nNote that these speculative models currently need to be run without tensor parallelism, although\nit is possible to run the main model using tensor parallelism (see example above). Since the\nspeculative models are relatively small, we still see significant speedups. However, this\nlimitation will be fixed in a future release.  \nA variety of speculative models of this type are available on HF hub:  \n- [llama-13b-accelerator](https://huggingface.co/ibm-ai-platform/llama-13b-accelerator)\n- [llama3-8b-accelerator](https://huggingface.co/ibm-ai-platform/llama3-8b-accelerator)\n- [codellama-34b-accelerator](https://huggingface.co/ibm-ai-platform/codellama-34b-accelerator)\n- [llama2-70b-accelerator](https://huggingface.co/ibm-ai-platform/llama2-70b-accelerator)\n- [llama3-70b-accelerator](https://huggingface.co/ibm-ai-platform/llama3-70b-accelerator)", "file_path": "features/spec_decode.md"}
{"id": "762131b89ee97a0c153f891f61c8206a4c663e204fe3d5ec06f63a407c8cfb58", "heading": "Speculative Decoding/Speculating using MLP speculators", "level": 2, "text": "- [granite-3b-code-instruct-accelerator](https://huggingface.co/ibm-granite/granite-3b-code-instruct-accelerator)\n- [granite-8b-code-instruct-accelerator](https://huggingface.co/ibm-granite/granite-8b-code-instruct-accelerator)\n- [granite-7b-instruct-accelerator](https://huggingface.co/ibm-granite/granite-7b-instruct-accelerator)\n- [granite-20b-code-instruct-accelerator](https://huggingface.co/ibm-granite/granite-20b-code-instruct-accelerator)", "file_path": "features/spec_decode.md"}
{"id": "762131b89ee97a0c153f891f61c8206a4c663e204fe3d5ec06f63a407c8cfb58", "heading": "Speculative Decoding/Speculating using EAGLE based draft models", "level": 2, "text": "## Speculating using EAGLE based draft models  \nThe following code configures vLLM to use speculative decoding where proposals are generated by\nan [EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency)](https://arxiv.org/pdf/2401.15077) based draft model. A more detailed example for offline mode, including how to extract request level acceptance rate, can be found [here](gh-file:examples/offline_inference/eagle.py).  \n??? code  \n```python\nfrom vllm import LLM, SamplingParams\n\nprompts = [\n\"The future of AI is\",\n]\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\nllm = LLM(\nmodel=\"meta-llama/Meta-Llama-3-8B-Instruct\",\ntensor_parallel_size=4,\nspeculative_config={\n\"model\": \"yuhuili/EAGLE-LLaMA3-Instruct-8B\",\n\"draft_tensor_parallel_size\": 1,\n\"num_speculative_tokens\": 2,\n\"method\": \"eagle\",\n},\n)\n\noutputs = llm.generate(prompts, sampling_params)", "file_path": "features/spec_decode.md"}
{"id": "762131b89ee97a0c153f891f61c8206a4c663e204fe3d5ec06f63a407c8cfb58", "heading": "Speculative Decoding/Speculating using EAGLE based draft models", "level": 2, "text": "outputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")", "file_path": "features/spec_decode.md"}
{"id": "762131b89ee97a0c153f891f61c8206a4c663e204fe3d5ec06f63a407c8cfb58", "heading": "Speculative Decoding/Speculating using EAGLE based draft models", "level": 2, "text": "```  \nA few important things to consider when using the EAGLE based draft models:  \n1. The EAGLE draft models available in the [HF repository for EAGLE models](https://huggingface.co/yuhuili) should\nbe able to be loaded and used directly by vLLM after <gh-pr:12304>.\nIf you are using vllm version before <gh-pr:12304>, please use the\n[script](https://gist.github.com/abhigoyal1997/1e7a4109ccb7704fbc67f625e86b2d6d) to convert the speculative model,\nand specify `\"model\": \"path/to/modified/eagle/model\"` in `speculative_config`. If weight-loading problems still occur when using the latest version of vLLM, please leave a comment or raise an issue.  \n2. The EAGLE based draft models need to be run without tensor parallelism\n(i.e. draft_tensor_parallel_size is set to 1 in `speculative_config`), although\nit is possible to run the main model using tensor parallelism (see example above).  \n3. When using EAGLE-based speculators with vLLM, the observed speedup is lower than what is", "file_path": "features/spec_decode.md"}
{"id": "762131b89ee97a0c153f891f61c8206a4c663e204fe3d5ec06f63a407c8cfb58", "heading": "Speculative Decoding/Speculating using EAGLE based draft models", "level": 2, "text": "reported in the reference implementation [here](https://github.com/SafeAILab/EAGLE). This issue is under\ninvestigation and tracked here: <gh-issue:9565>.  \n4. When using EAGLE-3 based draft model, option \"method\" must be set to \"eagle3\".\nThat is, to specify `\"method\": \"eagle3\"` in `speculative_config`.  \nA variety of EAGLE draft models are available on the Hugging Face hub:  \n| Base Model                                                           | EAGLE on Hugging Face                     | # EAGLE Parameters |\n|---------------------------------------------------------------------|-------------------------------------------|--------------------|\n| Vicuna-7B-v1.3                                                       | yuhuili/EAGLE-Vicuna-7B-v1.3             | 0.24B              |\n| Vicuna-13B-v1.3                                                      | yuhuili/EAGLE-Vicuna-13B-v1.3            | 0.37B              |", "file_path": "features/spec_decode.md"}
{"id": "762131b89ee97a0c153f891f61c8206a4c663e204fe3d5ec06f63a407c8cfb58", "heading": "Speculative Decoding/Speculating using EAGLE based draft models", "level": 2, "text": "| Vicuna-33B-v1.3                                                      | yuhuili/EAGLE-Vicuna-33B-v1.3            | 0.56B              |\n| LLaMA2-Chat 7B                                                       | yuhuili/EAGLE-llama2-chat-7B             | 0.24B              |\n| LLaMA2-Chat 13B                                                      | yuhuili/EAGLE-llama2-chat-13B            | 0.37B              |\n| LLaMA2-Chat 70B                                                      | yuhuili/EAGLE-llama2-chat-70B            | 0.99B              |\n| Mixtral-8x7B-Instruct-v0.1                                           | yuhuili/EAGLE-mixtral-instruct-8x7B      | 0.28B              |\n| LLaMA3-Instruct 8B                                                   | yuhuili/EAGLE-LLaMA3-Instruct-8B         | 0.25B              |\n| LLaMA3-Instruct 70B                                                  | yuhuili/EAGLE-LLaMA3-Instruct-70B        | 0.99B              |", "file_path": "features/spec_decode.md"}
{"id": "762131b89ee97a0c153f891f61c8206a4c663e204fe3d5ec06f63a407c8cfb58", "heading": "Speculative Decoding/Speculating using EAGLE based draft models", "level": 2, "text": "| Qwen2-7B-Instruct                                                    | yuhuili/EAGLE-Qwen2-7B-Instruct          | 0.26B              |\n| Qwen2-72B-Instruct                                                   | yuhuili/EAGLE-Qwen2-72B-Instruct         | 1.05B              |", "file_path": "features/spec_decode.md"}
{"id": "762131b89ee97a0c153f891f61c8206a4c663e204fe3d5ec06f63a407c8cfb58", "heading": "Speculative Decoding/Lossless guarantees of Speculative Decoding", "level": 2, "text": "## Lossless guarantees of Speculative Decoding  \nIn vLLM, speculative decoding aims to enhance inference efficiency while maintaining accuracy. This section addresses the lossless guarantees of\nspeculative decoding, breaking down the guarantees into three key areas:  \n1. **Theoretical Losslessness**\n\\- Speculative decoding sampling is theoretically lossless up to the precision limits of hardware numerics. Floating-point errors might\ncause slight variations in output distributions, as discussed\nin [Accelerating Large Language Model Decoding with Speculative Sampling](https://arxiv.org/pdf/2302.01318)  \n2. **Algorithmic Losslessness**\n\\- vLLMâ€™s implementation of speculative decoding is algorithmically validated to be lossless. Key validation tests include:  \n> - **Rejection Sampler Convergence**: Ensures that samples from vLLMâ€™s rejection sampler align with the target", "file_path": "features/spec_decode.md"}
{"id": "762131b89ee97a0c153f891f61c8206a4c663e204fe3d5ec06f63a407c8cfb58", "heading": "Speculative Decoding/Lossless guarantees of Speculative Decoding", "level": 2, "text": ">   distribution. [View Test Code](https://github.com/vllm-project/vllm/blob/47b65a550866c7ffbd076ecb74106714838ce7da/tests/samplers/test_rejection_sampler.py#L252)\n> - **Greedy Sampling Equality**: Confirms that greedy sampling with speculative decoding matches greedy sampling\n>   without it. This verifies that vLLM's speculative decoding framework, when integrated with the vLLM forward pass and the vLLM rejection sampler,\n>   provides a lossless guarantee. Almost all of the tests in <gh-dir:tests/spec_decode/e2e>.\n>   verify this property using [this assertion implementation](https://github.com/vllm-project/vllm/blob/b67ae00cdbbe1a58ffc8ff170f0c8d79044a684a/tests/spec_decode/e2e/conftest.py#L291)  \n3. **vLLM Logprob Stability**\n\\- vLLM does not currently guarantee stable token log probabilities (logprobs). This can result in different outputs for the\nsame request across runs. For more details, see the FAQ section", "file_path": "features/spec_decode.md"}
{"id": "762131b89ee97a0c153f891f61c8206a4c663e204fe3d5ec06f63a407c8cfb58", "heading": "Speculative Decoding/Lossless guarantees of Speculative Decoding", "level": 2, "text": "same request across runs. For more details, see the FAQ section\ntitled *Can the output of a prompt vary across runs in vLLM?* in the [FAQs](../usage/faq.md).  \nWhile vLLM strives to ensure losslessness in speculative decoding, variations in generated outputs with and without speculative decoding\ncan occur due to following factors:  \n- **Floating-Point Precision**: Differences in hardware numerical precision may lead to slight discrepancies in the output distribution.\n- **Batch Size and Numerical Stability**: Changes in batch size may cause variations in logprobs and output probabilities, potentially\ndue to non-deterministic behavior in batched operations or numerical instability.  \nFor mitigation strategies, please refer to the FAQ entry *Can the output of a prompt vary across runs in vLLM?* in the [FAQs](../usage/faq.md).", "file_path": "features/spec_decode.md"}
{"id": "762131b89ee97a0c153f891f61c8206a4c663e204fe3d5ec06f63a407c8cfb58", "heading": "Speculative Decoding/Resources for vLLM contributors", "level": 2, "text": "## Resources for vLLM contributors  \n- [A Hacker's Guide to Speculative Decoding in vLLM](https://www.youtube.com/watch?v=9wNAgpX6z_4)\n- [What is Lookahead Scheduling in vLLM?](https://docs.google.com/document/d/1Z9TvqzzBPnh5WHcRwjvK2UEeFeq5zMZb5mFE8jR0HCs/edit#heading=h.1fjfb0donq5a)\n- [Information on batch expansion](https://docs.google.com/document/d/1T-JaS2T1NRfdP51qzqpyakoCXxSXTtORppiwaj5asxA/edit#heading=h.kk7dq05lc6q8)\n- [Dynamic speculative decoding](gh-issue:4565)", "file_path": "features/spec_decode.md"}
{"id": "20773bfc08175deef801b49da755f62c20121688808ee8a2742029b66fd2276a", "heading": "Structured Outputs", "level": 1, "text": "# Structured Outputs  \nvLLM supports the generation of structured outputs using\n[xgrammar](https://github.com/mlc-ai/xgrammar) or\n[guidance](https://github.com/guidance-ai/llguidance) as backends.\nThis document shows you some examples of the different options that are\navailable to generate structured outputs.  \n!!! warning\nIf you are still using the following deprecated API fields, please update your code to use `structured_outputs` as demonstrated in the rest of this document:  \n- `guided_json` -> `{\"structured_outputs\": {\"json\": ...}}` or `StructuredOutputsParams(json=...)`\n- `guided_regex` -> `{\"structured_outputs\": {\"regex\": ...}}` or `StructuredOutputsParams(regex=...)`\n- `guided_choice` -> `{\"structured_outputs\": {\"choice\": ...}}` or `StructuredOutputsParams(choice=...)`\n- `guided_grammar` -> `{\"structured_outputs\": {\"grammar\": ...}}` or `StructuredOutputsParams(grammar=...)`", "file_path": "features/structured_outputs.md"}
{"id": "20773bfc08175deef801b49da755f62c20121688808ee8a2742029b66fd2276a", "heading": "Structured Outputs", "level": 1, "text": "- `guided_whitespace_pattern` -> `{\"structured_outputs\": {\"whitespace_pattern\": ...}}` or `StructuredOutputsParams(whitespace_pattern=...)`\n- `structural_tag` -> `{\"structured_outputs\": {\"structural_tag\": ...}}` or `StructuredOutputsParams(structural_tag=...)`\n- `guided_decoding_backend` -> Remove this field from your request", "file_path": "features/structured_outputs.md"}
{"id": "20773bfc08175deef801b49da755f62c20121688808ee8a2742029b66fd2276a", "heading": "Structured Outputs/Online Serving (OpenAI API)", "level": 2, "text": "## Online Serving (OpenAI API)  \nYou can generate structured outputs using the OpenAI's [Completions](https://platform.openai.com/docs/api-reference/completions) and [Chat](https://platform.openai.com/docs/api-reference/chat) API.  \nThe following parameters are supported, which must be added as extra parameters:  \n- `choice`: the output will be exactly one of the choices.\n- `regex`: the output will follow the regex pattern.\n- `json`: the output will follow the JSON schema.\n- `grammar`: the output will follow the context free grammar.\n- `structural_tag`: Follow a JSON schema within a set of specified tags within the generated text.  \nYou can see the complete list of supported parameters on the [OpenAI-Compatible Server](../serving/openai_compatible_server.md) page.  \nStructured outputs are supported by default in the OpenAI-Compatible Server. You\nmay choose to specify the backend to use by setting the\n`--structured-outputs-config.backend` flag to `vllm serve`. The default backend is `auto`,", "file_path": "features/structured_outputs.md"}
{"id": "20773bfc08175deef801b49da755f62c20121688808ee8a2742029b66fd2276a", "heading": "Structured Outputs/Online Serving (OpenAI API)", "level": 2, "text": "which will try to choose an appropriate backend based on the details of the\nrequest. You may also choose a specific backend, along with\nsome options. A full set of options is available in the `vllm serve --help`\ntext.  \nNow letÂ´s see an example for each of the cases, starting with the `choice`, as itÂ´s the easiest one:  \n??? code  \n```python\nfrom openai import OpenAI\nclient = OpenAI(\nbase_url=\"http://localhost:8000/v1\",\napi_key=\"-\",\n)\nmodel = client.models.list().data[0].id", "file_path": "features/structured_outputs.md"}
{"id": "20773bfc08175deef801b49da755f62c20121688808ee8a2742029b66fd2276a", "heading": "Structured Outputs/Online Serving (OpenAI API)", "level": 2, "text": "completion = client.chat.completions.create(\nmodel=model,\nmessages=[\n{\"role\": \"user\", \"content\": \"Classify this sentiment: vLLM is wonderful!\"}\n],\nextra_body={\"structured_outputs\": {\"choice\": [\"positive\", \"negative\"]}},\n)\nprint(completion.choices[0].message.content)\n```  \nThe next example shows how to use the `regex`. The idea is to generate an email address, given a simple regex template:  \n??? code  \n```python\ncompletion = client.chat.completions.create(\nmodel=model,\nmessages=[\n{\n\"role\": \"user\",\n\"content\": \"Generate an example email address for Alan Turing, who works in Enigma. End in .com and new line. Example result: alan.turing@enigma.com\\n\",\n}\n],\nextra_body={\"structured_outputs\": {\"regex\": r\"\\w+@\\w+\\.com\\n\"}, \"stop\": [\"\\n\"]},\n)\nprint(completion.choices[0].message.content)\n```  \nOne of the most relevant features in structured text generation is the option to generate a valid JSON with pre-defined fields and formats.\nFor this we can use the `json` parameter in two different ways:", "file_path": "features/structured_outputs.md"}
{"id": "20773bfc08175deef801b49da755f62c20121688808ee8a2742029b66fd2276a", "heading": "Structured Outputs/Online Serving (OpenAI API)", "level": 2, "text": "- Using directly a [JSON Schema](https://json-schema.org/)\n- Defining a [Pydantic model](https://docs.pydantic.dev/latest/) and then extracting the JSON Schema from it (which is normally an easier option).  \nThe next example shows how to use the `response_format` parameter with a Pydantic model:  \n??? code  \n```python\nfrom pydantic import BaseModel\nfrom enum import Enum", "file_path": "features/structured_outputs.md"}
{"id": "20773bfc08175deef801b49da755f62c20121688808ee8a2742029b66fd2276a", "heading": "Structured Outputs/Online Serving (OpenAI API)", "level": 2, "text": "class CarType(str, Enum):\nsedan = \"sedan\"\nsuv = \"SUV\"\ntruck = \"Truck\"\ncoupe = \"Coupe\"\n\nclass CarDescription(BaseModel):\nbrand: str\nmodel: str\ncar_type: CarType\n\njson_schema = CarDescription.model_json_schema()", "file_path": "features/structured_outputs.md"}
{"id": "20773bfc08175deef801b49da755f62c20121688808ee8a2742029b66fd2276a", "heading": "Structured Outputs/Online Serving (OpenAI API)", "level": 2, "text": "json_schema = CarDescription.model_json_schema()\n\ncompletion = client.chat.completions.create(\nmodel=model,\nmessages=[\n{\n\"role\": \"user\",\n\"content\": \"Generate a JSON with the brand, model and car_type of the most iconic car from the 90's\",\n}\n],\nresponse_format={\n\"type\": \"json_schema\",\n\"json_schema\": {\n\"name\": \"car-description\",\n\"schema\": CarDescription.model_json_schema()\n},\n},\n)\nprint(completion.choices[0].message.content)\n```  \n!!! tip\nWhile not strictly necessary, normally itÂ´s better to indicate in the prompt the\nJSON schema and how the fields should be populated. This can improve the\nresults notably in most cases.  \nFinally we have the `grammar` option, which is probably the most\ndifficult to use, but itÂ´s really powerful. It allows us to define complete\nlanguages like SQL queries. It works by using a context free EBNF grammar.\nAs an example, we can use to define a specific format of simplified SQL queries:  \n??? code  \n```python\nsimplified_sql_grammar = \"\"\"\nroot ::= select_statement", "file_path": "features/structured_outputs.md"}
{"id": "20773bfc08175deef801b49da755f62c20121688808ee8a2742029b66fd2276a", "heading": "Structured Outputs/Online Serving (OpenAI API)", "level": 2, "text": "select_statement ::= \"SELECT \" column \" from \" table \" where \" condition\n\ncolumn ::= \"col_1 \" | \"col_2 \"\n\ntable ::= \"table_1 \" | \"table_2 \"\n\ncondition ::= column \"= \" number\n\nnumber ::= \"1 \" | \"2 \"\n\"\"\"\n\ncompletion = client.chat.completions.create(\nmodel=model,\nmessages=[\n{\n\"role\": \"user\",\n\"content\": \"Generate an SQL query to show the 'username' and 'email' from the 'users' table.\",\n}\n],\nextra_body={\"structured_outputs\": {\"grammar\": simplified_sql_grammar}},\n)\nprint(completion.choices[0].message.content)\n```  \nSee also: [full example](../examples/online_serving/structured_outputs.md)", "file_path": "features/structured_outputs.md"}
{"id": "20773bfc08175deef801b49da755f62c20121688808ee8a2742029b66fd2276a", "heading": "Structured Outputs/Reasoning Outputs", "level": 2, "text": "## Reasoning Outputs  \nYou can also use structured outputs with <project:#reasoning-outputs> for reasoning models.  \n```bash\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --reasoning-parser deepseek_r1\n```  \nNote that you can use reasoning with any provided structured outputs feature. The following uses one with JSON schema:  \n??? code  \n```python\nfrom pydantic import BaseModel\n\n\nclass People(BaseModel):\nname: str\nage: int\n\n\ncompletion = client.chat.completions.create(\nmodel=model,\nmessages=[\n{\n\"role\": \"user\",\n\"content\": \"Generate a JSON with the name and age of one random person.\",\n}\n],\nresponse_format={\n\"type\": \"json_schema\",\n\"json_schema\": {\n\"name\": \"people\",\n\"schema\": People.model_json_schema()\n}\n},\n)\nprint(\"reasoning_content: \", completion.choices[0].message.reasoning_content)\nprint(\"content: \", completion.choices[0].message.content)\n```  \nSee also: [full example](../examples/online_serving/structured_outputs.md)", "file_path": "features/structured_outputs.md"}
{"id": "20773bfc08175deef801b49da755f62c20121688808ee8a2742029b66fd2276a", "heading": "Structured Outputs/Experimental Automatic Parsing (OpenAI API)", "level": 2, "text": "## Experimental Automatic Parsing (OpenAI API)  \nThis section covers the OpenAI beta wrapper over the `client.chat.completions.create()` method that provides richer integrations with Python specific types.  \nAt the time of writing (`openai==1.54.4`), this is a \"beta\" feature in the OpenAI client library. Code reference can be found [here](https://github.com/openai/openai-python/blob/52357cff50bee57ef442e94d78a0de38b4173fc2/src/openai/resources/beta/chat/completions.py#L100-L104).  \nFor the following examples, vLLM was set up using `vllm serve meta-llama/Llama-3.1-8B-Instruct`  \nHere is a simple example demonstrating how to get structured output using Pydantic models:  \n??? code  \n```python\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclass Info(BaseModel):\nname: str\nage: int", "file_path": "features/structured_outputs.md"}
{"id": "20773bfc08175deef801b49da755f62c20121688808ee8a2742029b66fd2276a", "heading": "Structured Outputs/Experimental Automatic Parsing (OpenAI API)", "level": 2, "text": "class Info(BaseModel):\nname: str\nage: int\n\nclient = OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"dummy\")\nmodel = client.models.list().data[0].id\ncompletion = client.beta.chat.completions.parse(\nmodel=model,\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"My name is Cameron, I'm 28. What's my name and age?\"},\n],\nresponse_format=Info,\n)\n\nmessage = completion.choices[0].message\nprint(message)\nassert message.parsed\nprint(\"Name:\", message.parsed.name)\nprint(\"Age:\", message.parsed.age)\n```  \n```console\nParsedChatCompletionMessage[Testing](content='{\"name\": \"Cameron\", \"age\": 28}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Testing(name='Cameron', age=28))\nName: Cameron\nAge: 28\n```  \nHere is a more complex example using nested Pydantic models to handle a step-by-step math solution:  \n??? code  \n```python\nfrom typing import List\nfrom pydantic import BaseModel\nfrom openai import OpenAI", "file_path": "features/structured_outputs.md"}
{"id": "20773bfc08175deef801b49da755f62c20121688808ee8a2742029b66fd2276a", "heading": "Structured Outputs/Experimental Automatic Parsing (OpenAI API)", "level": 2, "text": "class Step(BaseModel):\nexplanation: str\noutput: str\n\nclass MathResponse(BaseModel):\nsteps: list[Step]\nfinal_answer: str\n\ncompletion = client.beta.chat.completions.parse(\nmodel=model,\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful expert math tutor.\"},\n{\"role\": \"user\", \"content\": \"Solve 8x + 31 = 2.\"},\n],\nresponse_format=MathResponse,\n)", "file_path": "features/structured_outputs.md"}
{"id": "20773bfc08175deef801b49da755f62c20121688808ee8a2742029b66fd2276a", "heading": "Structured Outputs/Experimental Automatic Parsing (OpenAI API)", "level": 2, "text": "message = completion.choices[0].message\nprint(message)\nassert message.parsed\nfor i, step in enumerate(message.parsed.steps):\nprint(f\"Step #{i}:\", step)\nprint(\"Answer:\", message.parsed.final_answer)\n```  \nOutput:  \n```console", "file_path": "features/structured_outputs.md"}
{"id": "20773bfc08175deef801b49da755f62c20121688808ee8a2742029b66fd2276a", "heading": "Structured Outputs/Experimental Automatic Parsing (OpenAI API)", "level": 2, "text": "ParsedChatCompletionMessage[MathResponse](content='{ \"steps\": [{ \"explanation\": \"First, let\\'s isolate the term with the variable \\'x\\'. To do this, we\\'ll subtract 31 from both sides of the equation.\", \"output\": \"8x + 31 - 31 = 2 - 31\"}, { \"explanation\": \"By subtracting 31 from both sides, we simplify the equation to 8x = -29.\", \"output\": \"8x = -29\"}, { \"explanation\": \"Next, let\\'s isolate \\'x\\' by dividing both sides of the equation by 8.\", \"output\": \"8x / 8 = -29 / 8\"}], \"final_answer\": \"x = -29/8\" }', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=MathResponse(steps=[Step(explanation=\"First, let's isolate the term with the variable 'x'. To do this, we'll subtract 31 from both sides of the equation.\", output='8x + 31 - 31 = 2 - 31'), Step(explanation='By subtracting 31 from both sides, we simplify the equation to 8x = -29.', output='8x = -29'), Step(explanation=\"Next, let's isolate 'x' by dividing both sides of the equation by 8.\", output='8x / 8 = -29 / 8')],", "file_path": "features/structured_outputs.md"}
{"id": "20773bfc08175deef801b49da755f62c20121688808ee8a2742029b66fd2276a", "heading": "Structured Outputs/Experimental Automatic Parsing (OpenAI API)", "level": 2, "text": "both sides of the equation by 8.\", output='8x / 8 = -29 / 8')], final_answer='x = -29/8'))", "file_path": "features/structured_outputs.md"}
{"id": "20773bfc08175deef801b49da755f62c20121688808ee8a2742029b66fd2276a", "heading": "Structured Outputs/Experimental Automatic Parsing (OpenAI API)", "level": 2, "text": "Step #0: explanation=\"First, let's isolate the term with the variable 'x'. To do this, we'll subtract 31 from both sides of the equation.\" output='8x + 31 - 31 = 2 - 31'\nStep #1: explanation='By subtracting 31 from both sides, we simplify the equation to 8x = -29.' output='8x = -29'\nStep #2: explanation=\"Next, let's isolate 'x' by dividing both sides of the equation by 8.\" output='8x / 8 = -29 / 8'\nAnswer: x = -29/8\n```  \nAn example of using `structural_tag` can be found here: <gh-file:examples/online_serving/structured_outputs>", "file_path": "features/structured_outputs.md"}
{"id": "20773bfc08175deef801b49da755f62c20121688808ee8a2742029b66fd2276a", "heading": "Structured Outputs/Offline Inference", "level": 2, "text": "## Offline Inference  \nOffline inference allows for the same types of structured outputs.\nTo use it, weÂ´ll need to configure the structured outputs using the class `StructuredOutputsParams` inside `SamplingParams`.\nThe main available options inside `StructuredOutputsParams` are:  \n- `json`\n- `regex`\n- `choice`\n- `grammar`\n- `structural_tag`  \nThese parameters can be used in the same way as the parameters from the Online\nServing examples above. One example for the usage of the `choice` parameter is\nshown below:  \n??? code  \n```python\nfrom vllm import LLM, SamplingParams\nfrom vllm.sampling_params import StructuredOutputsParams\n\nllm = LLM(model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")", "file_path": "features/structured_outputs.md"}
{"id": "20773bfc08175deef801b49da755f62c20121688808ee8a2742029b66fd2276a", "heading": "Structured Outputs/Offline Inference", "level": 2, "text": "llm = LLM(model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n\nstructured_outputs_params = StructuredOutputsParams(choice=[\"Positive\", \"Negative\"])\nsampling_params = SamplingParams(structured_outputs=structured_outputs_params)\noutputs = llm.generate(\nprompts=\"Classify this sentiment: vLLM is wonderful!\",\nsampling_params=sampling_params,\n)\nprint(outputs[0].outputs[0].text)\n```  \nSee also: [full example](../examples/online_serving/structured_outputs.md)", "file_path": "features/structured_outputs.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling", "level": 1, "text": "# Tool Calling  \nvLLM currently supports named function calling, as well as the `auto`, `required` (as of `vllm>=0.8.3`), and `none` options for the `tool_choice` field in the chat completion API.", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Quickstart", "level": 2, "text": "## Quickstart  \nStart the server with tool calling enabled. This example uses Meta's Llama 3.1 8B model, so we need to use the `llama3_json` tool calling chat template from the vLLM examples directory:  \n```bash\nvllm serve meta-llama/Llama-3.1-8B-Instruct \\\n--enable-auto-tool-choice \\\n--tool-call-parser llama3_json \\\n--chat-template examples/tool_chat_template_llama3.1_json.jinja\n```  \nNext, make a request that triggers the model to use the available tools:  \n??? code  \n```python\nfrom openai import OpenAI\nimport json\n\nclient = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"dummy\")\n\ndef get_weather(location: str, unit: str):\nreturn f\"Getting the weather for {location} in {unit}...\"\ntool_functions = {\"get_weather\": get_weather}", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Quickstart", "level": 2, "text": "tools = [\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_weather\",\n\"description\": \"Get the current weather in a given location\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\"type\": \"string\", \"description\": \"City and state, e.g., 'San Francisco, CA'\"},\n\"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n},\n\"required\": [\"location\", \"unit\"],\n},\n},\n},\n]\n\nresponse = client.chat.completions.create(\nmodel=client.models.list().data[0].id,\nmessages=[{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}],\ntools=tools,\ntool_choice=\"auto\",\n)", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Quickstart", "level": 2, "text": "tool_call = response.choices[0].message.tool_calls[0].function\nprint(f\"Function called: {tool_call.name}\")\nprint(f\"Arguments: {tool_call.arguments}\")\nprint(f\"Result: {tool_functions[tool_call.name](**json.loads(tool_call.arguments))}\")\n```  \nExample output:  \n```text\nFunction called: get_weather\nArguments: {\"location\": \"San Francisco, CA\", \"unit\": \"fahrenheit\"}\nResult: Getting the weather for San Francisco, CA in fahrenheit...\n```  \nThis example demonstrates:  \n* Setting up the server with tool calling enabled\n* Defining an actual function to handle tool calls\n* Making a request with `tool_choice=\"auto\"`\n* Handling the structured response and executing the corresponding function", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Quickstart", "level": 2, "text": "You can also specify a particular function using named function calling by setting `tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}`. Note that this will use the structured outputs backend - so the first time this is used, there will be several seconds of latency (or more) as the FSM is compiled for the first time before it is cached for subsequent requests.  \nRemember that it's the caller's responsibility to:  \n1. Define appropriate tools in the request\n2. Include relevant context in the chat messages\n3. Handle the tool calls in your application logic  \nFor more advanced usage, including parallel tool calls and different model-specific parsers, see the sections below.", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Named Function Calling", "level": 2, "text": "## Named Function Calling  \nvLLM supports named function calling in the chat completion API by default. This should work with most structured outputs backends supported by vLLM. You are guaranteed a validly-parsable function call - not a\nhigh-quality one.  \nvLLM will use structured outputs to ensure the response matches the tool parameter object defined by the JSON schema in the `tools` parameter.\nFor best results, we recommend ensuring that the expected output format / schema is specified in the prompt to ensure that the model's intended generation is aligned with the schema that it's being forced to generate by the structured outputs backend.  \nTo use a named function, you need to define the functions in the `tools` parameter of the chat completion request, and\nspecify the `name` of one of the tools in the `tool_choice` parameter of the chat completion request.", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Required Function Calling", "level": 2, "text": "## Required Function Calling  \nvLLM supports the `tool_choice='required'` option in the chat completion API. Similar to the named function calling, it also uses structured outputs, so this is enabled by default and will work with any supported model. However, support for alternative decoding backends are on the [roadmap](../usage/v1_guide.md#features) for the V1 engine.  \nWhen tool_choice='required' is set, the model is guaranteed to generate one or more tool calls based on the specified tool list in the `tools` parameter. The number of tool calls depends on the user's query. The output format strictly follows the schema defined in the `tools` parameter.", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/None Function Calling", "level": 2, "text": "## None Function Calling  \nvLLM supports the `tool_choice='none'` option in the chat completion API. When this option is set, the model will not generate any tool calls and will respond with regular text content only, even if tools are defined in the request.  \n!!! note\nWhen tools are specified in the request, vLLM includes tool definitions in the prompt by default, regardless of the `tool_choice` setting. To exclude tool definitions when `tool_choice='none'`, use the `--exclude-tools-when-tool-choice-none` option.", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling", "level": 2, "text": "## Automatic Function Calling  \nTo enable this feature, you should set the following flags:  \n* `--enable-auto-tool-choice` -- **mandatory** Auto tool choice. It tells vLLM that you want to enable the model to generate its own tool calls when it\ndeems appropriate.\n* `--tool-call-parser` -- select the tool parser to use (listed below). Additional tool parsers\nwill continue to be added in the future. You can also register your own tool parsers in the `--tool-parser-plugin`.\n* `--tool-parser-plugin` -- **optional** tool parser plugin used to register user defined tool parsers into vllm, the registered tool parser name can be specified in `--tool-call-parser`.\n* `--chat-template` -- **optional** for auto tool choice. It's the path to the chat template which handles `tool`-role messages and `assistant`-role messages\nthat contain previously generated tool calls. Hermes, Mistral and Llama models have tool-compatible chat templates in their", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling", "level": 2, "text": "`tokenizer_config.json` files, but you can specify a custom template. This argument can be set to `tool_use` if your model has a tool use-specific chat\ntemplate configured in the `tokenizer_config.json`. In this case, it will be used per the `transformers` specification. More on this [here](https://huggingface.co/docs/transformers/en/chat_templating#why-do-some-models-have-multiple-templates)\nfrom HuggingFace; and you can find an example of this in a `tokenizer_config.json` [here](https://huggingface.co/NousResearch/Hermes-2-Pro-Llama-3-8B/blob/main/tokenizer_config.json).  \nIf your favorite tool-calling model is not supported, please feel free to contribute a parser & tool use chat template!", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/Hermes Models (`hermes`)", "level": 3, "text": "### Hermes Models (`hermes`)  \nAll Nous Research Hermes-series models newer than Hermes 2 Pro should be supported.  \n* `NousResearch/Hermes-2-Pro-*`\n* `NousResearch/Hermes-2-Theta-*`\n* `NousResearch/Hermes-3-*`  \n_Note that the Hermes 2 **Theta** models are known to have degraded tool call quality and capabilities due to the merge\nstep in their creation_.  \nFlags: `--tool-call-parser hermes`", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/Mistral Models (`mistral`)", "level": 3, "text": "### Mistral Models (`mistral`)  \nSupported models:  \n* `mistralai/Mistral-7B-Instruct-v0.3` (confirmed)\n* Additional mistral function-calling models are compatible as well.  \nKnown issues:  \n1. Mistral 7B struggles to generate parallel tool calls correctly.\n2. **For Transformers tokenization backend only**: Mistral's `tokenizer_config.json` chat template requires tool call IDs that are exactly 9 digits, which is\nmuch shorter than what vLLM generates. Since an exception is thrown when this condition\nis not met, the following additional chat templates are provided:  \n* <gh-file:examples/tool_chat_template_mistral.jinja> - this is the \"official\" Mistral chat template, but tweaked so that\nit works with vLLM's tool call IDs (provided `tool_call_id` fields are truncated to the last 9 digits)\n* <gh-file:examples/tool_chat_template_mistral_parallel.jinja> - this is a \"better\" version that adds a tool-use system prompt", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/Mistral Models (`mistral`)", "level": 3, "text": "when tools are provided, that results in much better reliability when working with parallel tool calling.  \nRecommended flags:  \n1. To use [mistral-common](https://github.com/mistralai/mistral-common) the official Mistral tokenization backend:  \n`--tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral`  \n2. To use the default Transformers tokenization backend:\n`--tool-call-parser mistral --chat-template examples/tool_chat_template_mistral_parallel.jinja`", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/Llama Models (`llama3_json`)", "level": 3, "text": "### Llama Models (`llama3_json`)  \nSupported models:  \nAll Llama 3.1, 3.2 and 4 models should be supported.  \n* `meta-llama/Llama-3.1-*`\n* `meta-llama/Llama-3.2-*`\n* `meta-llama/Llama-4-*`  \nThe tool calling that is supported is the [JSON-based tool calling](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/#json-based-tool-calling). For [pythonic tool calling](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/text_prompt_format.md#zero-shot-function-calling) introduced by the Llama-3.2 models, see the `pythonic` tool parser below. As for Llama 4 models, it is recommended to use the `llama4_pythonic` tool parser.  \nOther tool calling formats like the built-in python tool calling or custom tool calling are not supported.  \nKnown issues:  \n1. Parallel tool calls are not supported for Llama 3, but it is supported in Llama 4 models.\n2. The model can generate parameters in an incorrect format, such as generating\nan array serialized as string instead of an array.", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/Llama Models (`llama3_json`)", "level": 3, "text": "an array serialized as string instead of an array.  \nVLLM provides two JSON-based chat templates for Llama 3.1 and 3.2:  \n* <gh-file:examples/tool_chat_template_llama3.1_json.jinja> - this is the \"official\" chat template for the Llama 3.1\nmodels, but tweaked so that it works better with vLLM.\n* <gh-file:examples/tool_chat_template_llama3.2_json.jinja> - this extends upon the Llama 3.1 chat template by adding support for\nimages.  \nRecommended flags: `--tool-call-parser llama3_json --chat-template {see_above}`  \nVLLM also provides a pythonic and JSON-based chat template for Llama 4, but pythonic tool calling is recommended:  \n* <gh-file:examples/tool_chat_template_llama4_pythonic.jinja> - this is based on the [official chat template](https://www.llama.com/docs/model-cards-and-prompt-formats/llama4/) for the Llama 4 models.  \nFor Llama 4 model, use `--tool-call-parser llama4_pythonic --chat-template examples/tool_chat_template_llama4_pythonic.jinja`.", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/IBM Granite", "level": 3, "text": "### IBM Granite  \nSupported models:  \n* `ibm-granite/granite-4.0-h-small` and other Granite 4.0 models  \nRecommended flags: `--tool-call-parser hermes`  \n* `ibm-granite/granite-3.0-8b-instruct`  \nRecommended flags: `--tool-call-parser granite --chat-template examples/tool_chat_template_granite.jinja`  \n<gh-file:examples/tool_chat_template_granite.jinja>: this is a modified chat template from the original on Hugging Face. Parallel function calls are supported.  \n* `ibm-granite/granite-3.1-8b-instruct`  \nRecommended flags: `--tool-call-parser granite`  \nThe chat template from Huggingface can be used directly. Parallel function calls are supported.  \n* `ibm-granite/granite-20b-functioncalling`  \nRecommended flags: `--tool-call-parser granite-20b-fc --chat-template examples/tool_chat_template_granite_20b_fc.jinja`", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/IBM Granite", "level": 3, "text": "<gh-file:examples/tool_chat_template_granite_20b_fc.jinja>: this is a modified chat template from the original on Hugging Face, which is not vLLM-compatible. It blends function description elements from the Hermes template and follows the same system prompt as \"Response Generation\" mode from [the paper](https://arxiv.org/abs/2407.00121). Parallel function calls are supported.", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/InternLM Models (`internlm`)", "level": 3, "text": "### InternLM Models (`internlm`)  \nSupported models:  \n* `internlm/internlm2_5-7b-chat` (confirmed)\n* Additional internlm2.5 function-calling models are compatible as well  \nKnown issues:  \n* Although this implementation also supports InternLM2, the tool call results are not stable when testing with the `internlm/internlm2-chat-7b` model.  \nRecommended flags: `--tool-call-parser internlm --chat-template examples/tool_chat_template_internlm2_tool.jinja`", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/Jamba Models (`jamba`)", "level": 3, "text": "### Jamba Models (`jamba`)  \nAI21's Jamba-1.5 models are supported.  \n* `ai21labs/AI21-Jamba-1.5-Mini`\n* `ai21labs/AI21-Jamba-1.5-Large`  \nFlags: `--tool-call-parser jamba`", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/xLAM Models (`xlam`)", "level": 3, "text": "### xLAM Models (`xlam`)  \nThe xLAM tool parser is designed to support models that generate tool calls in various JSON formats. It detects function calls in several different output styles:  \n1. Direct JSON arrays: Output strings that are JSON arrays starting with `[` and ending with `]`\n2. Thinking tags: Using `<think>...</think>` tags containing JSON arrays\n3. Code blocks: JSON in code blocks (```json ...```)\n4. Tool calls tags: Using `[TOOL_CALLS]` or `<tool_call>...</tool_call>` tags  \nParallel function calls are supported, and the parser can effectively separate text content from tool calls.  \nSupported models:  \n* Salesforce Llama-xLAM models: `Salesforce/Llama-xLAM-2-8B-fc-r`, `Salesforce/Llama-xLAM-2-70B-fc-r`\n* Qwen-xLAM models: `Salesforce/xLAM-1B-fc-r`, `Salesforce/xLAM-3B-fc-r`, `Salesforce/Qwen-xLAM-32B-fc-r`  \nFlags:  \n* For Llama-based xLAM models: `--tool-call-parser xlam --chat-template examples/tool_chat_template_xlam_llama.jinja`", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/xLAM Models (`xlam`)", "level": 3, "text": "* For Qwen-based xLAM models: `--tool-call-parser xlam --chat-template examples/tool_chat_template_xlam_qwen.jinja`", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/Qwen Models", "level": 3, "text": "### Qwen Models  \nFor Qwen2.5, the chat template in tokenizer_config.json has already included support for the Hermes-style tool use. Therefore, you can use the `hermes` parser to enable tool calls for Qwen models. For more detailed information, please refer to the official [Qwen documentation](https://qwen.readthedocs.io/en/latest/framework/function_call.html#vllm)  \n* `Qwen/Qwen2.5-*`\n* `Qwen/QwQ-32B`  \nFlags: `--tool-call-parser hermes`", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/MiniMax Models (`minimax_m1`)", "level": 3, "text": "### MiniMax Models (`minimax_m1`)  \nSupported models:  \n* `MiniMaxAi/MiniMax-M1-40k` (use with <gh-file:examples/tool_chat_template_minimax_m1.jinja>)\n* `MiniMaxAi/MiniMax-M1-80k` (use with <gh-file:examples/tool_chat_template_minimax_m1.jinja>)  \nFlags: `--tool-call-parser minimax --chat-template examples/tool_chat_template_minimax_m1.jinja`", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/DeepSeek-V3 Models (`deepseek_v3`)", "level": 3, "text": "### DeepSeek-V3 Models (`deepseek_v3`)  \nSupported models:  \n* `deepseek-ai/DeepSeek-V3-0324` (use with <gh-file:examples/tool_chat_template_deepseekv3.jinja>)\n* `deepseek-ai/DeepSeek-R1-0528` (use with <gh-file:examples/tool_chat_template_deepseekr1.jinja>)  \nFlags: `--tool-call-parser deepseek_v3 --chat-template {see_above}`", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/DeepSeek-V3.1 Models (`deepseek_v31`)", "level": 3, "text": "### DeepSeek-V3.1 Models (`deepseek_v31`)  \nSupported models:  \n* `deepseek-ai/DeepSeek-V3.1` (use with <gh-file:examples/tool_chat_template_deepseekv31.jinja>)  \nFlags: `--tool-call-parser deepseek_v31 --chat-template {see_above}`", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/Kimi-K2 Models (`kimi_k2`)", "level": 3, "text": "### Kimi-K2 Models (`kimi_k2`)  \nSupported models:  \n* `moonshotai/Kimi-K2-Instruct`  \nFlags: `--tool-call-parser kimi_k2`", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/Hunyuan Models (`hunyuan_a13b`)", "level": 3, "text": "### Hunyuan Models (`hunyuan_a13b`)  \nSupported models:  \n* `tencent/Hunyuan-A13B-Instruct` (The chat template is already included in the Hugging Face model files.)  \nFlags:  \n* For non-reasoning: `--tool-call-parser hunyuan_a13b`\n* For reasoning: `--tool-call-parser hunyuan_a13b --reasoning-parser hunyuan_a13b --enable_reasoning`", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/LongCat-Flash-Chat Models (`longcat`)", "level": 3, "text": "### LongCat-Flash-Chat Models (`longcat`)  \nSupported models:  \n* `meituan-longcat/LongCat-Flash-Chat`\n* `meituan-longcat/LongCat-Flash-Chat-FP8`  \nFlags: `--tool-call-parser longcat`", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/GLM-4.5 Models (`glm45`)", "level": 3, "text": "### GLM-4.5 Models (`glm45`)  \nSupported models:  \n* `zai-org/GLM-4.5`\n* `zai-org/GLM-4.5-Air`\n* `zai-org/GLM-4.6`\n* `zai-org/GLM-4.6-Air`  \nFlags: `--tool-call-parser glm45`", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/Qwen3-Coder Models (`qwen3_xml`)", "level": 3, "text": "### Qwen3-Coder Models (`qwen3_xml`)  \nSupported models:  \n* `Qwen/Qwen3-480B-A35B-Instruct`\n* `Qwen/Qwen3-Coder-30B-A3B-Instruct`  \nFlags: `--tool-call-parser qwen3_xml`", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/Olmo 3 Models (`olmo3`)", "level": 3, "text": "### Olmo 3 Models (`olmo3`)  \nOlmo 3 models output tool calls in a format that is very similar to the one expected by the `pythonic` parser (see below), with a few differences. Each tool call is a pythonic string, but the parallel tool calls are newline-delimited, and the calls are wrapped within XML tags as `<function_calls>..</function_calls>`. In addition, the parser also allows JSON boolean and null literals (`true`, `false`, and `null`) in addition to the pythonic ones (`True`, `False`, and `None`).  \nSupported models:  \n* TODO (will be updated after Olmo 3 release)  \nFlags: `--tool-call-parser olmo3`", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/Models with Pythonic Tool Calls (`pythonic`)", "level": 3, "text": "### Models with Pythonic Tool Calls (`pythonic`)  \nA growing number of models output a python list to represent tool calls instead of using JSON. This has the advantage of inherently supporting parallel tool calls and removing ambiguity around the JSON schema required for tool calls. The `pythonic` tool parser can support such models.  \nAs a concrete example, these models may look up the weather in San Francisco and Seattle by generating:  \n```python\n[get_weather(city='San Francisco', metric='celsius'), get_weather(city='Seattle', metric='celsius')]\n```  \nLimitations:  \n* The model must not generate both text and tool calls in the same generation. This may not be hard to change for a specific model, but the community currently lacks consensus on which tokens to emit when starting and ending tool calls.  (In particular, the Llama 3.2 models emit no such tokens.)\n* Llama's smaller models struggle to use tools effectively.  \nExample supported models:", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/Automatic Function Calling/Models with Pythonic Tool Calls (`pythonic`)", "level": 3, "text": "Example supported models:  \n* `meta-llama/Llama-3.2-1B-Instruct` âš ï¸ (use with <gh-file:examples/tool_chat_template_llama3.2_pythonic.jinja>)\n* `meta-llama/Llama-3.2-3B-Instruct` âš ï¸ (use with <gh-file:examples/tool_chat_template_llama3.2_pythonic.jinja>)\n* `Team-ACE/ToolACE-8B` (use with <gh-file:examples/tool_chat_template_toolace.jinja>)\n* `fixie-ai/ultravox-v0_4-ToolACE-8B` (use with <gh-file:examples/tool_chat_template_toolace.jinja>)\n* `meta-llama/Llama-4-Scout-17B-16E-Instruct` âš ï¸ (use with <gh-file:examples/tool_chat_template_llama4_pythonic.jinja>)\n* `meta-llama/Llama-4-Maverick-17B-128E-Instruct` âš ï¸ (use with <gh-file:examples/tool_chat_template_llama4_pythonic.jinja>)  \nFlags: `--tool-call-parser pythonic --chat-template {see_above}`  \n!!! warning\nLlama's smaller models frequently fail to emit tool calls in the correct format. Results may vary depending on the model.", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/How to Write a Tool Parser Plugin", "level": 2, "text": "## How to Write a Tool Parser Plugin  \nA tool parser plugin is a Python file containing one or more ToolParser implementations. You can write a ToolParser similar to the `Hermes2ProToolParser` in <gh-file:vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py>.  \nHere is a summary of a plugin file:  \n??? code  \n```python\n\n# import the required packages\n\n# define a tool parser and register it to vllm\n# the name list in register_module can be used\n# in --tool-call-parser. you can define as many\n# tool parsers as you want here.\n@ToolParserManager.register_module([\"example\"])\nclass ExampleToolParser(ToolParser):\ndef __init__(self, tokenizer: AnyTokenizer):\nsuper().__init__(tokenizer)\n\n# adjust request. e.g.: set skip special tokens\n# to False for tool call output.\ndef adjust_request(self, request: ChatCompletionRequest) -> ChatCompletionRequest:\nreturn request", "file_path": "features/tool_calling.md"}
{"id": "453173d3f4c6dee40292cf83e3c631b36244b20545e440c5f511cc590e95bba3", "heading": "Tool Calling/How to Write a Tool Parser Plugin", "level": 2, "text": "# implement the tool call parse for stream call\ndef extract_tool_calls_streaming(\nself,\nprevious_text: str,\ncurrent_text: str,\ndelta_text: str,\nprevious_token_ids: Sequence[int],\ncurrent_token_ids: Sequence[int],\ndelta_token_ids: Sequence[int],\nrequest: ChatCompletionRequest,\n) -> DeltaMessage | None:\nreturn delta\n\n# implement the tool parse for non-stream call\ndef extract_tool_calls(\nself,\nmodel_output: str,\nrequest: ChatCompletionRequest,\n) -> ExtractedToolCallInformation:\nreturn ExtractedToolCallInformation(tools_called=False,\ntool_calls=[],\ncontent=text)\n\n```  \nThen you can use this plugin in the command line like this.  \n```bash\n--enable-auto-tool-choice \\\n--tool-parser-plugin <absolute path of the plugin file>\n--tool-call-parser example \\\n--chat-template <your chat template> \\\n```", "file_path": "features/tool_calling.md"}
{"id": "2c746fd7757836c04a77f6632cc76af2d7013c095eddd37a2a22868f91234ad3", "heading": "Installation", "level": 1, "text": "# Installation  \nvLLM supports the following hardware platforms:  \n- [GPU](gpu.md)\n- [NVIDIA CUDA](gpu.md#nvidia-cuda)\n- [AMD ROCm](gpu.md#amd-rocm)\n- [Intel XPU](gpu.md#intel-xpu)\n- [CPU](cpu.md)\n- [Intel/AMD x86](cpu.md#intelamd-x86)\n- [ARM AArch64](cpu.md#arm-aarch64)\n- [Apple silicon](cpu.md#apple-silicon)\n- [IBM Z (S390X)](cpu.md#ibm-z-s390x)\n- [Google TPU](google_tpu.md)", "file_path": "getting_started/installation/README.md"}
{"id": "2c746fd7757836c04a77f6632cc76af2d7013c095eddd37a2a22868f91234ad3", "heading": "Installation/Hardware Plugins", "level": 2, "text": "## Hardware Plugins  \nThe backends below live **outside** the main `vllm` repository and follow the\n[Hardware-Pluggable RFC](../../design/plugin_system.md).  \n| Accelerator | PyPI / package | Repository |\n|-------------|----------------|------------|\n| Ascend NPU | `vllm-ascend` | <https://github.com/vllm-project/vllm-ascend> |\n| Intel Gaudi (HPU) | N/A, install from source | <https://github.com/vllm-project/vllm-gaudi> |\n| MetaX MACA GPU | N/A, install from source | <https://github.com/MetaX-MACA/vLLM-metax> |\n| Rebellions ATOM / REBEL NPU | `vllm-rbln` | <https://github.com/rebellions-sw/vllm-rbln> |\n| IBM Spyre AIU | `vllm-spyre` | <https://github.com/vllm-project/vllm-spyre> |\n| Cambricon MLU | `vllm-mlu` | <https://github.com/Cambricon/vllm-mlu> |", "file_path": "getting_started/installation/README.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU", "level": 1, "text": "# CPU  \nvLLM is a Python library that supports the following CPU variants. Select your CPU type to see vendor specific instructions:  \n=== \"Intel/AMD x86\"  \nvLLM supports basic model inferencing and serving on x86 CPU platform, with data types FP32, FP16 and BF16.  \n=== \"ARM AArch64\"  \nvLLM has been adapted to work on ARM64 CPUs with NEON support, leveraging the CPU backend initially developed for the x86 platform.  \nARM CPU backend currently supports Float32, FP16 and BFloat16 datatypes.  \n!!! warning\nThere are no pre-built wheels or images for this device, so you must build vLLM from source.  \n=== \"Apple silicon\"  \nvLLM has experimental support for macOS with Apple Silicon. For now, users must build from source to natively run on macOS.  \nCurrently the CPU implementation for macOS supports FP32 and FP16 datatypes.  \n!!! warning\nThere are no pre-built wheels or images for this device, so you must build vLLM from source.  \n=== \"IBM Z (S390X)\"", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU", "level": 1, "text": "=== \"IBM Z (S390X)\"  \nvLLM has experimental support for s390x architecture on IBM Z platform. For now, users must build from source to natively run on IBM Z platform.  \nCurrently the CPU implementation for s390x architecture supports FP32 datatype only.  \n!!! warning\nThere are no pre-built wheels or images for this device, so you must build vLLM from source.", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/Requirements", "level": 2, "text": "## Requirements  \n- Python: 3.10 -- 3.13  \n=== \"Intel/AMD x86\"  \n- OS: Linux\n- CPU flags: `avx512f` (Recommended), `avx512_bf16` (Optional), `avx512_vnni` (Optional)  \n!!! tip\nUse `lscpu` to check the CPU flags.  \n=== \"ARM AArch64\"  \n- OS: Linux\n- Compiler: `gcc/g++ >= 12.3.0` (optional, recommended)\n- Instruction Set Architecture (ISA): NEON support is required  \n=== \"Apple silicon\"  \n- OS: `macOS Sonoma` or later\n- SDK: `XCode 15.4` or later with Command Line Tools\n- Compiler: `Apple Clang >= 15.0.0`  \n=== \"IBM Z (S390X)\"  \n- OS: `Linux`\n- SDK: `gcc/g++ >= 12.3.0` or later with Command Line Tools\n- Instruction Set Architecture (ISA): VXE support is required. Works with Z14 and above.\n- Build install python packages: `pyarrow`, `torch` and `torchvision`  \n-8<-- \"docs/getting_started/installation/cpu/s390x.inc.md:requirements\"", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/Set up using Python/Create a new Python environment", "level": 3, "text": "## Set up using Python  \n### Create a new Python environment  \nIt's recommended to use [uv](https://docs.astral.sh/uv/), a very fast Python environment manager, to create and manage Python environments. Please follow the [documentation](https://docs.astral.sh/uv/#getting-started) to install `uv`. After installing `uv`, you can create a new Python environment using the following commands:  \n```bash\nuv venv --python 3.12 --seed\nsource .venv/bin/activate\n```", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/Set up using Python/Pre-built wheels", "level": 3, "text": "### Pre-built wheels  \nCurrently, there are no pre-built CPU wheels.", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/Set up using Python/Build wheel from source", "level": 3, "text": "### Build wheel from source  \n=== \"Intel/AMD x86\"  \nInstall recommended compiler. We recommend to use `gcc/g++ >= 12.3.0` as the default compiler to avoid potential problems. For example, on Ubuntu 22.4, you can run:  \n```bash\nsudo apt-get update -y\nsudo apt-get install -y gcc-12 g++-12 libnuma-dev python3-dev\nsudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12\n```  \nClone the vLLM project:  \n```bash\ngit clone https://github.com/vllm-project/vllm.git vllm_source\ncd vllm_source\n```  \nInstall the required dependencies:  \n```bash\nuv pip install -r requirements/cpu-build.txt --torch-backend cpu\nuv pip install -r requirements/cpu.txt --torch-backend cpu\n```  \n??? console \"pip\"\n```bash\npip install --upgrade pip\npip install -v -r requirements/cpu-build.txt --extra-index-url https://download.pytorch.org/whl/cpu\npip install -v -r requirements/cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu\n```  \nBuild and install vLLM:  \n```bash", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/Set up using Python/Build wheel from source", "level": 3, "text": "```  \nBuild and install vLLM:  \n```bash\nVLLM_TARGET_DEVICE=cpu uv pip install . --no-build-isolation\n```  \nIf you want to develop vLLM, install it in editable mode instead.  \n```bash\nVLLM_TARGET_DEVICE=cpu uv pip install -e . --no-build-isolation\n```  \nOptionally, build a portable wheel which you can then install elsewhere:  \n```bash\nVLLM_TARGET_DEVICE=cpu uv build --wheel\n```  \n```bash\nuv pip install dist/*.whl\n```  \n??? console \"pip\"\n```bash\nVLLM_TARGET_DEVICE=cpu python -m build --wheel --no-isolation\n```  \n```bash\npip install dist/*.whl\n```  \n!!! example \"Troubleshooting\"\n- **NumPy â‰¥2.0 error**: Downgrade using `pip install \"numpy<2.0\"`.\n- **CMake picks up CUDA**: Add `CMAKE_DISABLE_FIND_PACKAGE_CUDA=ON` to prevent CUDA detection during CPU builds, even if CUDA is installed.\n- `AMD` requies at least 4th gen processors (Zen 4/Genoa) or higher to support [AVX512](https://www.phoronix.com/review/amd-zen4-avx512) to run vLLM on CPU.", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/Set up using Python/Build wheel from source", "level": 3, "text": "- If you receive an error such as: `Could not find a version that satisfies the requirement torch==X.Y.Z+cpu+cpu`, consider updating [pyproject.toml](https://github.com/vllm-project/vllm/blob/main/pyproject.toml) to help pip resolve the dependency.\n```toml title=\"pyproject.toml\"\n[build-system]\nrequires = [\n\"cmake>=3.26.1\",\n...\n\"torch==X.Y.Z+cpu\"   # <-------\n]", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/Set up using Python/Build wheel from source", "level": 3, "text": "```\n- If you are building vLLM from source and not using the pre-built images, remember to set `LD_PRELOAD=\"/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4:$LD_PRELOAD\"` on x86 machines before running vLLM.  \n=== \"ARM AArch64\"  \nFirst, install the recommended compiler. We recommend using `gcc/g++ >= 12.3.0` as the default compiler to avoid potential problems. For example, on Ubuntu 22.4, you can run:  \n```bash\nsudo apt-get update  -y\nsudo apt-get install -y --no-install-recommends ccache git curl wget ca-certificates gcc-12 g++-12 libtcmalloc-minimal4 libnuma-dev ffmpeg libsm6 libxext6 libgl1 jq lsof\nsudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12\n```  \nSecond, clone the vLLM project:  \n```bash\ngit clone https://github.com/vllm-project/vllm.git vllm_source\ncd vllm_source\n```  \nThird, install required dependencies:  \n```bash\nuv pip install -r requirements/cpu-build.txt --torch-backend cpu", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/Set up using Python/Build wheel from source", "level": 3, "text": "uv pip install -r requirements/cpu.txt --torch-backend cpu\n```  \n??? console \"pip\"\n```bash\npip install --upgrade pip\npip install -v -r requirements/cpu-build.txt --extra-index-url https://download.pytorch.org/whl/cpu\npip install -v -r requirements/cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu\n```  \nFinally, build and install vLLM:  \n```bash\nVLLM_TARGET_DEVICE=cpu uv pip install . --no-build-isolation\n```  \nIf you want to develop vLLM, install it in editable mode instead.  \n```bash\nVLLM_TARGET_DEVICE=cpu uv pip install -e . --no-build-isolation\n```  \nTesting has been conducted on AWS Graviton3 instances for compatibility.  \n=== \"Apple silicon\"  \nAfter installation of XCode and the Command Line Tools, which include Apple Clang, execute the following commands to build and install vLLM from source.  \n```bash\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\nuv pip install -r requirements/cpu.txt\nuv pip install -e .\n```  \n!!! note", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/Set up using Python/Build wheel from source", "level": 3, "text": "uv pip install -e .\n```  \n!!! note\nOn macOS the `VLLM_TARGET_DEVICE` is automatically set to `cpu`, which is currently the only supported device.  \n!!! example \"Troubleshooting\"\nIf the build fails with errors like the following where standard C++ headers cannot be found, try to remove and reinstall your\n[Command Line Tools for Xcode](https://developer.apple.com/download/all/).  \n```text\n[...] fatal error: 'map' file not found\n1 | #include <map>\n|          ^~~~~\n1 error generated.\n[2/8] Building CXX object CMakeFiles/_C.dir/csrc/cpu/pos_encoding.cpp.o", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/Set up using Python/Build wheel from source", "level": 3, "text": "[...] fatal error: 'cstddef' file not found\n10 | #include <cstddef>\n|          ^~~~~~~~~\n1 error generated.\n```  \n---  \nIf the build fails with C++11/C++17 compatibility errors like the following, the issue is that the build system is defaulting to an older C++ standard:  \n```text\n[...] error: 'constexpr' is not a type\n[...] error: expected ';' before 'constexpr'\n[...] error: 'constexpr' does not name a type\n```  \n**Solution**: Your compiler might be using an older C++ standard. Edit `cmake/cpu_extension.cmake` and add `set(CMAKE_CXX_STANDARD 17)` before `set(CMAKE_CXX_STANDARD_REQUIRED ON)`.  \nTo check your compiler's C++ standard support:\n```bash\nclang++ -std=c++17 -pedantic -dM -E -x c++ /dev/null | grep __cplusplus", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/Set up using Python/Build wheel from source", "level": 3, "text": "```\nOn Apple Clang 16 you should see: `#define __cplusplus 201703L`  \n=== \"IBM Z (s390x)\"  \nInstall the following packages from the package manager before building the vLLM. For example on RHEL 9.4:  \n```bash\ndnf install -y \\\nwhich procps findutils tar vim git gcc g++ make patch make cython zlib-devel \\\nlibjpeg-turbo-devel libtiff-devel libpng-devel libwebp-devel freetype-devel harfbuzz-devel \\\nopenssl-devel openblas openblas-devel wget autoconf automake libtool cmake numactl-devel\n```  \nInstall rust>=1.80 which is needed for `outlines-core` and `uvloop` python packages installation.  \n```bash\ncurl https://sh.rustup.rs -sSf | sh -s -- -y && \\\n. \"$HOME/.cargo/env\"\n```  \nExecute the following commands to build and install vLLM from source.  \n!!! tip\nPlease build the following dependencies, `torchvision`, `pyarrow` from source before building vLLM.  \n```bash\nsed -i '/^torch/d' requirements/build.txt    # remove torch from requirements/build.txt since we use nightly builds\nuv pip install -v \\", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/Set up using Python/Build wheel from source", "level": 3, "text": "uv pip install -v \\\n--torch-backend auto \\\n-r requirements/build.txt \\\n-r requirements/cpu.txt \\\nVLLM_TARGET_DEVICE=cpu python setup.py bdist_wheel && \\\nuv pip install dist/*.whl\n```  \n??? console \"pip\"\n```bash\nsed -i '/^torch/d' requirements/build.txt    # remove torch from requirements/build.txt since we use nightly builds\npip install -v \\\n--extra-index-url https://download.pytorch.org/whl/nightly/cpu \\\n-r requirements/build.txt \\\n-r requirements/cpu.txt \\\nVLLM_TARGET_DEVICE=cpu python setup.py bdist_wheel && \\\npip install dist/*.whl\n```", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/Set up using Docker/Pre-built images", "level": 3, "text": "## Set up using Docker  \n### Pre-built images  \n=== \"Intel/AMD x86\"  \n[https://gallery.ecr.aws/q9t5s3a7/vllm-cpu-release-repo](https://gallery.ecr.aws/q9t5s3a7/vllm-cpu-release-repo)  \n!!! warning\nIf deploying the pre-built images on machines without `avx512f`, `avx512_bf16`, or `avx512_vnni` support, an `Illegal instruction` error may be raised. It is recommended to build images for these machines with the appropriate build arguments (e.g., `--build-arg VLLM_CPU_DISABLE_AVX512=true`, `--build-arg VLLM_CPU_AVX512BF16=false`, or `--build-arg VLLM_CPU_AVX512VNNI=false`) to disable unsupported features. Please note that without `avx512f`, AVX2 will be used and this version is not recommended because it only has basic feature support.", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/Set up using Docker/Build image from source", "level": 3, "text": "### Build image from source  \n=== \"Intel/AMD x86\"  \n```bash\ndocker build -f docker/Dockerfile.cpu \\\n--build-arg VLLM_CPU_AVX512BF16=false (default)|true \\\n--build-arg VLLM_CPU_AVX512VNNI=false (default)|true \\\n--build-arg VLLM_CPU_DISABLE_AVX512=false (default)|true \\\n--tag vllm-cpu-env \\\n--target vllm-openai .\n\n# Launching OpenAI server\ndocker run --rm \\\n--security-opt seccomp=unconfined \\\n--cap-add SYS_NICE \\\n--shm-size=4g \\\n-p 8000:8000 \\\n-e VLLM_CPU_KVCACHE_SPACE=<KV cache space> \\\n-e VLLM_CPU_OMP_THREADS_BIND=<CPU cores for inference> \\\nvllm-cpu-env \\\n--model=meta-llama/Llama-3.2-1B-Instruct \\\n--dtype=bfloat16 \\\nother vLLM OpenAI server arguments\n```  \n=== \"ARM AArch64\"  \n```bash\ndocker build -f docker/Dockerfile.cpu \\\n--tag vllm-cpu-env .", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/Set up using Docker/Build image from source", "level": 3, "text": "# Launching OpenAI server\ndocker run --rm \\\n--privileged=true \\\n--shm-size=4g \\\n-p 8000:8000 \\\n-e VLLM_CPU_KVCACHE_SPACE=<KV cache space> \\\n-e VLLM_CPU_OMP_THREADS_BIND=<CPU cores for inference> \\\nvllm-cpu-env \\\n--model=meta-llama/Llama-3.2-1B-Instruct \\\n--dtype=bfloat16 \\\nother vLLM OpenAI server arguments\n```  \n!!! tip\nAn alternative of `--privileged=true` is `--cap-add SYS_NICE --security-opt seccomp=unconfined`.  \n=== \"Apple silicon\"", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/Set up using Docker/Build image from source", "level": 3, "text": "```\ndocker build -f docker/Dockerfile.cpu \\\n--tag vllm-cpu-env .\n\n# Launching OpenAI server\ndocker run --rm \\\n--privileged=true \\\n--shm-size=4g \\\n-p 8000:8000 \\\n-e VLLM_CPU_KVCACHE_SPACE=<KV cache space> \\\n-e VLLM_CPU_OMP_THREADS_BIND=<CPU cores for inference> \\\nvllm-cpu-env \\\n--model=meta-llama/Llama-3.2-1B-Instruct \\\n--dtype=bfloat16 \\\nother vLLM OpenAI server arguments\n```  \n!!! tip An alternative of --privileged=true is --cap-add SYS_NICE --security-opt seccomp=unconfined.  \n=== \"IBM Z (S390X)\"  \n```bash\ndocker build -f docker/Dockerfile.s390x \\\n--tag vllm-cpu-env .\n\n# Launch OpenAI server\ndocker run --rm \\\n--privileged true \\\n--shm-size 4g \\\n-p 8000:8000 \\\n-e VLLM_CPU_KVCACHE_SPACE=<KV cache space> \\\n-e VLLM_CPU_OMP_THREADS_BIND=<CPU cores for inference> \\\nvllm-cpu-env \\\n--model meta-llama/Llama-3.2-1B-Instruct \\\n--dtype float \\\nother vLLM OpenAI server arguments\n```  \n!!! tip\nAn alternative of `--privileged true` is `--cap-add SYS_NICE --security-opt seccomp=unconfined`.", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/Related runtime environment variables", "level": 2, "text": "## Related runtime environment variables  \n- `VLLM_CPU_KVCACHE_SPACE`: specify the KV Cache size (e.g, `VLLM_CPU_KVCACHE_SPACE=40` means 40 GiB space for KV cache), larger setting will allow vLLM running more requests in parallel. This parameter should be set based on the hardware configuration and memory management pattern of users. Default value is `0`.\n- `VLLM_CPU_OMP_THREADS_BIND`: specify the CPU cores dedicated to the OpenMP threads, can be set as CPU id lists or `auto` (by default). For example, `VLLM_CPU_OMP_THREADS_BIND=0-31` means there will be 32 OpenMP threads bound on 0-31 CPU cores. `VLLM_CPU_OMP_THREADS_BIND=0-31|32-63` means there will be 2 tensor parallel processes, 32 OpenMP threads of rank0 are bound on 0-31 CPU cores, and the OpenMP threads of rank1 are bound on 32-63 CPU cores. By setting to `auto`, the OpenMP threads of each rank are bound to the CPU cores in each NUMA node respectively.", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/Related runtime environment variables", "level": 2, "text": "- `VLLM_CPU_NUM_OF_RESERVED_CPU`: specify the number of CPU cores which are not dedicated to the OpenMP threads for each rank. The variable only takes effect when VLLM_CPU_OMP_THREADS_BIND is set to `auto`. Default value is `None`. If the value is not set and use `auto` thread binding, no CPU will be reserved for `world_size == 1`, 1 CPU per rank will be reserved for `world_size > 1`.\n- `CPU_VISIBLE_MEMORY_NODES`: specify visible NUMA memory nodes for vLLM CPU workers, similar to ```CUDA_VISIBLE_DEVICES```. The variable only takes effect when VLLM_CPU_OMP_THREADS_BIND is set to `auto`. The variable provides more control for the auto thread-binding feature, such as masking nodes and changing nodes binding sequence.\n- `VLLM_CPU_MOE_PREPACK` (x86 only): whether to use prepack for MoE layer. This will be passed to `ipex.llm.modules.GatedMLPMOE`. Default is `1` (True). On unsupported CPUs, you might need to set this to `0` (False).", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/Related runtime environment variables", "level": 2, "text": "- `VLLM_CPU_SGL_KERNEL` (x86 only, Experimental): whether to use small-batch optimized kernels for linear layer and MoE layer, especially for low-latency requirements like online serving. The kernels require AMX instruction set, BFloat16 weight type and weight shapes divisible by 32. Default is `0` (False).", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/FAQ/Which `dtype` should be used?", "level": 3, "text": "## FAQ  \n### Which `dtype` should be used?  \n- Currently vLLM CPU uses model default settings as `dtype`. However, due to unstable float16 support in torch CPU, it is recommended to explicitly set `dtype=bfloat16` if there are any performance or accuracy problem.", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/FAQ/How to launch a vLLM service on CPU?", "level": 3, "text": "### How to launch a vLLM service on CPU?  \n- When using the online serving, it is recommended to reserve 1-2 CPU cores for the serving framework to avoid CPU oversubscription. For example, on a platform with 32 physical CPU cores, reserving CPU 31 for the framework and using CPU 0-30 for inference threads:  \n```bash\nexport VLLM_CPU_KVCACHE_SPACE=40\nexport VLLM_CPU_OMP_THREADS_BIND=0-30\nvllm serve facebook/opt-125m --dtype=bfloat16\n```  \nor using default auto thread binding:  \n```bash\nexport VLLM_CPU_KVCACHE_SPACE=40\nexport VLLM_CPU_NUM_OF_RESERVED_CPU=1\nvllm serve facebook/opt-125m --dtype=bfloat16\n```  \nNote, it is recommended to manually reserve 1 CPU for vLLM front-end process when `world_size == 1`.", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/FAQ/How to decide `VLLM_CPU_OMP_THREADS_BIND`?", "level": 3, "text": "### How to decide `VLLM_CPU_OMP_THREADS_BIND`?  \n- Default `auto` thread-binding is recommended for most cases. Ideally, each OpenMP thread will be bound to a dedicated physical core respectively, threads of each rank will be bound to a same NUMA node respectively, and 1 CPU per rank will be reserved for other vLLM components when `world_size > 1`. If have any performance problems or unexpected binding behaviours, please try to bind threads as following.  \n- On a hyper-threading enabled platform with 16 logical CPU cores / 8 physical CPU cores:  \n??? console \"Commands\"  \n```console\n$ lscpu -e # check the mapping between logical CPU cores and physical CPU cores", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/FAQ/How to decide `VLLM_CPU_OMP_THREADS_BIND`?", "level": 3, "text": "# The \"CPU\" column means the logical CPU core IDs, and the \"CORE\" column means the physical core IDs. On this platform, two logical cores are sharing one physical core.\nCPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE    MAXMHZ   MINMHZ      MHZ\n0    0      0    0 0:0:0:0          yes 2401.0000 800.0000  800.000\n1    0      0    1 1:1:1:0          yes 2401.0000 800.0000  800.000\n2    0      0    2 2:2:2:0          yes 2401.0000 800.0000  800.000\n3    0      0    3 3:3:3:0          yes 2401.0000 800.0000  800.000\n4    0      0    4 4:4:4:0          yes 2401.0000 800.0000  800.000\n5    0      0    5 5:5:5:0          yes 2401.0000 800.0000  800.000\n6    0      0    6 6:6:6:0          yes 2401.0000 800.0000  800.000\n7    0      0    7 7:7:7:0          yes 2401.0000 800.0000  800.000\n8    0      0    0 0:0:0:0          yes 2401.0000 800.0000  800.000\n9    0      0    1 1:1:1:0          yes 2401.0000 800.0000  800.000\n10   0      0    2 2:2:2:0          yes 2401.0000 800.0000  800.000", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/FAQ/How to decide `VLLM_CPU_OMP_THREADS_BIND`?", "level": 3, "text": "11   0      0    3 3:3:3:0          yes 2401.0000 800.0000  800.000\n12   0      0    4 4:4:4:0          yes 2401.0000 800.0000  800.000\n13   0      0    5 5:5:5:0          yes 2401.0000 800.0000  800.000\n14   0      0    6 6:6:6:0          yes 2401.0000 800.0000  800.000\n15   0      0    7 7:7:7:0          yes 2401.0000 800.0000  800.000", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/FAQ/How to decide `VLLM_CPU_OMP_THREADS_BIND`?", "level": 3, "text": "# On this platform, it is recommend to only bind openMP threads on logical CPU cores 0-7 or 8-15\n$ export VLLM_CPU_OMP_THREADS_BIND=0-7\n$ python examples/offline_inference/basic/basic.py\n```  \n- When deploy vLLM CPU backend on a multi-socket machine with NUMA and enable tensor parallel or pipeline parallel, each NUMA node is treated as a TP/PP rank. So be aware to set CPU cores of a single rank on a same NUMA node to avoid cross NUMA node memory access.", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/FAQ/How to decide `VLLM_CPU_KVCACHE_SPACE`?", "level": 3, "text": "### How to decide `VLLM_CPU_KVCACHE_SPACE`?  \nThis value is 4GB by default. Larger space can support more concurrent requests, longer context length. However, users should take care of memory capacity of each NUMA node. The memory usage of each TP rank is the sum of `weight shard size` and `VLLM_CPU_KVCACHE_SPACE`, if it exceeds the capacity of a single NUMA node, the TP worker will be killed with `exitcode 9` due to out-of-memory.", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/FAQ/How to do performance tuning for vLLM CPU?", "level": 3, "text": "### How to do performance tuning for vLLM CPU?  \nFirst of all, please make sure the thread-binding and KV cache space are properly set and take effect. You can check the thread-binding by running a vLLM benchmark and observing CPU cores usage via `htop`.  \nInference batch size is an important parameter for the performance. Larger batch usually provides higher throughput, smaller batch provides lower latency. Tuning max batch size starts from default value to balance throughput and latency is an effective way to improve vLLM CPU performance on specific platforms. There are two important related parameters in vLLM:  \n- `--max-num-batched-tokens`, defines the limit of token numbers in a single batch, has more impacts on the first token performance. The default value is set as:\n- Offline Inference: `4096 * world_size`\n- Online Serving: `2048 * world_size`\n- `--max-num-seqs`, defines the limit of sequence numbers in a single batch, has more impacts on the output token performance.", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/FAQ/How to do performance tuning for vLLM CPU?", "level": 3, "text": "- Offline Inference: `256 * world_size`\n- Online Serving: `128 * world_size`  \nvLLM CPU supports data parallel (DP), tensor parallel (TP) and pipeline parallel (PP) to leverage multiple CPU sockets and memory nodes. For more details of tuning DP, TP and PP, please refer to [Optimization and Tuning](../../configuration/optimization.md). For vLLM CPU, it is recommended to use DP, TP and PP together if there are enough CPU sockets and memory nodes.", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/FAQ/Which quantization configs does vLLM CPU support?", "level": 3, "text": "### Which quantization configs does vLLM CPU support?  \n- vLLM CPU supports quantizations:\n- AWQ (x86 only)\n- GPTQ (x86 only)\n- compressed-tensor INT8 W8A8 (x86, s390x)", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/FAQ/(x86 only) What is the purpose of `VLLM_CPU_MOE_PREPACK` and `VLLM_CPU_SGL_KERNEL`?", "level": 3, "text": "### (x86 only) What is the purpose of `VLLM_CPU_MOE_PREPACK` and `VLLM_CPU_SGL_KERNEL`?  \n- Both of them require `amx` CPU flag.\n- `VLLM_CPU_MOE_PREPACK` can provides better performance for MoE models\n- `VLLM_CPU_SGL_KERNEL` can provides better performance for MoE models and small-batch scenarios.", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/FAQ/Why do I see `get_mempolicy: Operation not permitted` when running in Docker?", "level": 3, "text": "### Why do I see `get_mempolicy: Operation not permitted` when running in Docker?  \nIn some container environments (like Docker), NUMA-related syscalls used by vLLM (e.g., `get_mempolicy`, `migrate_pages`) are blocked/denied in the runtime's default seccomp/capabilities settings. This may lead to warnings like `get_mempolicy: Operation not permitted`. Functionality is not affected, but NUMA memory binding/migration optimizations may not take effect and performance can be suboptimal.  \nTo enable these optimizations inside Docker with the least privilege, you can follow below tips:  \n```bash\ndocker run ... --cap-add SYS_NICE --security-opt seccomp=unconfined  ...\n\n# 1) `--cap-add SYS_NICE` is to address `get_mempolicy` EPERM issue.", "file_path": "getting_started/installation/cpu.md"}
{"id": "89d3898a46241dce26d4606918fe386d75baf73cd001ed07399ec95981bbf862", "heading": "CPU/FAQ/Why do I see `get_mempolicy: Operation not permitted` when running in Docker?", "level": 3, "text": "# 2) `--security-opt seccomp=unconfined` is to enable `migrate_pages` for `numa_migrate_pages()`.\n# Actually, `seccomp=unconfined` bypasses the seccomp for container,\n# if it's unacceptable, you can customize your own seccomp profile,\n# based on docker/runtime default.json and add `migrate_pages` to `SCMP_ACT_ALLOW` list.\n\n# reference : https://docs.docker.com/engine/security/seccomp/\n```  \nAlternatively, running with `--privileged=true` also works but is broader and not generally recommended.  \nIn K8S, the following configuration can be added to workload yaml to achieve the same effect as above:  \n```yaml\nsecurityContext:\nseccompProfile:\ntype: Unconfined\ncapabilities:\nadd:\n- SYS_NICE\n```", "file_path": "getting_started/installation/cpu.md"}
{"id": "17f9b826ae87bf9f4f3e5eb7e526a9e692f93eec9ce8cc2e4598e809e45a7993", "heading": "Google TPU", "level": 1, "text": "# Google TPU  \nTensor Processing Units (TPUs) are Google's custom-developed application-specific\nintegrated circuits (ASICs) used to accelerate machine learning workloads. TPUs\nare available in different versions each with different hardware specifications.\nFor more information about TPUs, see [TPU System Architecture](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm).\nFor more information on the TPU versions supported with vLLM, see:  \n- [TPU v6e](https://cloud.google.com/tpu/docs/v6e)\n- [TPU v5e](https://cloud.google.com/tpu/docs/v5e)\n- [TPU v5p](https://cloud.google.com/tpu/docs/v5p)\n- [TPU v4](https://cloud.google.com/tpu/docs/v4)  \nThese TPU versions allow you to configure the physical arrangements of the TPU\nchips. This can improve throughput and networking performance. For more\ninformation see:  \n- [TPU v6e topologies](https://cloud.google.com/tpu/docs/v6e#configurations)\n- [TPU v5e topologies](https://cloud.google.com/tpu/docs/v5e#tpu-v5e-config)", "file_path": "getting_started/installation/google_tpu.md"}
{"id": "17f9b826ae87bf9f4f3e5eb7e526a9e692f93eec9ce8cc2e4598e809e45a7993", "heading": "Google TPU", "level": 1, "text": "- [TPU v5p topologies](https://cloud.google.com/tpu/docs/v5p#tpu-v5p-config)\n- [TPU v4 topologies](https://cloud.google.com/tpu/docs/v4#tpu-v4-config)  \nIn order for you to use Cloud TPUs you need to have TPU quota granted to your\nGoogle Cloud Platform project. TPU quotas specify how many TPUs you can use in a\nGPC project and are specified in terms of TPU version, the number of TPU you\nwant to use, and quota type. For more information, see [TPU quota](https://cloud.google.com/tpu/docs/quota#tpu_quota).  \nFor TPU pricing information, see [Cloud TPU pricing](https://cloud.google.com/tpu/pricing).  \nYou may need additional persistent storage for your TPU VMs. For more\ninformation, see [Storage options for Cloud TPU data](https://cloud.devsite.corp.google.com/tpu/docs/storage-options).  \n!!! warning\nThere are no pre-built wheels for this device, so you must either use the pre-built Docker image or build vLLM from source.", "file_path": "getting_started/installation/google_tpu.md"}
{"id": "17f9b826ae87bf9f4f3e5eb7e526a9e692f93eec9ce8cc2e4598e809e45a7993", "heading": "Google TPU/Requirements", "level": 2, "text": "## Requirements  \n- Google Cloud TPU VM\n- TPU versions: v6e, v5e, v5p, v4\n- Python: 3.11 or newer", "file_path": "getting_started/installation/google_tpu.md"}
{"id": "17f9b826ae87bf9f4f3e5eb7e526a9e692f93eec9ce8cc2e4598e809e45a7993", "heading": "Google TPU/Requirements/Provision Cloud TPUs", "level": 3, "text": "### Provision Cloud TPUs  \nYou can provision Cloud TPUs using the [Cloud TPU API](https://cloud.google.com/tpu/docs/reference/rest)\nor the [queued resources](https://cloud.google.com/tpu/docs/queued-resources)\nAPI (preferred). This section shows how to create TPUs using the queued resource API. For\nmore information about using the Cloud TPU API, see [Create a Cloud TPU using the Create Node API](https://cloud.google.com/tpu/docs/managing-tpus-tpu-vm#create-node-api).\nQueued resources enable you to request Cloud TPU resources in a queued manner.\nWhen you request queued resources, the request is added to a queue maintained by\nthe Cloud TPU service. When the requested resource becomes available, it's\nassigned to your Google Cloud project for your immediate exclusive use.  \n!!! note\nIn all of the following commands, replace the ALL CAPS parameter names with\nappropriate values. See the parameter descriptions table for more information.", "file_path": "getting_started/installation/google_tpu.md"}
{"id": "17f9b826ae87bf9f4f3e5eb7e526a9e692f93eec9ce8cc2e4598e809e45a7993", "heading": "Google TPU/Requirements/Provision Cloud TPUs with GKE", "level": 3, "text": "### Provision Cloud TPUs with GKE  \nFor more information about using TPUs with GKE, see:  \n- [About TPUs in GKE](https://cloud.google.com/kubernetes-engine/docs/concepts/tpus)\n- [Deploy TPU workloads in GKE Standard](https://cloud.google.com/kubernetes-engine/docs/how-to/tpus)\n- [Plan for TPUs in GKE](https://cloud.google.com/kubernetes-engine/docs/concepts/plan-tpus)", "file_path": "getting_started/installation/google_tpu.md"}
{"id": "17f9b826ae87bf9f4f3e5eb7e526a9e692f93eec9ce8cc2e4598e809e45a7993", "heading": "Google TPU/Configure a new environment/Provision a Cloud TPU with the queued resource API", "level": 3, "text": "## Configure a new environment  \n### Provision a Cloud TPU with the queued resource API  \nCreate a TPU v5e with 4 TPU chips:  \n```bash\ngcloud alpha compute tpus queued-resources create QUEUED_RESOURCE_ID \\\n--node-id TPU_NAME \\\n--project PROJECT_ID \\\n--zone ZONE \\\n--accelerator-type ACCELERATOR_TYPE \\\n--runtime-version RUNTIME_VERSION \\\n--service-account SERVICE_ACCOUNT\n```  \n| Parameter name     | Description                                                                                                                                                                                              |\n|--------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|", "file_path": "getting_started/installation/google_tpu.md"}
{"id": "17f9b826ae87bf9f4f3e5eb7e526a9e692f93eec9ce8cc2e4598e809e45a7993", "heading": "Google TPU/Configure a new environment/Provision a Cloud TPU with the queued resource API", "level": 3, "text": "| QUEUED_RESOURCE_ID | The user-assigned ID of the queued resource request.                                                                                                                                                     |\n| TPU_NAME           | The user-assigned name of the TPU which is created when the queued resource request is allocated.                                                                                                        |\n| PROJECT_ID         | Your Google Cloud project                                                                                                                                                                                |\n| ZONE               | The GCP zone where you want to create your Cloud TPU. The value you use depends on the version of TPUs you are using. For more information, see [TPU regions and zones]                                  |", "file_path": "getting_started/installation/google_tpu.md"}
{"id": "17f9b826ae87bf9f4f3e5eb7e526a9e692f93eec9ce8cc2e4598e809e45a7993", "heading": "Google TPU/Configure a new environment/Provision a Cloud TPU with the queued resource API", "level": 3, "text": "| ACCELERATOR_TYPE   | The TPU version you want to use. Specify the TPU version, for example `v5litepod-4` specifies a v5e TPU with 4 cores, `v6e-1` specifies a v6e TPU with 1 core. For more information, see [TPU versions]. |\n| RUNTIME_VERSION    | The TPU VM runtime version to use. For example, use `v2-alpha-tpuv6e` for a VM loaded with one or more v6e TPU(s).                                              |\n| SERVICE_ACCOUNT    | The email address for your service account. You can find it in the IAM Cloud Console under *Service Accounts*. For example: `tpu-service-account@<your_project_ID>.iam.gserviceaccount.com`              |  \nConnect to your TPU VM using SSH:  \n```bash\ngcloud compute tpus tpu-vm ssh TPU_NAME --project PROJECT_ID --zone ZONE\n```  \n!!! note\nWhen configuring `RUNTIME_VERSION` (\"TPU software version\") on GCP, ensure it matches the TPU generation you've selected by referencing the [TPU VM images] compatibility matrix. Using an incompatible version may prevent vLLM from running correctly.", "file_path": "getting_started/installation/google_tpu.md"}
{"id": "17f9b826ae87bf9f4f3e5eb7e526a9e692f93eec9ce8cc2e4598e809e45a7993", "heading": "Google TPU/Configure a new environment/Provision a Cloud TPU with the queued resource API", "level": 3, "text": "[TPU versions]: https://cloud.google.com/tpu/docs/runtimes\n[TPU VM images]: https://cloud.google.com/tpu/docs/runtimes\n[TPU regions and zones]: https://cloud.google.com/tpu/docs/regions-zones", "file_path": "getting_started/installation/google_tpu.md"}
{"id": "17f9b826ae87bf9f4f3e5eb7e526a9e692f93eec9ce8cc2e4598e809e45a7993", "heading": "Google TPU/Set up using Python/Pre-built wheels", "level": 3, "text": "## Set up using Python  \n### Pre-built wheels  \nCurrently, there are no pre-built TPU wheels.", "file_path": "getting_started/installation/google_tpu.md"}
{"id": "17f9b826ae87bf9f4f3e5eb7e526a9e692f93eec9ce8cc2e4598e809e45a7993", "heading": "Google TPU/Set up using Python/Build wheel from source", "level": 3, "text": "### Build wheel from source  \nInstall Miniconda:  \n```bash\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\nsource ~/.bashrc\n```  \nCreate and activate a Conda environment for vLLM:  \n```bash\nconda create -n vllm python=3.12 -y\nconda activate vllm\n```  \nClone the vLLM repository and go to the vLLM directory:  \n```bash\ngit clone https://github.com/vllm-project/vllm.git && cd vllm\n```  \nUninstall the existing `torch` and `torch_xla` packages:  \n```bash\npip uninstall torch torch-xla -y\n```  \nInstall build dependencies:  \n```bash\npip install -r requirements/tpu.txt\nsudo apt-get install --no-install-recommends --yes libopenblas-base libopenmpi-dev libomp-dev\n```  \nRun the setup script:  \n```bash\nVLLM_TARGET_DEVICE=\"tpu\" python -m pip install -e .\n```", "file_path": "getting_started/installation/google_tpu.md"}
{"id": "17f9b826ae87bf9f4f3e5eb7e526a9e692f93eec9ce8cc2e4598e809e45a7993", "heading": "Google TPU/Set up using Docker/Pre-built images", "level": 3, "text": "## Set up using Docker  \n### Pre-built images  \nSee [deployment-docker-pre-built-image][deployment-docker-pre-built-image] for instructions on using the official Docker image, making sure to substitute the image name `vllm/vllm-openai` with `vllm/vllm-tpu`.", "file_path": "getting_started/installation/google_tpu.md"}
{"id": "17f9b826ae87bf9f4f3e5eb7e526a9e692f93eec9ce8cc2e4598e809e45a7993", "heading": "Google TPU/Set up using Docker/Build image from source", "level": 3, "text": "### Build image from source  \nYou can use <gh-file:docker/Dockerfile.tpu> to build a Docker image with TPU support.  \n```bash\ndocker build -f docker/Dockerfile.tpu -t vllm-tpu .\n```  \nRun the Docker image with the following command:  \n```bash\n# Make sure to add `--privileged --net host --shm-size=16G`.\ndocker run --privileged --net host --shm-size=16G -it vllm-tpu\n```  \n!!! note\nSince TPU relies on XLA which requires static shapes, vLLM bucketizes the\npossible input shapes and compiles an XLA graph for each shape. The\ncompilation time may take 20~30 minutes in the first run. However, the\ncompilation time reduces to ~5 minutes afterwards because the XLA graphs are\ncached in the disk (in `VLLM_XLA_CACHE_PATH` or `~/.cache/vllm/xla_cache` by default).  \n!!! tip\nIf you encounter the following error:  \n```console\nfrom torch._C import *  # noqa: F403\nImportError: libopenblas.so.0: cannot open shared object file: No such\nfile or directory\n```  \nInstall OpenBLAS with the following command:  \n```bash", "file_path": "getting_started/installation/google_tpu.md"}
{"id": "17f9b826ae87bf9f4f3e5eb7e526a9e692f93eec9ce8cc2e4598e809e45a7993", "heading": "Google TPU/Set up using Docker/Build image from source", "level": 3, "text": "```  \nInstall OpenBLAS with the following command:  \n```bash\nsudo apt-get install --no-install-recommends --yes libopenblas-base libopenmpi-dev libomp-dev\n```", "file_path": "getting_started/installation/google_tpu.md"}
{"id": "ac1a5e0a54c52e05c124f1111c66fb46b72614606be2378890bc6a55e59a8ee4", "heading": "Install on Cuda GPU", "level": 1, "text": "# Install on Cuda GPU  \nvLLM contains pre-compiled C++ and CUDA (12.8) binaries.", "file_path": "getting_started/installation/gpu/cuda.md"}
{"id": "ac1a5e0a54c52e05c124f1111c66fb46b72614606be2378890bc6a55e59a8ee4", "heading": "Install on Cuda GPU/Requirements", "level": 2, "text": "### Requirements\n- GPU: compute capability 7.0 or higher (e.g., V100, T4, RTX20xx, A100, L4, H100, etc.)", "file_path": "getting_started/installation/gpu/cuda.md"}
{"id": "ac1a5e0a54c52e05c124f1111c66fb46b72614606be2378890bc6a55e59a8ee4", "heading": "Install on Cuda GPU/Set up using Python", "level": 2, "text": "## Set up using Python  \n!!! note\nPyTorch installed via `conda` will statically link `NCCL` library, which can cause issues when vLLM tries to use `NCCL`. See <gh-issue:8420> for more details.  \nIn order to be performant, vLLM has to compile many cuda kernels. The compilation unfortunately introduces binary incompatibility with other CUDA versions and PyTorch versions, even for the same PyTorch version with different building configurations.  \nTherefore, it is recommended to install vLLM with a **fresh new** environment. If either you have a different CUDA version or you want to use an existing PyTorch installation, you need to build vLLM from source. See [below][build-from-source] for more details.", "file_path": "getting_started/installation/gpu/cuda.md"}
{"id": "ac1a5e0a54c52e05c124f1111c66fb46b72614606be2378890bc6a55e59a8ee4", "heading": "Install on Cuda GPU/Set up using Python/Pre-build wheels", "level": 3, "text": "### Pre-build wheels  \n```bash\nuv pip install vllm --torch-backend=auto\n```  \n??? console \"pip\"\n```bash\n# Install vLLM with CUDA 12.8.\npip install vllm --extra-index-url https://download.pytorch.org/whl/cu128\n```  \nWe recommend leveraging `uv` to [automatically select the appropriate PyTorch index at runtime](https://docs.astral.sh/uv/guides/integration/pytorch/#automatic-backend-selection) by inspecting the installed CUDA driver version via `--torch-backend=auto` (or `UV_TORCH_BACKEND=auto`). To select a specific backend (e.g., `cu126`), set `--torch-backend=cu126` (or `UV_TORCH_BACKEND=cu126`). If this doesn't work, try running `uv self update` to update `uv` first.  \n!!! note\nNVIDIA Blackwell GPUs (B200, GB200) require a minimum of CUDA 12.8, so make sure you are installing PyTorch wheels with at least that version. PyTorch itself offers a [dedicated interface](https://pytorch.org/get-started/locally/) to determine the appropriate pip command to run for a given target configuration.", "file_path": "getting_started/installation/gpu/cuda.md"}
{"id": "ac1a5e0a54c52e05c124f1111c66fb46b72614606be2378890bc6a55e59a8ee4", "heading": "Install on Cuda GPU/Set up using Python/Pre-build wheels", "level": 3, "text": "As of now, vLLM's binaries are compiled with CUDA 12.8 and public PyTorch release versions by default. We also provide vLLM binaries compiled with CUDA 12.6, 11.8, and public PyTorch release versions:  \n```bash\n# Install vLLM with a specific CUDA version (e.g., 11.8 or 12.6).\nexport VLLM_VERSION=$(curl -s https://api.github.com/repos/vllm-project/vllm/releases/latest | jq -r .tag_name | sed 's/^v//')\nexport CUDA_VERSION=118 # or 126\nuv pip install https://github.com/vllm-project/vllm/releases/download/v${VLLM_VERSION}/vllm-${VLLM_VERSION}+cu${CUDA_VERSION}-cp38-abi3-manylinux1_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu${CUDA_VERSION}\n```  \n#### Install the latest code", "file_path": "getting_started/installation/gpu/cuda.md"}
{"id": "ac1a5e0a54c52e05c124f1111c66fb46b72614606be2378890bc6a55e59a8ee4", "heading": "Install on Cuda GPU/Set up using Python/Pre-build wheels", "level": 3, "text": "```  \n#### Install the latest code  \nLLM inference is a fast-evolving field, and the latest code may contain bug fixes, performance improvements, and new features that are not released yet. To allow users to try the latest code without waiting for the next release, vLLM provides wheels for Linux running on an x86 platform with CUDA 12 for every commit since `v0.5.3`.  \n```bash\nuv pip install -U vllm \\\n--torch-backend=auto \\\n--extra-index-url https://wheels.vllm.ai/nightly\n```  \n??? console \"pip\"\n```bash\npip install -U vllm \\\n--pre \\\n--extra-index-url https://wheels.vllm.ai/nightly\n```  \n`--pre` is required for `pip` to consider pre-released versions.  \n#### Install specific revisions  \nIf you want to access the wheels for previous commits (e.g. to bisect the behavior change, performance regression), you can specify the commit hash in the URL:  \n```bash\nexport VLLM_COMMIT=72d9c316d3f6ede485146fe5aabd4e61dbc59069 # use full commit hash from the main branch\nuv pip install vllm \\\n--torch-backend=auto \\", "file_path": "getting_started/installation/gpu/cuda.md"}
{"id": "ac1a5e0a54c52e05c124f1111c66fb46b72614606be2378890bc6a55e59a8ee4", "heading": "Install on Cuda GPU/Set up using Python/Pre-build wheels", "level": 3, "text": "uv pip install vllm \\\n--torch-backend=auto \\\n--extra-index-url https://wheels.vllm.ai/${VLLM_COMMIT}\n```  \nThe `uv` approach works for vLLM `v0.6.6` and later and offers an easy-to-remember command. A unique feature of `uv` is that packages in `--extra-index-url` have [higher priority than the default index](https://docs.astral.sh/uv/pip/compatibility/#packages-that-exist-on-multiple-indexes). If the latest public release is `v0.6.6.post1`, `uv`'s behavior allows installing a commit before `v0.6.6.post1` by specifying the `--extra-index-url`. In contrast, `pip` combines packages from `--extra-index-url` and the default index, choosing only the latest version, which makes it difficult to install a development version prior to the released version.  \n??? note \"pip\"\nIf you want to access the wheels for previous commits (e.g. to bisect the behavior change,\nperformance regression), due to the limitation of `pip`, you have to specify the full URL of the\nwheel file by embedding the commit hash in the URL:  \n```bash", "file_path": "getting_started/installation/gpu/cuda.md"}
{"id": "ac1a5e0a54c52e05c124f1111c66fb46b72614606be2378890bc6a55e59a8ee4", "heading": "Install on Cuda GPU/Set up using Python/Pre-build wheels", "level": 3, "text": "wheel file by embedding the commit hash in the URL:  \n```bash\nexport VLLM_COMMIT=33f460b17a54acb3b6cc0b03f4a17876cff5eafd # use full commit hash from the main branch\npip install https://wheels.vllm.ai/${VLLM_COMMIT}/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl\n```  \nNote that the wheels are built with Python 3.8 ABI (see [PEP\n425](https://peps.python.org/pep-0425/) for more details about ABI), so **they are compatible\nwith Python 3.8 and later**. The version string in the wheel file name (`1.0.0.dev`) is just a\nplaceholder to have a unified URL for the wheels, the actual versions of wheels are contained in\nthe wheel metadata (the wheels listed in the extra index url have correct versions). Although we\ndon't support Python 3.8 any more (because PyTorch 2.5 dropped support for Python 3.8), the\nwheels are still built with Python 3.8 ABI to keep the same wheel name as before.", "file_path": "getting_started/installation/gpu/cuda.md"}
{"id": "ac1a5e0a54c52e05c124f1111c66fb46b72614606be2378890bc6a55e59a8ee4", "heading": "Install on Cuda GPU/Set up using Python/Build wheels from source", "level": 3, "text": "### Build wheels from source  \n#### Set up using Python-only build (without compilation)  \nIf you only need to change Python code, you can build and install vLLM without compilation. Using `uv pip`'s [`--editable` flag](https://docs.astral.sh/uv/pip/packages/#editable-packages), changes you make to the code will be reflected when you run vLLM:  \n```bash\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\nVLLM_USE_PRECOMPILED=1 uv pip install --editable .\n```  \nThis command will do the following:  \n1. Look for the current branch in your vLLM clone.\n1. Identify the corresponding base commit in the main branch.\n1. Download the pre-built wheel of the base commit.\n1. Use its compiled libraries in the installation.  \n!!! note\n1. If you change C++ or kernel code, you cannot use Python-only build; otherwise you will see an import error about library not found or undefined symbol.", "file_path": "getting_started/installation/gpu/cuda.md"}
{"id": "ac1a5e0a54c52e05c124f1111c66fb46b72614606be2378890bc6a55e59a8ee4", "heading": "Install on Cuda GPU/Set up using Python/Build wheels from source", "level": 3, "text": "2. If you rebase your dev branch, it is recommended to uninstall vllm and re-run the above command to make sure your libraries are up to date.  \nIn case you see an error about wheel not found when running the above command, it might be because the commit you based on in the main branch was just merged and the wheel is being built. In this case, you can wait for around an hour to try again, or manually assign the previous commit in the installation using the `VLLM_PRECOMPILED_WHEEL_LOCATION` environment variable.  \n```bash\nexport VLLM_COMMIT=72d9c316d3f6ede485146fe5aabd4e61dbc59069 # use full commit hash from the main branch\nexport VLLM_PRECOMPILED_WHEEL_LOCATION=https://wheels.vllm.ai/${VLLM_COMMIT}/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl\nuv pip install --editable .\n```  \nYou can find more information about vLLM's wheels in [install-the-latest-code][install-the-latest-code].  \n!!! note", "file_path": "getting_started/installation/gpu/cuda.md"}
{"id": "ac1a5e0a54c52e05c124f1111c66fb46b72614606be2378890bc6a55e59a8ee4", "heading": "Install on Cuda GPU/Set up using Python/Build wheels from source", "level": 3, "text": "!!! note\nThere is a possibility that your source code may have a different commit ID compared to the latest vLLM wheel, which could potentially lead to unknown errors.\nIt is recommended to use the same commit ID for the source code as the vLLM wheel you have installed. Please refer to [install-the-latest-code][install-the-latest-code] for instructions on how to install a specified wheel.  \n#### Full build (with compilation)  \nIf you want to modify C++ or CUDA code, you'll need to build vLLM from source. This can take several minutes:  \n```bash\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\nuv pip install -e .\n```  \n!!! tip\nBuilding from source requires a lot of compilation. If you are building from source repeatedly, it's more efficient to cache the compilation results.  \nFor example, you can install [ccache](https://github.com/ccache/ccache) using `conda install ccache` or `apt install ccache` .", "file_path": "getting_started/installation/gpu/cuda.md"}
{"id": "ac1a5e0a54c52e05c124f1111c66fb46b72614606be2378890bc6a55e59a8ee4", "heading": "Install on Cuda GPU/Set up using Python/Build wheels from source", "level": 3, "text": "As long as `which ccache` command can find the `ccache` binary, it will be used automatically by the build system. After the first build, subsequent builds will be much faster.  \nWhen using `ccache` with `pip install -e .`, you should run `CCACHE_NOHASHDIR=\"true\" pip install --no-build-isolation -e .`. This is because `pip` creates a new folder with a random name for each build, preventing `ccache` from recognizing that the same files are being built.  \n[sccache](https://github.com/mozilla/sccache) works similarly to `ccache`, but has the capability to utilize caching in remote storage environments.\nThe following environment variables can be set to configure the vLLM `sccache` remote: `SCCACHE_BUCKET=vllm-build-sccache SCCACHE_REGION=us-west-2 SCCACHE_S3_NO_CREDENTIALS=1`. We also recommend setting `SCCACHE_IDLE_TIMEOUT=0`.  \n!!! note \"Faster Kernel Development\"", "file_path": "getting_started/installation/gpu/cuda.md"}
{"id": "ac1a5e0a54c52e05c124f1111c66fb46b72614606be2378890bc6a55e59a8ee4", "heading": "Install on Cuda GPU/Set up using Python/Build wheels from source", "level": 3, "text": "!!! note \"Faster Kernel Development\"\nFor frequent C++/CUDA kernel changes, after the initial `uv pip install -e .` setup, consider using the [Incremental Compilation Workflow](../../contributing/incremental_build.md) for significantly faster rebuilds of only the modified kernel code.  \n##### Use an existing PyTorch installation  \nThere are scenarios where the PyTorch dependency cannot be easily installed with `uv`, e.g.:  \n- Building vLLM with PyTorch nightly or a custom PyTorch build.\n- Building vLLM with aarch64 and CUDA (GH200), where the PyTorch wheels are not available on PyPI. Currently, only the PyTorch nightly has wheels for aarch64 with CUDA. You can run `uv pip install --index-url https://download.pytorch.org/whl/nightly/cu128 torch torchvision torchaudio` to [install PyTorch nightly](https://pytorch.org/get-started/locally/) and then build vLLM on top of it.  \nTo build vLLM using an existing PyTorch installation:  \n```bash\n# install PyTorch first, either from PyPI or from source", "file_path": "getting_started/installation/gpu/cuda.md"}
{"id": "ac1a5e0a54c52e05c124f1111c66fb46b72614606be2378890bc6a55e59a8ee4", "heading": "Install on Cuda GPU/Set up using Python/Build wheels from source", "level": 3, "text": "# install PyTorch first, either from PyPI or from source\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\npython use_existing_torch.py\nuv pip install -r requirements/build.txt\nuv pip install --no-build-isolation -e .\n```  \nAlternatively: if you are exclusively using `uv` to create and manage virtual environments, it has [a unique mechanism](https://docs.astral.sh/uv/concepts/projects/config/#disabling-build-isolation)\nfor disabling build isolation for specific packages. vLLM can leverage this mechanism to specify `torch` as the package to disable build isolation for:  \n```bash\n# install PyTorch first, either from PyPI or from source\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\n# pip install -e . does not work directly, only uv can do this\nuv pip install -e .\n```  \n##### Use the local cutlass for compilation  \nCurrently, before starting the build process, vLLM fetches cutlass code from GitHub. However, there may be scenarios where you want to use a local version of cutlass instead.", "file_path": "getting_started/installation/gpu/cuda.md"}
{"id": "ac1a5e0a54c52e05c124f1111c66fb46b72614606be2378890bc6a55e59a8ee4", "heading": "Install on Cuda GPU/Set up using Python/Build wheels from source", "level": 3, "text": "To achieve this, you can set the environment variable VLLM_CUTLASS_SRC_DIR to point to your local cutlass directory.  \n```bash\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\nVLLM_CUTLASS_SRC_DIR=/path/to/cutlass uv pip install -e .\n```  \n#### Troubleshooting  \nTo avoid your system being overloaded, you can limit the number of compilation jobs\nto be run simultaneously, via the environment variable `MAX_JOBS`. For example:  \n```bash\nexport MAX_JOBS=6\nuv pip install -e .\n```  \nThis is especially useful when you are building on less powerful machines. For example, when you use WSL it only [assigns 50% of the total memory by default](https://learn.microsoft.com/en-us/windows/wsl/wsl-config#main-wsl-settings), so using `export MAX_JOBS=1` can avoid compiling multiple files simultaneously and running out of memory.\nA side effect is a much slower build process.  \nAdditionally, if you have trouble building vLLM, we recommend using the NVIDIA PyTorch Docker image.  \n```bash", "file_path": "getting_started/installation/gpu/cuda.md"}
{"id": "ac1a5e0a54c52e05c124f1111c66fb46b72614606be2378890bc6a55e59a8ee4", "heading": "Install on Cuda GPU/Set up using Python/Build wheels from source", "level": 3, "text": "```bash\n# Use `--ipc=host` to make sure the shared memory is large enough.\ndocker run \\\n--gpus all \\\n-it \\\n--rm \\\n--ipc=host nvcr.io/nvidia/pytorch:23.10-py3\n```  \nIf you don't want to use docker, it is recommended to have a full installation of CUDA Toolkit. You can download and install it from [the official website](https://developer.nvidia.com/cuda-toolkit-archive). After installation, set the environment variable `CUDA_HOME` to the installation path of CUDA Toolkit, and make sure that the `nvcc` compiler is in your `PATH`, e.g.:  \n```bash\nexport CUDA_HOME=/usr/local/cuda\nexport PATH=\"${CUDA_HOME}/bin:$PATH\"\n```  \nHere is a sanity check to verify that the CUDA Toolkit is correctly installed:  \n```bash\nnvcc --version # verify that nvcc is in your PATH\n${CUDA_HOME}/bin/nvcc --version # verify that nvcc is in your CUDA_HOME\n```  \n#### Unsupported OS build", "file_path": "getting_started/installation/gpu/cuda.md"}
{"id": "ac1a5e0a54c52e05c124f1111c66fb46b72614606be2378890bc6a55e59a8ee4", "heading": "Install on Cuda GPU/Set up using Python/Build wheels from source", "level": 3, "text": "```  \n#### Unsupported OS build  \nvLLM can fully run only on Linux but for development purposes, you can still build it on other systems (for example, macOS), allowing for imports and a more convenient development environment. The binaries will not be compiled and won't work on non-Linux systems.  \nSimply disable the `VLLM_TARGET_DEVICE` environment variable before installing:  \n```bash\nexport VLLM_TARGET_DEVICE=empty\nuv pip install -e .\n```", "file_path": "getting_started/installation/gpu/cuda.md"}
{"id": "ac1a5e0a54c52e05c124f1111c66fb46b72614606be2378890bc6a55e59a8ee4", "heading": "Install on Cuda GPU/Set up using Docker/Use pre-built images", "level": 3, "text": "## Set up using Docker  \n### Use pre-built images  \nSee [deployment-docker-pre-built-image][deployment-docker-pre-built-image] for instructions on using the official Docker image.  \nAnother way to access the latest code is to use the docker images:  \n```bash\nexport VLLM_COMMIT=33f460b17a54acb3b6cc0b03f4a17876cff5eafd # use full commit hash from the main branch\ndocker pull public.ecr.aws/q9t5s3a7/vllm-ci-postmerge-repo:${VLLM_COMMIT}\n```  \nThese docker images are used for CI and testing only, and they are not intended for production use. They will be expired after several days.  \nThe latest code can contain bugs and may not be stable. Please use it with caution.", "file_path": "getting_started/installation/gpu/cuda.md"}
{"id": "ac1a5e0a54c52e05c124f1111c66fb46b72614606be2378890bc6a55e59a8ee4", "heading": "Install on Cuda GPU/Set up using Docker/Build Image from source", "level": 3, "text": "### Build Image from source\nSee [deployment-docker-build-image-from-source][deployment-docker-build-image-from-source] for instructions on building the Docker image.", "file_path": "getting_started/installation/gpu/cuda.md"}
{"id": "a9310d90ff72728fa8121962ad9c9989e21cf88273ec8cefe7afe0cfbb5ae7f9", "heading": "Install on ROCm GPU", "level": 1, "text": "# Install on ROCm GPU  \nvLLM supports AMD GPUs with ROCm 6.3 or above.  \n!!! tip\n[Docker](#set-up-using-docker) is the recommended way to use vLLM on ROCm.  \n!!! warning\nThere are no pre-built wheels for this device, so you must either use the pre-built Docker image or build vLLM from source.", "file_path": "getting_started/installation/gpu/rocm.md"}
{"id": "a9310d90ff72728fa8121962ad9c9989e21cf88273ec8cefe7afe0cfbb5ae7f9", "heading": "Install on ROCm GPU/Requirements", "level": 2, "text": "## Requirements  \n- GPU: MI200s (gfx90a), MI300 (gfx942), MI350 (gfx950), Radeon RX 7900 series (gfx1100/1101), Radeon RX 9000 series (gfx1200/1201)\n- ROCm 6.3 or above\n- MI350 requires ROCm 7.0 or above", "file_path": "getting_started/installation/gpu/rocm.md"}
{"id": "a9310d90ff72728fa8121962ad9c9989e21cf88273ec8cefe7afe0cfbb5ae7f9", "heading": "Install on ROCm GPU/Setup using Python", "level": 2, "text": "## Setup using Python  \nThere is no extra information on creating a new Python environment for this device.", "file_path": "getting_started/installation/gpu/rocm.md"}
{"id": "a9310d90ff72728fa8121962ad9c9989e21cf88273ec8cefe7afe0cfbb5ae7f9", "heading": "Install on ROCm GPU/Setup using Python/Pre-built wheels", "level": 3, "text": "### Pre-built wheels\nCurrently, there are no pre-built ROCm wheels.", "file_path": "getting_started/installation/gpu/rocm.md"}
{"id": "a9310d90ff72728fa8121962ad9c9989e21cf88273ec8cefe7afe0cfbb5ae7f9", "heading": "Install on ROCm GPU/Setup using Python/Build wheels from source", "level": 3, "text": "### Build wheels from source\n0. Install prerequisites (skip if you are already in an environment/docker with the following installed):  \n- [ROCm](https://rocm.docs.amd.com/en/latest/deploy/linux/index.html)\n- [PyTorch](https://pytorch.org/)  \nFor installing PyTorch, you can start from a fresh docker image, e.g, `rocm/pytorch:rocm6.4.3_ubuntu24.04_py3.12_pytorch_release_2.6.0`, `rocm/pytorch-nightly`. If you are using docker image, you can skip to Step 3.  \nAlternatively, you can install PyTorch using PyTorch wheels. You can check PyTorch installation guide in PyTorch [Getting Started](https://pytorch.org/get-started/locally/). Example:  \n```bash\n# Install PyTorch\npip uninstall torch -y\npip install --no-cache-dir torch torchvision --index-url https://download.pytorch.org/whl/rocm6.4\n```  \n1. Install [Triton for ROCm](https://github.com/triton-lang/triton)", "file_path": "getting_started/installation/gpu/rocm.md"}
{"id": "a9310d90ff72728fa8121962ad9c9989e21cf88273ec8cefe7afe0cfbb5ae7f9", "heading": "Install on ROCm GPU/Setup using Python/Build wheels from source", "level": 3, "text": "Install ROCm's Triton (the default triton-mlir branch) following the instructions from [ROCm/triton](https://github.com/ROCm/triton/blob/triton-mlir/README.md)  \n```bash\npython3 -m pip install ninja cmake wheel pybind11\npip uninstall -y triton\ngit clone https://github.com/triton-lang/triton.git\ncd triton\ngit checkout e5be006\nif [ ! -f setup.py ]; then cd python; fi\npython3 setup.py install\ncd ../..\n```  \n!!! note\nIf you see HTTP issue related to downloading packages during building triton, please try again as the HTTP error is intermittent.  \n2. Optionally, if you choose to use CK flash attention, you can install [flash attention for ROCm](https://github.com/Dao-AILab/flash-attention)  \nInstall ROCm's flash attention (v2.7.2) following the instructions from [ROCm/flash-attention](https://github.com/ROCm/flash-attention#amd-rocm-support)\nAlternatively, wheels intended for vLLM use can be accessed under the releases.", "file_path": "getting_started/installation/gpu/rocm.md"}
{"id": "a9310d90ff72728fa8121962ad9c9989e21cf88273ec8cefe7afe0cfbb5ae7f9", "heading": "Install on ROCm GPU/Setup using Python/Build wheels from source", "level": 3, "text": "For example, for ROCm 6.3, suppose your gfx arch is `gfx90a`. To get your gfx architecture, run `rocminfo |grep gfx`.  \n```bash\ngit clone https://github.com/Dao-AILab/flash-attention.git\ncd flash-attention\ngit checkout 1a7f4dfa\ngit submodule update --init\nGPU_ARCHS=\"gfx90a\" python3 setup.py install\ncd ..\n```  \n!!! note\nYou might need to downgrade the \"ninja\" version to 1.10 as it is not used when compiling flash-attention-2 (e.g. `pip install ninja==1.10.2.4`)  \n3. If you choose to build AITER yourself to use a certain branch or commit, you can build AITER using the following steps:  \n```bash\npython3 -m pip uninstall -y aiter\ngit clone --recursive https://github.com/ROCm/aiter.git\ncd aiter\ngit checkout $AITER_BRANCH_OR_COMMIT\ngit submodule sync; git submodule update --init --recursive\npython3 setup.py develop\n```  \n!!! note\nYou will need to config the `$AITER_BRANCH_OR_COMMIT` for your purpose.  \n4. Build vLLM. For example, vLLM on ROCM 6.3 can be built with the following steps:  \n??? console \"Commands\"", "file_path": "getting_started/installation/gpu/rocm.md"}
{"id": "a9310d90ff72728fa8121962ad9c9989e21cf88273ec8cefe7afe0cfbb5ae7f9", "heading": "Install on ROCm GPU/Setup using Python/Build wheels from source", "level": 3, "text": "??? console \"Commands\"  \n```bash\npip install --upgrade pip", "file_path": "getting_started/installation/gpu/rocm.md"}
{"id": "a9310d90ff72728fa8121962ad9c9989e21cf88273ec8cefe7afe0cfbb5ae7f9", "heading": "Install on ROCm GPU/Setup using Python/Build wheels from source", "level": 3, "text": "# Build & install AMD SMI\npip install /opt/rocm/share/amd_smi\n\n# Install dependencies\npip install --upgrade numba \\\nscipy \\\nhuggingface-hub[cli,hf_transfer] \\\nsetuptools_scm\npip install \"numpy<2\"\npip install -r requirements/rocm.txt", "file_path": "getting_started/installation/gpu/rocm.md"}
{"id": "a9310d90ff72728fa8121962ad9c9989e21cf88273ec8cefe7afe0cfbb5ae7f9", "heading": "Install on ROCm GPU/Setup using Python/Build wheels from source", "level": 3, "text": "# Build vLLM for MI210/MI250/MI300.\nexport PYTORCH_ROCM_ARCH=\"gfx90a;gfx942\"\npython3 setup.py develop\n```  \nThis may take 5-10 minutes. Currently, `pip install .` does not work for ROCm installation.  \n!!! tip\n- Triton flash attention is used by default. For benchmarking purposes, it is recommended to run a warm-up step before collecting perf numbers.\n- Triton flash attention does not currently support sliding window attention. If using half precision, please use CK flash-attention for sliding window support.\n- To use CK flash-attention or PyTorch naive attention, please use this flag `export VLLM_USE_TRITON_FLASH_ATTN=0` to turn off triton flash attention.\n- The ROCm version of PyTorch, ideally, should match the ROCm driver version.  \n!!! tip\n- For MI300x (gfx942) users, to achieve optimal performance, please refer to [MI300x tuning guide](https://rocm.docs.amd.com/en/latest/how-to/tuning-guides/mi300x/index.html) for performance optimization and tuning tips on system and workflow level.", "file_path": "getting_started/installation/gpu/rocm.md"}
{"id": "a9310d90ff72728fa8121962ad9c9989e21cf88273ec8cefe7afe0cfbb5ae7f9", "heading": "Install on ROCm GPU/Setup using Python/Build wheels from source", "level": 3, "text": "For vLLM, please refer to [vLLM performance optimization](https://rocm.docs.amd.com/en/latest/how-to/tuning-guides/mi300x/workload.html#vllm-performance-optimization).", "file_path": "getting_started/installation/gpu/rocm.md"}
{"id": "a9310d90ff72728fa8121962ad9c9989e21cf88273ec8cefe7afe0cfbb5ae7f9", "heading": "Install on ROCm GPU/Docker Images/Pre-build images", "level": 3, "text": "## Docker Images  \n### Pre-build images  \nThe [AMD Infinity hub for vLLM](https://hub.docker.com/r/rocm/vllm/tags) offers a prebuilt, optimized\ndocker image designed for validating inference performance on the AMD Instinctâ„¢ MI300X accelerator.  \n!!! tip\nPlease check [LLM inference performance validation on AMD Instinct MI300X](https://rocm.docs.amd.com/en/latest/how-to/performance-validation/mi300x/vllm-benchmark.html)\nfor instructions on how to use this prebuilt docker image.", "file_path": "getting_started/installation/gpu/rocm.md"}
{"id": "a9310d90ff72728fa8121962ad9c9989e21cf88273ec8cefe7afe0cfbb5ae7f9", "heading": "Install on ROCm GPU/Docker Images/Build Image from source", "level": 3, "text": "### Build Image from source  \nBuilding the Docker image from source is the recommended way to use vLLM with ROCm.  \n#### (Optional) Build an image with ROCm software stack  \nBuild a docker image from <gh-file:docker/Dockerfile.rocm_base> which setup ROCm software stack needed by the vLLM.\n**This step is optional as this rocm_base image is usually prebuilt and store at [Docker Hub](https://hub.docker.com/r/rocm/vllm-dev) under tag `rocm/vllm-dev:base` to speed up user experience.**\nIf you choose to build this rocm_base image yourself, the steps are as follows.  \nIt is important that the user kicks off the docker build using buildkit. Either the user put DOCKER_BUILDKIT=1 as environment variable when calling docker build command, or the user needs to set up buildkit in the docker daemon configuration /etc/docker/daemon.json as follows and restart the daemon:  \n```json\n{\n\"features\": {\n\"buildkit\": true\n}\n}\n```  \nTo build vllm on ROCm 6.3 for MI200 and MI300 series, you can use the default:  \n```bash", "file_path": "getting_started/installation/gpu/rocm.md"}
{"id": "a9310d90ff72728fa8121962ad9c9989e21cf88273ec8cefe7afe0cfbb5ae7f9", "heading": "Install on ROCm GPU/Docker Images/Build Image from source", "level": 3, "text": "```bash\nDOCKER_BUILDKIT=1 docker build \\\n-f docker/Dockerfile.rocm_base \\\n-t rocm/vllm-dev:base .\n```  \n#### Build an image with vLLM  \nFirst, build a docker image from <gh-file:docker/Dockerfile.rocm> and launch a docker container from the image.\nIt is important that the user kicks off the docker build using buildkit. Either the user put `DOCKER_BUILDKIT=1` as environment variable when calling docker build command, or the user needs to set up buildkit in the docker daemon configuration /etc/docker/daemon.json as follows and restart the daemon:  \n```bash\n{\n\"features\": {\n\"buildkit\": true\n}\n}\n```  \n<gh-file:docker/Dockerfile.rocm> uses ROCm 6.3 by default, but also supports ROCm 5.7, 6.0, 6.1, and 6.2, in older vLLM branches.\nIt provides flexibility to customize the build of docker image using the following arguments:", "file_path": "getting_started/installation/gpu/rocm.md"}
{"id": "a9310d90ff72728fa8121962ad9c9989e21cf88273ec8cefe7afe0cfbb5ae7f9", "heading": "Install on ROCm GPU/Docker Images/Build Image from source", "level": 3, "text": "- `BASE_IMAGE`: specifies the base image used when running `docker build`. The default value `rocm/vllm-dev:base` is an image published and maintained by AMD. It is being built using <gh-file:docker/Dockerfile.rocm_base>\n- `ARG_PYTORCH_ROCM_ARCH`: Allows to override the gfx architecture values from the base docker image  \nTheir values can be passed in when running `docker build` with `--build-arg` options.  \nTo build vllm on ROCm 6.3 for MI200 and MI300 series, you can use the default:  \n```bash\nDOCKER_BUILDKIT=1 docker build -f docker/Dockerfile.rocm -t vllm-rocm .\n```  \nTo run the above docker image `vllm-rocm`, use the below command:  \n??? console \"Command\"  \n```bash\ndocker run -it \\\n--network=host \\\n--group-add=video \\\n--ipc=host \\\n--cap-add=SYS_PTRACE \\\n--security-opt seccomp=unconfined \\\n--device /dev/kfd \\\n--device /dev/dri \\\n-v <path/to/model>:/app/model \\\nvllm-rocm\n```  \nWhere the `<path/to/model>` is the location where the model is stored, for example, the weights for llama2 or llama3 models.", "file_path": "getting_started/installation/gpu/rocm.md"}
{"id": "3b64a0e55c595e496e157d2976e4d633d0616d12f877235fda7f4eea4b0eedd9", "heading": "Install on XPU", "level": 1, "text": "# Install on XPU  \nvLLM initially supports basic model inference and serving on Intel GPU platform.  \n!!! warning\nThere are no pre-built wheels for this device, so you need build vLLM from source. Or you can use pre-built images which are based on vLLM released versions.", "file_path": "getting_started/installation/gpu/xpu.md"}
{"id": "3b64a0e55c595e496e157d2976e4d633d0616d12f877235fda7f4eea4b0eedd9", "heading": "Install on XPU/Requirements", "level": 2, "text": "## Requirements  \n- Supported Hardware: Intel Data Center GPU, Intel ARC GPU\n- OneAPI requirements: oneAPI 2025.1\n- Python: 3.12\n!!! warning\nThe provided IPEX whl is Python3.12 specific so this version is a MUST.", "file_path": "getting_started/installation/gpu/xpu.md"}
{"id": "3b64a0e55c595e496e157d2976e4d633d0616d12f877235fda7f4eea4b0eedd9", "heading": "Install on XPU/Set up using Python", "level": 2, "text": "## Set up using Python  \nThere is no extra information on creating a new Python environment for this device.", "file_path": "getting_started/installation/gpu/xpu.md"}
{"id": "3b64a0e55c595e496e157d2976e4d633d0616d12f877235fda7f4eea4b0eedd9", "heading": "Install on XPU/Set up using Python/Pre-built wheels", "level": 3, "text": "### Pre-built wheels  \nCurrently, there are no pre-built XPU wheels.", "file_path": "getting_started/installation/gpu/xpu.md"}
{"id": "3b64a0e55c595e496e157d2976e4d633d0616d12f877235fda7f4eea4b0eedd9", "heading": "Install on XPU/Set up using Python/Build wheel from source", "level": 3, "text": "### Build wheel from source  \n- First, install required [driver](https://dgpu-docs.intel.com/driver/installation.html#installing-gpu-drivers) and [Intel OneAPI](https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit.html) 2025.1 or later.\n- Second, install Python packages for vLLM XPU backend building:  \n```bash\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\npip install --upgrade pip\npip install -v -r requirements/xpu.txt\n```  \n- Then, build and install vLLM XPU backend:  \n```bash\nVLLM_TARGET_DEVICE=xpu python setup.py install\n```", "file_path": "getting_started/installation/gpu/xpu.md"}
{"id": "3b64a0e55c595e496e157d2976e4d633d0616d12f877235fda7f4eea4b0eedd9", "heading": "Install on XPU/Set up using Docker/Pre-built images", "level": 3, "text": "## Set up using Docker  \n### Pre-built images  \nCurrently, we release prebuilt XPU images at docker [hub](https://hub.docker.com/r/intel/vllm/tags) based on vLLM released version. For more information, please refer release [note](https://github.com/intel/ai-containers/blob/main/vllm).", "file_path": "getting_started/installation/gpu/xpu.md"}
{"id": "3b64a0e55c595e496e157d2976e4d633d0616d12f877235fda7f4eea4b0eedd9", "heading": "Install on XPU/Set up using Docker/Build image from source", "level": 3, "text": "### Build image from source  \n```bash\ndocker build -f docker/Dockerfile.xpu -t vllm-xpu-env --shm-size=4g .\ndocker run -it \\\n--rm \\\n--network=host \\\n--device /dev/dri \\\n-v /dev/dri/by-path:/dev/dri/by-path \\\nvllm-xpu-env\n```", "file_path": "getting_started/installation/gpu/xpu.md"}
{"id": "3b64a0e55c595e496e157d2976e4d633d0616d12f877235fda7f4eea4b0eedd9", "heading": "Install on XPU/Supported features", "level": 2, "text": "## Supported features  \nXPU platform supports **tensor parallel** inference/serving and also supports **pipeline parallel** as a beta feature for online serving. For **pipeline parallel**, we support it on single node with mp as the backend. For example, a reference execution like following:  \n```bash\nvllm serve facebook/opt-13b \\\n--dtype=bfloat16 \\\n--max_model_len=1024 \\\n--distributed-executor-backend=mp \\\n--pipeline-parallel-size=2 \\\n-tp=8\n```  \nBy default, a ray instance will be launched automatically if no existing one is detected in the system, with `num-gpus` equals to `parallel_config.world_size`. We recommend properly starting a ray cluster before execution, referring to the <gh-file:examples/online_serving/run_cluster.sh> helper script.", "file_path": "getting_started/installation/gpu/xpu.md"}
{"id": "3b64a0e55c595e496e157d2976e4d633d0616d12f877235fda7f4eea4b0eedd9", "heading": "Install on XPU/Distributed backend", "level": 2, "text": "## Distributed backend  \nXPU platform uses **torch-ccl** for torch<2.8 and **xccl** for torch>=2.8 as distributed backend, since torch 2.8 supports **xccl** as built-in backend for XPU.", "file_path": "getting_started/installation/gpu/xpu.md"}
{"id": "4220a196f22dad7395cfdb94b51b44954576c860502e1f1c08196a85a3b5a7a6", "heading": "Quickstart", "level": 1, "text": "# Quickstart  \nThis guide will help you quickly get started with vLLM to perform:  \n- [Offline batched inference][quickstart-offline]\n- [Online serving using OpenAI-compatible server][quickstart-online]", "file_path": "getting_started/quickstart.md"}
{"id": "4220a196f22dad7395cfdb94b51b44954576c860502e1f1c08196a85a3b5a7a6", "heading": "Quickstart/Prerequisites", "level": 2, "text": "## Prerequisites  \n- OS: Linux\n- Python: 3.10 -- 3.13", "file_path": "getting_started/quickstart.md"}
{"id": "4220a196f22dad7395cfdb94b51b44954576c860502e1f1c08196a85a3b5a7a6", "heading": "Quickstart/Installation", "level": 2, "text": "## Installation  \nIf you are using NVIDIA GPUs, you can install vLLM using [pip](https://pypi.org/project/vllm/) directly.  \nIt's recommended to use [uv](https://docs.astral.sh/uv/), a very fast Python environment manager, to create and manage Python environments. Please follow the [documentation](https://docs.astral.sh/uv/#getting-started) to install `uv`. After installing `uv`, you can create a new Python environment and install vLLM using the following commands:  \n```bash\nuv venv --python 3.12 --seed\nsource .venv/bin/activate\nuv pip install vllm --torch-backend=auto\n```  \n`uv` can [automatically select the appropriate PyTorch index at runtime](https://docs.astral.sh/uv/guides/integration/pytorch/#automatic-backend-selection) by inspecting the installed CUDA driver version via `--torch-backend=auto` (or `UV_TORCH_BACKEND=auto`). To select a specific backend (e.g., `cu126`), set `--torch-backend=cu126` (or `UV_TORCH_BACKEND=cu126`).", "file_path": "getting_started/quickstart.md"}
{"id": "4220a196f22dad7395cfdb94b51b44954576c860502e1f1c08196a85a3b5a7a6", "heading": "Quickstart/Installation", "level": 2, "text": "Another delightful way is to use `uv run` with `--with [dependency]` option, which allows you to run commands such as `vllm serve` without creating any permanent environment:  \n```bash\nuv run --with vllm vllm --help\n```  \nYou can also use [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html) to create and manage Python environments. You can install `uv` to the conda environment through `pip` if you want to manage it within the environment.  \n```bash\nconda create -n myenv python=3.12 -y\nconda activate myenv\npip install --upgrade uv\nuv pip install vllm --torch-backend=auto\n```  \n!!! note\nFor more detail and non-CUDA platforms, please refer [here](installation/README.md) for specific instructions on how to install vLLM.  \n[](){ #quickstart-offline }", "file_path": "getting_started/quickstart.md"}
{"id": "4220a196f22dad7395cfdb94b51b44954576c860502e1f1c08196a85a3b5a7a6", "heading": "Quickstart/Offline Batched Inference", "level": 2, "text": "## Offline Batched Inference  \nWith vLLM installed, you can start generating texts for list of input prompts (i.e. offline batch inferencing). See the example script: <gh-file:examples/offline_inference/basic/basic.py>  \nThe first line of this example imports the classes [LLM][vllm.LLM] and [SamplingParams][vllm.SamplingParams]:  \n- [LLM][vllm.LLM] is the main class for running offline inference with vLLM engine.\n- [SamplingParams][vllm.SamplingParams] specifies the parameters for the sampling process.  \n```python\nfrom vllm import LLM, SamplingParams\n```  \nThe next section defines a list of input prompts and sampling parameters for text generation. The [sampling temperature](https://arxiv.org/html/2402.05201v1) is set to `0.8` and the [nucleus sampling probability](https://en.wikipedia.org/wiki/Top-p_sampling) is set to `0.95`. You can find more information about the sampling parameters [here][sampling-params].  \n!!! important", "file_path": "getting_started/quickstart.md"}
{"id": "4220a196f22dad7395cfdb94b51b44954576c860502e1f1c08196a85a3b5a7a6", "heading": "Quickstart/Offline Batched Inference", "level": 2, "text": "!!! important\nBy default, vLLM will use sampling parameters recommended by model creator by applying the `generation_config.json` from the Hugging Face model repository if it exists. In most cases, this will provide you with the best results by default if [SamplingParams][vllm.SamplingParams] is not specified.  \nHowever, if vLLM's default sampling parameters are preferred, please set `generation_config=\"vllm\"` when creating the [LLM][vllm.LLM] instance.  \n```python\nprompts = [\n\"Hello, my name is\",\n\"The president of the United States is\",\n\"The capital of France is\",\n\"The future of AI is\",\n]\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n```  \nThe [LLM][vllm.LLM] class initializes vLLM's engine and the [OPT-125M model](https://arxiv.org/abs/2205.01068) for offline inference. The list of supported models can be found [here](../models/supported_models.md).  \n```python\nllm = LLM(model=\"facebook/opt-125m\")\n```  \n!!! note", "file_path": "getting_started/quickstart.md"}
{"id": "4220a196f22dad7395cfdb94b51b44954576c860502e1f1c08196a85a3b5a7a6", "heading": "Quickstart/Offline Batched Inference", "level": 2, "text": "```python\nllm = LLM(model=\"facebook/opt-125m\")\n```  \n!!! note\nBy default, vLLM downloads models from [Hugging Face](https://huggingface.co/). If you would like to use models from [ModelScope](https://www.modelscope.cn), set the environment variable `VLLM_USE_MODELSCOPE` before initializing the engine.  \n```shell\nexport VLLM_USE_MODELSCOPE=True\n```  \nNow, the fun part! The outputs are generated using `llm.generate`. It adds the input prompts to the vLLM engine's waiting queue and executes the vLLM engine to generate the outputs with high throughput. The outputs are returned as a list of `RequestOutput` objects, which include all of the output tokens.  \n```python\noutputs = llm.generate(prompts, sampling_params)", "file_path": "getting_started/quickstart.md"}
{"id": "4220a196f22dad7395cfdb94b51b44954576c860502e1f1c08196a85a3b5a7a6", "heading": "Quickstart/Offline Batched Inference", "level": 2, "text": "for output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```  \n!!! note\nThe `llm.generate` method does not automatically apply the model's chat template to the input prompt. Therefore, if you are using an Instruct model or Chat model, you should manually apply the corresponding chat template to ensure the expected behavior. Alternatively, you can use the `llm.chat` method and pass a list of messages which have the same format as those passed to OpenAI's `client.chat.completions`:  \n??? code  \n```python\n# Using tokenizer to apply chat template\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"/path/to/chat_model\")\nmessages_list = [\n[{\"role\": \"user\", \"content\": prompt}]\nfor prompt in prompts\n]\ntexts = tokenizer.apply_chat_template(\nmessages_list,\ntokenize=False,\nadd_generation_prompt=True,\n)\n\n# Generate outputs\noutputs = llm.generate(texts, sampling_params)", "file_path": "getting_started/quickstart.md"}
{"id": "4220a196f22dad7395cfdb94b51b44954576c860502e1f1c08196a85a3b5a7a6", "heading": "Quickstart/Offline Batched Inference", "level": 2, "text": "# Print the outputs.\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n\n# Using chat interface.\noutputs = llm.chat(messages_list, sampling_params)\nfor idx, output in enumerate(outputs):\nprompt = prompts[idx]\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```  \n[](){ #quickstart-online }", "file_path": "getting_started/quickstart.md"}
{"id": "4220a196f22dad7395cfdb94b51b44954576c860502e1f1c08196a85a3b5a7a6", "heading": "Quickstart/OpenAI-Compatible Server", "level": 2, "text": "## OpenAI-Compatible Server  \nvLLM can be deployed as a server that implements the OpenAI API protocol. This allows vLLM to be used as a drop-in replacement for applications using OpenAI API.\nBy default, it starts the server at `http://localhost:8000`. You can specify the address with `--host` and `--port` arguments. The server currently hosts one model at a time and implements endpoints such as [list models](https://platform.openai.com/docs/api-reference/models/list), [create chat completion](https://platform.openai.com/docs/api-reference/chat/completions/create), and [create completion](https://platform.openai.com/docs/api-reference/completions/create) endpoints.  \nRun the following command to start the vLLM server with the [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct) model:  \n```bash\nvllm serve Qwen/Qwen2.5-1.5B-Instruct\n```  \n!!! note\nBy default, the server uses a predefined chat template stored in the tokenizer.\nYou can learn about overriding it [here][chat-template].", "file_path": "getting_started/quickstart.md"}
{"id": "4220a196f22dad7395cfdb94b51b44954576c860502e1f1c08196a85a3b5a7a6", "heading": "Quickstart/OpenAI-Compatible Server", "level": 2, "text": "You can learn about overriding it [here][chat-template].\n!!! important\nBy default, the server applies `generation_config.json` from the huggingface model repository if it exists. This means the default values of certain sampling parameters can be overridden by those recommended by the model creator.  \nTo disable this behavior, please pass `--generation-config vllm` when launching the server.  \nThis server can be queried in the same format as OpenAI API. For example, to list the models:  \n```bash\ncurl http://localhost:8000/v1/models\n```  \nYou can pass in the argument `--api-key` or environment variable `VLLM_API_KEY` to enable the server to check for API key in the header.\nYou can pass multiple keys after `--api-key`, and the server will accept any of the keys passed, this can be useful for key rotation.", "file_path": "getting_started/quickstart.md"}
{"id": "4220a196f22dad7395cfdb94b51b44954576c860502e1f1c08196a85a3b5a7a6", "heading": "Quickstart/OpenAI-Compatible Server/OpenAI Completions API with vLLM", "level": 3, "text": "### OpenAI Completions API with vLLM  \nOnce your server is started, you can query the model with input prompts:  \n```bash\ncurl http://localhost:8000/v1/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n\"prompt\": \"San Francisco is a\",\n\"max_tokens\": 7,\n\"temperature\": 0\n}'\n```  \nSince this server is compatible with OpenAI API, you can use it as a drop-in replacement for any applications using OpenAI API. For example, another way to query the server is via the `openai` Python package:  \n??? code  \n```python\nfrom openai import OpenAI", "file_path": "getting_started/quickstart.md"}
{"id": "4220a196f22dad7395cfdb94b51b44954576c860502e1f1c08196a85a3b5a7a6", "heading": "Quickstart/OpenAI-Compatible Server/OpenAI Completions API with vLLM", "level": 3, "text": "# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\ncompletion = client.completions.create(\nmodel=\"Qwen/Qwen2.5-1.5B-Instruct\",\nprompt=\"San Francisco is a\",\n)\nprint(\"Completion result:\", completion)\n```  \nA more detailed client example can be found here: <gh-file:examples/online_serving/openai_completion_client.py>", "file_path": "getting_started/quickstart.md"}
{"id": "4220a196f22dad7395cfdb94b51b44954576c860502e1f1c08196a85a3b5a7a6", "heading": "Quickstart/OpenAI-Compatible Server/OpenAI Chat Completions API with vLLM", "level": 3, "text": "### OpenAI Chat Completions API with vLLM  \nvLLM is designed to also support the OpenAI Chat Completions API. The chat interface is a more dynamic, interactive way to communicate with the model, allowing back-and-forth exchanges that can be stored in the chat history. This is useful for tasks that require context or more detailed explanations.  \nYou can use the [create chat completion](https://platform.openai.com/docs/api-reference/chat/completions/create) endpoint to interact with the model:  \n```bash\ncurl http://localhost:8000/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n\"messages\": [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n]\n}'\n```  \nAlternatively, you can use the `openai` Python package:  \n??? code  \n```python\nfrom openai import OpenAI\n# Set OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"", "file_path": "getting_started/quickstart.md"}
{"id": "4220a196f22dad7395cfdb94b51b44954576c860502e1f1c08196a85a3b5a7a6", "heading": "Quickstart/OpenAI-Compatible Server/OpenAI Chat Completions API with vLLM", "level": 3, "text": "openai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"", "file_path": "getting_started/quickstart.md"}
{"id": "4220a196f22dad7395cfdb94b51b44954576c860502e1f1c08196a85a3b5a7a6", "heading": "Quickstart/OpenAI-Compatible Server/OpenAI Chat Completions API with vLLM", "level": 3, "text": "client = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\n\nchat_response = client.chat.completions.create(\nmodel=\"Qwen/Qwen2.5-1.5B-Instruct\",\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n],\n)\nprint(\"Chat response:\", chat_response)\n```", "file_path": "getting_started/quickstart.md"}
{"id": "4220a196f22dad7395cfdb94b51b44954576c860502e1f1c08196a85a3b5a7a6", "heading": "Quickstart/On Attention Backends", "level": 2, "text": "## On Attention Backends  \nCurrently, vLLM supports multiple backends for efficient Attention computation across different platforms and accelerator architectures. It automatically selects the most performant backend compatible with your system and model specifications.  \nIf desired, you can also manually set the backend of your choice by configuring the environment variable `VLLM_ATTENTION_BACKEND` to one of the following options: `FLASH_ATTN`, `FLASHINFER` or `XFORMERS`.  \n!!! warning\nThere are no pre-built vllm wheels containing Flash Infer, so you must install it in your environment first. Refer to the [Flash Infer official docs](https://docs.flashinfer.ai/) or see <gh-file:docker/Dockerfile> for instructions on how to install it.", "file_path": "getting_started/quickstart.md"}
{"id": "3a4ed79c96e1b29f66b89ae89e2c16b3712d696e5f360abe95c1d38f6d23b645", "heading": "Generative Models", "level": 1, "text": "# Generative Models  \nvLLM provides first-class support for generative models, which covers most of LLMs.  \nIn vLLM, generative models implement the[VllmModelForTextGeneration][vllm.model_executor.models.VllmModelForTextGeneration] interface.\nBased on the final hidden states of the input, these models output log probabilities of the tokens to generate,\nwhich are then passed through [Sampler][vllm.v1.sample.sampler.Sampler] to obtain the final text.", "file_path": "models/generative_models.md"}
{"id": "3a4ed79c96e1b29f66b89ae89e2c16b3712d696e5f360abe95c1d38f6d23b645", "heading": "Generative Models/Configuration/Model Runner (`--runner`)", "level": 3, "text": "## Configuration  \n### Model Runner (`--runner`)  \nRun a model in generation mode via the option `--runner generate`.  \n!!! tip\nThere is no need to set this option in the vast majority of cases as vLLM can automatically\ndetect the model runner to use via `--runner auto`.", "file_path": "models/generative_models.md"}
{"id": "3a4ed79c96e1b29f66b89ae89e2c16b3712d696e5f360abe95c1d38f6d23b645", "heading": "Generative Models/Offline Inference", "level": 2, "text": "## Offline Inference  \nThe [LLM][vllm.LLM] class provides various methods for offline inference.\nSee [configuration](../api/README.md#configuration) for a list of options when initializing the model.", "file_path": "models/generative_models.md"}
{"id": "3a4ed79c96e1b29f66b89ae89e2c16b3712d696e5f360abe95c1d38f6d23b645", "heading": "Generative Models/Offline Inference/`LLM.generate`", "level": 3, "text": "### `LLM.generate`  \nThe [generate][vllm.LLM.generate] method is available to all generative models in vLLM.\nIt is similar to [its counterpart in HF Transformers](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate),\nexcept that tokenization and detokenization are also performed automatically.  \n```python\nfrom vllm import LLM\n\nllm = LLM(model=\"facebook/opt-125m\")\noutputs = llm.generate(\"Hello, my name is\")\n\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```  \nYou can optionally control the language generation by passing [SamplingParams][vllm.SamplingParams].\nFor example, you can use greedy sampling by setting `temperature=0`:  \n```python\nfrom vllm import LLM, SamplingParams\n\nllm = LLM(model=\"facebook/opt-125m\")\nparams = SamplingParams(temperature=0)\noutputs = llm.generate(\"Hello, my name is\", params)", "file_path": "models/generative_models.md"}
{"id": "3a4ed79c96e1b29f66b89ae89e2c16b3712d696e5f360abe95c1d38f6d23b645", "heading": "Generative Models/Offline Inference/`LLM.generate`", "level": 3, "text": "for output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```  \n!!! important\nBy default, vLLM will use sampling parameters recommended by model creator by applying the `generation_config.json` from the huggingface model repository if it exists. In most cases, this will provide you with the best results by default if [SamplingParams][vllm.SamplingParams] is not specified.  \nHowever, if vLLM's default sampling parameters are preferred, please pass `generation_config=\"vllm\"` when creating the [LLM][vllm.LLM] instance.\nA code example can be found here: <gh-file:examples/offline_inference/basic/basic.py>", "file_path": "models/generative_models.md"}
{"id": "3a4ed79c96e1b29f66b89ae89e2c16b3712d696e5f360abe95c1d38f6d23b645", "heading": "Generative Models/Offline Inference/`LLM.beam_search`", "level": 3, "text": "### `LLM.beam_search`  \nThe [beam_search][vllm.LLM.beam_search] method implements [beam search](https://huggingface.co/docs/transformers/en/generation_strategies#beam-search) on top of [generate][vllm.LLM.generate].\nFor example, to search using 5 beams and output at most 50 tokens:  \n```python\nfrom vllm import LLM\nfrom vllm.sampling_params import BeamSearchParams\n\nllm = LLM(model=\"facebook/opt-125m\")\nparams = BeamSearchParams(beam_width=5, max_tokens=50)\noutputs = llm.beam_search([{\"prompt\": \"Hello, my name is \"}], params)\n\nfor output in outputs:\ngenerated_text = output.sequences[0].text\nprint(f\"Generated text: {generated_text!r}\")\n```", "file_path": "models/generative_models.md"}
{"id": "3a4ed79c96e1b29f66b89ae89e2c16b3712d696e5f360abe95c1d38f6d23b645", "heading": "Generative Models/Offline Inference/`LLM.chat`", "level": 3, "text": "### `LLM.chat`  \nThe [chat][vllm.LLM.chat] method implements chat functionality on top of [generate][vllm.LLM.generate].\nIn particular, it accepts input similar to [OpenAI Chat Completions API](https://platform.openai.com/docs/api-reference/chat)\nand automatically applies the model's [chat template](https://huggingface.co/docs/transformers/en/chat_templating) to format the prompt.  \n!!! important\nIn general, only instruction-tuned models have a chat template.\nBase models may perform poorly as they are not trained to respond to the chat conversation.  \n??? code  \n```python\nfrom vllm import LLM\n\nllm = LLM(model=\"meta-llama/Meta-Llama-3-8B-Instruct\")\nconversation = [\n{\n\"role\": \"system\",\n\"content\": \"You are a helpful assistant\",\n},\n{\n\"role\": \"user\",\n\"content\": \"Hello\",\n},\n{\n\"role\": \"assistant\",\n\"content\": \"Hello! How can I assist you today?\",\n},\n{\n\"role\": \"user\",\n\"content\": \"Write an essay about the importance of higher education.\",\n},\n]\noutputs = llm.chat(conversation)", "file_path": "models/generative_models.md"}
{"id": "3a4ed79c96e1b29f66b89ae89e2c16b3712d696e5f360abe95c1d38f6d23b645", "heading": "Generative Models/Offline Inference/`LLM.chat`", "level": 3, "text": "for output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```  \nA code example can be found here: <gh-file:examples/offline_inference/basic/chat.py>  \nIf the model doesn't have a chat template or you want to specify another one,\nyou can explicitly pass a chat template:  \n```python\nfrom vllm.entrypoints.chat_utils import load_chat_template\n\n# You can find a list of existing chat templates under `examples/`\ncustom_template = load_chat_template(chat_template=\"<path_to_template>\")\nprint(\"Loaded chat template:\", custom_template)\n\noutputs = llm.chat(conversation, chat_template=custom_template)\n```", "file_path": "models/generative_models.md"}
{"id": "3a4ed79c96e1b29f66b89ae89e2c16b3712d696e5f360abe95c1d38f6d23b645", "heading": "Generative Models/Online Serving", "level": 2, "text": "## Online Serving  \nOur [OpenAI-Compatible Server](../serving/openai_compatible_server.md) provides endpoints that correspond to the offline APIs:  \n- [Completions API][completions-api] is similar to `LLM.generate` but only accepts text.\n- [Chat API][chat-api]  is similar to `LLM.chat`, accepting both text and [multi-modal inputs](../features/multimodal_inputs.md) for models with a chat template.", "file_path": "models/generative_models.md"}
{"id": "4547b60c10db756ca8abe972d3d116b2be8f69961355fc0ae99eb09aab9c9aec", "heading": "Pooling Models", "level": 1, "text": "# Pooling Models  \nvLLM also supports pooling models, such as embedding, classification and reward models.  \nIn vLLM, pooling models implement the [VllmModelForPooling][vllm.model_executor.models.VllmModelForPooling] interface.\nThese models use a [Pooler][vllm.model_executor.layers.pooler.Pooler] to extract the final hidden states of the input\nbefore returning them.  \n!!! note\nWe currently support pooling models primarily as a matter of convenience. This is not guaranteed to have any performance improvement over using HF Transformers / Sentence Transformers directly.  \nWe are now planning to optimize pooling models in vLLM. Please comment on <gh-issue:21796> if you have any suggestions!", "file_path": "models/pooling_models.md"}
{"id": "4547b60c10db756ca8abe972d3d116b2be8f69961355fc0ae99eb09aab9c9aec", "heading": "Pooling Models/Configuration/Model Runner", "level": 3, "text": "## Configuration  \n### Model Runner  \nRun a model in pooling mode via the option `--runner pooling`.  \n!!! tip\nThere is no need to set this option in the vast majority of cases as vLLM can automatically\ndetect the model runner to use via `--runner auto`.", "file_path": "models/pooling_models.md"}
{"id": "4547b60c10db756ca8abe972d3d116b2be8f69961355fc0ae99eb09aab9c9aec", "heading": "Pooling Models/Configuration/Model Conversion", "level": 3, "text": "### Model Conversion  \nvLLM can adapt models for various pooling tasks via the option `--convert <type>`.  \nIf `--runner pooling` has been set (manually or automatically) but the model does not implement the\n[VllmModelForPooling][vllm.model_executor.models.VllmModelForPooling] interface,\nvLLM will attempt to automatically convert the model according to the architecture names\nshown in the table below.  \n| Architecture                                    | `--convert` | Supported pooling tasks       |\n|-------------------------------------------------|-------------|-------------------------------|\n| `*ForTextEncoding`, `*EmbeddingModel`, `*Model` | `embed`     | `encode`, `embed`             |\n| `*For*Classification`, `*ClassificationModel`   | `classify`  | `encode`, `classify`, `score` |\n| `*ForRewardModeling`, `*RewardModel`            | `reward`    | `encode`                      |  \n!!! tip\nYou can explicitly set `--convert <type>` to specify how to convert the model.", "file_path": "models/pooling_models.md"}
{"id": "4547b60c10db756ca8abe972d3d116b2be8f69961355fc0ae99eb09aab9c9aec", "heading": "Pooling Models/Configuration/Pooling Tasks", "level": 3, "text": "### Pooling Tasks  \nEach pooling model in vLLM supports one or more of these tasks according to\n[Pooler.get_supported_tasks][vllm.model_executor.layers.pooler.Pooler.get_supported_tasks],\nenabling the corresponding APIs:  \n| Task       | APIs                                 |\n|------------|--------------------------------------|\n| `encode`   | `LLM.reward(...)`                    |\n| `embed`    | `LLM.embed(...)`, `LLM.score(...)`\\* |\n| `classify` | `LLM.classify(...)`                  |\n| `score`    | `LLM.score(...)`                     |  \n\\* The `LLM.score(...)` API falls back to `embed` task if the model does not support `score` task.", "file_path": "models/pooling_models.md"}
{"id": "4547b60c10db756ca8abe972d3d116b2be8f69961355fc0ae99eb09aab9c9aec", "heading": "Pooling Models/Configuration/Pooler Configuration", "level": 3, "text": "### Pooler Configuration  \n#### Predefined models  \nIf the [Pooler][vllm.model_executor.layers.pooler.Pooler] defined by the model accepts `pooler_config`,\nyou can override some of its attributes via the `--pooler-config` option.  \n#### Converted models  \nIf the model has been converted via `--convert` (see above),\nthe pooler assigned to each task has the following attributes by default:  \n| Task       | Pooling Type | Normalization | Softmax |\n|------------|--------------|---------------|---------|\n| `reward`   | `ALL`        | âŒ            | âŒ     |\n| `embed`    | `LAST`       | âœ…ï¸Ž            | âŒ      |\n| `classify` | `LAST`       | âŒ            | âœ…ï¸Ž      |  \nWhen loading [Sentence Transformers](https://huggingface.co/sentence-transformers) models,\nits Sentence Transformers configuration file (`modules.json`) takes priority over the model's defaults.  \nYou can further customize this via the `--pooler-config` option,\nwhich takes priority over both the model's and Sentence Transformers's defaults.", "file_path": "models/pooling_models.md"}
{"id": "4547b60c10db756ca8abe972d3d116b2be8f69961355fc0ae99eb09aab9c9aec", "heading": "Pooling Models/Offline Inference", "level": 2, "text": "## Offline Inference  \nThe [LLM][vllm.LLM] class provides various methods for offline inference.\nSee [configuration](../api/README.md#configuration) for a list of options when initializing the model.", "file_path": "models/pooling_models.md"}
{"id": "4547b60c10db756ca8abe972d3d116b2be8f69961355fc0ae99eb09aab9c9aec", "heading": "Pooling Models/Offline Inference/`LLM.embed`", "level": 3, "text": "### `LLM.embed`  \nThe [embed][vllm.LLM.embed] method outputs an embedding vector for each prompt.\nIt is primarily designed for embedding models.  \n```python\nfrom vllm import LLM\n\nllm = LLM(model=\"intfloat/e5-small\", runner=\"pooling\")\n(output,) = llm.embed(\"Hello, my name is\")\n\nembeds = output.outputs.embedding\nprint(f\"Embeddings: {embeds!r} (size={len(embeds)})\")\n```  \nA code example can be found here: <gh-file:examples/offline_inference/basic/embed.py>", "file_path": "models/pooling_models.md"}
{"id": "4547b60c10db756ca8abe972d3d116b2be8f69961355fc0ae99eb09aab9c9aec", "heading": "Pooling Models/Offline Inference/`LLM.classify`", "level": 3, "text": "### `LLM.classify`  \nThe [classify][vllm.LLM.classify] method outputs a probability vector for each prompt.\nIt is primarily designed for classification models.  \n```python\nfrom vllm import LLM\n\nllm = LLM(model=\"jason9693/Qwen2.5-1.5B-apeach\", runner=\"pooling\")\n(output,) = llm.classify(\"Hello, my name is\")\n\nprobs = output.outputs.probs\nprint(f\"Class Probabilities: {probs!r} (size={len(probs)})\")\n```  \nA code example can be found here: <gh-file:examples/offline_inference/basic/classify.py>", "file_path": "models/pooling_models.md"}
{"id": "4547b60c10db756ca8abe972d3d116b2be8f69961355fc0ae99eb09aab9c9aec", "heading": "Pooling Models/Offline Inference/`LLM.score`", "level": 3, "text": "### `LLM.score`  \nThe [score][vllm.LLM.score] method outputs similarity scores between sentence pairs.\nIt is designed for embedding models and cross-encoder models. Embedding models use cosine similarity, and [cross-encoder models](https://www.sbert.net/examples/applications/cross-encoder/README.html) serve as rerankers between candidate query-document pairs in RAG systems.  \n!!! note\nvLLM can only perform the model inference component (e.g. embedding, reranking) of RAG.\nTo handle RAG at a higher level, you should use integration frameworks such as [LangChain](https://github.com/langchain-ai/langchain).  \n```python\nfrom vllm import LLM\n\nllm = LLM(model=\"BAAI/bge-reranker-v2-m3\", runner=\"pooling\")\n(output,) = llm.score(\n\"What is the capital of France?\",\n\"The capital of Brazil is Brasilia.\",\n)\n\nscore = output.outputs.score\nprint(f\"Score: {score}\")\n```  \nA code example can be found here: <gh-file:examples/offline_inference/basic/score.py>", "file_path": "models/pooling_models.md"}
{"id": "4547b60c10db756ca8abe972d3d116b2be8f69961355fc0ae99eb09aab9c9aec", "heading": "Pooling Models/Offline Inference/`LLM.reward`", "level": 3, "text": "### `LLM.reward`  \nThe [reward][vllm.LLM.reward] method is available to all reward models in vLLM.\nIt returns the extracted hidden states directly.  \n```python\nfrom vllm import LLM\n\nllm = LLM(model=\"internlm/internlm2-1_8b-reward\", runner=\"pooling\", trust_remote_code=True)\n(output,) = llm.reward(\"Hello, my name is\")\n\ndata = output.outputs.data\nprint(f\"Data: {data!r}\")\n```  \nA code example can be found here: <gh-file:examples/offline_inference/basic/reward.py>", "file_path": "models/pooling_models.md"}
{"id": "4547b60c10db756ca8abe972d3d116b2be8f69961355fc0ae99eb09aab9c9aec", "heading": "Pooling Models/Offline Inference/`LLM.encode`", "level": 3, "text": "### `LLM.encode`  \nThe [encode][vllm.LLM.encode] method is available to all pooling models in vLLM.\nIt returns the extracted hidden states directly.  \n!!! note\nPlease use one of the more specific methods or set the task directly when using `LLM.encode`:  \n- For embeddings, use `LLM.embed(...)` or `pooling_task=\"embed\"`.\n- For classification logits, use `LLM.classify(...)` or `pooling_task=\"classify\"`.\n- For rewards, use `LLM.reward(...)` or `pooling_task=\"reward\"`.\n- For similarity scores, use `LLM.score(...)`.  \n```python\nfrom vllm import LLM\n\nllm = LLM(model=\"intfloat/e5-small\", runner=\"pooling\")\n(output,) = llm.encode(\"Hello, my name is\", pooling_task=\"embed\")\n\ndata = output.outputs.data\nprint(f\"Data: {data!r}\")\n```", "file_path": "models/pooling_models.md"}
{"id": "4547b60c10db756ca8abe972d3d116b2be8f69961355fc0ae99eb09aab9c9aec", "heading": "Pooling Models/Online Serving", "level": 2, "text": "## Online Serving  \nOur [OpenAI-Compatible Server](../serving/openai_compatible_server.md) provides endpoints that correspond to the offline APIs:  \n- [Pooling API][pooling-api] is similar to `LLM.encode`, being applicable to all types of pooling models.\n- [Embeddings API][embeddings-api] is similar to `LLM.embed`, accepting both text and [multi-modal inputs](../features/multimodal_inputs.md) for embedding models.\n- [Classification API][classification-api] is similar to `LLM.classify` and is applicable to sequence classification models.\n- [Score API][score-api] is similar to `LLM.score` for cross-encoder models.", "file_path": "models/pooling_models.md"}
{"id": "4547b60c10db756ca8abe972d3d116b2be8f69961355fc0ae99eb09aab9c9aec", "heading": "Pooling Models/Matryoshka Embeddings", "level": 2, "text": "## Matryoshka Embeddings  \n[Matryoshka Embeddings](https://sbert.net/examples/sentence_transformer/training/matryoshka/README.html#matryoshka-embeddings) or [Matryoshka Representation Learning (MRL)](https://arxiv.org/abs/2205.13147) is a technique used in training embedding models. It allows user to trade off between performance and cost.  \n!!! warning\nNot all embedding models are trained using Matryoshka Representation Learning. To avoid misuse of the `dimensions` parameter, vLLM returns an error for requests that attempt to change the output dimension of models that do not support Matryoshka Embeddings.  \nFor example, setting `dimensions` parameter while using the `BAAI/bge-m3` model will result in the following error.  \n```json\n{\"object\":\"error\",\"message\":\"Model \\\"BAAI/bge-m3\\\" does not support matryoshka representation, changing output dimensions will lead to poor results.\",\"type\":\"BadRequestError\",\"param\":null,\"code\":400}\n```", "file_path": "models/pooling_models.md"}
{"id": "4547b60c10db756ca8abe972d3d116b2be8f69961355fc0ae99eb09aab9c9aec", "heading": "Pooling Models/Matryoshka Embeddings/Manually enable Matryoshka Embeddings", "level": 3, "text": "### Manually enable Matryoshka Embeddings  \nThere is currently no official interface for specifying support for Matryoshka Embeddings. In vLLM, if `is_matryoshka` is `True` in `config.json,` it is allowed to change the output to arbitrary dimensions. Using `matryoshka_dimensions` can control the allowed output dimensions.  \nFor models that support Matryoshka Embeddings but not recognized by vLLM, please manually override the config using `hf_overrides={\"is_matryoshka\": True}`, `hf_overrides={\"matryoshka_dimensions\": [<allowed output dimensions>]}` (offline) or `--hf-overrides '{\"is_matryoshka\": true}'`,  `--hf-overrides '{\"matryoshka_dimensions\": [<allowed output dimensions>]}'`(online).  \nHere is an example to serve a model with Matryoshka Embeddings enabled.  \n```bash\nvllm serve Snowflake/snowflake-arctic-embed-m-v1.5 --hf-overrides '{\"matryoshka_dimensions\":[256]}'\n```", "file_path": "models/pooling_models.md"}
{"id": "4547b60c10db756ca8abe972d3d116b2be8f69961355fc0ae99eb09aab9c9aec", "heading": "Pooling Models/Matryoshka Embeddings/Offline Inference", "level": 3, "text": "### Offline Inference  \nYou can change the output dimensions of embedding models that support Matryoshka Embeddings by using the dimensions parameter in [PoolingParams][vllm.PoolingParams].  \n```python\nfrom vllm import LLM, PoolingParams\n\nllm = LLM(\nmodel=\"jinaai/jina-embeddings-v3\",\nrunner=\"pooling\",\ntrust_remote_code=True,\n)\noutputs = llm.embed(\n[\"Follow the white rabbit.\"],\npooling_params=PoolingParams(dimensions=32),\n)\nprint(outputs[0].outputs)\n```  \nA code example can be found here: <gh-file:examples/offline_inference/pooling/embed_matryoshka_fy.py>", "file_path": "models/pooling_models.md"}
{"id": "4547b60c10db756ca8abe972d3d116b2be8f69961355fc0ae99eb09aab9c9aec", "heading": "Pooling Models/Matryoshka Embeddings/Online Inference", "level": 3, "text": "### Online Inference  \nUse the following command to start vllm server.  \n```bash\nvllm serve jinaai/jina-embeddings-v3 --trust-remote-code\n```  \nYou can change the output dimensions of embedding models that support Matryoshka Embeddings by using the dimensions parameter.  \n```bash\ncurl http://127.0.0.1:8000/v1/embeddings \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '{\n\"input\": \"Follow the white rabbit.\",\n\"model\": \"jinaai/jina-embeddings-v3\",\n\"encoding_format\": \"float\",\n\"dimensions\": 32\n}'\n```  \nExpected output:  \n```json", "file_path": "models/pooling_models.md"}
{"id": "4547b60c10db756ca8abe972d3d116b2be8f69961355fc0ae99eb09aab9c9aec", "heading": "Pooling Models/Matryoshka Embeddings/Online Inference", "level": 3, "text": "\"dimensions\": 32\n}'\n```  \nExpected output:  \n```json\n{\"id\":\"embd-5c21fc9a5c9d4384a1b021daccaf9f64\",\"object\":\"list\",\"created\":1745476417,\"model\":\"jinaai/jina-embeddings-v3\",\"data\":[{\"index\":0,\"object\":\"embedding\",\"embedding\":[-0.3828125,-0.1357421875,0.03759765625,0.125,0.21875,0.09521484375,-0.003662109375,0.1591796875,-0.130859375,-0.0869140625,-0.1982421875,0.1689453125,-0.220703125,0.1728515625,-0.2275390625,-0.0712890625,-0.162109375,-0.283203125,-0.055419921875,-0.0693359375,0.031982421875,-0.04052734375,-0.2734375,0.1826171875,-0.091796875,0.220703125,0.37890625,-0.0888671875,-0.12890625,-0.021484375,-0.0091552734375,0.23046875]}],\"usage\":{\"prompt_tokens\":8,\"total_tokens\":8,\"completion_tokens\":0,\"prompt_tokens_details\":null}}\n```  \nAn OpenAI client example can be found here: <gh-file:examples/online_serving/pooling/openai_embedding_matryoshka_fy.py>", "file_path": "models/pooling_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models", "level": 1, "text": "# Supported Models  \nvLLM supports [generative](./generative_models.md) and [pooling](./pooling_models.md) models across various tasks.  \nFor each task, we list the model architectures that have been implemented in vLLM.\nAlongside each architecture, we include some popular models that use it.", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Model Implementation/vLLM", "level": 3, "text": "## Model Implementation  \n### vLLM  \nIf vLLM natively supports a model, its implementation can be found in <gh-file:vllm/model_executor/models>.  \nThese models are what we list in [supported-text-models][supported-text-models] and [supported-mm-models][supported-mm-models].  \n[](){ #transformers-backend }", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Model Implementation/Transformers", "level": 3, "text": "### Transformers  \nvLLM also supports model implementations that are available in Transformers. You should expect the performance of a Transformers model implementation used in vLLM to be within <5% of the performance of a dedicated vLLM model implementation. We call this feature the \"Transformers backend\".  \nCurrently, the Transformers backend works for the following:  \n- Modalities: embedding models, language models and vision-language models*\n- Architectures: encoder-only, decoder-only, mixture-of-experts\n- Attention types: full attention and/or sliding attention  \n_*Vision-language models currently accept only image inputs. Support for video inputs will be added in a future release._  \nIf the Transformers model implementation follows all the steps in [writing a custom model](#writing-custom-models) then, when used with the Transformers backend, it will be compatible with the following features of vLLM:  \n- All the features listed in the [compatibility matrix](../features/README.md#feature-x-feature)", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Model Implementation/Transformers", "level": 3, "text": "- Any combination of the following vLLM parallelisation schemes:\n- Data parallel\n- Tensor parallel\n- Expert parallel\n- Pipeline parallel  \nChecking if the modeling backend is Transformers is as simple as:  \n```python\nfrom vllm import LLM\nllm = LLM(model=...)  # Name or path of your model\nllm.apply_model(lambda model: print(type(model)))\n```  \nIf the printed type starts with `Transformers...` then it's using the Transformers model implementation!  \nIf a model has a vLLM implementation but you would prefer to use the Transformers implementation via the Transformers backend, set `model_impl=\"transformers\"` for [offline inference](../serving/offline_inference.md) or `--model-impl transformers` for the [online serving](../serving/openai_compatible_server.md).  \n!!! note", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Model Implementation/Transformers", "level": 3, "text": "!!! note\nFor vision-language models, if you are loading with `dtype=\"auto\"`, vLLM loads the whole model with config's `dtype` if it exists. In contrast the native Transformers will respect the `dtype` attribute of each backbone in the model. That might cause a slight difference in performance.  \n#### Custom models  \nIf a model is neither supported natively by vLLM nor Transformers, it can still be used in vLLM!  \nFor a model to be compatible with the Transformers backend for vLLM it must:  \n- be a Transformers compatible custom model (see [Transformers - Customizing models](https://huggingface.co/docs/transformers/en/custom_models)):\n- The model directory must have the correct structure (e.g. `config.json` is present).\n- `config.json` must contain `auto_map.AutoModel`.\n- be a Transformers backend for vLLM compatible model (see [writing-custom-models][writing-custom-models]):\n- Customisation should be done in the base model (e.g. in `MyModel`, not `MyModelForCausalLM`).  \nIf the compatible model is:", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Model Implementation/Transformers", "level": 3, "text": "If the compatible model is:  \n- on the Hugging Face Model Hub, simply set `trust_remote_code=True` for [offline-inference](../serving/offline_inference.md) or `--trust-remote-code` for the [openai-compatible-server](../serving/openai_compatible_server.md).\n- in a local directory, simply pass directory path to `model=<MODEL_DIR>` for [offline-inference](../serving/offline_inference.md) or `vllm serve <MODEL_DIR>` for the [openai-compatible-server](../serving/openai_compatible_server.md).  \nThis means that, with the Transformers backend for vLLM, new models can be used before they are officially supported in Transformers or vLLM!  \n[](){ #writing-custom-models }  \n#### Writing custom models", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Model Implementation/Transformers", "level": 3, "text": "[](){ #writing-custom-models }  \n#### Writing custom models  \nThis section details the necessary modifications to make to a Transformers compatible custom model that make it compatible with the Transformers backend for vLLM. (We assume that a Transformers compatible custom model has already been created, see [Transformers - Customizing models](https://huggingface.co/docs/transformers/en/custom_models)).  \nTo make your model compatible with the Transformers backend, it needs:  \n1. `kwargs` passed down through all modules from `MyModel` to `MyAttention`.\n1. If your model is encoder-only, you must also add `is_causal = False` to `MyAttention`.\n2. `MyAttention` must use `ALL_ATTENTION_FUNCTIONS` to call attention.\n3. `MyModel` must contain `_supports_attention_backend = True`.  \n<details class=\"code\">\n<summary>modeling_my_model.py</summary>  \n```python", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Model Implementation/Transformers", "level": 3, "text": "from transformers import PreTrainedModel\nfrom torch import nn\n\nclass MyAttention(nn.Module):\nis_causal = False  # Only do this for encoder-only models\n\ndef forward(self, hidden_states, **kwargs):\n...\nattention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\nattn_output, attn_weights = attention_interface(\nself,\nquery_states,\nkey_states,\nvalue_states,\n**kwargs,\n)\n...", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Model Implementation/Transformers", "level": 3, "text": "class MyModel(PreTrainedModel):\n_supports_attention_backend = True\n```  \n</details>  \nHere is what happens in the background when this model is loaded:  \n1. The config is loaded.\n2. `MyModel` Python class is loaded from the `auto_map` in config, and we check that the model `is_backend_compatible()`.\n3. `MyModel` is loaded into one of the Transformers backend classes in <gh-file:vllm/model_executor/models/transformers.py> which sets `self.config._attn_implementation = \"vllm\"` so that vLLM's attention layer is used.  \nThat's it!  \nFor your model to be compatible with vLLM's tensor parallel and/or pipeline parallel features, you must add `base_model_tp_plan` and/or `base_model_pp_plan` to your model's config class:  \n<details class=\"code\">\n<summary>configuration_my_model.py</summary>  \n```python\n\nfrom transformers import PretrainedConfig", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Model Implementation/Transformers", "level": 3, "text": "class MyConfig(PretrainedConfig):\nbase_model_tp_plan = {\n\"layers.*.self_attn.k_proj\": \"colwise\",\n\"layers.*.self_attn.v_proj\": \"colwise\",\n\"layers.*.self_attn.o_proj\": \"rowwise\",\n\"layers.*.mlp.gate_proj\": \"colwise\",\n\"layers.*.mlp.up_proj\": \"colwise\",\n\"layers.*.mlp.down_proj\": \"rowwise\",\n}\nbase_model_pp_plan = {\n\"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n\"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n\"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n}\n```  \n</details>  \n- `base_model_tp_plan` is a `dict` that maps fully qualified layer name patterns to tensor parallel styles (currently only `\"colwise\"` and `\"rowwise\"` are supported).\n- `base_model_pp_plan` is a `dict` that maps direct child layer names to `tuple`s of `list`s of `str`s:\n- You only need to do this for layers which are not present on all pipeline stages\n- vLLM assumes that there will be only one `nn.ModuleList`, which is distributed across the pipeline stages", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Model Implementation/Transformers", "level": 3, "text": "- The `list` in the first element of the `tuple` contains the names of the input arguments\n- The `list` in the last element of the `tuple` contains the names of the variables the layer outputs to in your modeling code", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Loading a Model/Hugging Face Hub", "level": 3, "text": "## Loading a Model  \n### Hugging Face Hub  \nBy default, vLLM loads models from [Hugging Face (HF) Hub](https://huggingface.co/models). To change the download path for models, you can set the `HF_HOME` environment variable; for more details, refer to [their official documentation](https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables#hfhome).  \nTo determine whether a given model is natively supported, you can check the `config.json` file inside the HF repository.\nIf the `\"architectures\"` field contains a model architecture listed below, then it should be natively supported.  \nModels do not _need_ to be natively supported to be used in vLLM.\nThe [Transformers backend][transformers-backend] enables you to run models directly using their Transformers implementation (or even remote code on the Hugging Face Model Hub!).  \n!!! tip\nThe easiest way to check if your model is really supported at runtime is to run the program below:  \n```python\nfrom vllm import LLM", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Loading a Model/Hugging Face Hub", "level": 3, "text": "# For generative models (runner=generate) only\nllm = LLM(model=..., runner=\"generate\")  # Name or path of your model\noutput = llm.generate(\"Hello, my name is\")\nprint(output)", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Loading a Model/Hugging Face Hub", "level": 3, "text": "# For pooling models (runner=pooling) only\nllm = LLM(model=..., runner=\"pooling\")  # Name or path of your model\noutput = llm.encode(\"Hello, my name is\")\nprint(output)\n```  \nIf vLLM successfully returns text (for generative models) or hidden states (for pooling models), it indicates that your model is supported.  \nOtherwise, please refer to [Adding a New Model](../contributing/model/README.md) for instructions on how to implement your model in vLLM.\nAlternatively, you can [open an issue on GitHub](https://github.com/vllm-project/vllm/issues/new/choose) to request vLLM support.  \n#### Download a model  \nIf you prefer, you can use the Hugging Face CLI to [download a model](https://huggingface.co/docs/huggingface_hub/guides/cli#huggingface-cli-download) or specific files from a model repository:  \n```bash\n# Download a model\nhuggingface-cli download HuggingFaceH4/zephyr-7b-beta\n\n# Specify a custom cache directory\nhuggingface-cli download HuggingFaceH4/zephyr-7b-beta --cache-dir ./path/to/cache", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Loading a Model/Hugging Face Hub", "level": 3, "text": "# Download a specific file from a model repo\nhuggingface-cli download HuggingFaceH4/zephyr-7b-beta eval_results.json\n```  \n#### List the downloaded models  \nUse the Hugging Face CLI to [manage models](https://huggingface.co/docs/huggingface_hub/guides/manage-cache#scan-your-cache) stored in local cache:  \n```bash\n# List cached models\nhuggingface-cli scan-cache\n\n# Show detailed (verbose) output\nhuggingface-cli scan-cache -v\n\n# Specify a custom cache directory\nhuggingface-cli scan-cache --dir ~/.cache/huggingface/hub\n```  \n#### Delete a cached model  \nUse the Hugging Face CLI to interactively [delete downloaded model](https://huggingface.co/docs/huggingface_hub/guides/manage-cache#clean-your-cache) from the cache:  \n<details>\n<summary>Commands</summary>  \n```console\n# The `delete-cache` command requires extra dependencies to work with the TUI.\n# Please run `pip install huggingface_hub[cli]` to install them.", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Loading a Model/Hugging Face Hub", "level": 3, "text": "# Launch the interactive TUI to select models to delete\n$ huggingface-cli delete-cache\n? Select revisions to delete: 1 revisions selected counting for 438.9M.\nâ—‹ None of the following (if selected, nothing will be deleted).\nModel BAAI/bge-base-en-v1.5 (438.9M, used 1 week ago)\nâ¯ â—‰ a5beb1e3: main # modified 1 week ago\n\nModel BAAI/bge-large-en-v1.5 (1.3G, used 1 week ago)\nâ—‹ d4aa6901: main # modified 1 week ago\n\nModel BAAI/bge-reranker-base (1.1G, used 4 weeks ago)\nâ—‹ 2cfc18c9: main # modified 4 weeks ago\n\nPress <space> to select, <enter> to validate and <ctrl+c> to quit without modification.", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Loading a Model/Hugging Face Hub", "level": 3, "text": "# Need to confirm after selected\n? Select revisions to delete: 1 revision(s) selected.\n? 1 revisions selected counting for 438.9M. Confirm deletion ? Yes\nStart deletion.\nDone. Deleted 1 repo(s) and 0 revision(s) for a total of 438.9M.\n```  \n</details>  \n#### Using a proxy  \nHere are some tips for loading/downloading models from Hugging Face using a proxy:  \n- Set the proxy globally for your session (or set it in the profile file):  \n```shell\nexport http_proxy=http://your.proxy.server:port\nexport https_proxy=http://your.proxy.server:port\n```  \n- Set the proxy for just the current command:  \n```shell\nhttps_proxy=http://your.proxy.server:port huggingface-cli download <model_name>\n\n# or use vllm cmd directly\nhttps_proxy=http://your.proxy.server:port  vllm serve <model_name>\n```  \n- Set the proxy in Python interpreter:  \n```python\nimport os\n\nos.environ[\"http_proxy\"] = \"http://your.proxy.server:port\"\nos.environ[\"https_proxy\"] = \"http://your.proxy.server:port\"\n```", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Loading a Model/ModelScope", "level": 3, "text": "### ModelScope  \nTo use models from [ModelScope](https://www.modelscope.cn) instead of Hugging Face Hub, set an environment variable:  \n```shell\nexport VLLM_USE_MODELSCOPE=True\n```  \nAnd use with `trust_remote_code=True`.  \n```python\nfrom vllm import LLM\n\nllm = LLM(model=..., revision=..., runner=..., trust_remote_code=True)\n\n# For generative models (runner=generate) only\noutput = llm.generate(\"Hello, my name is\")\nprint(output)\n\n# For pooling models (runner=pooling) only\noutput = llm.encode(\"Hello, my name is\")\nprint(output)\n```  \n[](){ #feature-status-legend }", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Feature Status Legend", "level": 2, "text": "## Feature Status Legend  \n- âœ…ï¸Ž indicates that the feature is supported for the model.  \n- ðŸš§ indicates that the feature is planned but not yet supported for the model.  \n- âš ï¸ indicates that the feature is available but may have known issues or limitations.  \n[](){ #supported-text-models }", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Generative Models", "level": 3, "text": "## List of Text-only Language Models  \n### Generative Models  \nSee [this page](generative_models.md) for more information on how to use generative models.  \n#### Text Generation  \nThese models primarily accept the [`LLM.generate`](./generative_models.md#llmgenerate) API. Chat/Instruct models additionally support the [`LLM.chat`](./generative_models.md#llmchat) API.  \n<style>\nth {\nwhite-space: nowrap;\nmin-width: 0 !important;\n}\n</style>  \n| Architecture | Models | Example HF Models | [LoRA](../features/lora.md) | [PP](../serving/parallelism_scaling.md) |\n|--------------|--------|-------------------|----------------------|---------------------------|\n| `ApertusForCausalLM` | Apertus | `swiss-ai/Apertus-8B-2509`, `swiss-ai/Apertus-70B-Instruct-2509`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `AquilaForCausalLM` | Aquila, Aquila2 | `BAAI/Aquila-7B`, `BAAI/AquilaChat-7B`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `ArceeForCausalLM` | Arcee (AFM) | `arcee-ai/AFM-4.5B-Base`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Generative Models", "level": 3, "text": "| `ArcticForCausalLM` | Arctic | `Snowflake/snowflake-arctic-base`, `Snowflake/snowflake-arctic-instruct`, etc. | | âœ…ï¸Ž |\n| `BaiChuanForCausalLM` | Baichuan2, Baichuan | `baichuan-inc/Baichuan2-13B-Chat`, `baichuan-inc/Baichuan-7B`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `BailingMoeForCausalLM` | Ling | `inclusionAI/Ling-lite-1.5`, `inclusionAI/Ling-plus`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `BailingMoeV2ForCausalLM` | Ling | `inclusionAI/Ling-mini-2.0`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `BambaForCausalLM` | Bamba | `ibm-ai-platform/Bamba-9B-fp8`, `ibm-ai-platform/Bamba-9B` | âœ…ï¸Ž | âœ…ï¸Ž |\n| `BloomForCausalLM` | BLOOM, BLOOMZ, BLOOMChat | `bigscience/bloom`, `bigscience/bloomz`, etc. | | âœ…ï¸Ž |\n| `ChatGLMModel`, `ChatGLMForConditionalGeneration` | ChatGLM | `zai-org/chatglm2-6b`, `zai-org/chatglm3-6b`, `ShieldLM-6B-chatglm3`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Generative Models", "level": 3, "text": "| `CohereForCausalLM`, `Cohere2ForCausalLM` | Command-R, Command-A | `CohereLabs/c4ai-command-r-v01`, `CohereLabs/c4ai-command-r7b-12-2024`, `CohereLabs/c4ai-command-a-03-2025`, `CohereLabs/command-a-reasoning-08-2025`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `DbrxForCausalLM` | DBRX | `databricks/dbrx-base`, `databricks/dbrx-instruct`, etc. | | âœ…ï¸Ž |\n| `DeciLMForCausalLM` | DeciLM | `nvidia/Llama-3_3-Nemotron-Super-49B-v1`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `DeepseekForCausalLM` | DeepSeek | `deepseek-ai/deepseek-llm-67b-base`, `deepseek-ai/deepseek-llm-7b-chat`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `DeepseekV2ForCausalLM` | DeepSeek-V2 | `deepseek-ai/DeepSeek-V2`, `deepseek-ai/DeepSeek-V2-Chat`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `DeepseekV3ForCausalLM` | DeepSeek-V3 | `deepseek-ai/DeepSeek-V3`, `deepseek-ai/DeepSeek-R1`, `deepseek-ai/DeepSeek-V3.1`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Dots1ForCausalLM` | dots.llm1 | `rednote-hilab/dots.llm1.base`, `rednote-hilab/dots.llm1.inst`, etc. | | âœ…ï¸Ž |\n| `DotsOCRForCausalLM` | dots_ocr | `rednote-hilab/dots.ocr` | | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Generative Models", "level": 3, "text": "| `Ernie4_5ForCausalLM` | Ernie4.5 | `baidu/ERNIE-4.5-0.3B-PT`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Ernie4_5_MoeForCausalLM` | Ernie4.5MoE | `baidu/ERNIE-4.5-21B-A3B-PT`, `baidu/ERNIE-4.5-300B-A47B-PT`, etc. |âœ…ï¸Ž| âœ…ï¸Ž |\n| `ExaoneForCausalLM` | EXAONE-3 | `LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Exaone4ForCausalLM` | EXAONE-4 | `LGAI-EXAONE/EXAONE-4.0-32B`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Fairseq2LlamaForCausalLM` | Llama (fairseq2 format) | `mgleize/fairseq2-dummy-Llama-3.2-1B`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `FalconForCausalLM` | Falcon | `tiiuae/falcon-7b`, `tiiuae/falcon-40b`, `tiiuae/falcon-rw-7b`, etc. | | âœ…ï¸Ž |\n| `FalconMambaForCausalLM` | FalconMamba | `tiiuae/falcon-mamba-7b`, `tiiuae/falcon-mamba-7b-instruct`, etc. | | âœ…ï¸Ž |\n| `FalconH1ForCausalLM` | Falcon-H1 | `tiiuae/Falcon-H1-34B-Base`, `tiiuae/Falcon-H1-34B-Instruct`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `FlexOlmoForCausalLM` | FlexOlmo | `allenai/FlexOlmo-7x7B-1T`, `allenai/FlexOlmo-7x7B-1T-RT`, etc. | | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Generative Models", "level": 3, "text": "| `GemmaForCausalLM` | Gemma | `google/gemma-2b`, `google/gemma-1.1-2b-it`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Gemma2ForCausalLM` | Gemma 2 | `google/gemma-2-9b`, `google/gemma-2-27b`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Gemma3ForCausalLM` | Gemma 3 | `google/gemma-3-1b-it`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Gemma3nForCausalLM` | Gemma 3n | `google/gemma-3n-E2B-it`, `google/gemma-3n-E4B-it`, etc. | | |\n| `GlmForCausalLM` | GLM-4 | `zai-org/glm-4-9b-chat-hf`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Glm4ForCausalLM` | GLM-4-0414 | `zai-org/GLM-4-32B-0414`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Glm4MoeForCausalLM` | GLM-4.5, GLM-4.6 | `zai-org/GLM-4.5`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `GPT2LMHeadModel` | GPT-2 | `gpt2`, `gpt2-xl`, etc. | | âœ…ï¸Ž |\n| `GPTBigCodeForCausalLM` | StarCoder, SantaCoder, WizardCoder | `bigcode/starcoder`, `bigcode/gpt_bigcode-santacoder`, `WizardLM/WizardCoder-15B-V1.0`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `GPTJForCausalLM` | GPT-J | `EleutherAI/gpt-j-6b`, `nomic-ai/gpt4all-j`, etc. | | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Generative Models", "level": 3, "text": "| `GPTNeoXForCausalLM` | GPT-NeoX, Pythia, OpenAssistant, Dolly V2, StableLM | `EleutherAI/gpt-neox-20b`, `EleutherAI/pythia-12b`, `OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5`, `databricks/dolly-v2-12b`, `stabilityai/stablelm-tuned-alpha-7b`, etc. | | âœ…ï¸Ž |\n| `GptOssForCausalLM` | GPT-OSS | `openai/gpt-oss-120b`, `openai/gpt-oss-20b` | | âœ…ï¸Ž |\n| `GraniteForCausalLM` | Granite 3.0, Granite 3.1, PowerLM | `ibm-granite/granite-3.0-2b-base`, `ibm-granite/granite-3.1-8b-instruct`, `ibm/PowerLM-3b`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `GraniteMoeForCausalLM` | Granite 3.0 MoE, PowerMoE | `ibm-granite/granite-3.0-1b-a400m-base`, `ibm-granite/granite-3.0-3b-a800m-instruct`, `ibm/PowerMoE-3b`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `GraniteMoeHybridForCausalLM` | Granite 4.0 MoE Hybrid | `ibm-granite/granite-4.0-tiny-preview`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `GraniteMoeSharedForCausalLM` | Granite MoE Shared | `ibm-research/moe-7b-1b-active-shared-experts` (test model) | âœ…ï¸Ž | âœ…ï¸Ž |\n| `GritLM` | GritLM | `parasail-ai/GritLM-7B-vllm`. | âœ…ï¸Ž | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Generative Models", "level": 3, "text": "| `GritLM` | GritLM | `parasail-ai/GritLM-7B-vllm`. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Grok1ModelForCausalLM` | Grok1 | `hpcai-tech/grok-1`. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `HunYuanDenseV1ForCausalLM` | Hunyuan-7B-Instruct-0124 | `tencent/Hunyuan-7B-Instruct-0124` | âœ…ï¸Ž | âœ…ï¸Ž |\n| `HunYuanMoEV1ForCausalLM` | Hunyuan-80B-A13B | `tencent/Hunyuan-A13B-Instruct`, `tencent/Hunyuan-A13B-Pretrain`, `tencent/Hunyuan-A13B-Instruct-FP8`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `HCXVisionForCausalLM` | HyperCLOVAX-SEED-Vision-Instruct-3B | `naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B` | | |\n| `InternLMForCausalLM` | InternLM | `internlm/internlm-7b`, `internlm/internlm-chat-7b`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `InternLM2ForCausalLM` | InternLM2 | `internlm/internlm2-7b`, `internlm/internlm2-chat-7b`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `InternLM3ForCausalLM` | InternLM3 | `internlm/internlm3-8b-instruct`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `JAISLMHeadModel` | Jais | `inceptionai/jais-13b`, `inceptionai/jais-13b-chat`, `inceptionai/jais-30b-v3`, `inceptionai/jais-30b-chat-v3`, etc. | | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Generative Models", "level": 3, "text": "| `JambaForCausalLM` | Jamba | `ai21labs/AI21-Jamba-1.5-Large`, `ai21labs/AI21-Jamba-1.5-Mini`, `ai21labs/Jamba-v0.1`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Lfm2ForCausalLM`  | LFM2  | `LiquidAI/LFM2-1.2B`, `LiquidAI/LFM2-700M`, `LiquidAI/LFM2-350M`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Lfm2MoeForCausalLM`  | LFM2MoE  | `LiquidAI/LFM2-8B-A1B-preview`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `LlamaForCausalLM` | Llama 3.1, Llama 3, Llama 2, LLaMA, Yi | `meta-llama/Meta-Llama-3.1-405B-Instruct`, `meta-llama/Meta-Llama-3.1-70B`, `meta-llama/Meta-Llama-3-70B-Instruct`, `meta-llama/Llama-2-70b-hf`, `01-ai/Yi-34B`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `MambaForCausalLM` | Mamba | `state-spaces/mamba-130m-hf`, `state-spaces/mamba-790m-hf`, `state-spaces/mamba-2.8b-hf`, etc. | | âœ…ï¸Ž |\n| `Mamba2ForCausalLM` | Mamba2 | `mistralai/Mamba-Codestral-7B-v0.1`, etc. | | âœ…ï¸Ž |\n| `MiMoForCausalLM` | MiMo | `XiaomiMiMo/MiMo-7B-RL`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `MiniCPMForCausalLM` | MiniCPM | `openbmb/MiniCPM-2B-sft-bf16`, `openbmb/MiniCPM-2B-dpo-bf16`, `openbmb/MiniCPM-S-1B-sft`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Generative Models", "level": 3, "text": "| `MiniCPM3ForCausalLM` | MiniCPM3 | `openbmb/MiniCPM3-4B`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `MistralForCausalLM` | Mistral, Mistral-Instruct | `mistralai/Mistral-7B-v0.1`, `mistralai/Mistral-7B-Instruct-v0.1`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `MixtralForCausalLM` | Mixtral-8x7B, Mixtral-8x7B-Instruct | `mistralai/Mixtral-8x7B-v0.1`, `mistralai/Mixtral-8x7B-Instruct-v0.1`, `mistral-community/Mixtral-8x22B-v0.1`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `MPTForCausalLM` | MPT, MPT-Instruct, MPT-Chat, MPT-StoryWriter | `mosaicml/mpt-7b`, `mosaicml/mpt-7b-storywriter`, `mosaicml/mpt-30b`, etc. | | âœ…ï¸Ž |\n| `NemotronForCausalLM` | Nemotron-3, Nemotron-4, Minitron | `nvidia/Minitron-8B-Base`, `mgoin/Nemotron-4-340B-Base-hf-FP8`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `NemotronHForCausalLM` | Nemotron-H | `nvidia/Nemotron-H-8B-Base-8K`, `nvidia/Nemotron-H-47B-Base-8K`, `nvidia/Nemotron-H-56B-Base-8K`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `OLMoForCausalLM` | OLMo | `allenai/OLMo-1B-hf`, `allenai/OLMo-7B-hf`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `OLMo2ForCausalLM` | OLMo2 | `allenai/OLMo-2-0425-1B`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Generative Models", "level": 3, "text": "| `OLMo3ForCausalLM` | OLMo3 | TBA | âœ…ï¸Ž | âœ…ï¸Ž |\n| `OLMoEForCausalLM` | OLMoE | `allenai/OLMoE-1B-7B-0924`, `allenai/OLMoE-1B-7B-0924-Instruct`, etc. | | âœ…ï¸Ž |\n| `OPTForCausalLM` | OPT, OPT-IML | `facebook/opt-66b`, `facebook/opt-iml-max-30b`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `OrionForCausalLM` | Orion | `OrionStarAI/Orion-14B-Base`, `OrionStarAI/Orion-14B-Chat`, etc. | | âœ…ï¸Ž |\n| `PhiForCausalLM` | Phi | `microsoft/phi-1_5`, `microsoft/phi-2`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Phi3ForCausalLM` | Phi-4, Phi-3 | `microsoft/Phi-4-mini-instruct`, `microsoft/Phi-4`, `microsoft/Phi-3-mini-4k-instruct`, `microsoft/Phi-3-mini-128k-instruct`, `microsoft/Phi-3-medium-128k-instruct`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `PhiMoEForCausalLM` | Phi-3.5-MoE | `microsoft/Phi-3.5-MoE-instruct`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `PersimmonForCausalLM` | Persimmon | `adept/persimmon-8b-base`, `adept/persimmon-8b-chat`, etc. | | âœ…ï¸Ž |\n| `Plamo2ForCausalLM` | PLaMo2 | `pfnet/plamo-2-1b`, `pfnet/plamo-2-8b`, etc. | | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Generative Models", "level": 3, "text": "| `QWenLMHeadModel` | Qwen | `Qwen/Qwen-7B`, `Qwen/Qwen-7B-Chat`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Qwen2ForCausalLM` | QwQ, Qwen2 | `Qwen/QwQ-32B-Preview`, `Qwen/Qwen2-7B-Instruct`, `Qwen/Qwen2-7B`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Qwen2MoeForCausalLM` | Qwen2MoE | `Qwen/Qwen1.5-MoE-A2.7B`, `Qwen/Qwen1.5-MoE-A2.7B-Chat`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Qwen3ForCausalLM` | Qwen3 | `Qwen/Qwen3-8B`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Qwen3MoeForCausalLM` | Qwen3MoE | `Qwen/Qwen3-30B-A3B`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Qwen3NextForCausalLM` | Qwen3NextMoE | `Qwen/Qwen3-Next-80B-A3B-Instruct`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `SeedOssForCausalLM` | SeedOss | `ByteDance-Seed/Seed-OSS-36B-Instruct`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `StableLmForCausalLM` | StableLM | `stabilityai/stablelm-3b-4e1t`, `stabilityai/stablelm-base-alpha-7b-v2`, etc. | | |\n| `Starcoder2ForCausalLM` | Starcoder2 | `bigcode/starcoder2-3b`, `bigcode/starcoder2-7b`, `bigcode/starcoder2-15b`, etc. | | âœ…ï¸Ž |\n| `SolarForCausalLM` | Solar Pro | `upstage/solar-pro-preview-instruct`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Generative Models", "level": 3, "text": "| `TeleChat2ForCausalLM` | TeleChat2 | `Tele-AI/TeleChat2-3B`, `Tele-AI/TeleChat2-7B`, `Tele-AI/TeleChat2-35B`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `TeleFLMForCausalLM` | TeleFLM | `CofeAI/FLM-2-52B-Instruct-2407`, `CofeAI/Tele-FLM`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `XverseForCausalLM` | XVERSE | `xverse/XVERSE-7B-Chat`, `xverse/XVERSE-13B-Chat`, `xverse/XVERSE-65B-Chat`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `MiniMaxM1ForCausalLM` | MiniMax-Text | `MiniMaxAI/MiniMax-M1-40k`, `MiniMaxAI/MiniMax-M1-80k`, etc. | | |\n| `MiniMaxText01ForCausalLM` | MiniMax-Text | `MiniMaxAI/MiniMax-Text-01`, etc. | | |\n| `Zamba2ForCausalLM` | Zamba2 | `Zyphra/Zamba2-7B-instruct`, `Zyphra/Zamba2-2.7B-instruct`, `Zyphra/Zamba2-1.2B-instruct`, etc. | | |\n| `LongcatFlashForCausalLM` | LongCat-Flash | `meituan-longcat/LongCat-Flash-Chat`, `meituan-longcat/LongCat-Flash-Chat-FP8` | âœ…ï¸Ž | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Generative Models", "level": 3, "text": "Some models are supported only via the [Transformers backend](#transformers). The purpose of the table below is to acknowledge models which we officially support in this way. The logs will say that the Transformers backend is being used, and you will see no warning that this is fallback behaviour. This means that, if you have issues with any of the models listed below, please [make an issue](https://github.com/vllm-project/vllm/issues/new/choose) and we'll do our best to fix it!  \n| Architecture | Models | Example HF Models | [LoRA](../features/lora.md) | [PP](../serving/parallelism_scaling.md) |\n|--------------|--------|-------------------|----------------------|---------------------------|\n| `SmolLM3ForCausalLM` | SmolLM3 | `HuggingFaceTB/SmolLM3-3B` | âœ…ï¸Ž | âœ…ï¸Ž |  \n!!! note\nCurrently, the ROCm version of vLLM supports Mistral and Mixtral only for context lengths up to 4096.", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Pooling Models", "level": 3, "text": "### Pooling Models  \nSee [this page](./pooling_models.md) for more information on how to use pooling models.  \n!!! important\nSince some model architectures support both generative and pooling tasks,\nyou should explicitly specify `--runner pooling` to ensure that the model is used in pooling mode instead of generative mode.  \n#### Embedding  \nThese models primarily support the [`LLM.embed`](./pooling_models.md#llmembed) API.  \n| Architecture | Models | Example HF Models | [LoRA](../features/lora.md) | [PP](../serving/parallelism_scaling.md) |\n|--------------|--------|-------------------|----------------------|---------------------------|\n| `BertModel`<sup>C</sup> | BERT-based | `BAAI/bge-base-en-v1.5`, `Snowflake/snowflake-arctic-embed-xs`, etc. | | |\n| `Gemma2Model`<sup>C</sup> | Gemma 2-based | `BAAI/bge-multilingual-gemma2`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Gemma3TextModel`<sup>C</sup> | Gemma 3-based | `google/embeddinggemma-300m`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `GritLM` | GritLM | `parasail-ai/GritLM-7B-vllm`. | âœ…ï¸Ž | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Pooling Models", "level": 3, "text": "| `GritLM` | GritLM | `parasail-ai/GritLM-7B-vllm`. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `GteModel`<sup>C</sup> | Arctic-Embed-2.0-M | `Snowflake/snowflake-arctic-embed-m-v2.0`. |  |  |\n| `GteNewModel`<sup>C</sup> | mGTE-TRM (see note) | `Alibaba-NLP/gte-multilingual-base`, etc. |  |  |\n| `ModernBertModel`<sup>C</sup> | ModernBERT-based | `Alibaba-NLP/gte-modernbert-base`, etc. |  |  |\n| `NomicBertModel`<sup>C</sup> | Nomic BERT | `nomic-ai/nomic-embed-text-v1`, `nomic-ai/nomic-embed-text-v2-moe`, `Snowflake/snowflake-arctic-embed-m-long`, etc. |  |  |\n| `LlamaModel`<sup>C</sup>, `LlamaForCausalLM`<sup>C</sup>, `MistralModel`<sup>C</sup>, etc. | Llama-based | `intfloat/e5-mistral-7b-instruct`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Qwen2Model`<sup>C</sup>, `Qwen2ForCausalLM`<sup>C</sup> | Qwen2-based | `ssmits/Qwen2-7B-Instruct-embed-base` (see note), `Alibaba-NLP/gte-Qwen2-7B-instruct` (see note), etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Qwen3Model`<sup>C</sup>, `Qwen3ForCausalLM`<sup>C</sup> | Qwen3-based | `Qwen/Qwen3-Embedding-0.6B`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Pooling Models", "level": 3, "text": "| `RobertaModel`, `RobertaForMaskedLM` | RoBERTa-based | `sentence-transformers/all-roberta-large-v1`, etc. | | |\n| `*Model`<sup>C</sup>, `*ForCausalLM`<sup>C</sup>, etc. | Generative models | N/A | \\* | \\* |  \n<sup>C</sup> Automatically converted into an embedding model via `--convert embed`. ([details](./pooling_models.md#model-conversion))\n\\* Feature support is the same as that of the original model.  \n!!! note\n`ssmits/Qwen2-7B-Instruct-embed-base` has an improperly defined Sentence Transformers config.\nYou need to manually set mean pooling by passing `--pooler-config '{\"pooling_type\": \"MEAN\"}'`.  \n!!! note\nFor `Alibaba-NLP/gte-Qwen2-*`, you need to enable `--trust-remote-code` for the correct tokenizer to be loaded.\nSee [relevant issue on HF Transformers](https://github.com/huggingface/transformers/issues/34882).  \n!!! note\n`jinaai/jina-embeddings-v3` supports multiple tasks through LoRA, while vllm temporarily only supports text-matching tasks by merging LoRA weights.  \n!!! note", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Pooling Models", "level": 3, "text": "!!! note\nThe second-generation GTE model (mGTE-TRM) is named `NewModel`. The name `NewModel` is too generic, you should set `--hf-overrides '{\"architectures\": [\"GteNewModel\"]}'` to specify the use of the `GteNewModel` architecture.  \nIf your model is not in the above list, we will try to automatically convert the model using\n[as_embedding_model][vllm.model_executor.models.adapters.as_embedding_model]. By default, the embeddings\nof the whole prompt are extracted from the normalized hidden state corresponding to the last token.  \n#### Classification  \nThese models primarily support the [`LLM.classify`](./pooling_models.md#llmclassify) API.  \n| Architecture | Models | Example HF Models | [LoRA](../features/lora.md) | [PP](../serving/parallelism_scaling.md) |\n|--------------|--------|-------------------|----------------------|---------------------------|\n| `JambaForSequenceClassification` | Jamba | `ai21labs/Jamba-tiny-reward-dev`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Pooling Models", "level": 3, "text": "| `GPT2ForSequenceClassification` | GPT2 | `nie3e/sentiment-polish-gpt2-small` | | |\n| `*Model`<sup>C</sup>, `*ForCausalLM`<sup>C</sup>, etc. | Generative models | N/A | \\* | \\* |  \n<sup>C</sup> Automatically converted into a classification model via `--convert classify`. ([details](./pooling_models.md#model-conversion))\n\\* Feature support is the same as that of the original model.  \nIf your model is not in the above list, we will try to automatically convert the model using\n[as_seq_cls_model][vllm.model_executor.models.adapters.as_seq_cls_model]. By default, the class probabilities are extracted from the softmaxed hidden state corresponding to the last token.  \n#### Cross-encoder / Reranker  \nCross-encoder and reranker models are a subset of classification models that accept two prompts as input.\nThese models primarily support the [`LLM.score`](./pooling_models.md#llmscore) API.  \n| Architecture | Models | Example HF Models | [LoRA](../features/lora.md) | [PP](../serving/parallelism_scaling.md) |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Pooling Models", "level": 3, "text": "|--------------|--------|-------------------|----------------------|---------------------------|\n| `BertForSequenceClassification` | BERT-based | `cross-encoder/ms-marco-MiniLM-L-6-v2`, etc. | | |\n| `GemmaForSequenceClassification` | Gemma-based | `BAAI/bge-reranker-v2-gemma` (see note), etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `GteNewForSequenceClassification` | mGTE-TRM (see note) | `Alibaba-NLP/gte-multilingual-reranker-base`, etc. |  |  |\n| `Qwen2ForSequenceClassification` | Qwen2-based | `mixedbread-ai/mxbai-rerank-base-v2` (see note), etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Qwen3ForSequenceClassification` | Qwen3-based | `tomaarsen/Qwen3-Reranker-0.6B-seq-cls`, `Qwen/Qwen3-Reranker-0.6B` (see note), etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `RobertaForSequenceClassification` | RoBERTa-based | `cross-encoder/quora-roberta-base`, etc. | | |\n| `XLMRobertaForSequenceClassification` | XLM-RoBERTa-based | `BAAI/bge-reranker-v2-m3`, etc. | | |\n| `*Model`<sup>C</sup>, `*ForCausalLM`<sup>C</sup>, etc. | Generative models | N/A | \\* | \\* |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Pooling Models", "level": 3, "text": "<sup>C</sup> Automatically converted into a classification model via `--convert classify`. ([details](./pooling_models.md#model-conversion))\n\\* Feature support is the same as that of the original model.  \n!!! note\nLoad the official original `BAAI/bge-reranker-v2-gemma` by using the following command.  \n```bash\nvllm serve BAAI/bge-reranker-v2-gemma --hf_overrides '{\"architectures\": [\"GemmaForSequenceClassification\"],\"classifier_from_token\": [\"Yes\"],\"method\": \"no_post_processing\"}'\n```  \n!!! note\nThe second-generation GTE model (mGTE-TRM) is named `NewForSequenceClassification`. The name `NewForSequenceClassification` is too generic, you should set `--hf-overrides '{\"architectures\": [\"GteNewForSequenceClassification\"]}'` to specify the use of the `GteNewForSequenceClassification` architecture.  \n!!! note\nLoad the official original `mxbai-rerank-v2` by using the following command.  \n```bash", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Pooling Models", "level": 3, "text": "```bash\nvllm serve mixedbread-ai/mxbai-rerank-base-v2 --hf_overrides '{\"architectures\": [\"Qwen2ForSequenceClassification\"],\"classifier_from_token\": [\"0\", \"1\"], \"method\": \"from_2_way_softmax\"}'\n```  \n!!! note\nLoad the official original `Qwen3 Reranker` by using the following command. More information can be found at: <gh-file:examples/offline_inference/pooling/qwen3_reranker.py>.  \n```bash\nvllm serve Qwen/Qwen3-Reranker-0.6B --hf_overrides '{\"architectures\": [\"Qwen3ForSequenceClassification\"],\"classifier_from_token\": [\"no\", \"yes\"],\"is_original_qwen3_reranker\": true}'\n```  \n#### Reward Modeling  \nThese models primarily support the [`LLM.reward`](./pooling_models.md#llmreward) API.  \n| Architecture | Models | Example HF Models | [LoRA](../features/lora.md) | [PP](../serving/parallelism_scaling.md) |\n|--------------|--------|-------------------|----------------------|---------------------------|", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Pooling Models", "level": 3, "text": "| `InternLM2ForRewardModel` | InternLM2-based | `internlm/internlm2-1_8b-reward`, `internlm/internlm2-7b-reward`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `LlamaForCausalLM`<sup>C</sup> | Llama-based | `peiyi9979/math-shepherd-mistral-7b-prm`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Qwen2ForRewardModel` | Qwen2-based | `Qwen/Qwen2.5-Math-RM-72B`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Qwen2ForProcessRewardModel` | Qwen2-based | `Qwen/Qwen2.5-Math-PRM-7B`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `*Model`<sup>C</sup>, `*ForCausalLM`<sup>C</sup>, etc. | Generative models | N/A | \\* | \\* |  \n<sup>C</sup> Automatically converted into a reward model via `--convert reward`. ([details](./pooling_models.md#model-conversion))\n\\* Feature support is the same as that of the original model.  \nIf your model is not in the above list, we will try to automatically convert the model using\n[as_reward_model][vllm.model_executor.models.adapters.as_reward_model]. By default, we return the hidden states of each token directly.  \n!!! important", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Text-only Language Models/Pooling Models", "level": 3, "text": "!!! important\nFor process-supervised reward models such as `peiyi9979/math-shepherd-mistral-7b-prm`, the pooling config should be set explicitly,\ne.g.: `--pooler-config '{\"pooling_type\": \"STEP\", \"step_tag_id\": 123, \"returned_token_ids\": [456, 789]}'`.  \n#### Token Classification  \nThese models primarily support the [`LLM.encode`](./pooling_models.md#llmencode) API.  \n| Architecture | Models | Example HF Models | [LoRA](../features/lora.md) | [PP](../serving/parallelism_scaling.md) |\n|--------------|--------|-------------------|-----------------------------|-----------------------------------------|\n| `BertForTokenClassification` | bert-based | `boltuix/NeuroBERT-NER` (see note), etc. |  |  |\n| `ModernBertForTokenClassification` | ModernBERT-based | `disham993/electrical-ner-ModernBERT-base` |  |  |  \n!!! note\nNamed Entity Recognition (NER) usage, please refer to <gh-file:examples/offline_inference/pooling/ner.py>, <gh-file:examples/online_serving/pooling/ner_client.py>.  \n[](){ #supported-mm-models }", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models", "level": 2, "text": "## List of Multimodal Language Models  \nThe following modalities are supported depending on the model:  \n- **T**ext\n- **I**mage\n- **V**ideo\n- **A**udio  \nAny combination of modalities joined by `+` are supported.  \n- e.g.: `T + I` means that the model supports text-only, image-only, and text-with-image inputs.  \nOn the other hand, modalities separated by `/` are mutually exclusive.  \n- e.g.: `T / I` means that the model supports text-only and image-only inputs, but not text-with-image inputs.  \nSee [this page](../features/multimodal_inputs.md) on how to pass multi-modal inputs to the model.  \n!!! tip\nFor hybrid-only models such as Llama-4, Step3 and Mistral-3, a text-only mode can be enabled by setting all supported multimodal modalities to 0 (e.g, `--limit-mm-per-prompt '{\"image\":0}`) so that their multimodal modules will not be loaded to free up more GPU memory for KV cache.  \n!!! note\nvLLM currently only supports dynamic LoRA adapters on the language backbone of multimodal models.", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models", "level": 2, "text": "If you wish to use a model with LoRA in the multi-modal encoder,\nplease merge the weights into the base model first before running it in vLLM like a regular model.  \n```python\nfrom peft import PeftConfig, PeftModel\nfrom transformers import AutoModelForImageTextToText, AutoProcessor", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models", "level": 2, "text": "def merge_and_save(model_id: str, output_dir: str):\nbase_model = AutoModelForImageTextToText.from_pretrained(model_id)\nlora_model = PeftModel.from_pretrained(\nbase_model,\nmodel_id,\nconfig=PeftConfig.from_pretrained(model_id),\n)\nmodel = lora_model.merge_and_unload().to(dtype=base_model.dtype)\nmodel._hf_peft_config_loaded = False  # Needed to save the merged model\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nmodel.save_pretrained(output_dir)\nprocessor.save_pretrained(output_dir)\n```", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models/Generative Models", "level": 3, "text": "### Generative Models  \nSee [this page](generative_models.md) for more information on how to use generative models.  \n#### Text Generation  \nThese models primarily accept the [`LLM.generate`](./generative_models.md#llmgenerate) API. Chat/Instruct models additionally support the [`LLM.chat`](./generative_models.md#llmchat) API.  \n| Architecture | Models | Inputs | Example HF Models | [LoRA](../features/lora.md) | [PP](../serving/parallelism_scaling.md) |\n|--------------|--------|--------|-------------------|----------------------|---------------------------|\n| `AriaForConditionalGeneration` | Aria | T + I<sup>+</sup> | `rhymes-ai/Aria` | | |\n| `AyaVisionForConditionalGeneration` | Aya Vision | T + I<sup>+</sup> | `CohereForAI/aya-vision-8b`, `CohereForAI/aya-vision-32b`, etc. | | âœ…ï¸Ž |\n| `Blip2ForConditionalGeneration` | BLIP-2 | T + I<sup>E</sup> | `Salesforce/blip2-opt-2.7b`, `Salesforce/blip2-opt-6.7b`, etc. | | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models/Generative Models", "level": 3, "text": "| `ChameleonForConditionalGeneration` | Chameleon | T + I | `facebook/chameleon-7b`, etc. | | âœ…ï¸Ž |\n| `Cohere2VisionForConditionalGeneration` | Command A Vision | T + I<sup>+</sup> | `CohereLabs/command-a-vision-07-2025`, etc. | | âœ…ï¸Ž |\n| `DeepseekVLV2ForCausalLM`<sup>^</sup> | DeepSeek-VL2 | T + I<sup>+</sup> | `deepseek-ai/deepseek-vl2-tiny`, `deepseek-ai/deepseek-vl2-small`, `deepseek-ai/deepseek-vl2`, etc. | | âœ…ï¸Ž |\n| `Ernie4_5_VLMoeForConditionalGeneration` | Ernie4.5-VL | T + I<sup>+</sup>/ V<sup>+</sup> | `baidu/ERNIE-4.5-VL-28B-A3B-PT`, `baidu/ERNIE-4.5-VL-424B-A47B-PT` | | âœ…ï¸Ž |\n| `FuyuForCausalLM` | Fuyu | T + I | `adept/fuyu-8b`, etc. | | âœ…ï¸Ž |\n| `Gemma3ForConditionalGeneration` | Gemma 3 | T + I<sup>+</sup> | `google/gemma-3-4b-it`, `google/gemma-3-27b-it`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Gemma3nForConditionalGeneration` | Gemma 3n | T + I + A | `google/gemma-3n-E2B-it`, `google/gemma-3n-E4B-it`, etc. | | |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models/Generative Models", "level": 3, "text": "| `GLM4VForCausalLM`<sup>^</sup> | GLM-4V | T + I | `zai-org/glm-4v-9b`, `zai-org/cogagent-9b-20241220`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Glm4vForConditionalGeneration` | GLM-4.1V-Thinking | T + I<sup>E+</sup> + V<sup>E+</sup> | `zai-org/GLM-4.1V-9B-Thinking`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Glm4vMoeForConditionalGeneration` | GLM-4.5V | T + I<sup>E+</sup> + V<sup>E+</sup> | `zai-org/GLM-4.5V`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `GraniteSpeechForConditionalGeneration` | Granite Speech | T + A | `ibm-granite/granite-speech-3.3-8b` | âœ…ï¸Ž | âœ…ï¸Ž |\n| `H2OVLChatModel` | H2OVL | T + I<sup>E+</sup> | `h2oai/h2ovl-mississippi-800m`, `h2oai/h2ovl-mississippi-2b`, etc. | | âœ…ï¸Ž |\n| `Idefics3ForConditionalGeneration` | Idefics3 | T + I | `HuggingFaceM4/Idefics3-8B-Llama3`, etc. | âœ…ï¸Ž | |\n| `InternS1ForConditionalGeneration` | Intern-S1 | T + I<sup>E+</sup> + V<sup>E+</sup> | `internlm/Intern-S1`, `internlm/Intern-S1-mini`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models/Generative Models", "level": 3, "text": "| `InternVLChatModel` | InternVL 3.5, InternVL 3.0, InternVideo 2.5, InternVL 2.5, Mono-InternVL, InternVL 2.0 | T + I<sup>E+</sup> + (V<sup>E+</sup>) | `OpenGVLab/InternVL3_5-14B`, `OpenGVLab/InternVL3-9B`, `OpenGVLab/InternVideo2_5_Chat_8B`, `OpenGVLab/InternVL2_5-4B`, `OpenGVLab/Mono-InternVL-2B`, `OpenGVLab/InternVL2-4B`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `InternVLForConditionalGeneration` | InternVL 3.0 (HF format) | T + I<sup>E+</sup> + V<sup>E+</sup> | `OpenGVLab/InternVL3-1B-hf`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `KeyeForConditionalGeneration` | Keye-VL-8B-Preview | T + I<sup>E+</sup> + V<sup>E+</sup> | `Kwai-Keye/Keye-VL-8B-Preview` | âœ…ï¸Ž | âœ…ï¸Ž |\n| `KeyeVL1_5ForConditionalGeneration` | Keye-VL-1_5-8B | T + I<sup>E+</sup> + V<sup>E+</sup> | `Kwai-Keye/Keye-VL-1_5-8B` | âœ…ï¸Ž | âœ…ï¸Ž |\n| `KimiVLForConditionalGeneration` | Kimi-VL-A3B-Instruct, Kimi-VL-A3B-Thinking | T + I<sup>+</sup> | `moonshotai/Kimi-VL-A3B-Instruct`, `moonshotai/Kimi-VL-A3B-Thinking` | | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models/Generative Models", "level": 3, "text": "| `Llama4ForConditionalGeneration` | Llama 4 | T + I<sup>+</sup> | `meta-llama/Llama-4-Scout-17B-16E-Instruct`, `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8`, `meta-llama/Llama-4-Maverick-17B-128E-Instruct`, etc. | | âœ…ï¸Ž |\n| `Llama_Nemotron_Nano_VL` | Llama Nemotron Nano VL | T + I<sup>E+</sup> | `nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1` | âœ…ï¸Ž | âœ…ï¸Ž |\n| `LlavaForConditionalGeneration` | LLaVA-1.5, Pixtral (HF Transformers) | T + I<sup>E+</sup> | `llava-hf/llava-1.5-7b-hf`, `TIGER-Lab/Mantis-8B-siglip-llama3` (see note), `mistral-community/pixtral-12b`, etc. | | âœ…ï¸Ž |\n| `LlavaNextForConditionalGeneration` | LLaVA-NeXT | T + I<sup>E+</sup> | `llava-hf/llava-v1.6-mistral-7b-hf`, `llava-hf/llava-v1.6-vicuna-7b-hf`, etc. | | âœ…ï¸Ž |\n| `LlavaNextVideoForConditionalGeneration` | LLaVA-NeXT-Video | T + V | `llava-hf/LLaVA-NeXT-Video-7B-hf`, etc. | | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models/Generative Models", "level": 3, "text": "| `LlavaOnevisionForConditionalGeneration` | LLaVA-Onevision | T + I<sup>+</sup> + V<sup>+</sup> | `llava-hf/llava-onevision-qwen2-7b-ov-hf`, `llava-hf/llava-onevision-qwen2-0.5b-ov-hf`, etc. | | âœ…ï¸Ž |\n| `MiDashengLMModel` | MiDashengLM | T + A<sup>+</sup> | `mispeech/midashenglm-7b` | | âœ…ï¸Ž |\n| `MiniCPMO` | MiniCPM-O | T + I<sup>E+</sup> + V<sup>E+</sup> + A<sup>E+</sup> | `openbmb/MiniCPM-o-2_6`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `MiniCPMV` | MiniCPM-V | T + I<sup>E+</sup> + V<sup>E+</sup> | `openbmb/MiniCPM-V-2` (see note), `openbmb/MiniCPM-Llama3-V-2_5`, `openbmb/MiniCPM-V-2_6`, `openbmb/MiniCPM-V-4`, `openbmb/MiniCPM-V-4_5`, etc. | âœ…ï¸Ž | |\n| `MiniMaxVL01ForConditionalGeneration` | MiniMax-VL | T + I<sup>E+</sup> | `MiniMaxAI/MiniMax-VL-01`, etc. | | âœ…ï¸Ž |\n| `Mistral3ForConditionalGeneration` | Mistral3 (HF Transformers) | T + I<sup>+</sup> | `mistralai/Mistral-Small-3.1-24B-Instruct-2503`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models/Generative Models", "level": 3, "text": "| `MolmoForCausalLM` | Molmo | T + I<sup>+</sup> | `allenai/Molmo-7B-D-0924`, `allenai/Molmo-7B-O-0924`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `NVLM_D_Model` | NVLM-D 1.0 | T + I<sup>+</sup> | `nvidia/NVLM-D-72B`, etc. | | âœ…ï¸Ž |\n| `Ovis` | Ovis2, Ovis1.6 | T + I<sup>+</sup> | `AIDC-AI/Ovis2-1B`, `AIDC-AI/Ovis1.6-Llama3.2-3B`, etc. | | âœ…ï¸Ž |\n| `Ovis2_5` | Ovis2.5 | T + I<sup>+</sup> + V | `AIDC-AI/Ovis2.5-9B`, etc. | | |\n| `PaliGemmaForConditionalGeneration` | PaliGemma, PaliGemma 2 | T + I<sup>E</sup> | `google/paligemma-3b-pt-224`, `google/paligemma-3b-mix-224`, `google/paligemma2-3b-ft-docci-448`, etc. | | âœ…ï¸Ž |\n| `Phi3VForCausalLM` | Phi-3-Vision, Phi-3.5-Vision | T + I<sup>E+</sup> | `microsoft/Phi-3-vision-128k-instruct`, `microsoft/Phi-3.5-vision-instruct`, etc. | | âœ…ï¸Ž |\n| `Phi4MMForCausalLM` | Phi-4-multimodal | T + I<sup>+</sup> / T + A<sup>+</sup> / I<sup>+</sup> + A<sup>+</sup> | `microsoft/Phi-4-multimodal-instruct`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models/Generative Models", "level": 3, "text": "| `Phi4MultimodalForCausalLM` | Phi-4-multimodal (HF Transformers) | T + I<sup>+</sup> / T + A<sup>+</sup> / I<sup>+</sup> + A<sup>+</sup> | `microsoft/Phi-4-multimodal-instruct` (with revision `refs/pr/70`), etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `PixtralForConditionalGeneration` | Mistral 3 (Mistral format), Pixtral (Mistral format) | T + I<sup>+</sup> | `mistralai/Mistral-Small-3.1-24B-Instruct-2503`, `mistralai/Pixtral-12B-2409`, etc. | | âœ…ï¸Ž |\n| `QwenVLForConditionalGeneration`<sup>^</sup> | Qwen-VL | T + I<sup>E+</sup> | `Qwen/Qwen-VL`, `Qwen/Qwen-VL-Chat`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Qwen2AudioForConditionalGeneration` | Qwen2-Audio | T + A<sup>+</sup> | `Qwen/Qwen2-Audio-7B-Instruct` | | âœ…ï¸Ž |\n| `Qwen2VLForConditionalGeneration` | QVQ, Qwen2-VL | T + I<sup>E+</sup> + V<sup>E+</sup> | `Qwen/QVQ-72B-Preview`, `Qwen/Qwen2-VL-7B-Instruct`, `Qwen/Qwen2-VL-72B-Instruct`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models/Generative Models", "level": 3, "text": "| `Qwen2_5_VLForConditionalGeneration` | Qwen2.5-VL | T + I<sup>E+</sup> + V<sup>E+</sup> | `Qwen/Qwen2.5-VL-3B-Instruct`, `Qwen/Qwen2.5-VL-72B-Instruct`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Qwen2_5OmniThinkerForConditionalGeneration` | Qwen2.5-Omni | T + I<sup>E+</sup> + V<sup>E+</sup> + A<sup>+</sup> | `Qwen/Qwen2.5-Omni-3B`, `Qwen/Qwen2.5-Omni-7B` | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Qwen3VLForConditionalGeneration` | Qwen3-VL | T + I<sup>E+</sup> + V<sup>E+</sup> | `Qwen/Qwen3-VL-4B-Instruct`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Qwen3VLMoeForConditionalGeneration` | Qwen3-VL-MOE | T + I<sup>E+</sup> + V<sup>E+</sup> | `Qwen/Qwen3-VL-30B-A3B-Instruct`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Qwen3OmniMoeThinkerForConditionalGeneration` | Qwen3-Omni | T + I<sup>E+</sup> + V<sup>E+</sup> + A<sup>+</sup> | `Qwen/Qwen3-Omni-30B-A3B-Instruct`, `Qwen/Qwen3-Omni-30B-A3B-Thinking` | âœ…ï¸Ž | âœ…ï¸Ž |\n| `RForConditionalGeneration` | R-VL-4B | T + I<sup>E+</sup> | `YannQi/R-4B` | | âœ…ï¸Ž |\n| `SkyworkR1VChatModel` | Skywork-R1V-38B | T + I | `Skywork/Skywork-R1V-38B` | | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models/Generative Models", "level": 3, "text": "| `SmolVLMForConditionalGeneration` | SmolVLM2 | T + I | `SmolVLM2-2.2B-Instruct` | âœ…ï¸Ž | |\n| `Step3VLForConditionalGeneration` | Step3-VL | T + I<sup>+</sup> | `stepfun-ai/step3` | | âœ…ï¸Ž |\n| `TarsierForConditionalGeneration` | Tarsier | T + I<sup>E+</sup> | `omni-search/Tarsier-7b`, `omni-search/Tarsier-34b` | | âœ…ï¸Ž |\n| `Tarsier2ForConditionalGeneration`<sup>^</sup> | Tarsier2 | T + I<sup>E+</sup> + V<sup>E+</sup> | `omni-research/Tarsier2-Recap-7b`, `omni-research/Tarsier2-7b-0115` | | âœ…ï¸Ž |  \nSome models are supported only via the [Transformers backend](#transformers). The purpose of the table below is to acknowledge models which we officially support in this way. The logs will say that the Transformers backend is being used, and you will see no warning that this is fallback behaviour. This means that, if you have issues with any of the models listed below, please [make an issue](https://github.com/vllm-project/vllm/issues/new/choose) and we'll do our best to fix it!", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models/Generative Models", "level": 3, "text": "| Architecture | Models | Inputs | Example HF Models | [LoRA](../features/lora.md) | [PP](../serving/parallelism_scaling.md) |\n|--------------|--------|--------|-------------------|-----------------------------|-----------------------------------------|\n| `Emu3ForConditionalGeneration` | Emu3 | T + I | `BAAI/Emu3-Chat-hf` | âœ…ï¸Ž | âœ…ï¸Ž |  \n<sup>^</sup> You need to set the architecture name via `--hf-overrides` to match the one in vLLM.\n&nbsp;&nbsp;&nbsp;&nbsp;â€¢ For example, to use DeepSeek-VL2 series models:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`--hf-overrides '{\"architectures\": [\"DeepseekVLV2ForCausalLM\"]}'`\n<sup>E</sup> Pre-computed embeddings can be inputted for this modality.\n<sup>+</sup> Multiple items can be inputted per text prompt for this modality.  \n!!! warning\nBoth V0 and V1 support `Gemma3ForConditionalGeneration` for text-only inputs.\nHowever, there are differences in how they handle text + image inputs:  \nV0 correctly implements the model's attention pattern:", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models/Generative Models", "level": 3, "text": "V0 correctly implements the model's attention pattern:\n- Uses bidirectional attention between the image tokens corresponding to the same image\n- Uses causal attention for other tokens\n- Implemented via (naive) PyTorch SDPA with masking tensors\n- Note: May use significant memory for long prompts with image  \nV1 currently uses a simplified attention pattern:\n- Uses causal attention for all tokens, including image tokens\n- Generates reasonable outputs but does not match the original model's attention for text + image inputs, especially when `{\"do_pan_and_scan\": true}`\n- Will be updated in the future to support the correct behavior  \nThis limitation exists because the model's mixed attention pattern (bidirectional for images, causal otherwise) is not yet supported by vLLM's attention backends.  \n!!! note\n`Gemma3nForConditionalGeneration` is only supported on V1 due to shared KV caching and it depends on `timm>=1.0.17` to make use of its\nMobileNet-v5 vision backbone.", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models/Generative Models", "level": 3, "text": "MobileNet-v5 vision backbone.  \nPerformance is not yet fully optimized mainly due to:  \n- Both audio and vision MM encoders use `transformers.AutoModel` implementation.\n- There's no PLE caching or out-of-memory swapping support, as described in [Google's blog](https://developers.googleblog.com/en/introducing-gemma-3n/). These features might be too model-specific for vLLM, and swapping in particular may be better suited for constrained setups.  \n!!! note\nFor `InternVLChatModel`, only InternVL2.5 with Qwen2.5 text backbone (`OpenGVLab/InternVL2.5-1B` etc), InternVL3 and InternVL3.5 have video inputs support currently.  \n!!! note\nTo use `TIGER-Lab/Mantis-8B-siglip-llama3`, you have to pass `--hf_overrides '{\"architectures\": [\"MantisForConditionalGeneration\"]}'` when running vLLM.  \n!!! warning\nThe output quality of `AllenAI/Molmo-7B-D-0924` (especially in object localization tasks) has deteriorated in recent updates.", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models/Generative Models", "level": 3, "text": "For the best results, we recommend using the following dependency versions (tested on A10 and L40):  \n??? code \"Dependency versions\"  \n```text\n# Core vLLM-compatible dependencies with Molmo accuracy setup (tested on L40)\ntorch==2.5.1\ntorchvision==0.20.1\ntransformers==4.48.1\ntokenizers==0.21.0\ntiktoken==0.7.0\nvllm==0.7.0", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models/Generative Models", "level": 3, "text": "# Optional but recommended for improved performance and stability\ntriton==3.1.0\nxformers==0.0.28.post3\nuvloop==0.21.0\nprotobuf==5.29.3\nopenai==1.60.2\nopencv-python-headless==4.11.0.86\npillow==10.4.0", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models/Generative Models", "level": 3, "text": "# Installed FlashAttention (for float16 only)\nflash-attn>=2.5.6  # Not used in float32, but should be documented\n```  \n**Note:** Make sure you understand the security implications of using outdated packages.  \n!!! note\nThe official `openbmb/MiniCPM-V-2` doesn't work yet, so we need to use a fork (`HwwwH/MiniCPM-V-2`) for now.\nFor more details, please see: <gh-pr:4087#issuecomment-2250397630>  \n!!! warning\nOur PaliGemma implementations have the same problem as Gemma 3 (see above) for both V0 and V1.  \n!!! note\nFor Qwen2.5-Omni and Qwen3-Omni, reading audio from video pre-processing (`--mm-processor-kwargs '{\"use_audio_in_video\": true}'`) is currently work in progress and not yet supported.  \n#### Transcription  \nSpeech2Text models trained specifically for Automatic Speech Recognition.  \n| Architecture | Models | Example HF Models | [LoRA](../features/lora.md) | [PP](../serving/parallelism_scaling.md) |\n|--------------|--------|-------------------|----------------------|---------------------------|", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models/Generative Models", "level": 3, "text": "| `WhisperForConditionalGeneration` | Whisper | `openai/whisper-small`, `openai/whisper-large-v3-turbo`, etc. | | |\n| `VoxtralForConditionalGeneration` | Voxtral (Mistral format) | `mistralai/Voxtral-Mini-3B-2507`, `mistralai/Voxtral-Small-24B-2507`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |\n| `Gemma3nForConditionalGeneration` | Gemma3n | `google/gemma-3n-E2B-it`, `google/gemma-3n-E4B-it`, etc. | | |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models/Pooling Models", "level": 3, "text": "### Pooling Models  \nSee [this page](./pooling_models.md) for more information on how to use pooling models.  \n#### Embedding  \nThese models primarily support the [`LLM.embed`](./pooling_models.md#llmembed) API.  \n!!! note\nTo get the best results, you should use pooling models that are specifically trained as such.  \nThe following table lists those that are tested in vLLM.  \n| Architecture | Models | Inputs | Example HF Models | [LoRA](../features/lora.md) | [PP](../serving/parallelism_scaling.md) |\n|--------------|--------|--------|-------------------|----------------------|---------------------------|\n| `CLIPModel` | CLIP | T / I | `openai/clip-vit-base-patch32`, `openai/clip-vit-large-patch14`, etc. | | |\n| `LlavaNextForConditionalGeneration`<sup>C</sup> | LLaVA-NeXT-based | T / I | `royokong/e5-v` | | âœ…ï¸Ž |\n| `Phi3VForCausalLM`<sup>C</sup> | Phi-3-Vision-based | T + I | `TIGER-Lab/VLM2Vec-Full` | | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models/Pooling Models", "level": 3, "text": "| `*ForConditionalGeneration`<sup>C</sup>, `*ForCausalLM`<sup>C</sup>, etc. | Generative models | \\* | N/A | \\* | \\* |  \n<sup>C</sup> Automatically converted into an embedding model via `--convert embed`. ([details](./pooling_models.md#model-conversion))\n\\* Feature support is the same as that of the original model.  \n---  \n#### Cross-encoder / Reranker  \nCross-encoder and reranker models are a subset of classification models that accept two prompts as input.\nThese models primarily support the [`LLM.score`](./pooling_models.md#llmscore) API.  \n| Architecture | Models | Inputs | Example HF Models | [LoRA](../features/lora.md) | [PP](../serving/parallelism_scaling.md) |\n|--------------|--------|--------|-------------------|----------------------|---------------------------|\n| `JinaVLForSequenceClassification` | JinaVL-based | T + I<sup>E+</sup> | `jinaai/jina-reranker-m0`, etc. | âœ…ï¸Ž | âœ…ï¸Ž |", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/List of Multimodal Language Models/Pooling Models", "level": 3, "text": "<sup>C</sup> Automatically converted into a classification model via `--convert classify`. ([details](./pooling_models.md#model-conversion))\n\\* Feature support is the same as that of the original model.", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Model Support Policy", "level": 2, "text": "## Model Support Policy  \nAt vLLM, we are committed to facilitating the integration and support of third-party models within our ecosystem. Our approach is designed to balance the need for robustness and the practical limitations of supporting a wide range of models. Hereâ€™s how we manage third-party model support:  \n1. **Community-Driven Support**: We encourage community contributions for adding new models. When a user requests support for a new model, we welcome pull requests (PRs) from the community. These contributions are evaluated primarily on the sensibility of the output they generate, rather than strict consistency with existing implementations such as those in transformers. **Call for contribution:** PRs coming directly from model vendors are greatly appreciated!", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Model Support Policy", "level": 2, "text": "2. **Best-Effort Consistency**: While we aim to maintain a level of consistency between the models implemented in vLLM and other frameworks like transformers, complete alignment is not always feasible. Factors like acceleration techniques and the use of low-precision computations can introduce discrepancies. Our commitment is to ensure that the implemented models are functional and produce sensible results.  \n!!! tip\nWhen comparing the output of `model.generate` from Hugging Face Transformers with the output of `llm.generate` from vLLM, note that the former reads the model's generation config file (i.e., [generation_config.json](https://github.com/huggingface/transformers/blob/19dabe96362803fb0a9ae7073d03533966598b17/src/transformers/generation/utils.py#L1945)) and applies the default parameters for generation, while the latter only uses the parameters passed to the function. Ensure all sampling parameters are identical when comparing outputs.", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Model Support Policy", "level": 2, "text": "3. **Issue Resolution and Model Updates**: Users are encouraged to report any bugs or issues they encounter with third-party models. Proposed fixes should be submitted via PRs, with a clear explanation of the problem and the rationale behind the proposed solution. If a fix for one model impacts another, we rely on the community to highlight and address these cross-model dependencies. Note: for bugfix PRs, it is good etiquette to inform the original author to seek their feedback.  \n4. **Monitoring and Updates**: Users interested in specific models should monitor the commit history for those models (e.g., by tracking changes in the main/vllm/model_executor/models directory). This proactive approach helps users stay informed about updates and changes that may affect the models they use.", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Model Support Policy", "level": 2, "text": "5. **Selective Focus**: Our resources are primarily directed towards models with significant user interest and impact. Models that are less frequently used may receive less attention, and we rely on the community to play a more active role in their upkeep and improvement.  \nThrough this approach, vLLM fosters a collaborative environment where both the core development team and the broader community contribute to the robustness and diversity of the third-party models supported in our ecosystem.  \nNote that, as an inference engine, vLLM does not introduce new models. Therefore, all models supported by vLLM are third-party models in this regard.  \nWe have the following levels of testing for models:", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Model Support Policy", "level": 2, "text": "We have the following levels of testing for models:  \n1. **Strict Consistency**: We compare the output of the model with the output of the model in the HuggingFace Transformers library under greedy decoding. This is the most stringent test. Please refer to [models tests](https://github.com/vllm-project/vllm/blob/main/tests/models) for the models that have passed this test.\n2. **Output Sensibility**: We check if the output of the model is sensible and coherent, by measuring the perplexity of the output and checking for any obvious errors. This is a less stringent test.\n3. **Runtime Functionality**: We check if the model can be loaded and run without errors. This is the least stringent test. Please refer to [functionality tests](gh-dir:tests) and [examples](gh-dir:examples) for the models that have passed this test.", "file_path": "models/supported_models.md"}
{"id": "4ad1cf317272e8e6984162d57141a332dd73189c4a6bd75d217c4b7a6f148530", "heading": "Supported Models/Model Support Policy", "level": 2, "text": "4. **Community Feedback**: We rely on the community to provide feedback on the models. If a model is broken or not working as expected, we encourage users to raise issues to report it or open pull requests to fix it. The rest of the models fall under this category.", "file_path": "models/supported_models.md"}
{"id": "bb9853ac25ba59804a27891eec24d1c37a41acd500e36e9bf58b80113fc28d52", "heading": "Context Parallel Deployment", "level": 1, "text": "# Context Parallel Deployment  \nContext parallel mainly solves the problem of serving long context requests. As prefill and decode present quite different characteristics and have quite different SLO (service level objectives), we need to implement context parallel separately for them. The major considerations are:  \n- For long context prefill, we need to control the TTFT (time to first token) by amortizing the computation time of the prefill across query tokens.\n- For long context decode, we need more space for KV cache to increase the batchsize (and hence the throughput).", "file_path": "serving/context_parallel_deployment.md"}
{"id": "bb9853ac25ba59804a27891eec24d1c37a41acd500e36e9bf58b80113fc28d52", "heading": "Context Parallel Deployment/Prefill Context Parallel", "level": 2, "text": "## Prefill Context Parallel  \nDuring prefill, for a long request with `T` new tokens, we need to compute query/key/value tensors for these new tokens. Say we have `N` GPUs, we can split the request into `N` chunks, and each GPU computes one chunk of the query/key/value tensors.  \nDepending on the use case, there're two possible strategies:  \n1. Partial query, full key/value: If the request token length is moderately long (we can afford holding the full key/value tensors), and the goal is to accelerate the prefill (and amortize the computation time of the prefill across query tokens), then we can gather the key/value tensors from all GPUs and let each GPU compute the attention output corresponding to the query tokens of its chunk.", "file_path": "serving/context_parallel_deployment.md"}
{"id": "bb9853ac25ba59804a27891eec24d1c37a41acd500e36e9bf58b80113fc28d52", "heading": "Context Parallel Deployment/Prefill Context Parallel", "level": 2, "text": "2. Partial query, partial key/value: If the request token length is too long, we cannot afford holding the full key/value tensors anymore, then we can only compute one chunk of query/key/value tensors for each GPU, and use techniques like [ring-attention](http://arxiv.org/abs/2310.01889) to send/recv key/value tensors chunk by chunk.  \nBoth approaches are under active development.", "file_path": "serving/context_parallel_deployment.md"}
{"id": "bb9853ac25ba59804a27891eec24d1c37a41acd500e36e9bf58b80113fc28d52", "heading": "Context Parallel Deployment/Decode Context Parallel", "level": 2, "text": "## Decode Context Parallel  \nDue to the auto-regressive nature of decoding, every decoding step needs to compute a small amount of query tokens w.r.t. a large number of key/value tokens stored in the paged KV cache. The core of decode context parallel is how to shard the KV cache across GPUs.  \nFor a model with `H` kv-heads, a request with `T` tokens in the context needs to store `H * T` key/value tensors in the KV cache.  \n1. If one GPU can hold them all, and the performance is good enough, then no parallelization is needed.\n2. If one GPU cannot hold them all, or we want to hold more requests in the KV cache, we can first shard the KV cache along the `H` dimension, that's the plain tensor parallel sharding. It's as simple as adding `-tp <num_gpus>` to the command line.", "file_path": "serving/context_parallel_deployment.md"}
{"id": "bb9853ac25ba59804a27891eec24d1c37a41acd500e36e9bf58b80113fc28d52", "heading": "Context Parallel Deployment/Decode Context Parallel", "level": 2, "text": "3. Since `H` is limited (determined by the model architecture), when we continue to increase the tensor parallel size, the KV cache for each GPU will be duplicated for `tp_size / H` times. Of course, duplication is not good for efficiency. Then we need to add decode context parallel to further shard the KV cache along the `T` dimension. This is as simple as adding `-dcp <size>` to the command line. Note that `size` does not increase the number of GPUs we need to launch, but just reduces the KV cache duplication. The dcp size should lie in the range of `[1, tp_size/H]`. With larger dcp size, the KV cache duplication is reduced, but the communication overhead increases.", "file_path": "serving/context_parallel_deployment.md"}
{"id": "bb9853ac25ba59804a27891eec24d1c37a41acd500e36e9bf58b80113fc28d52", "heading": "Context Parallel Deployment/Decode Context Parallel", "level": 2, "text": "Theoretically, it is possible to extend the dcp size beyond `tp_size / H` to further shard the KV cache and accelerate the decoding phase. However, since the number of query tokens is limited in decoding, it's unclear what should we do for the remaining `dcp_size - tp_size / H` GPUs for non-attention layers. For the sake of simplicity, dcp size is upper bounded by `tp_size / H`. If you want to further accelerate the decoding phase, you can consider increasing the `tp_size` first, and then increasing the dcp size.  \nNote that kv cache can grow during decoding, and the sharding strategy needs to be carefully implemented. We use an interleaving strategy to shard the KV cache along the `T` dimension, so that kv cache for future tokens can be naturally sharded along the `T` dimension. This is proposed by [Chao Hong from Moonshot](https://github.com/youzhedian), and also explained in details in [this paper](http://arxiv.org/abs/2507.07120).  \nCase study:", "file_path": "serving/context_parallel_deployment.md"}
{"id": "bb9853ac25ba59804a27891eec24d1c37a41acd500e36e9bf58b80113fc28d52", "heading": "Context Parallel Deployment/Decode Context Parallel", "level": 2, "text": "Case study:  \nFor DeepSeek-R1, we have 1 kv-head when MLA is enabled. The typical single-node deployment with `-tp 8` causes 8x KV cache duplication. We can consider adding `-dcp 8` to reduce the KV cache duplication.  \nFor Kimi-K2, the architecture is similar to DeepSeek-R1, but with more parameters. When we deploy it with `-tp 16`, the KV cache duplication is 16x. We can add `-dcp 16` to completely remove the KV cache duplication, at the cost of more communication overhead. We can also add `-dcp 8` to reduce the KV cache duplication to 2x. Although it still duplicates the KV cache twice, the communication overhead is smaller since the DCP communication only happens inside one node.  \nFor Qwen3-235B-A22B, we have 4 kv-heads. When we deploy it with `-tp 8`, the KV cache duplication is 2x. Then we can add `-dcp 2` to remove the KV cache duplication.", "file_path": "serving/context_parallel_deployment.md"}
{"id": "bb9853ac25ba59804a27891eec24d1c37a41acd500e36e9bf58b80113fc28d52", "heading": "Context Parallel Deployment/Decode Context Parallel", "level": 2, "text": "In short, for decode context parallel, try to increase `-tp` size until you get satisfactory performance, and then add `-dcp` to reduce the KV cache duplication.  \nDecode context parallel is supported in vLLM, for both MLA and GQA models. Some attention backends also support the combination of decode context parallel and MTP (multi-token prediction) to further accelerate the decoding phase.", "file_path": "serving/context_parallel_deployment.md"}
{"id": "bb9853ac25ba59804a27891eec24d1c37a41acd500e36e9bf58b80113fc28d52", "heading": "Context Parallel Deployment/Technical Discussions", "level": 2, "text": "## Technical Discussions  \nThe main discussions happen in the `#sig-context-parallel` channel of [vLLM Slack](https://slack.vllm.ai/).", "file_path": "serving/context_parallel_deployment.md"}
{"id": "961b9a00add7eda3efa2908caef76e5ee8dd06b65e138966f13342d011414fff", "heading": "Data Parallel Deployment", "level": 1, "text": "# Data Parallel Deployment  \nvLLM supports Data Parallel deployment, where model weights are replicated across separate instances/GPUs to process independent batches of requests.  \nThis will work with both dense and MoE models.  \nFor MoE models, particularly those like DeepSeek that employ MLA (Multi-head Latent Attention), it can be advantageous to use data parallel for the attention layers and expert or tensor parallel (EP or TP) for the expert layers.  \nIn these cases, the data parallel ranks are not completely independent. Forward passes must be aligned, and expert layers across all ranks are required to synchronize during every forward pass, even when there are fewer requests to be processed than DP ranks.  \nThe expert layers will by default form a (DP x TP) sized tensor parallel group. To enable expert parallelism, include the `--enable-expert-parallel` CLI arg (on all nodes in the multi-node case).", "file_path": "serving/data_parallel_deployment.md"}
{"id": "961b9a00add7eda3efa2908caef76e5ee8dd06b65e138966f13342d011414fff", "heading": "Data Parallel Deployment", "level": 1, "text": "In vLLM, each DP rank is deployed as a separate \"core engine\" process that communicates with front-end process(es) via ZMQ sockets. Data Parallel attention can be combined with Tensor Parallel attention, in which case each DP engine owns a number of per-GPU worker processes equal to the configured TP size.  \nFor MoE models, when any requests are in progress in any rank, we must ensure that empty \"dummy\" forward passes are performed in all ranks that don't currently have any requests scheduled. This is handled via a separate DP Coordinator process that communicates with all ranks, and a collective operation performed every N steps to determine when all ranks become idle and can be paused. When TP is used in conjunction with DP, expert layers form an EP or TP group of size (DP x TP).", "file_path": "serving/data_parallel_deployment.md"}
{"id": "961b9a00add7eda3efa2908caef76e5ee8dd06b65e138966f13342d011414fff", "heading": "Data Parallel Deployment", "level": 1, "text": "In all cases, it is beneficial to load-balance requests between DP ranks. For online deployments, this balancing can be optimized by taking into account the state of each DP engine - in particular its currently scheduled and waiting (queued) requests, and KV cache state. Each DP engine has an independent KV cache, and the benefit of prefix caching can be maximized by directing prompts intelligently.  \nThis document focuses on online deployments (with the API server). DP + EP is also supported for offline usage (via the LLM class), for an example see <gh-file:examples/offline_inference/data_parallel.py>.  \nThere are two distinct modes supported for online deployments - self-contained with internal load balancing, or externally per-rank process deployment and load balancing.", "file_path": "serving/data_parallel_deployment.md"}
{"id": "961b9a00add7eda3efa2908caef76e5ee8dd06b65e138966f13342d011414fff", "heading": "Data Parallel Deployment/Internal Load Balancing", "level": 2, "text": "## Internal Load Balancing  \nvLLM supports \"self-contained\" data parallel deployments that expose a single API endpoint.  \nIt can be configured by simply including e.g. `--data-parallel-size=4` in the vllm serve command line arguments. This will require 4 GPUs. It can be combined with tensor parallel, for example `--data-parallel-size=4 --tensor-parallel-size=2`, which would require 8 GPUs.  \nRunning a single data parallel deployment across multiple nodes requires a different `vllm serve` to be run on each node, specifying which DP ranks should run on that node. In this case, there will still be a single HTTP entrypoint - the API server(s) will run only on one node, but it doesn't necessarily need to be co-located with the DP ranks.  \nThis will run DP=4, TP=2 on a single 8-GPU node:  \n```bash\nvllm serve $MODEL --data-parallel-size 4 --tensor-parallel-size 2\n```  \nThis will run DP=4 with DP ranks 0 and 1 on the head node and ranks 2 and 3 on the second node:  \n```bash\n# Node 0  (with ip address 10.99.48.128)", "file_path": "serving/data_parallel_deployment.md"}
{"id": "961b9a00add7eda3efa2908caef76e5ee8dd06b65e138966f13342d011414fff", "heading": "Data Parallel Deployment/Internal Load Balancing", "level": 2, "text": "```bash\n# Node 0  (with ip address 10.99.48.128)\nvllm serve $MODEL --data-parallel-size 4 --data-parallel-size-local 2 \\\n--data-parallel-address 10.99.48.128 --data-parallel-rpc-port 13345\n# Node 1\nvllm serve $MODEL --headless --data-parallel-size 4 --data-parallel-size-local 2 \\\n--data-parallel-start-rank 2 \\\n--data-parallel-address 10.99.48.128 --data-parallel-rpc-port 13345\n```  \nThis will run DP=4 with only the API server on the first node and all engines on the second node:  \n```bash\n# Node 0  (with ip address 10.99.48.128)\nvllm serve $MODEL --data-parallel-size 4 --data-parallel-size-local 0 \\\n--data-parallel-address 10.99.48.128 --data-parallel-rpc-port 13345\n# Node 1\nvllm serve $MODEL --headless --data-parallel-size 4 --data-parallel-size-local 4 \\\n--data-parallel-address 10.99.48.128 --data-parallel-rpc-port 13345\n```  \nThis DP mode can also be used with Ray by specifying `--data-parallel-backend=ray`:  \n```bash\nvllm serve $MODEL --data-parallel-size 4 --data-parallel-size-local 2 \\", "file_path": "serving/data_parallel_deployment.md"}
{"id": "961b9a00add7eda3efa2908caef76e5ee8dd06b65e138966f13342d011414fff", "heading": "Data Parallel Deployment/Internal Load Balancing", "level": 2, "text": "--data-parallel-backend=ray\n```  \nThere are several notable differences when using Ray:  \n- A single launch command (on any node) is needed to start all local and remote DP ranks, therefore it is more convenient compared to launching on each node\n- There is no need to specify `--data-parallel-address`, and the node where the command is run is used as `--data-parallel-address`\n- There is no need to specify `--data-parallel-rpc-port`\n- Remote DP ranks will be allocated based on node resources of the Ray cluster  \nCurrently, the internal DP load balancing is done within the API server process(es) and is based on the running and waiting queues in each of the engines. This could be made more sophisticated in future by incorporating KV cache aware logic.", "file_path": "serving/data_parallel_deployment.md"}
{"id": "961b9a00add7eda3efa2908caef76e5ee8dd06b65e138966f13342d011414fff", "heading": "Data Parallel Deployment/Internal Load Balancing", "level": 2, "text": "When deploying large DP sizes using this method, the API server process can become a bottleneck. In this case, the orthogonal `--api-server-count` command line option can be used to scale this out (for example `--api-server-count=4`). This is transparent to users - a single HTTP endpoint / port is still exposed. Note that this API server scale-out is \"internal\" and still confined to the \"head\" node.  \n<figure markdown=\"1\">\n![DP Internal LB Diagram](../assets/deployment/dp_internal_lb.png)\n</figure>", "file_path": "serving/data_parallel_deployment.md"}
{"id": "961b9a00add7eda3efa2908caef76e5ee8dd06b65e138966f13342d011414fff", "heading": "Data Parallel Deployment/External Load Balancing", "level": 2, "text": "## External Load Balancing  \nFor larger scale deployments especially, it can make sense to handle the orchestration and load balancing of data parallel ranks externally.  \nIn this case, it's more convenient to treat each DP rank like a separate vLLM deployment, with its own endpoint, and have an external router balance HTTP requests between them, making use of appropriate real-time telemetry from each server for routing decisions.  \nThis can already be done trivially for non-MoE models, since each deployed server is fully independent. No data parallel CLI options need to be used for this.  \nWe support an equivalent topology for MoE DP+EP which can be configured via the following CLI arguments.  \nIf DP ranks are co-located (same node / ip address), a default RPC port is used, but a different HTTP server port must be specified for each rank:  \n```bash\n# Rank 0\nCUDA_VISIBLE_DEVICES=0 vllm serve $MODEL --data-parallel-size 2 --data-parallel-rank 0 \\\n--port 8000\n# Rank 1", "file_path": "serving/data_parallel_deployment.md"}
{"id": "961b9a00add7eda3efa2908caef76e5ee8dd06b65e138966f13342d011414fff", "heading": "Data Parallel Deployment/External Load Balancing", "level": 2, "text": "--port 8000\n# Rank 1\nCUDA_VISIBLE_DEVICES=1 vllm serve $MODEL --data-parallel-size 2 --data-parallel-rank 1 \\\n--port 8001\n```  \nFor multi-node cases, the address/port of rank 0 must also be specified:  \n```bash\n# Rank 0  (with ip address 10.99.48.128)\nvllm serve $MODEL --data-parallel-size 2 --data-parallel-rank 0 \\\n--data-parallel-address 10.99.48.128 --data-parallel-rpc-port 13345\n# Rank 1\nvllm serve $MODEL --data-parallel-size 2 --data-parallel-rank 1 \\\n--data-parallel-address 10.99.48.128 --data-parallel-rpc-port 13345\n```  \nThe coordinator process also runs in this scenario, co-located with the DP rank 0 engine.  \n<figure markdown=\"1\">\n![DP External LB Diagram](../assets/deployment/dp_external_lb.png)\n</figure>  \nIn the above diagram, each of the dotted boxes corresponds to a separate launch of `vllm serve` - these could be separate Kubernetes pods, for example.", "file_path": "serving/data_parallel_deployment.md"}
{"id": "32e14997c5e18a1cc1f89ffdbc471a0b10a61a1b2a83b5564e998179c54752a3", "heading": "Troubleshooting distributed deployments", "level": 1, "text": "# Troubleshooting distributed deployments  \nFor general troubleshooting, see [Troubleshooting](../usage/troubleshooting.md).", "file_path": "serving/distributed_troubleshooting.md"}
{"id": "32e14997c5e18a1cc1f89ffdbc471a0b10a61a1b2a83b5564e998179c54752a3", "heading": "Troubleshooting distributed deployments/Verify inter-node GPU communication", "level": 2, "text": "## Verify inter-node GPU communication  \nAfter you start the Ray cluster, verify GPU-to-GPU communication across nodes. Proper configuration can be non-trivial. For more information, see [troubleshooting script][troubleshooting-incorrect-hardware-driver]. If you need additional environment variables for communication configuration, append them to <gh-file:examples/online_serving/run_cluster.sh>, for example `-e NCCL_SOCKET_IFNAME=eth0`. Setting environment variables during cluster creation is recommended because the variables propagate to all nodes. In contrast, setting environment variables in the shell affects only the local node. For more information, see <gh-issue:6803>.", "file_path": "serving/distributed_troubleshooting.md"}
{"id": "32e14997c5e18a1cc1f89ffdbc471a0b10a61a1b2a83b5564e998179c54752a3", "heading": "Troubleshooting distributed deployments/No available node types can fulfill resource request", "level": 2, "text": "## No available node types can fulfill resource request  \nThe error message `Error: No available node types can fulfill resource request` can appear even when the cluster has enough GPUs. The issue often occurs when nodes have multiple IP addresses and vLLM can't select the correct one. Ensure that vLLM and Ray use the same IP address by setting `VLLM_HOST_IP` in <gh-file:examples/online_serving/run_cluster.sh> (with a different value on each node). Use `ray status` and `ray list nodes` to verify the chosen IP address. For more information, see <gh-issue:7815>.", "file_path": "serving/distributed_troubleshooting.md"}
{"id": "32e14997c5e18a1cc1f89ffdbc471a0b10a61a1b2a83b5564e998179c54752a3", "heading": "Troubleshooting distributed deployments/Ray observability", "level": 2, "text": "## Ray observability  \nDebugging a distributed system can be challenging due to the large scale and complexity. Ray provides a suite of tools to help monitor, debug, and optimize Ray applications and clusters. For more information about Ray observability, visit the [official Ray observability docs](https://docs.ray.io/en/latest/ray-observability/index.html). For more information about debugging Ray applications, visit the [Ray Debugging Guide](https://docs.ray.io/en/latest/ray-observability/user-guides/debug-apps/index.html). For information about troubleshooting Kubernetes clusters, see the\n[official KubeRay troubleshooting guide](https://docs.ray.io/en/latest/serve/advanced-guides/multi-node-gpu-troubleshooting.html).", "file_path": "serving/distributed_troubleshooting.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment", "level": 1, "text": "# Expert Parallel Deployment  \nvLLM supports Expert Parallelism (EP), which allows experts in Mixture-of-Experts (MoE) models to be deployed on separate GPUs, increasing locality, efficiency, and throughput overall.  \nEP is typically coupled with Data Parallelism (DP). While DP can be used independently of EP, EP is more efficient when used in conjunction with DP. You can read more about data parallelism [here](data_parallel_deployment.md).", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Prerequisites", "level": 2, "text": "## Prerequisites  \nBefore using EP, you need to install the necessary dependencies. We are actively working on making this easier in the future:  \n1. **Install DeepEP and pplx-kernels**: Set up host environment following vLLM's guide for EP kernels [here](gh-file:tools/ep_kernels).\n2. **Install DeepGEMM library**: Follow the [official instructions](https://github.com/deepseek-ai/DeepGEMM#installation).\n3. **For disaggregated serving**: Install `gdrcopy` by running the [`install_gdrcopy.sh`](gh-file:tools/install_gdrcopy.sh) script (e.g., `install_gdrcopy.sh \"${GDRCOPY_OS_VERSION}\" \"12.8\" \"x64\"`). You can find available OS versions [here](https://developer.download.nvidia.com/compute/redist/gdrcopy/CUDA%2012.8/).", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Prerequisites/Backend Selection Guide", "level": 3, "text": "### Backend Selection Guide  \nvLLM provides multiple communication backends for EP. Use `--all2all-backend` to select one:  \n| Backend | Use Case | Features | Best For |\n|---------|----------|----------|----------|\n| `allgather_reducescatter` | Default backend | Standard all2all using allgather/reducescatter primitives | General purpose, works with any EP+DP configuration |\n| `pplx` | Single node | Chunked prefill support, efficient intra-node communication | Single-node deployments, development |\n| `deepep_high_throughput` | Multi-node prefill | Grouped GEMM with continuous layout, optimized for prefill | Prefill-dominated workloads, high-throughput scenarios |\n| `deepep_low_latency` | Multi-node decode | CUDA graph support, masked layout, optimized for decode | Decode-dominated workloads, low-latency scenarios |\n| `flashinfer_all2allv` | MNNVL systems | FlashInfer alltoallv kernels for multi-node NVLink | Systems with NVLink across nodes |", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Prerequisites/Backend Selection Guide", "level": 3, "text": "| `naive` | Testing/debugging | Simple broadcast-based implementation | Debugging, not recommended for production |", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Single Node Deployment", "level": 2, "text": "## Single Node Deployment  \n!!! warning\nEP is an experimental feature. Argument names and default values may change in the future.", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Single Node Deployment/Configuration", "level": 3, "text": "### Configuration  \nEnable EP by setting the `--enable-expert-parallel` flag. The EP size is automatically calculated as:  \n```text\nEP_SIZE = TP_SIZE Ã— DP_SIZE\n```  \nWhere:  \n- `TP_SIZE`: Tensor parallel size (always 1 for now)\n- `DP_SIZE`: Data parallel size\n- `EP_SIZE`: Expert parallel size (computed automatically)", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Single Node Deployment/Example Command", "level": 3, "text": "### Example Command  \nThe following command serves a `DeepSeek-V3-0324` model with 1-way tensor parallel, 8-way (attention) data parallel, and 8-way expert parallel. The attention weights are replicated across all GPUs, while the expert weights are split across GPUs. It will work on a H200 (or H20) node with 8 GPUs. For H100, you can try to serve a smaller model or refer to the multi-node deployment section.  \n```bash\n# Single node EP deployment with pplx backend\nvllm serve deepseek-ai/DeepSeek-V3-0324 \\\n--tensor-parallel-size 1 \\       # Tensor parallelism across 1 GPU\n--data-parallel-size 8 \\         # Data parallelism across 8 processes\n--enable-expert-parallel \\       # Enable expert parallelism\n--all2all-backend pplx           # Use pplx communication backend\n```", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Multi-Node Deployment", "level": 2, "text": "## Multi-Node Deployment  \nFor multi-node deployment, use the DeepEP communication kernel with one of two modes (see [Backend Selection Guide](#backend-selection-guide) above).", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Multi-Node Deployment/Deployment Steps", "level": 3, "text": "### Deployment Steps  \n1. **Run one command per node** - Each node requires its own launch command\n2. **Configure networking** - Ensure proper IP addresses and port configurations\n3. **Set node roles** - First node handles requests, additional nodes run in headless mode", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Multi-Node Deployment/Example: 2-Node Deployment", "level": 3, "text": "### Example: 2-Node Deployment  \nThe following example deploys `DeepSeek-V3-0324` across 2 nodes using `deepep_low_latency` mode:  \n```bash\n# Node 1 (Primary - handles incoming requests)\nvllm serve deepseek-ai/DeepSeek-V3-0324 \\\n--all2all-backend deepep_low_latency \\\n--tensor-parallel-size 1 \\               # TP size per node\n--enable-expert-parallel \\               # Enable EP\n--data-parallel-size 16 \\                # Total DP size across all nodes\n--data-parallel-size-local 8 \\           # Local DP size on this node (8 GPUs per node)\n--data-parallel-address 192.168.1.100 \\  # Replace with actual IP of Node 1\n--data-parallel-rpc-port 13345 \\         # RPC communication port, can be any port as long as reachable by all nodes\n--api-server-count=8                     # Number of API servers for load handling (scaling this out to total ranks are recommended)", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Multi-Node Deployment/Example: 2-Node Deployment", "level": 3, "text": "# Node 2 (Secondary - headless mode, no API server)\nvllm serve deepseek-ai/DeepSeek-V3-0324 \\\n--all2all-backend deepep_low_latency \\\n--tensor-parallel-size 1 \\               # TP size per node\n--enable-expert-parallel \\               # Enable EP\n--data-parallel-size 16 \\                # Total DP size across all nodes\n--data-parallel-size-local 8 \\           # Local DP size on this node\n--data-parallel-start-rank 8 \\           # Starting rank offset for this node\n--data-parallel-address 192.168.1.100 \\  # IP of primary node (Node 1)\n--data-parallel-rpc-port 13345 \\         # Same RPC port as primary\n--headless                               # No API server, worker only\n```", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Multi-Node Deployment/Key Configuration Notes", "level": 3, "text": "### Key Configuration Notes  \n- **Headless mode**: Secondary nodes run with `--headless` flag, meaning all client requests are handled by the primary node\n- **Rank calculation**: `--data-parallel-start-rank` should equal the cumulative local DP size of previous nodes\n- **Load scaling**: Adjust `--api-server-count` on the primary node to handle higher request loads", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Multi-Node Deployment/Network Configuration", "level": 3, "text": "### Network Configuration  \n!!! important \"InfiniBand Clusters\"\nOn InfiniBand networked clusters, set this environment variable to prevent initialization hangs:\n```bash\nexport GLOO_SOCKET_IFNAME=eth0\n```\nThis ensures torch distributed group discovery uses Ethernet instead of InfiniBand for initial setup.", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Expert Parallel Load Balancer (EPLB)", "level": 2, "text": "## Expert Parallel Load Balancer (EPLB)  \nWhile MoE models are typically trained so that each expert receives a similar number of tokens, in practice the distribution of tokens across experts can be highly skewed. vLLM provides an Expert Parallel Load Balancer (EPLB) to redistribute expert mappings across EP ranks, evening the load across experts.", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Expert Parallel Load Balancer (EPLB)/Configuration", "level": 3, "text": "### Configuration  \nEnable EPLB with the `--enable-eplb` flag.  \n!!! note \"Model Support\"\nCurrently only DeepSeek V3 architecture is supported.  \nWhen enabled, vLLM collects load statistics with every forward pass and periodically rebalances expert distribution.", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Expert Parallel Load Balancer (EPLB)/EPLB Parameters", "level": 3, "text": "### EPLB Parameters  \nConfigure EPLB with the `--eplb-config` argument, which accepts a JSON string. The available keys and their descriptions are:  \n| Parameter | Description | Default |\n|-----------|-------------|---------|\n| `window_size`| Number of engine steps to track for rebalancing decisions | 1000 |\n| `step_interval`| Frequency of rebalancing (every N engine steps) | 3000 |\n| `log_balancedness` | Log balancedness metrics (avg tokens per expert Ã· max tokens per expert) | `false` |\n| `num_redundant_experts` | Additional global experts per EP rank beyond equal distribution | `0` |  \nFor example:  \n```bash\nvllm serve Qwen/Qwen3-30B-A3B \\\n--enable-eplb \\\n--eplb-config '{\"window_size\":1000,\"step_interval\":3000,\"num_redundant_experts\":2,\"log_balancedness\":true}'\n```  \n??? tip \"Prefer individual arguments instead of JSON?\"  \n```bash\nvllm serve Qwen/Qwen3-30B-A3B \\\n--enable-eplb \\\n--eplb-config.window_size 1000 \\\n--eplb-config.step_interval 3000 \\\n--eplb-config.num_redundant_experts 2 \\", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Expert Parallel Load Balancer (EPLB)/EPLB Parameters", "level": 3, "text": "--eplb-config.num_redundant_experts 2 \\\n--eplb-config.log_balancedness true\n```", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Expert Parallel Load Balancer (EPLB)/Expert Distribution Formula", "level": 3, "text": "### Expert Distribution Formula  \n- **Default**: Each EP rank has `NUM_TOTAL_EXPERTS Ã· NUM_EP_RANKS` experts\n- **With redundancy**: Each EP rank has `(NUM_TOTAL_EXPERTS + NUM_REDUNDANT_EXPERTS) Ã· NUM_EP_RANKS` experts", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Expert Parallel Load Balancer (EPLB)/Memory Footprint Overhead", "level": 3, "text": "### Memory Footprint Overhead  \nEPLB uses redundant experts that need to fit in GPU memory. This means that EPLB may not be a good fit for memory constrained environments or when KV cache space is at a premium.  \nThis overhead equals `NUM_MOE_LAYERS * BYTES_PER_EXPERT * (NUM_TOTAL_EXPERTS + NUM_REDUNDANT_EXPERTS) Ã· NUM_EP_RANKS`.\nFor DeepSeekV3, this is approximately `2.4 GB` for one redundant expert per EP rank.", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Expert Parallel Load Balancer (EPLB)/Example Command", "level": 3, "text": "### Example Command  \nSingle node deployment with EPLB enabled:  \n```bash\n# Single node with EPLB load balancing\nvllm serve deepseek-ai/DeepSeek-V3-0324 \\\n--tensor-parallel-size 1 \\       # Tensor parallelism\n--data-parallel-size 8 \\         # Data parallelism\n--enable-expert-parallel \\       # Enable EP\n--all2all-backend pplx \\         # Use pplx communication backend\n--enable-eplb \\                  # Enable load balancer\n--eplb-config '{\"window_size\":1000,\"step_interval\":3000,\"num_redundant_experts\":2,\"log_balancedness\":true}'\n```  \nFor multi-node deployment, add these EPLB flags to each node's command. We recommend setting `--eplb-config '{\"num_redundant_experts\":32}'` to 32 in large scale use cases so the most popular experts are always available.", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Disaggregated Serving (Prefill/Decode Split)", "level": 2, "text": "## Disaggregated Serving (Prefill/Decode Split)  \nFor production deployments requiring strict SLA guarantees for time-to-first-token and inter-token latency, disaggregated serving allows independent scaling of prefill and decode operations.", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Disaggregated Serving (Prefill/Decode Split)/Architecture Overview", "level": 3, "text": "### Architecture Overview  \n- **Prefill Instance**: Uses `deepep_high_throughput` backend for optimal prefill performance\n- **Decode Instance**: Uses `deepep_low_latency` backend for minimal decode latency\n- **KV Cache Transfer**: Connects instances via NIXL or other KV connectors", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Disaggregated Serving (Prefill/Decode Split)/Setup Steps", "level": 3, "text": "### Setup Steps  \n1. **Install gdrcopy/ucx/nixl**: For maximum performance, run the [install_gdrcopy.sh](gh-file:tools/install_gdrcopy.sh) script to install `gdrcopy` (e.g., `install_gdrcopy.sh \"${GDRCOPY_OS_VERSION}\" \"12.8\" \"x64\"`). You can find available OS versions [here](https://developer.download.nvidia.com/compute/redist/gdrcopy/CUDA%2012.8/). If `gdrcopy` is not installed, things will still work with a plain `pip install nixl`, just with lower performance. `nixl` and `ucx` are installed as dependencies via pip. For non-cuda platform to install nixl with non-cuda UCX build, run the [install_nixl_from_source_ubuntu.py](gh-file:tools/install_nixl_from_source_ubuntu.py) script.", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Disaggregated Serving (Prefill/Decode Split)/Setup Steps", "level": 3, "text": "2. **Configure Both Instances**: Add this flag to both prefill and decode instances `--kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\"}`. Noted, you may also specify one or multiple NIXL_Backend. Such as: `--kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\", \"kv_connector_extra_config\":{\"backends\":[\"UCX\", \"GDS\"]}}'`  \n3. **Client Orchestration**: Use the client-side script below to coordinate prefill/decode operations. We are actively working on routing solutions.", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Disaggregated Serving (Prefill/Decode Split)/Client Orchestration Example", "level": 3, "text": "### Client Orchestration Example  \n```python\nfrom openai import OpenAI\nimport uuid\n\ntry:\n# 1: Set up clients for prefill and decode instances\nopenai_api_key = \"EMPTY\"  # vLLM doesn't require a real API key\n\n# Replace these IP addresses with your actual instance addresses\nprefill_client = OpenAI(\napi_key=openai_api_key,\nbase_url=\"http://192.168.1.100:8000/v1\",  # Prefill instance URL\n)\ndecode_client = OpenAI(\napi_key=openai_api_key,\nbase_url=\"http://192.168.1.101:8001/v1\",  # Decode instance URL\n)\n\n# Get model name from prefill instance\nmodels = prefill_client.models.list()\nmodel = models.data[0].id\nprint(f\"Using model: {model}\")\n\n# 2: Prefill Phase\n# Generate unique request ID to link prefill and decode operations\nrequest_id = str(uuid.uuid4())\nprint(f\"Request ID: {request_id}\")", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Disaggregated Serving (Prefill/Decode Split)/Client Orchestration Example", "level": 3, "text": "prefill_response = prefill_client.completions.create(\nmodel=model,\n# Prompt must exceed vLLM's block size (16 tokens) for PD to work\nprompt=\"Write a detailed explanation of Paged Attention for Transformers works including the management of KV cache for multi-turn conversations\",\nmax_tokens=1,  # Force prefill-only operation\nextra_body={\n\"kv_transfer_params\": {\n\"do_remote_decode\": True,     # Enable remote decode\n\"do_remote_prefill\": False,   # This is the prefill instance\n\"remote_engine_id\": None,     # Will be populated by vLLM\n\"remote_block_ids\": None,     # Will be populated by vLLM\n\"remote_host\": None,          # Will be populated by vLLM\n\"remote_port\": None,          # Will be populated by vLLM\n}\n},\nextra_headers={\"X-Request-Id\": request_id},\n)\n\nprint(\"-\" * 50)\nprint(\"âœ“ Prefill completed successfully\")\nprint(f\"Prefill response: {prefill_response.choices[0].text}\")", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "e9c35027b6ac3d648ccfa7d3d1319f1d4296eb28cf4006dacaae370ed86131dd", "heading": "Expert Parallel Deployment/Disaggregated Serving (Prefill/Decode Split)/Client Orchestration Example", "level": 3, "text": "# 3: Decode Phase\n# Transfer KV cache parameters from prefill to decode instance\ndecode_response = decode_client.completions.create(\nmodel=model,\nprompt=\"This prompt is ignored during decode\",  # Original prompt not needed\nmax_tokens=150,  # Generate up to 150 tokens\nextra_body={\n\"kv_transfer_params\": prefill_response.kv_transfer_params  # Pass KV cache info\n},\nextra_headers={\"X-Request-Id\": request_id},  # Same request ID\n)\n\nprint(\"-\" * 50)\nprint(\"âœ“ Decode completed successfully\")\nprint(f\"Final response: {decode_response.choices[0].text}\")\n\nexcept Exception as e:\nprint(f\"âŒ Error during disaggregated serving: {e}\")\nprint(\"Check that both prefill and decode instances are running and accessible\")\n```", "file_path": "serving/expert_parallel_deployment.md"}
{"id": "f5c667fb0d613d526696aad3eee9d513ecb7340c816c5fb90c56244bb5170ff8", "heading": "LangChain", "level": 1, "text": "# LangChain  \nvLLM is also available via [LangChain](https://github.com/langchain-ai/langchain) .  \nTo install LangChain, run  \n```bash\npip install langchain langchain_community -q\n```  \nTo run inference on a single or multiple GPUs, use `VLLM` class from `langchain`.  \n??? code  \n```python\nfrom langchain_community.llms import VLLM\n\nllm = VLLM(\nmodel=\"mosaicml/mpt-7b\",\ntrust_remote_code=True,  # mandatory for hf models\nmax_new_tokens=128,\ntop_k=10,\ntop_p=0.95,\ntemperature=0.8,\n# for distributed inference\n# tensor_parallel_size=...,\n)\n\nprint(llm(\"What is the capital of France ?\"))\n```  \nPlease refer to this [Tutorial](https://python.langchain.com/docs/integrations/llms/vllm) for more details.", "file_path": "serving/integrations/langchain.md"}
{"id": "6c5c286bd8d7ec975cb6a57e3b12c22d2cf88c88b32449ab314e42a7828202af", "heading": "LlamaIndex", "level": 1, "text": "# LlamaIndex  \nvLLM is also available via [LlamaIndex](https://github.com/run-llama/llama_index) .  \nTo install LlamaIndex, run  \n```bash\npip install llama-index-llms-vllm -q\n```  \nTo run inference on a single or multiple GPUs, use `Vllm` class from `llamaindex`.  \n```python\nfrom llama_index.llms.vllm import Vllm\n\nllm = Vllm(\nmodel=\"microsoft/Orca-2-7b\",\ntensor_parallel_size=4,\nmax_new_tokens=100,\nvllm_kwargs={\"swap_space\": 1, \"gpu_memory_utilization\": 0.5},\n)\n```  \nPlease refer to this [Tutorial](https://docs.llamaindex.ai/en/latest/examples/llm/vllm/) for more details.", "file_path": "serving/integrations/llamaindex.md"}
{"id": "f13efc7928b7b03984f38d620c9a20777e8682f803491e6744dc7a2033183b6c", "heading": "Offline Inference", "level": 1, "text": "# Offline Inference  \nOffline inference is possible in your own code using vLLM's [`LLM`][vllm.LLM] class.  \nFor example, the following code downloads the [`facebook/opt-125m`](https://huggingface.co/facebook/opt-125m) model from HuggingFace\nand runs it in vLLM using the default configuration.  \n```python\nfrom vllm import LLM\n\n# Initialize the vLLM engine.\nllm = LLM(model=\"facebook/opt-125m\")\n```  \nAfter initializing the `LLM` instance, use the available APIs to perform model inference.\nThe available APIs depend on the model type:  \n- [Generative models](../models/generative_models.md) output logprobs which are sampled from to obtain the final output text.\n- [Pooling models](../models/pooling_models.md) output their hidden states directly.  \n!!! info\n[API Reference][offline-inference-api]", "file_path": "serving/offline_inference.md"}
{"id": "f13efc7928b7b03984f38d620c9a20777e8682f803491e6744dc7a2033183b6c", "heading": "Offline Inference/Ray Data LLM API", "level": 2, "text": "## Ray Data LLM API  \nRay Data LLM is an alternative offline inference API that uses vLLM as the underlying engine.\nThis API adds several batteries-included capabilities that simplify large-scale, GPU-efficient inference:  \n- Streaming execution processes datasets that exceed aggregate cluster memory.\n- Automatic sharding, load balancing, and autoscaling distribute work across a Ray cluster with built-in fault tolerance.\n- Continuous batching keeps vLLM replicas saturated and maximizes GPU utilization.\n- Transparent support for tensor and pipeline parallelism enables efficient multi-GPU inference.\n- Reading and writing to most popular file formats and cloud object storage.\n- Scaling up the workload without code changes.  \n??? code  \n```python\nimport ray  # Requires ray>=2.44.1\nfrom ray.data.llm import vLLMEngineProcessorConfig, build_llm_processor", "file_path": "serving/offline_inference.md"}
{"id": "f13efc7928b7b03984f38d620c9a20777e8682f803491e6744dc7a2033183b6c", "heading": "Offline Inference/Ray Data LLM API", "level": 2, "text": "config = vLLMEngineProcessorConfig(model_source=\"unsloth/Llama-3.2-1B-Instruct\")\nprocessor = build_llm_processor(\nconfig,\npreprocess=lambda row: {\n\"messages\": [\n{\"role\": \"system\", \"content\": \"You are a bot that completes unfinished haikus.\"},\n{\"role\": \"user\", \"content\": row[\"item\"]},\n],\n\"sampling_params\": {\"temperature\": 0.3, \"max_tokens\": 250},\n},\npostprocess=lambda row: {\"answer\": row[\"generated_text\"]},\n)\n\nds = ray.data.from_items([\"An old silent pond...\"])\nds = processor(ds)\nds.write_parquet(\"local:///tmp/data/\")\n```  \nFor more information about the Ray Data LLM API, see the [Ray Data LLM documentation](https://docs.ray.io/en/latest/data/working-with-llms.html).", "file_path": "serving/offline_inference.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server", "level": 1, "text": "# OpenAI-Compatible Server  \nvLLM provides an HTTP server that implements OpenAI's [Completions API](https://platform.openai.com/docs/api-reference/completions), [Chat API](https://platform.openai.com/docs/api-reference/chat), and more! This functionality lets you serve models and interact with them using an HTTP client.  \nIn your terminal, you can [install](../getting_started/installation/README.md) vLLM, then start the server with the [`vllm serve`](../configuration/serve_args.md) command. (You can also use our [Docker](../deployment/docker.md) image.)  \n```bash\nvllm serve NousResearch/Meta-Llama-3-8B-Instruct \\\n--dtype auto \\\n--api-key token-abc123\n```  \nTo call the server, in your preferred text editor, create a script that uses an HTTP client. Include any messages that you want to send to the model. Then run that script. Below is an example script using the [official OpenAI Python client](https://github.com/openai/openai-python).  \n??? code  \n```python\nfrom openai import OpenAI\nclient = OpenAI(", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server", "level": 1, "text": "??? code  \n```python\nfrom openai import OpenAI\nclient = OpenAI(\nbase_url=\"http://localhost:8000/v1\",\napi_key=\"token-abc123\",\n)", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server", "level": 1, "text": "completion = client.chat.completions.create(\nmodel=\"NousResearch/Meta-Llama-3-8B-Instruct\",\nmessages=[\n{\"role\": \"user\", \"content\": \"Hello!\"},\n],\n)\n\nprint(completion.choices[0].message)\n```  \n!!! tip\nvLLM supports some parameters that are not supported by OpenAI, `top_k` for example.\nYou can pass these parameters to vLLM using the OpenAI client in the `extra_body` parameter of your requests, i.e. `extra_body={\"top_k\": 50}` for `top_k`.  \n!!! important\nBy default, the server applies `generation_config.json` from the Hugging Face model repository if it exists. This means the default values of certain sampling parameters can be overridden by those recommended by the model creator.  \nTo disable this behavior, please pass `--generation-config vllm` when launching the server.", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/Supported APIs", "level": 2, "text": "## Supported APIs  \nWe currently support the following OpenAI APIs:  \n- [Completions API][completions-api] (`/v1/completions`)\n- Only applicable to [text generation models](../models/generative_models.md).\n- *Note: `suffix` parameter is not supported.*\n- [Chat Completions API][chat-api] (`/v1/chat/completions`)\n- Only applicable to [text generation models](../models/generative_models.md) with a [chat template][chat-template].\n- *Note: `parallel_tool_calls` and `user` parameters are ignored.*\n- [Embeddings API][embeddings-api] (`/v1/embeddings`)\n- Only applicable to [embedding models](../models/pooling_models.md).\n- [Transcriptions API][transcriptions-api] (`/v1/audio/transcriptions`)\n- Only applicable to [Automatic Speech Recognition (ASR) models](../models/supported_models.md#transcription).\n- [Translation API][translations-api] (`/v1/audio/translations`)\n- Only applicable to [Automatic Speech Recognition (ASR) models](../models/supported_models.md#transcription).", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/Supported APIs", "level": 2, "text": "In addition, we have the following custom APIs:  \n- [Tokenizer API][tokenizer-api] (`/tokenize`, `/detokenize`)\n- Applicable to any model with a tokenizer.\n- [Pooling API][pooling-api] (`/pooling`)\n- Applicable to all [pooling models](../models/pooling_models.md).\n- [Classification API][classification-api] (`/classify`)\n- Only applicable to [classification models](../models/pooling_models.md).\n- [Score API][score-api] (`/score`)\n- Applicable to [embedding models and cross-encoder models](../models/pooling_models.md).\n- [Re-rank API][rerank-api] (`/rerank`, `/v1/rerank`, `/v2/rerank`)\n- Implements [Jina AI's v1 re-rank API](https://jina.ai/reranker/)\n- Also compatible with [Cohere's v1 & v2 re-rank APIs](https://docs.cohere.com/v2/reference/rerank)\n- Jina and Cohere's APIs are very similar; Jina's includes extra information in the rerank endpoint's response.\n- Only applicable to [cross-encoder models](../models/pooling_models.md).  \n[](){ #chat-template }", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/Chat Template", "level": 2, "text": "## Chat Template  \nIn order for the language model to support chat protocol, vLLM requires the model to include\na chat template in its tokenizer configuration. The chat template is a Jinja2 template that\nspecifies how are roles, messages, and other chat-specific tokens are encoded in the input.  \nAn example chat template for `NousResearch/Meta-Llama-3-8B-Instruct` can be found [here](https://github.com/meta-llama/llama3?tab=readme-ov-file#instruction-tuned-models)  \nSome models do not provide a chat template even though they are instruction/chat fine-tuned. For those model,\nyou can manually specify their chat template in the `--chat-template` parameter with the file path to the chat\ntemplate, or the template in string form. Without a chat template, the server will not be able to process chat\nand all chat requests will error.  \n```bash\nvllm serve <model> --chat-template ./path-to-chat-template.jinja\n```", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/Chat Template", "level": 2, "text": "```  \nvLLM community provides a set of chat templates for popular models. You can find them under the <gh-dir:examples> directory.  \nWith the inclusion of multi-modal chat APIs, the OpenAI spec now accepts chat messages in a new format which specifies\nboth a `type` and a `text` field. An example is provided below:  \n```python\ncompletion = client.chat.completions.create(\nmodel=\"NousResearch/Meta-Llama-3-8B-Instruct\",\nmessages=[\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"Classify this sentiment: vLLM is wonderful!\"},\n],\n},\n],\n)\n```  \nMost chat templates for LLMs expect the `content` field to be a string, but there are some newer models like\n`meta-llama/Llama-Guard-3-1B` that expect the content to be formatted according to the OpenAI schema in the\nrequest. vLLM provides best-effort support to detect this automatically, which is logged as a string like\n*\"Detected the chat template content format to be...\"*, and internally converts incoming requests to match", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/Chat Template", "level": 2, "text": "the detected format, which can be one of:  \n- `\"string\"`: A string.\n- Example: `\"Hello world\"`\n- `\"openai\"`: A list of dictionaries, similar to OpenAI schema.\n- Example: `[{\"type\": \"text\", \"text\": \"Hello world!\"}]`  \nIf the result is not what you expect, you can set the `--chat-template-content-format` CLI argument\nto override which format to use.", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/Extra Parameters", "level": 2, "text": "## Extra Parameters  \nvLLM supports a set of parameters that are not part of the OpenAI API.\nIn order to use them, you can pass them as extra parameters in the OpenAI client.\nOr directly merge them into the JSON payload if you are using HTTP call directly.  \n```python\ncompletion = client.chat.completions.create(\nmodel=\"NousResearch/Meta-Llama-3-8B-Instruct\",\nmessages=[\n{\"role\": \"user\", \"content\": \"Classify this sentiment: vLLM is wonderful!\"},\n],\nextra_body={\n\"structured_outputs\": {\"choice\": [\"positive\", \"negative\"]},\n},\n)\n```", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/Extra HTTP Headers", "level": 2, "text": "## Extra HTTP Headers  \nOnly `X-Request-Id` HTTP request header is supported for now. It can be enabled\nwith `--enable-request-id-headers`.  \n??? code  \n```python\ncompletion = client.chat.completions.create(\nmodel=\"NousResearch/Meta-Llama-3-8B-Instruct\",\nmessages=[\n{\"role\": \"user\", \"content\": \"Classify this sentiment: vLLM is wonderful!\"},\n],\nextra_headers={\n\"x-request-id\": \"sentiment-classification-00001\",\n},\n)\nprint(completion._request_id)\n\ncompletion = client.completions.create(\nmodel=\"NousResearch/Meta-Llama-3-8B-Instruct\",\nprompt=\"A robot may not injure a human being\",\nextra_headers={\n\"x-request-id\": \"completion-test\",\n},\n)\nprint(completion._request_id)\n```", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference", "level": 2, "text": "## API Reference  \n[](){ #completions-api }", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Completions API", "level": 3, "text": "### Completions API  \nOur Completions API is compatible with [OpenAI's Completions API](https://platform.openai.com/docs/api-reference/completions);\nyou can use the [official OpenAI Python client](https://github.com/openai/openai-python) to interact with it.  \nCode example: <gh-file:examples/online_serving/openai_completion_client.py>  \n#### Extra parameters  \nThe following [sampling parameters][sampling-params] are supported.  \n??? code  \n```python\n--8<-- \"vllm/entrypoints/openai/protocol.py:completion-sampling-params\"\n```  \nThe following extra parameters are supported:  \n??? code  \n```python\n--8<-- \"vllm/entrypoints/openai/protocol.py:completion-extra-params\"\n```  \n[](){ #chat-api }", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Chat API", "level": 3, "text": "### Chat API  \nOur Chat API is compatible with [OpenAI's Chat Completions API](https://platform.openai.com/docs/api-reference/chat);\nyou can use the [official OpenAI Python client](https://github.com/openai/openai-python) to interact with it.  \nWe support both [Vision](https://platform.openai.com/docs/guides/vision)- and\n[Audio](https://platform.openai.com/docs/guides/audio?audio-generation-quickstart-example=audio-in)-related parameters;\nsee our [Multimodal Inputs](../features/multimodal_inputs.md) guide for more information.  \n- *Note: `image_url.detail` parameter is not supported.*  \nCode example: <gh-file:examples/online_serving/openai_chat_completion_client.py>  \n#### Extra parameters  \nThe following [sampling parameters][sampling-params] are supported.  \n??? code  \n```python\n--8<-- \"vllm/entrypoints/openai/protocol.py:chat-completion-sampling-params\"\n```  \nThe following extra parameters are supported:  \n??? code  \n```python\n--8<-- \"vllm/entrypoints/openai/protocol.py:chat-completion-extra-params\"\n```", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Chat API", "level": 3, "text": "```  \n[](){ #embeddings-api }", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Embeddings API", "level": 3, "text": "### Embeddings API  \nOur Embeddings API is compatible with [OpenAI's Embeddings API](https://platform.openai.com/docs/api-reference/embeddings);\nyou can use the [official OpenAI Python client](https://github.com/openai/openai-python) to interact with it.  \nCode example: <gh-file:examples/online_serving/pooling/openai_embedding_client.py>  \nIf the model has a [chat template][chat-template], you can replace `inputs` with a list of `messages` (same schema as [Chat API][chat-api])\nwhich will be treated as a single prompt to the model. Here is a convenience function for calling the API while retaining OpenAI's type annotations:  \n??? code  \n```python\nfrom openai import OpenAI\nfrom openai._types import NOT_GIVEN, NotGiven\nfrom openai.types.chat import ChatCompletionMessageParam\nfrom openai.types.create_embedding_response import CreateEmbeddingResponse", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Embeddings API", "level": 3, "text": "def create_chat_embeddings(\nclient: OpenAI,\n*,\nmessages: list[ChatCompletionMessageParam],\nmodel: str,\nencoding_format: Union[Literal[\"base64\", \"float\"], NotGiven] = NOT_GIVEN,\n) -> CreateEmbeddingResponse:\nreturn client.post(\n\"/embeddings\",\ncast_to=CreateEmbeddingResponse,\nbody={\"messages\": messages, \"model\": model, \"encoding_format\": encoding_format},\n)\n```  \n#### Multi-modal inputs  \nYou can pass multi-modal inputs to embedding models by defining a custom chat template for the server\nand passing a list of `messages` in the request. Refer to the examples below for illustration.  \n=== \"VLM2Vec\"  \nTo serve the model:  \n```bash\nvllm serve TIGER-Lab/VLM2Vec-Full --runner pooling \\\n--trust-remote-code \\\n--max-model-len 4096 \\\n--chat-template examples/template_vlm2vec_phi3v.jinja\n```  \n!!! important\nSince VLM2Vec has the same model architecture as Phi-3.5-Vision, we have to explicitly pass `--runner pooling`\nto run this model in embedding mode instead of text generation mode.", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Embeddings API", "level": 3, "text": "The custom chat template is completely different from the original one for this model,\nand can be found here: <gh-file:examples/template_vlm2vec_phi3v.jinja>  \nSince the request schema is not defined by OpenAI client, we post a request to the server using the lower-level `requests` library:  \n??? code  \n```python\nfrom openai import OpenAI\nclient = OpenAI(\nbase_url=\"http://localhost:8000/v1\",\napi_key=\"EMPTY\",\n)\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Embeddings API", "level": 3, "text": "response = create_chat_embeddings(\nclient,\nmodel=\"TIGER-Lab/VLM2Vec-Full\",\nmessages=[\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n{\"type\": \"text\", \"text\": \"Represent the given image.\"},\n],\n}\n],\nencoding_format=\"float\",\n)", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Embeddings API", "level": 3, "text": "print(\"Image embedding output:\", response.data[0].embedding)\n```  \n=== \"DSE-Qwen2-MRL\"  \nTo serve the model:  \n```bash\nvllm serve MrLight/dse-qwen2-2b-mrl-v1 --runner pooling \\\n--trust-remote-code \\\n--max-model-len 8192 \\\n--chat-template examples/template_dse_qwen2_vl.jinja\n```  \n!!! important\nLike with VLM2Vec, we have to explicitly pass `--runner pooling`.  \nAdditionally, `MrLight/dse-qwen2-2b-mrl-v1` requires an EOS token for embeddings, which is handled\nby a custom chat template: <gh-file:examples/template_dse_qwen2_vl.jinja>  \n!!! important\n`MrLight/dse-qwen2-2b-mrl-v1` requires a placeholder image of the minimum image size for text query embeddings. See the full code\nexample below for details.  \nFull example: <gh-file:examples/online_serving/pooling/openai_chat_embedding_client_for_multimodal.py>  \n#### Extra parameters  \nThe following [pooling parameters][vllm.PoolingParams] are supported.  \n```python\n--8<-- \"vllm/pooling_params.py:common-pooling-params\"", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Embeddings API", "level": 3, "text": "```python\n--8<-- \"vllm/pooling_params.py:common-pooling-params\"\n--8<-- \"vllm/pooling_params.py:embedding-pooling-params\"\n```  \nThe following extra parameters are supported by default:  \n??? code  \n```python\n--8<-- \"vllm/entrypoints/openai/protocol.py:embedding-extra-params\"\n```  \nFor chat-like input (i.e. if `messages` is passed), these extra parameters are supported instead:  \n??? code  \n```python\n--8<-- \"vllm/entrypoints/openai/protocol.py:chat-embedding-extra-params\"\n```  \n[](){ #transcriptions-api }", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Transcriptions API", "level": 3, "text": "### Transcriptions API  \nOur Transcriptions API is compatible with [OpenAI's Transcriptions API](https://platform.openai.com/docs/api-reference/audio/createTranscription);\nyou can use the [official OpenAI Python client](https://github.com/openai/openai-python) to interact with it.  \n!!! note\nTo use the Transcriptions API, please install with extra audio dependencies using `pip install vllm[audio]`.  \nCode example: <gh-file:examples/online_serving/openai_transcription_client.py>  \n#### API Enforced Limits  \nSet the maximum audio file size (in MB) that VLLM will accept, via the\n`VLLM_MAX_AUDIO_CLIP_FILESIZE_MB` environment variable. Default is 25 MB.  \n#### Uploading Audio Files  \nThe Transcriptions API supports uploading audio files in various formats including FLAC, MP3, MP4, MPEG, MPGA, M4A, OGG, WAV, and WEBM.  \n**Using OpenAI Python Client:**  \n??? code  \n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\nbase_url=\"http://localhost:8000/v1\",\napi_key=\"token-abc123\",\n)", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Transcriptions API", "level": 3, "text": "# Upload audio file from disk\nwith open(\"audio.mp3\", \"rb\") as audio_file:\ntranscription = client.audio.transcriptions.create(\nmodel=\"openai/whisper-large-v3-turbo\",\nfile=audio_file,\nlanguage=\"en\",\nresponse_format=\"verbose_json\",\n)", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Transcriptions API", "level": 3, "text": "print(transcription.text)\n```  \n**Using curl with multipart/form-data:**  \n??? code  \n```bash\ncurl -X POST \"http://localhost:8000/v1/audio/transcriptions\" \\\n-H \"Authorization: Bearer token-abc123\" \\\n-F \"file=@audio.mp3\" \\\n-F \"model=openai/whisper-large-v3-turbo\" \\\n-F \"language=en\" \\\n-F \"response_format=verbose_json\"\n```  \n**Supported Parameters:**  \n- `file`: The audio file to transcribe (required)\n- `model`: The model to use for transcription (required)\n- `language`: The language code (e.g., \"en\", \"zh\") (optional)\n- `prompt`: Optional text to guide the transcription style (optional)\n- `response_format`: Format of the response (\"json\", \"text\") (optional)\n- `temperature`: Sampling temperature between 0 and 1 (optional)  \nFor the complete list of supported parameters including sampling parameters and vLLM extensions, see the [protocol definitions](https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/openai/protocol.py#L2182).  \n**Response Format:**  \nFor `verbose_json` response format:  \n??? code", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Transcriptions API", "level": 3, "text": "For `verbose_json` response format:  \n??? code  \n```json\n{\n\"text\": \"Hello, this is a transcription of the audio file.\",\n\"language\": \"en\",\n\"duration\": 5.42,\n\"segments\": [\n{\n\"id\": 0,\n\"seek\": 0,\n\"start\": 0.0,\n\"end\": 2.5,\n\"text\": \"Hello, this is a transcription\",\n\"tokens\": [50364, 938, 428, 307, 275, 28347],\n\"temperature\": 0.0,\n\"avg_logprob\": -0.245,\n\"compression_ratio\": 1.235,\n\"no_speech_prob\": 0.012\n}\n]\n}\n```  \n#### Extra Parameters  \nThe following [sampling parameters][sampling-params] are supported.  \n??? code  \n```python\n--8<-- \"vllm/entrypoints/openai/protocol.py:transcription-sampling-params\"\n```  \nThe following extra parameters are supported:  \n??? code  \n```python\n--8<-- \"vllm/entrypoints/openai/protocol.py:transcription-extra-params\"\n```  \n[](){ #translations-api }", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Translations API", "level": 3, "text": "### Translations API  \nOur Translation API is compatible with [OpenAI's Translations API](https://platform.openai.com/docs/api-reference/audio/createTranslation);\nyou can use the [official OpenAI Python client](https://github.com/openai/openai-python) to interact with it.\nWhisper models can translate audio from one of the 55 non-English supported languages into English.\nPlease mind that the popular `openai/whisper-large-v3-turbo` model does not support translating.  \n!!! note\nTo use the Translation API, please install with extra audio dependencies using `pip install vllm[audio]`.  \nCode example: <gh-file:examples/online_serving/openai_translation_client.py>  \n#### Extra Parameters  \nThe following [sampling parameters][sampling-params] are supported.  \n```python\n--8<-- \"vllm/entrypoints/openai/protocol.py:translation-sampling-params\"\n```  \nThe following extra parameters are supported:  \n```python\n--8<-- \"vllm/entrypoints/openai/protocol.py:translation-extra-params\"\n```  \n[](){ #tokenizer-api }", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Tokenizer API", "level": 3, "text": "### Tokenizer API  \nOur Tokenizer API is a simple wrapper over [HuggingFace-style tokenizers](https://huggingface.co/docs/transformers/en/main_classes/tokenizer).\nIt consists of two endpoints:  \n- `/tokenize` corresponds to calling `tokenizer.encode()`.\n- `/detokenize` corresponds to calling `tokenizer.decode()`.  \n[](){ #pooling-api }", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Pooling API", "level": 3, "text": "### Pooling API  \nOur Pooling API encodes input prompts using a [pooling model](../models/pooling_models.md) and returns the corresponding hidden states.  \nThe input format is the same as [Embeddings API][embeddings-api], but the output data can contain an arbitrary nested list, not just a 1-D list of floats.  \nCode example: <gh-file:examples/online_serving/pooling/openai_pooling_client.py>  \n[](){ #classification-api }", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Classification API", "level": 3, "text": "### Classification API  \nOur Classification API directly supports Hugging Face sequence-classification models such as [ai21labs/Jamba-tiny-reward-dev](https://huggingface.co/ai21labs/Jamba-tiny-reward-dev) and [jason9693/Qwen2.5-1.5B-apeach](https://huggingface.co/jason9693/Qwen2.5-1.5B-apeach).  \nWe automatically wrap any other transformer via `as_seq_cls_model()`, which pools on the last token, attaches a `RowParallelLinear` head, and applies a softmax to produce per-class probabilities.  \nCode example: <gh-file:examples/online_serving/pooling/openai_classification_client.py>  \n#### Example Requests  \nYou can classify multiple texts by passing an array of strings:  \n```bash\ncurl -v \"http://127.0.0.1:8000/classify\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"jason9693/Qwen2.5-1.5B-apeach\",\n\"input\": [\n\"Loved the new cafÃ©â€”coffee was great.\",\n\"This update broke everything. Frustrating.\"\n]\n}'\n```  \n??? console \"Response\"  \n```json\n{\n\"id\": \"classify-7c87cac407b749a6935d8c7ce2a8fba2\",\n\"object\": \"list\",", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Classification API", "level": 3, "text": "\"object\": \"list\",\n\"created\": 1745383065,\n\"model\": \"jason9693/Qwen2.5-1.5B-apeach\",\n\"data\": [\n{\n\"index\": 0,\n\"label\": \"Default\",\n\"probs\": [\n0.565970778465271,\n0.4340292513370514\n],\n\"num_classes\": 2\n},\n{\n\"index\": 1,\n\"label\": \"Spoiled\",\n\"probs\": [\n0.26448777318000793,\n0.7355121970176697\n],\n\"num_classes\": 2\n}\n],\n\"usage\": {\n\"prompt_tokens\": 20,\n\"total_tokens\": 20,\n\"completion_tokens\": 0,\n\"prompt_tokens_details\": null\n}\n}\n```  \nYou can also pass a string directly to the `input` field:  \n```bash\ncurl -v \"http://127.0.0.1:8000/classify\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"jason9693/Qwen2.5-1.5B-apeach\",\n\"input\": \"Loved the new cafÃ©â€”coffee was great.\"\n}'\n```  \n??? console \"Response\"  \n```json\n{\n\"id\": \"classify-9bf17f2847b046c7b2d5495f4b4f9682\",\n\"object\": \"list\",\n\"created\": 1745383213,\n\"model\": \"jason9693/Qwen2.5-1.5B-apeach\",\n\"data\": [\n{\n\"index\": 0,\n\"label\": \"Default\",\n\"probs\": [\n0.565970778465271,\n0.4340292513370514\n],\n\"num_classes\": 2\n}\n],\n\"usage\": {\n\"prompt_tokens\": 10,\n\"total_tokens\": 10,", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Classification API", "level": 3, "text": "}\n],\n\"usage\": {\n\"prompt_tokens\": 10,\n\"total_tokens\": 10,\n\"completion_tokens\": 0,\n\"prompt_tokens_details\": null\n}\n}\n```  \n#### Extra parameters  \nThe following [pooling parameters][vllm.PoolingParams] are supported.  \n```python\n--8<-- \"vllm/pooling_params.py:common-pooling-params\"\n--8<-- \"vllm/pooling_params.py:classification-pooling-params\"\n```  \nThe following extra parameters are supported:  \n```python\n--8<-- \"vllm/entrypoints/openai/protocol.py:classification-extra-params\"\n```  \n[](){ #score-api }", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Score API", "level": 3, "text": "### Score API  \nOur Score API can apply a cross-encoder model or an embedding model to predict scores for sentence or multimodal pairs. When using an embedding model the score corresponds to the cosine similarity between each embedding pair.\nUsually, the score for a sentence pair refers to the similarity between two sentences, on a scale of 0 to 1.  \nYou can find the documentation for cross encoder models at [sbert.net](https://www.sbert.net/docs/package_reference/cross_encoder/cross_encoder.html).  \nCode example: <gh-file:examples/online_serving/openai_cross_encoder_score.py>  \n#### Single inference  \nYou can pass a string to both `text_1` and `text_2`, forming a single sentence pair.  \n```bash\ncurl -X 'POST' \\\n'http://127.0.0.1:8000/score' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '{\n\"model\": \"BAAI/bge-reranker-v2-m3\",\n\"encoding_format\": \"float\",\n\"text_1\": \"What is the capital of France?\",\n\"text_2\": \"The capital of France is Paris.\"\n}'\n```  \n??? console \"Response\"  \n```json", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Score API", "level": 3, "text": "}'\n```  \n??? console \"Response\"  \n```json\n{\n\"id\": \"score-request-id\",\n\"object\": \"list\",\n\"created\": 693447,\n\"model\": \"BAAI/bge-reranker-v2-m3\",\n\"data\": [\n{\n\"index\": 0,\n\"object\": \"score\",\n\"score\": 1\n}\n],\n\"usage\": {}\n}\n```  \n#### Batch inference  \nYou can pass a string to `text_1` and a list to `text_2`, forming multiple sentence pairs\nwhere each pair is built from `text_1` and a string in `text_2`.\nThe total number of pairs is `len(text_2)`.  \n??? console \"Request\"  \n```bash\ncurl -X 'POST' \\\n'http://127.0.0.1:8000/score' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '{\n\"model\": \"BAAI/bge-reranker-v2-m3\",\n\"text_1\": \"What is the capital of France?\",\n\"text_2\": [\n\"The capital of Brazil is Brasilia.\",\n\"The capital of France is Paris.\"\n]\n}'\n```  \n??? console \"Response\"  \n```json\n{\n\"id\": \"score-request-id\",\n\"object\": \"list\",\n\"created\": 693570,\n\"model\": \"BAAI/bge-reranker-v2-m3\",\n\"data\": [\n{\n\"index\": 0,\n\"object\": \"score\",\n\"score\": 0.001094818115234375\n},\n{\n\"index\": 1,\n\"object\": \"score\",", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Score API", "level": 3, "text": "},\n{\n\"index\": 1,\n\"object\": \"score\",\n\"score\": 1\n}\n],\n\"usage\": {}\n}\n```  \nYou can pass a list to both `text_1` and `text_2`, forming multiple sentence pairs\nwhere each pair is built from a string in `text_1` and the corresponding string in `text_2` (similar to `zip()`).\nThe total number of pairs is `len(text_2)`.  \n??? console \"Request\"  \n```bash\ncurl -X 'POST' \\\n'http://127.0.0.1:8000/score' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '{\n\"model\": \"BAAI/bge-reranker-v2-m3\",\n\"encoding_format\": \"float\",\n\"text_1\": [\n\"What is the capital of Brazil?\",\n\"What is the capital of France?\"\n],\n\"text_2\": [\n\"The capital of Brazil is Brasilia.\",\n\"The capital of France is Paris.\"\n]\n}'\n```  \n??? console \"Response\"  \n```json\n{\n\"id\": \"score-request-id\",\n\"object\": \"list\",\n\"created\": 693447,\n\"model\": \"BAAI/bge-reranker-v2-m3\",\n\"data\": [\n{\n\"index\": 0,\n\"object\": \"score\",\n\"score\": 1\n},\n{\n\"index\": 1,\n\"object\": \"score\",\n\"score\": 1\n}\n],\n\"usage\": {}\n}\n```  \n#### Multi-modal inputs", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Score API", "level": 3, "text": "\"score\": 1\n}\n],\n\"usage\": {}\n}\n```  \n#### Multi-modal inputs  \nYou can pass multi-modal inputs to scoring models by passing `content` including a list of multi-modal input (image, etc.) in the request. Refer to the examples below for illustration.  \n=== \"JinaVL-Reranker\"  \nTo serve the model:  \n```bash\nvllm serve jinaai/jina-reranker-m0\n```  \nSince the request schema is not defined by OpenAI client, we post a request to the server using the lower-level `requests` library:  \n??? Code  \n```python\nimport requests", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Score API", "level": 3, "text": "response = requests.post(\n\"http://localhost:8000/v1/score\",\njson={\n\"model\": \"jinaai/jina-reranker-m0\",\n\"text_1\": \"slm markdown\",\n\"text_2\": {\n\"content\": [\n{\n\"type\": \"image_url\",\n\"image_url\": {\n\"url\": \"https://raw.githubusercontent.com/jina-ai/multimodal-reranker-test/main/handelsblatt-preview.png\"\n},\n},\n{\n\"type\": \"image_url\",\n\"image_url\": {\n\"url\": \"https://raw.githubusercontent.com/jina-ai/multimodal-reranker-test/main/paper-11.png\"\n},\n},\n],\n},\n},\n)\nresponse.raise_for_status()\nresponse_json = response.json()\nprint(\"Scoring output:\", response_json[\"data\"][0][\"score\"])\nprint(\"Scoring output:\", response_json[\"data\"][1][\"score\"])", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Score API", "level": 3, "text": "```\nFull example: <gh-file:examples/online_serving/openai_cross_encoder_score_for_multimodal.py>  \n#### Extra parameters  \nThe following [pooling parameters][vllm.PoolingParams] are supported.  \n```python\n--8<-- \"vllm/pooling_params.py:common-pooling-params\"\n--8<-- \"vllm/pooling_params.py:classification-pooling-params\"\n```  \nThe following extra parameters are supported:  \n```python\n--8<-- \"vllm/entrypoints/openai/protocol.py:score-extra-params\"\n```  \n[](){ #rerank-api }", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Re-rank API", "level": 3, "text": "### Re-rank API  \nOur Re-rank API can apply an embedding model or a cross-encoder model to predict relevant scores between a single query, and\neach of a list of documents. Usually, the score for a sentence pair refers to the similarity between two sentences or multi-modal inputs (image, etc.), on a scale of 0 to 1.  \nYou can find the documentation for cross encoder models at [sbert.net](https://www.sbert.net/docs/package_reference/cross_encoder/cross_encoder.html).  \nThe rerank endpoints support popular re-rank models such as `BAAI/bge-reranker-base` and other models supporting the\n`score` task. Additionally, `/rerank`, `/v1/rerank`, and `/v2/rerank`\nendpoints are compatible with both [Jina AI's re-rank API interface](https://jina.ai/reranker/) and\n[Cohere's re-rank API interface](https://docs.cohere.com/v2/reference/rerank) to ensure compatibility with\npopular open-source tools.  \nCode example: <gh-file:examples/online_serving/pooling/jinaai_rerank_client.py>  \n#### Example Request", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Re-rank API", "level": 3, "text": "#### Example Request  \nNote that the `top_n` request parameter is optional and will default to the length of the `documents` field.\nResult documents will be sorted by relevance, and the `index` property can be used to determine original order.  \n??? console \"Request\"  \n```bash\ncurl -X 'POST' \\\n'http://127.0.0.1:8000/v1/rerank' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '{\n\"model\": \"BAAI/bge-reranker-base\",\n\"query\": \"What is the capital of France?\",\n\"documents\": [\n\"The capital of Brazil is Brasilia.\",\n\"The capital of France is Paris.\",\n\"Horses and cows are both animals\"\n]\n}'\n```  \n??? console \"Response\"  \n```json\n{\n\"id\": \"rerank-fae51b2b664d4ed38f5969b612edff77\",\n\"model\": \"BAAI/bge-reranker-base\",\n\"usage\": {\n\"total_tokens\": 56\n},\n\"results\": [\n{\n\"index\": 1,\n\"document\": {\n\"text\": \"The capital of France is Paris.\"\n},\n\"relevance_score\": 0.99853515625\n},\n{\n\"index\": 0,\n\"document\": {\n\"text\": \"The capital of Brazil is Brasilia.\"\n},\n\"relevance_score\": 0.0005860328674316406\n}\n]\n}\n```", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/API Reference/Re-rank API", "level": 3, "text": "},\n\"relevance_score\": 0.0005860328674316406\n}\n]\n}\n```  \n#### Extra parameters  \nThe following [pooling parameters][vllm.PoolingParams] are supported.  \n```python\n--8<-- \"vllm/pooling_params.py:common-pooling-params\"\n--8<-- \"vllm/pooling_params.py:classification-pooling-params\"\n```  \nThe following extra parameters are supported:  \n```python\n--8<-- \"vllm/entrypoints/openai/protocol.py:rerank-extra-params\"\n```", "file_path": "serving/openai_compatible_server.md"}
{"id": "cba3ce951f31232cb5f206c4f2c29cf0a3a5a4b57301f519e56e2c3690c3ee39", "heading": "OpenAI-Compatible Server/Ray Serve LLM", "level": 2, "text": "## Ray Serve LLM  \nRay Serve LLM enables scalable, production-grade serving of the vLLM engine. It integrates tightly with vLLM and extends it with features such as auto-scaling, load balancing, and back-pressure.  \nKey capabilities:  \n- Exposes an OpenAI-compatible HTTP API as well as a Pythonic API.\n- Scales from a single GPU to a multi-node cluster without code changes.\n- Provides observability and autoscaling policies through Ray dashboards and metrics.  \nThe following example shows how to deploy a large model like DeepSeek R1 with Ray Serve LLM: <gh-file:examples/online_serving/ray_serve_deepseek.py>.  \nLearn more about Ray Serve LLM with the official [Ray Serve LLM documentation](https://docs.ray.io/en/latest/serve/llm/serving-llms.html).", "file_path": "serving/openai_compatible_server.md"}
{"id": "c41cbcc2d5dc6835ddd74bf9b587384a575015d26aa189d8d2118f5803ff3c0b", "heading": "Parallelism and Scaling/Distributed inference strategies for a single-model replica", "level": 2, "text": "# Parallelism and Scaling  \n## Distributed inference strategies for a single-model replica  \nTo choose a distributed inference strategy for a single-model replica, use the following guidelines:  \n- **Single GPU (no distributed inference):** if the model fits on a single GPU, distributed inference is probably unnecessary. Run inference on that GPU.\n- **Single-node multi-GPU using tensor parallel inference:** if the model is too large for a single GPU but fits on a single node with multiple GPUs, use *tensor parallelism*. For example, set `tensor_parallel_size=4` when using a node with 4 GPUs.\n- **Multi-node multi-GPU using tensor parallel and pipeline parallel inference:** if the model is too large for a single node, combine *tensor parallelism* with *pipeline parallelism*. Set `tensor_parallel_size` to the number of GPUs per node and `pipeline_parallel_size` to the number of nodes. For example, set `tensor_parallel_size=8` and `pipeline_parallel_size=2` when using 2 nodes with 8 GPUs per node.", "file_path": "serving/parallelism_scaling.md"}
{"id": "c41cbcc2d5dc6835ddd74bf9b587384a575015d26aa189d8d2118f5803ff3c0b", "heading": "Parallelism and Scaling/Distributed inference strategies for a single-model replica", "level": 2, "text": "Increase the number of GPUs and nodes until there is enough GPU memory for the model. Set `tensor_parallel_size` to the number of GPUs per node and `pipeline_parallel_size` to the number of nodes.  \nAfter you provision sufficient resources to fit the model, run `vllm`. Look for log messages like:  \n```text\nINFO 07-23 13:56:04 [kv_cache_utils.py:775] GPU KV cache size: 643,232 tokens\nINFO 07-23 13:56:04 [kv_cache_utils.py:779] Maximum concurrency for 40,960 tokens per request: 15.70x\n```  \nThe `GPU KV cache size` line reports the total number of tokens that can be stored in the GPU KV cache at once. The `Maximum concurrency` line provides an estimate of how many requests can be served concurrently if each request requires the specified number of tokens (40,960 in the example above). The tokens-per-request number is taken from the model configuration's maximum sequence length, `ModelConfig.max_model_len`. If these numbers are lower than your throughput requirements, add more GPUs or nodes to your cluster.", "file_path": "serving/parallelism_scaling.md"}
{"id": "c41cbcc2d5dc6835ddd74bf9b587384a575015d26aa189d8d2118f5803ff3c0b", "heading": "Parallelism and Scaling/Distributed inference strategies for a single-model replica", "level": 2, "text": "!!! note \"Edge case: uneven GPU splits\"\nIf the model fits within a single node but the GPU count doesn't evenly divide the model size, enable pipeline parallelism, which splits the model along layers and supports uneven splits. In this scenario, set `tensor_parallel_size=1` and `pipeline_parallel_size` to the number of GPUs. Furthermore, if the GPUs on the node do not have NVLINK interconnect (e.g. L40S), leverage pipeline parallelism instead of tensor parallelism for higher throughput and lower communication overhead.", "file_path": "serving/parallelism_scaling.md"}
{"id": "c41cbcc2d5dc6835ddd74bf9b587384a575015d26aa189d8d2118f5803ff3c0b", "heading": "Parallelism and Scaling/Distributed inference strategies for a single-model replica/Distributed serving of *Mixture of Experts* (*MoE*) models", "level": 3, "text": "### Distributed serving of *Mixture of Experts* (*MoE*) models  \nIt's often advantageous to exploit the inherent parallelism of experts by using a separate parallelism strategy for the expert layers. vLLM supports large-scale deployment combining Data Parallel attention with Expert or Tensor Parallel MoE layers. For more information, see [Data Parallel Deployment](data_parallel_deployment.md).", "file_path": "serving/parallelism_scaling.md"}
{"id": "c41cbcc2d5dc6835ddd74bf9b587384a575015d26aa189d8d2118f5803ff3c0b", "heading": "Parallelism and Scaling/Single-node deployment", "level": 2, "text": "## Single-node deployment  \nvLLM supports distributed tensor-parallel and pipeline-parallel inference and serving. The implementation includes [Megatron-LM's tensor parallel algorithm](https://arxiv.org/pdf/1909.08053.pdf).  \nThe default distributed runtimes are [Ray](https://github.com/ray-project/ray) for multi-node inference and native Python `multiprocessing` for single-node inference. You can override the defaults by setting `distributed_executor_backend` in the `LLM` class or `--distributed-executor-backend` in the API server. Use `mp` for `multiprocessing` or `ray` for Ray.  \nFor multi-GPU inference, set `tensor_parallel_size` in the `LLM` class to the desired GPU count. For example, to run inference on 4 GPUs:  \n```python\nfrom vllm import LLM\nllm = LLM(\"facebook/opt-13b\", tensor_parallel_size=4)\noutput = llm.generate(\"San Francisco is a\")\n```  \nFor multi-GPU serving, include `--tensor-parallel-size` when starting the server. For example, to run the API server on 4 GPUs:  \n```bash", "file_path": "serving/parallelism_scaling.md"}
{"id": "c41cbcc2d5dc6835ddd74bf9b587384a575015d26aa189d8d2118f5803ff3c0b", "heading": "Parallelism and Scaling/Single-node deployment", "level": 2, "text": "```bash\nvllm serve facebook/opt-13b \\\n--tensor-parallel-size 4\n```  \nTo enable pipeline parallelism, add `--pipeline-parallel-size`. For example, to run the API server on 8 GPUs with pipeline parallelism and tensor parallelism:  \n```bash\n# Eight GPUs total\nvllm serve gpt2 \\\n--tensor-parallel-size 4 \\\n--pipeline-parallel-size 2\n```", "file_path": "serving/parallelism_scaling.md"}
{"id": "c41cbcc2d5dc6835ddd74bf9b587384a575015d26aa189d8d2118f5803ff3c0b", "heading": "Parallelism and Scaling/Multi-node deployment", "level": 2, "text": "## Multi-node deployment  \nIf a single node lacks sufficient GPUs to hold the model, deploy vLLM across multiple nodes. Ensure that every node provides an identical execution environment, including the model path and Python packages. Using container images is recommended because they provide a convenient way to keep environments consistent and to hide host heterogeneity.", "file_path": "serving/parallelism_scaling.md"}
{"id": "c41cbcc2d5dc6835ddd74bf9b587384a575015d26aa189d8d2118f5803ff3c0b", "heading": "Parallelism and Scaling/Multi-node deployment/What is Ray?", "level": 3, "text": "### What is Ray?  \nRay is a distributed computing framework for scaling Python programs. Multi-node vLLM deployments require Ray as the runtime engine.  \nvLLM uses Ray to manage the distributed execution of tasks across multiple nodes and control where execution happens.  \nRay also offers high-level APIs for large-scale [offline batch inference](https://docs.ray.io/en/latest/data/working-with-llms.html) and [online serving](https://docs.ray.io/en/latest/serve/llm) that can leverage vLLM as the engine. These APIs add production-grade fault tolerance, scaling, and distributed observability to vLLM workloads.  \nFor details, see the [Ray documentation](https://docs.ray.io/en/latest/index.html).", "file_path": "serving/parallelism_scaling.md"}
{"id": "c41cbcc2d5dc6835ddd74bf9b587384a575015d26aa189d8d2118f5803ff3c0b", "heading": "Parallelism and Scaling/Multi-node deployment/Ray cluster setup with containers", "level": 3, "text": "### Ray cluster setup with containers  \nThe helper script <gh-file:examples/online_serving/run_cluster.sh> starts containers across nodes and initializes Ray. By default, the script runs Docker without administrative privileges, which prevents access to the GPU performance counters when profiling or tracing. To enable admin privileges, add the `--cap-add=CAP_SYS_ADMIN` flag to the Docker command.  \nChoose one node as the head node and run:  \n```bash\nbash run_cluster.sh \\\nvllm/vllm-openai \\\n<HEAD_NODE_IP> \\\n--head \\\n/path/to/the/huggingface/home/in/this/node \\\n-e VLLM_HOST_IP=<HEAD_NODE_IP>\n```  \nOn each worker node, run:  \n```bash\nbash run_cluster.sh \\\nvllm/vllm-openai \\\n<HEAD_NODE_IP> \\\n--worker \\\n/path/to/the/huggingface/home/in/this/node \\\n-e VLLM_HOST_IP=<WORKER_NODE_IP>\n```  \nNote that `VLLM_HOST_IP` is unique for each worker. Keep the shells running these commands open; closing any shell terminates the cluster. Ensure that all nodes can communicate with each other through their IP addresses.", "file_path": "serving/parallelism_scaling.md"}
{"id": "c41cbcc2d5dc6835ddd74bf9b587384a575015d26aa189d8d2118f5803ff3c0b", "heading": "Parallelism and Scaling/Multi-node deployment/Ray cluster setup with containers", "level": 3, "text": "!!! warning \"Network security\"\nFor security, set `VLLM_HOST_IP` to an address on a private network segment. Traffic sent over this network is unencrypted, and the endpoints exchange data in a format that can be exploited to execute arbitrary code if an adversary gains network access. Ensure that untrusted parties cannot reach the network.  \nFrom any node, enter a container and run `ray status` and `ray list nodes` to verify that Ray finds the expected number of nodes and GPUs.  \n!!! tip\nAlternatively, set up the Ray cluster using KubeRay. For more information, see [KubeRay vLLM documentation](https://docs.ray.io/en/latest/cluster/kubernetes/examples/rayserve-llm-example.html).", "file_path": "serving/parallelism_scaling.md"}
{"id": "c41cbcc2d5dc6835ddd74bf9b587384a575015d26aa189d8d2118f5803ff3c0b", "heading": "Parallelism and Scaling/Multi-node deployment/Running vLLM on a Ray cluster", "level": 3, "text": "### Running vLLM on a Ray cluster  \n!!! tip\nIf Ray is running inside containers, run the commands in the remainder of this guide *inside the containers*, not on the host. To open a shell inside a container, connect to a node and use `docker exec -it <container_name> /bin/bash`.  \nOnce a Ray cluster is running, use vLLM as you would in a single-node setting. All resources across the Ray cluster are visible to vLLM, so a single `vllm` command on a single node is sufficient.  \nThe common practice is to set the tensor parallel size to the number of GPUs in each node, and the pipeline parallel size to the number of nodes. For example, if you have 16 GPUs across 2 nodes (8 GPUs per node), set the tensor parallel size to 8 and the pipeline parallel size to 2:  \n```bash\nvllm serve /path/to/the/model/in/the/container \\\n--tensor-parallel-size 8 \\\n--pipeline-parallel-size 2\n```  \nAlternatively, you can set `tensor_parallel_size` to the total number of GPUs in the cluster:  \n```bash", "file_path": "serving/parallelism_scaling.md"}
{"id": "c41cbcc2d5dc6835ddd74bf9b587384a575015d26aa189d8d2118f5803ff3c0b", "heading": "Parallelism and Scaling/Multi-node deployment/Running vLLM on a Ray cluster", "level": 3, "text": "```bash\nvllm serve /path/to/the/model/in/the/container \\\n--tensor-parallel-size 16\n```", "file_path": "serving/parallelism_scaling.md"}
{"id": "c41cbcc2d5dc6835ddd74bf9b587384a575015d26aa189d8d2118f5803ff3c0b", "heading": "Parallelism and Scaling/Optimizing network communication for tensor parallelism", "level": 2, "text": "## Optimizing network communication for tensor parallelism  \nEfficient tensor parallelism requires fast inter-node communication, preferably through high-speed network adapters such as InfiniBand.\nTo set up the cluster to use InfiniBand, append additional arguments like `--privileged -e NCCL_IB_HCA=mlx5` to the\n<gh-file:examples/online_serving/run_cluster.sh> helper script.\nContact your system administrator for more information about the required flags.", "file_path": "serving/parallelism_scaling.md"}
{"id": "c41cbcc2d5dc6835ddd74bf9b587384a575015d26aa189d8d2118f5803ff3c0b", "heading": "Parallelism and Scaling/Enabling GPUDirect RDMA", "level": 2, "text": "## Enabling GPUDirect RDMA  \nGPUDirect RDMA (Remote Direct Memory Access) is an NVIDIA technology that allows network adapters to directly access GPU memory, bypassing the CPU and system memory. This direct access reduces latency and CPU overhead, which is beneficial for large data transfers between GPUs across nodes.  \nTo enable GPUDirect RDMA with vLLM, configure the following settings:  \n- `IPC_LOCK` security context: add the `IPC_LOCK` capability to the container's security context to lock memory pages and prevent swapping to disk.\n- Shared memory with `/dev/shm`: mount `/dev/shm` in the pod spec to provide shared memory for interprocess communication (IPC).  \nIf you use Docker, set up the container as follows:  \n```bash\ndocker run --gpus all \\\n--ipc=host \\\n--shm-size=16G \\\n-v /dev/shm:/dev/shm \\\nvllm/vllm-openai\n```  \nIf you use Kubernetes, set up the pod spec as follows:  \n```yaml\n...\nspec:\ncontainers:\n- name: vllm\nimage: vllm/vllm-openai\nsecurityContext:\ncapabilities:\nadd: [\"IPC_LOCK\"]\nvolumeMounts:", "file_path": "serving/parallelism_scaling.md"}
{"id": "c41cbcc2d5dc6835ddd74bf9b587384a575015d26aa189d8d2118f5803ff3c0b", "heading": "Parallelism and Scaling/Enabling GPUDirect RDMA", "level": 2, "text": "securityContext:\ncapabilities:\nadd: [\"IPC_LOCK\"]\nvolumeMounts:\n- mountPath: /dev/shm\nname: dshm\nresources:\nlimits:\nnvidia.com/gpu: 8\nrequests:\nnvidia.com/gpu: 8\nvolumes:\n- name: dshm\nemptyDir:\nmedium: Memory\n...\n```  \n!!! tip \"Confirm GPUDirect RDMA operation\"\nTo confirm your InfiniBand card is using GPUDirect RDMA, run vLLM with detailed NCCL logs: `NCCL_DEBUG=TRACE vllm serve ...`.  \nThen look for the NCCL version and the network used.  \n- If you find `[send] via NET/IB/GDRDMA` in the logs, then NCCL is using InfiniBand with GPUDirect RDMA, which *is* efficient.\n- If you find `[send] via NET/Socket` in the logs, NCCL used a raw TCP socket, which *is not* efficient for cross-node tensor parallelism.  \n!!! tip \"Pre-download Hugging Face models\"", "file_path": "serving/parallelism_scaling.md"}
{"id": "c41cbcc2d5dc6835ddd74bf9b587384a575015d26aa189d8d2118f5803ff3c0b", "heading": "Parallelism and Scaling/Enabling GPUDirect RDMA", "level": 2, "text": "!!! tip \"Pre-download Hugging Face models\"\nIf you use Hugging Face models, downloading the model before starting vLLM is recommended. Download the model on every node to the same path, or store the model on a distributed file system accessible by all nodes. Then pass the path to the model in place of the repository ID. Otherwise, supply a Hugging Face token by appending `-e HF_TOKEN=<TOKEN>` to `run_cluster.sh`.", "file_path": "serving/parallelism_scaling.md"}
{"id": "c41cbcc2d5dc6835ddd74bf9b587384a575015d26aa189d8d2118f5803ff3c0b", "heading": "Parallelism and Scaling/Troubleshooting distributed deployments", "level": 2, "text": "## Troubleshooting distributed deployments  \nFor information about distributed debugging, see [Troubleshooting distributed deployments](distributed_troubleshooting.md).", "file_path": "serving/parallelism_scaling.md"}
{"id": "9e008ea9b9a54138d7e198f095f20880f38be1936d8bb281558168fd3674afbe", "heading": "Frequently Asked Questions", "level": 1, "text": "# Frequently Asked Questions  \n> Q: How can I serve multiple models on a single port using the OpenAI API?  \nA: Assuming that you're referring to using OpenAI compatible server to serve multiple models at once, that is not currently supported, you can run multiple instances of the server (each serving a different model) at the same time, and have another layer to route the incoming request to the correct server accordingly.  \n---  \n> Q: Which model to use for offline inference embedding?  \nA: You can try [e5-mistral-7b-instruct](https://huggingface.co/intfloat/e5-mistral-7b-instruct) and [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5);\nmore are listed [here](../models/supported_models.md).  \nBy extracting hidden states, vLLM can automatically convert text generation models like [Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B),\n[Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) into embedding models,", "file_path": "usage/faq.md"}
{"id": "9e008ea9b9a54138d7e198f095f20880f38be1936d8bb281558168fd3674afbe", "heading": "Frequently Asked Questions", "level": 1, "text": "but they are expected to be inferior to models that are specifically trained on embedding tasks.  \n---  \n> Q: Can the output of a prompt vary across runs in vLLM?  \nA: Yes, it can. vLLM does not guarantee stable log probabilities (logprobs) for the output tokens. Variations in logprobs may occur due to\nnumerical instability in Torch operations or non-deterministic behavior in batched Torch operations when batching changes. For more details,\nsee the [Numerical Accuracy section](https://pytorch.org/docs/stable/notes/numerical_accuracy.html#batched-computations-or-slice-computations).  \nIn vLLM, the same requests might be batched differently due to factors such as other concurrent requests,\nchanges in batch size, or batch expansion in speculative decoding. These batching variations, combined with numerical instability of Torch operations,\ncan lead to slightly different logit/logprob values at each step. Such differences can accumulate, potentially resulting in", "file_path": "usage/faq.md"}
{"id": "9e008ea9b9a54138d7e198f095f20880f38be1936d8bb281558168fd3674afbe", "heading": "Frequently Asked Questions", "level": 1, "text": "different tokens being sampled. Once a different token is sampled, further divergence is likely.", "file_path": "usage/faq.md"}
{"id": "9e008ea9b9a54138d7e198f095f20880f38be1936d8bb281558168fd3674afbe", "heading": "Frequently Asked Questions/Mitigation Strategies", "level": 2, "text": "## Mitigation Strategies  \n- For improved stability and reduced variance, use `float32`. Note that this will require more memory.\n- If using `bfloat16`, switching to `float16` can also help.\n- Using request seeds can aid in achieving more stable generation for temperature > 0, but discrepancies due to precision differences may still occur.", "file_path": "usage/faq.md"}
{"id": "7d7198c6b57f1277aea53759a66257df596a96c36cf44a63958a90803f8c6a1a", "heading": "Production Metrics", "level": 1, "text": "# Production Metrics  \nvLLM exposes a number of metrics that can be used to monitor the health of the\nsystem. These metrics are exposed via the `/metrics` endpoint on the vLLM\nOpenAI compatible API server.  \nYou can start the server using Python, or using [Docker](../deployment/docker.md):  \n```bash\nvllm serve unsloth/Llama-3.2-1B-Instruct\n```  \nThen query the endpoint to get the latest metrics from the server:  \n??? console \"Output\"  \n```console\n$ curl http://0.0.0.0:8000/metrics", "file_path": "usage/metrics.md"}
{"id": "7d7198c6b57f1277aea53759a66257df596a96c36cf44a63958a90803f8c6a1a", "heading": "Production Metrics", "level": 1, "text": "# HELP vllm:iteration_tokens_total Histogram of number of tokens per engine_step.\n# TYPE vllm:iteration_tokens_total histogram\nvllm:iteration_tokens_total_sum{model_name=\"unsloth/Llama-3.2-1B-Instruct\"} 0.0\nvllm:iteration_tokens_total_bucket{le=\"1.0\",model_name=\"unsloth/Llama-3.2-1B-Instruct\"} 3.0\nvllm:iteration_tokens_total_bucket{le=\"8.0\",model_name=\"unsloth/Llama-3.2-1B-Instruct\"} 3.0\nvllm:iteration_tokens_total_bucket{le=\"16.0\",model_name=\"unsloth/Llama-3.2-1B-Instruct\"} 3.0\nvllm:iteration_tokens_total_bucket{le=\"32.0\",model_name=\"unsloth/Llama-3.2-1B-Instruct\"} 3.0\nvllm:iteration_tokens_total_bucket{le=\"64.0\",model_name=\"unsloth/Llama-3.2-1B-Instruct\"} 3.0\nvllm:iteration_tokens_total_bucket{le=\"128.0\",model_name=\"unsloth/Llama-3.2-1B-Instruct\"} 3.0\nvllm:iteration_tokens_total_bucket{le=\"256.0\",model_name=\"unsloth/Llama-3.2-1B-Instruct\"} 3.0\nvllm:iteration_tokens_total_bucket{le=\"512.0\",model_name=\"unsloth/Llama-3.2-1B-Instruct\"} 3.0\n...\n```  \nThe following metrics are exposed:  \n??? code  \n```python", "file_path": "usage/metrics.md"}
{"id": "7d7198c6b57f1277aea53759a66257df596a96c36cf44a63958a90803f8c6a1a", "heading": "Production Metrics", "level": 1, "text": "```  \nThe following metrics are exposed:  \n??? code  \n```python\n--8<-- \"vllm/engine/metrics.py:metrics-definitions\"\n```  \nNote: when metrics are deprecated in version `X.Y`, they are hidden in version `X.Y+1`\nbut can be re-enabled using the `--show-hidden-metrics-for-version=X.Y` escape hatch,\nand are then removed in version `X.Y+2`.", "file_path": "usage/metrics.md"}
{"id": "8a2a3d5e0ad6ab28b5949ddd0d51f4bf174ffb29aeb59fcb1c554bc5a57885ae", "heading": "Reproducibility", "level": 1, "text": "# Reproducibility  \nvLLM does not guarantee the reproducibility of the results by default, for the sake of performance. You need to do the following to achieve\nreproducible results:  \n- For V1: Turn off multiprocessing to make the scheduling deterministic by setting `VLLM_ENABLE_V1_MULTIPROCESSING=0`.\n- For V0: Set the global seed (see below).  \nExample: <gh-file:examples/offline_inference/reproducibility.py>  \n!!! warning  \nApplying the above settings [changes the random state in user code](#locality-of-random-state).  \n!!! note  \nEven with the above settings, vLLM only provides reproducibility\nwhen it runs on the same hardware and the same vLLM version.\nAlso, the online serving API (`vllm serve`) does not support reproducibility\nbecause it is almost impossible to make the scheduling deterministic in the\nonline setting.", "file_path": "usage/reproducibility.md"}
{"id": "8a2a3d5e0ad6ab28b5949ddd0d51f4bf174ffb29aeb59fcb1c554bc5a57885ae", "heading": "Reproducibility/Setting the global seed", "level": 2, "text": "## Setting the global seed  \nThe `seed` parameter in vLLM is used to control the random states for various random number generators.  \nIf a specific seed value is provided, the random states for `random`, `np.random`, and `torch.manual_seed` will be set accordingly.  \nHowever, in some cases, setting the seed will also [change the random state in user code](#locality-of-random-state).", "file_path": "usage/reproducibility.md"}
{"id": "8a2a3d5e0ad6ab28b5949ddd0d51f4bf174ffb29aeb59fcb1c554bc5a57885ae", "heading": "Reproducibility/Setting the global seed/Default Behavior", "level": 3, "text": "### Default Behavior  \nIn V0, the `seed` parameter defaults to `None`. When the `seed` parameter is `None`, the random states for `random`, `np.random`, and `torch.manual_seed` are not set. This means that each run of vLLM will produce different results if `temperature > 0`, as expected.  \nIn V1, the `seed` parameter defaults to `0` which sets the random state for each worker, so the results will remain consistent for each vLLM run even if `temperature > 0`.  \n!!! note  \nIt is impossible to un-specify a seed for V1 because different workers need to sample the same outputs\nfor workflows such as speculative decoding.  \nFor more information, see: <gh-pr:17929>", "file_path": "usage/reproducibility.md"}
{"id": "8a2a3d5e0ad6ab28b5949ddd0d51f4bf174ffb29aeb59fcb1c554bc5a57885ae", "heading": "Reproducibility/Setting the global seed/Locality of random state", "level": 3, "text": "### Locality of random state  \nThe random state in user code (i.e. the code that constructs [LLM][vllm.LLM] class) is updated by vLLM under the following conditions:  \n- For V0: The seed is specified.\n- For V1: The workers are run in the same process as user code, i.e.: `VLLM_ENABLE_V1_MULTIPROCESSING=0`.  \nBy default, these conditions are not active so you can use vLLM without having to worry about\naccidentally making deterministic subsequent operations that rely on random state.", "file_path": "usage/reproducibility.md"}
{"id": "e255bfc189c6d4afaf3966f0e7b05ff34f4c7b4281b7a91d321c1ab6f9a44b73", "heading": "Security/Inter-Node Communication", "level": 2, "text": "# Security  \n## Inter-Node Communication  \nAll communications between nodes in a multi-node vLLM deployment are **insecure by default** and must be protected by placing the nodes on an isolated network. This includes:  \n1. PyTorch Distributed communications\n2. KV cache transfer communications\n3. Tensor, Pipeline, and Data parallel communications", "file_path": "usage/security.md"}
{"id": "e255bfc189c6d4afaf3966f0e7b05ff34f4c7b4281b7a91d321c1ab6f9a44b73", "heading": "Security/Inter-Node Communication/Configuration Options for Inter-Node Communications", "level": 3, "text": "### Configuration Options for Inter-Node Communications  \nThe following options control inter-node communications in vLLM:  \n#### 1. **Environment Variables:**  \n- `VLLM_HOST_IP`: Sets the IP address for vLLM processes to communicate on  \n#### 2. **KV Cache Transfer Configuration:**  \n- `--kv-ip`: The IP address for KV cache transfer communications (default: 127.0.0.1)\n- `--kv-port`: The port for KV cache transfer communications (default: 14579)  \n#### 3. **Data Parallel Configuration:**  \n- `data_parallel_master_ip`: IP of the data parallel master (default: 127.0.0.1)\n- `data_parallel_master_port`: Port of the data parallel master (default: 29500)", "file_path": "usage/security.md"}
{"id": "e255bfc189c6d4afaf3966f0e7b05ff34f4c7b4281b7a91d321c1ab6f9a44b73", "heading": "Security/Inter-Node Communication/Notes on PyTorch Distributed", "level": 3, "text": "### Notes on PyTorch Distributed  \nvLLM uses PyTorch's distributed features for some inter-node communication. For\ndetailed information about PyTorch Distributed security considerations, please\nrefer to the [PyTorch Security\nGuide](https://github.com/pytorch/pytorch/security/policy#using-distributed-features).  \nKey points from the PyTorch security guide:  \n- PyTorch Distributed features are intended for internal communication only\n- They are not built for use in untrusted environments or networks\n- No authorization protocol is included for performance reasons\n- Messages are sent unencrypted\n- Connections are accepted from anywhere without checks", "file_path": "usage/security.md"}
{"id": "e255bfc189c6d4afaf3966f0e7b05ff34f4c7b4281b7a91d321c1ab6f9a44b73", "heading": "Security/Inter-Node Communication/Security Recommendations", "level": 3, "text": "### Security Recommendations  \n#### 1. **Network Isolation:**  \n- Deploy vLLM nodes on a dedicated, isolated network\n- Use network segmentation to prevent unauthorized access\n- Implement appropriate firewall rules  \n#### 2. **Configuration Best Practices:**  \n- Always set `VLLM_HOST_IP` to a specific IP address rather than using defaults\n- Configure firewalls to only allow necessary ports between nodes  \n#### 3. **Access Control:**  \n- Restrict physical and network access to the deployment environment\n- Implement proper authentication and authorization for management interfaces\n- Follow the principle of least privilege for all system components", "file_path": "usage/security.md"}
{"id": "e255bfc189c6d4afaf3966f0e7b05ff34f4c7b4281b7a91d321c1ab6f9a44b73", "heading": "Security/Inter-Node Communication/4. **Restrict Domains Access for Media URLs:**", "level": 3, "text": "### 4. **Restrict Domains Access for Media URLs:**  \nRestrict domains that vLLM can access for media URLs by setting\n`--allowed-media-domains` to prevent Server-Side Request Forgery (SSRF) attacks.\n(e.g. `--allowed-media-domains upload.wikimedia.org github.com www.bogotobogo.com`)  \nAlso, consider setting `VLLM_MEDIA_URL_ALLOW_REDIRECTS=0` to prevent HTTP\nredirects from being followed to bypass domain restrictions.", "file_path": "usage/security.md"}
{"id": "e255bfc189c6d4afaf3966f0e7b05ff34f4c7b4281b7a91d321c1ab6f9a44b73", "heading": "Security/Security and Firewalls: Protecting Exposed vLLM Systems", "level": 2, "text": "## Security and Firewalls: Protecting Exposed vLLM Systems  \nWhile vLLM is designed to allow unsafe network services to be isolated to\nprivate networks, there are componentsâ€”such as dependencies and underlying\nframeworksâ€”that may open insecure services listening on all network interfaces,\nsometimes outside of vLLM's direct control.  \nA major concern is the use of `torch.distributed`, which vLLM leverages for\ndistributed communication, including when using vLLM on a single host. When vLLM\nuses TCP initialization (see [PyTorch TCP Initialization\ndocumentation](https://docs.pytorch.org/docs/stable/distributed.html#tcp-initialization)),\nPyTorch creates a `TCPStore` that, by default, listens on all network\ninterfaces. This means that unless additional protections are put in place,\nthese services may be accessible to any host that can reach your machine via any\nnetwork interface.  \n**From a PyTorch perspective, any use of `torch.distributed` should be", "file_path": "usage/security.md"}
{"id": "e255bfc189c6d4afaf3966f0e7b05ff34f4c7b4281b7a91d321c1ab6f9a44b73", "heading": "Security/Security and Firewalls: Protecting Exposed vLLM Systems", "level": 2, "text": "considered insecure by default.** This is a known and intentional behavior from\nthe PyTorch team.", "file_path": "usage/security.md"}
{"id": "e255bfc189c6d4afaf3966f0e7b05ff34f4c7b4281b7a91d321c1ab6f9a44b73", "heading": "Security/Security and Firewalls: Protecting Exposed vLLM Systems/Firewall Configuration Guidance", "level": 3, "text": "### Firewall Configuration Guidance  \nThe best way to protect your vLLM system is to carefully configure a firewall to\nexpose only the minimum network surface area necessary. In most cases, this\nmeans:  \n- **Block all incoming connections except to the TCP port the API server is\nlistening on.**  \n- Ensure that ports used for internal communication (such as those for\n`torch.distributed` and KV cache transfer) are only accessible from trusted\nhosts or networks.  \n- Never expose these internal ports to the public internet or untrusted\nnetworks.  \nConsult your operating system or application platform documentation for specific\nfirewall configuration instructions.", "file_path": "usage/security.md"}
{"id": "e255bfc189c6d4afaf3966f0e7b05ff34f4c7b4281b7a91d321c1ab6f9a44b73", "heading": "Security/Reporting Security Vulnerabilities", "level": 2, "text": "## Reporting Security Vulnerabilities  \nIf you believe you have found a security vulnerability in vLLM, please report it following the project's security policy. For more information on how to report security issues and the project's security policy, please see the [vLLM Security Policy](https://github.com/vllm-project/vllm/blob/main/SECURITY.md).", "file_path": "usage/security.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting", "level": 1, "text": "# Troubleshooting  \nThis document outlines some troubleshooting strategies you can consider. If you think you've discovered a bug, please [search existing issues](https://github.com/vllm-project/vllm/issues?q=is%3Aissue) first to see if it has already been reported. If not, please [file a new issue](https://github.com/vllm-project/vllm/issues/new/choose), providing as much relevant information as possible.  \n!!! note\nOnce you've debugged a problem, remember to turn off any debugging environment variables defined, or simply start a new shell to avoid being affected by lingering debugging settings. Otherwise, the system might be slow with debugging functionalities left activated.", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Hangs downloading a model", "level": 2, "text": "## Hangs downloading a model  \nIf the model isn't already downloaded to disk, vLLM will download it from the internet which can take time and depend on your internet connection.\nIt's recommended to download the model first using the [huggingface-cli](https://huggingface.co/docs/huggingface_hub/en/guides/cli) and passing the local path to the model to vLLM. This way, you can isolate the issue.", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Hangs loading a model from disk", "level": 2, "text": "## Hangs loading a model from disk  \nIf the model is large, it can take a long time to load it from disk. Pay attention to where you store the model. Some clusters have shared filesystems across nodes, e.g. a distributed filesystem or a network filesystem, which can be slow.\nIt'd be better to store the model in a local disk. Additionally, have a look at the CPU memory usage, when the model is too large it might take a lot of CPU memory, slowing down the operating system because it needs to frequently swap between disk and memory.  \n!!! note\nTo isolate the model downloading and loading issue, you can use the `--load-format dummy` argument to skip loading the model weights. This way, you can check if the model downloading and loading is the bottleneck.", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Out of memory", "level": 2, "text": "## Out of memory  \nIf the model is too large to fit in a single GPU, you will get an out-of-memory (OOM) error. Consider adopting [these options](../configuration/conserving_memory.md) to reduce the memory consumption.", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Generation quality changed", "level": 2, "text": "## Generation quality changed  \nIn v0.8.0, the source of default sampling parameters was changed in <gh-pr:12622>. Prior to v0.8.0, the default sampling parameters came from vLLM's set of neutral defaults. From v0.8.0 onwards, the default sampling parameters come from the `generation_config.json` provided by the model creator.  \nIn most cases, this should lead to higher quality responses, because the model creator is likely to know which sampling parameters are best for their model. However, in some cases the defaults provided by the model creator can lead to degraded performance.  \nYou can check if this is happening by trying the old defaults with `--generation-config vllm` for online and `generation_config=\"vllm\"` for offline. If, after trying this, your generation quality improves we would recommend continuing to use the vLLM defaults and petition the model creator on <https://huggingface.co> to update their default `generation_config.json` so that it produces better quality generations.", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Enable more logging", "level": 2, "text": "## Enable more logging  \nIf other strategies don't solve the problem, it's likely that the vLLM instance is stuck somewhere. You can use the following environment variables to help debug the issue:  \n- `export VLLM_LOGGING_LEVEL=DEBUG` to turn on more logging.\n- `export VLLM_LOG_STATS_INTERVAL=1.` to get log statistics more frequently for tracking running queue, waiting queue and cache hit states.\n- `export CUDA_LAUNCH_BLOCKING=1` to identify which CUDA kernel is causing the problem.\n- `export NCCL_DEBUG=TRACE` to turn on more logging for NCCL.\n- `export VLLM_TRACE_FUNCTION=1` to record all function calls for inspection in the log files to tell which function crashes or hangs. Do not use this flag unless absolutely needed for debugging, it will cause significant delays in startup time.", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Breakpoints", "level": 2, "text": "## Breakpoints  \nSetting normal `pdb` breakpoints may not work in vLLM's codebase if they are executed in a subprocess. You will experience something like:  \n``` text\nFile \"/usr/local/uv/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/bdb.py\", line 100, in trace_dispatch\nreturn self.dispatch_line(frame)\n^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/usr/local/uv/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/bdb.py\", line 125, in dispatch_line\nif self.quitting: raise BdbQuit\n^^^^^^^^^^^^^\nbdb.BdbQuit\n```  \nOne solution is using [forked-pdb](https://github.com/Lightning-AI/forked-pdb). Install with `pip install fpdb` and set a breakpoint with something like:  \n``` python\n__import__('fpdb').ForkedPdb().set_trace()\n```  \nAnother option is to disable multiprocessing entirely, with the `VLLM_ENABLE_V1_MULTIPROCESSING` environment variable.\nThis keeps the scheduler in the same process, so you can use stock `pdb` breakpoints:  \n``` python\nimport os\nos.environ[\"VLLM_ENABLE_V1_MULTIPROCESSING\"] = \"0\"\n```", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Incorrect network setup", "level": 2, "text": "## Incorrect network setup  \nThe vLLM instance cannot get the correct IP address if you have a complicated network config. You can find a log such as `DEBUG 06-10 21:32:17 parallel_state.py:88] world_size=8 rank=0 local_rank=0 distributed_init_method=tcp://xxx.xxx.xxx.xxx:54641 backend=nccl` and the IP address should be the correct one.\nIf it's not, override the IP address using the environment variable `export VLLM_HOST_IP=<your_ip_address>`.  \nYou might also need to set `export NCCL_SOCKET_IFNAME=<your_network_interface>` and `export GLOO_SOCKET_IFNAME=<your_network_interface>` to specify the network interface for the IP address.", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Error near `self.graph.replay()`", "level": 2, "text": "## Error near `self.graph.replay()`  \nIf vLLM crashes and the error trace captures it somewhere around `self.graph.replay()` in `vllm/worker/model_runner.py`, it is a CUDA error inside CUDAGraph.\nTo identify the particular CUDA operation that causes the error, you can add `--enforce-eager` to the command line, or `enforce_eager=True` to the [LLM][vllm.LLM] class to disable the CUDAGraph optimization and isolate the exact CUDA operation that causes the error.  \n[](){ #troubleshooting-incorrect-hardware-driver }", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Incorrect hardware/driver", "level": 2, "text": "## Incorrect hardware/driver  \nIf GPU/CPU communication cannot be established, you can use the following Python script and follow the instructions below to confirm whether the GPU/CPU communication is working correctly.  \n??? code  \n```python\n# Test PyTorch NCCL\nimport torch\nimport torch.distributed as dist\ndist.init_process_group(backend=\"nccl\")\nlocal_rank = dist.get_rank() % torch.cuda.device_count()\ntorch.cuda.set_device(local_rank)\ndata = torch.FloatTensor([1,] * 128).to(\"cuda\")\ndist.all_reduce(data, op=dist.ReduceOp.SUM)\ntorch.cuda.synchronize()\nvalue = data.mean().item()\nworld_size = dist.get_world_size()\nassert value == world_size, f\"Expected {world_size}, got {value}\"\n\nprint(\"PyTorch NCCL is successful!\")\n\n# Test PyTorch GLOO\ngloo_group = dist.new_group(ranks=list(range(world_size)), backend=\"gloo\")\ncpu_data = torch.FloatTensor([1,] * 128)\ndist.all_reduce(cpu_data, op=dist.ReduceOp.SUM, group=gloo_group)\nvalue = cpu_data.mean().item()\nassert value == world_size, f\"Expected {world_size}, got {value}\"", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Incorrect hardware/driver", "level": 2, "text": "print(\"PyTorch GLOO is successful!\")\n\nif world_size <= 1:\nexit()\n\n# Test vLLM NCCL, with cuda graph\nfrom vllm.distributed.device_communicators.pynccl import PyNcclCommunicator\n\npynccl = PyNcclCommunicator(group=gloo_group, device=local_rank)\n# pynccl is enabled by default for 0.6.5+,\n# but for 0.6.4 and below, we need to enable it manually.\n# keep the code for backward compatibility when because people\n# prefer to read the latest documentation.\npynccl.disabled = False\n\ns = torch.cuda.Stream()\nwith torch.cuda.stream(s):\ndata.fill_(1)\nout = pynccl.all_reduce(data, stream=s)\nvalue = out.mean().item()\nassert value == world_size, f\"Expected {world_size}, got {value}\"\n\nprint(\"vLLM NCCL is successful!\")\n\ng = torch.cuda.CUDAGraph()\nwith torch.cuda.graph(cuda_graph=g, stream=s):\nout = pynccl.all_reduce(data, stream=torch.cuda.current_stream())\n\ndata.fill_(1)\ng.replay()\ntorch.cuda.current_stream().synchronize()\nvalue = out.mean().item()\nassert value == world_size, f\"Expected {world_size}, got {value}\"", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Incorrect hardware/driver", "level": 2, "text": "print(\"vLLM NCCL with cuda graph is successful!\")", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Incorrect hardware/driver", "level": 2, "text": "dist.destroy_process_group(gloo_group)\ndist.destroy_process_group()\n```  \nIf you are testing with a single node, adjust `--nproc-per-node` to the number of GPUs you want to use:  \n```bash\nNCCL_DEBUG=TRACE torchrun --nproc-per-node=<number-of-GPUs> test.py\n```  \nIf you are testing with multi-nodes, adjust `--nproc-per-node` and `--nnodes` according to your setup and set `MASTER_ADDR` to the correct IP address of the master node, reachable from all nodes. Then, run:  \n```bash\nNCCL_DEBUG=TRACE torchrun --nnodes 2 \\\n--nproc-per-node=2 \\\n--rdzv_backend=c10d \\\n--rdzv_endpoint=$MASTER_ADDR test.py\n```  \nIf the script runs successfully, you should see the message `sanity check is successful!`.", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Incorrect hardware/driver", "level": 2, "text": "If the test script hangs or crashes, usually it means the hardware/drivers are broken in some sense. You should try to contact your system administrator or hardware vendor for further assistance. As a common workaround, you can try to tune some NCCL environment variables, such as `export NCCL_P2P_DISABLE=1` to see if it helps. Please check [their documentation](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html) for more information. Please only use these environment variables as a temporary workaround, as they might affect the performance of the system. The best solution is still to fix the hardware/drivers so that the test script can run successfully.  \n!!! note\nA multi-node environment is more complicated than a single-node one. If you see errors such as `torch.distributed.DistNetworkError`, it is likely that the network/DNS setup is incorrect. In that case, you can manually assign node rank and specify the IP via command line arguments:", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Incorrect hardware/driver", "level": 2, "text": "- In the first node, run `NCCL_DEBUG=TRACE torchrun --nnodes 2 --nproc-per-node=2 --node-rank 0 --master_addr $MASTER_ADDR test.py`.\n- In the second node, run `NCCL_DEBUG=TRACE torchrun --nnodes 2 --nproc-per-node=2 --node-rank 1 --master_addr $MASTER_ADDR test.py`.  \nAdjust `--nproc-per-node`, `--nnodes`, and `--node-rank` according to your setup, being sure to execute different commands (with different `--node-rank`) on different nodes.  \n[](){ #troubleshooting-python-multiprocessing }", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Python multiprocessing/`RuntimeError` Exception", "level": 3, "text": "## Python multiprocessing  \n### `RuntimeError` Exception  \nIf you have seen a warning in your logs like this:  \n```console\nWARNING 12-11 14:50:37 multiproc_worker_utils.py:281] CUDA was previously\ninitialized. We must use the `spawn` multiprocessing start method. Setting\nVLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See\nhttps://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing\nfor more information.\n```  \nor an error from Python that looks like this:  \n??? console \"Logs\"  \n```console\nRuntimeError:\nAn attempt has been made to start a new process before the\ncurrent process has finished its bootstrapping phase.\n\nThis probably means that you are not using fork to start your\nchild processes and you have forgotten to use the proper idiom\nin the main module:\n\nif __name__ == '__main__':\nfreeze_support()\n...\n\nThe \"freeze_support()\" line can be omitted if the program\nis not going to be frozen to produce an executable.", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Python multiprocessing/`RuntimeError` Exception", "level": 3, "text": "To fix this issue, refer to the \"Safe importing of main module\"\nsection in https://docs.python.org/3/library/multiprocessing.html\n```  \nthen you must update your Python code to guard usage of `vllm` behind a `if\n__name__ == '__main__':` block. For example, instead of this:  \n```python\nimport vllm\n\nllm = vllm.LLM(...)\n```  \ntry this instead:  \n```python\nif __name__ == '__main__':\nimport vllm\n\nllm = vllm.LLM(...)\n```", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/`torch.compile` Error", "level": 2, "text": "## `torch.compile` Error  \nvLLM heavily depends on `torch.compile` to optimize the model for better performance, which introduces the dependency on the `torch.compile` functionality and the `triton` library. By default, we use `torch.compile` to [optimize some functions](gh-pr:10406) in the model. Before running vLLM, you can check if `torch.compile` is working as expected by running the following script:  \n??? code  \n```python\nimport torch\n\n@torch.compile\ndef f(x):\n# a simple function to test torch.compile\nx = x + 1\nx = x * 2\nx = x.sin()\nreturn x\n\nx = torch.randn(4, 4).cuda()\nprint(f(x))\n```  \nIf it raises errors from `torch/_inductor` directory, usually it means you have a custom `triton` library that is not compatible with the version of PyTorch you are using. See <gh-issue:12219> for example.", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Model failed to be inspected", "level": 2, "text": "## Model failed to be inspected  \nIf you see an error like:  \n```text\nFile \"vllm/model_executor/models/registry.py\", line xxx, in _raise_for_unsupported\nraise ValueError(\nValueError: Model architectures ['<arch>'] failed to be inspected. Please check the logs for more details.\n```  \nIt means that vLLM failed to import the model file.\nUsually, it is related to missing dependencies or outdated binaries in the vLLM build.\nPlease read the logs carefully to determine the root cause of the error.", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Model not supported", "level": 2, "text": "## Model not supported  \nIf you see an error like:  \n```text\nTraceback (most recent call last):\n...\nFile \"vllm/model_executor/models/registry.py\", line xxx, in inspect_model_cls\nfor arch in architectures:\nTypeError: 'NoneType' object is not iterable\n```  \nor:  \n```text\nFile \"vllm/model_executor/models/registry.py\", line xxx, in _raise_for_unsupported\nraise ValueError(\nValueError: Model architectures ['<arch>'] are not supported for now. Supported architectures: [...]\n```  \nBut you are sure that the model is in the [list of supported models](../models/supported_models.md), there may be some issue with vLLM's model resolution. In that case, please follow [these steps](../configuration/model_resolution.md) to explicitly specify the vLLM implementation for the model.", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Failed to infer device type", "level": 2, "text": "## Failed to infer device type  \nIf you see an error like `RuntimeError: Failed to infer device type`, it means that vLLM failed to infer the device type of the runtime environment. You can check [the code](gh-file:vllm/platforms/__init__.py) to see how vLLM infers the device type and why it is not working as expected. After [this PR](gh-pr:14195), you can also set the environment variable `VLLM_LOGGING_LEVEL=DEBUG` to see more detailed logs to help debug the issue.", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/NCCL error: unhandled system error during `ncclCommInitRank`", "level": 2, "text": "## NCCL error: unhandled system error during `ncclCommInitRank`  \nIf your serving workload uses GPUDirect RDMA for distributed serving across multiple nodes and encounters an error during `ncclCommInitRank`, with no clear error message even with `NCCL_DEBUG=INFO` set, it might look like this:  \n```text\nError executing method 'init_device'. This might cause deadlock in distributed execution.\nTraceback (most recent call last):\n...\nFile \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl.py\", line 99, in __init__\nself.comm: ncclComm_t = self.nccl.ncclCommInitRank(\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 277, in ncclCommInitRank\nself.NCCL_CHECK(self._funcs[\"ncclCommInitRank\"](ctypes.byref(comm),\nFile \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 256, in NCCL_CHECK\nraise RuntimeError(f\"NCCL error: {error_str}\")", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/NCCL error: unhandled system error during `ncclCommInitRank`", "level": 2, "text": "raise RuntimeError(f\"NCCL error: {error_str}\")\nRuntimeError: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details)\n...\n```  \nThis indicates vLLM failed to initialize the NCCL communicator, possibly due to a missing `IPC_LOCK` linux capability  or an unmounted `/dev/shm`. Refer to [Enabling GPUDirect RDMA](../serving/parallelism_scaling.md#enabling-gpudirect-rdma) for guidance on properly configuring the environment for GPUDirect RDMA.", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Known Issues", "level": 2, "text": "## Known Issues  \n- In `v0.5.2`, `v0.5.3`, and `v0.5.3.post1`, there is a bug caused by [zmq](https://github.com/zeromq/pyzmq/issues/2000) , which can occasionally cause vLLM to hang depending on the machine configuration. The solution is to upgrade to the latest version of `vllm` to include the [fix](gh-pr:6759).\n- To address a memory overhead issue in older NCCL versions (see [bug](https://github.com/NVIDIA/nccl/issues/1234)), vLLM versions `>= 0.4.3, <= 0.10.1.1` would set the environment variable `NCCL_CUMEM_ENABLE=0`. External processes connecting to vLLM also needed to set this variable to prevent hangs or crashes. Since the underlying NCCL bug was fixed in NCCL 2.22.3, this override was removed in newer vLLM versions to allow for NCCL performance optimizations.", "file_path": "usage/troubleshooting.md"}
{"id": "9fbc29aaf057a1a5eb4df2af2847247d2e07d7a5cb4ebf5896b115f1c53d0494", "heading": "Troubleshooting/Known Issues", "level": 2, "text": "- In some PCIe machines (e.g. machines without NVLink), if you see an error like `transport/shm.cc:590 NCCL WARN Cuda failure 217 'peer access is not supported between these two devices'`, it's likely caused by a driver bug. See [this issue](https://github.com/NVIDIA/nccl/issues/1838) for more details. In that case, you can try to set `NCCL_CUMEM_HOST_ENABLE=0` to disable the feature, or upgrade your driver to the latest version.", "file_path": "usage/troubleshooting.md"}
{"id": "38639859f1ff9a03686ba8e5b4b950db68f1c46f4ba65a544cdb4a2fd19eeea3", "heading": "Usage Stats Collection", "level": 1, "text": "# Usage Stats Collection  \nvLLM collects anonymous usage data by default to help the engineering team better understand which hardware and model configurations are widely used. This data allows them to prioritize their efforts on the most common workloads. The collected data is transparent, does not contain any sensitive information.  \nA subset of the data, after cleaning and aggregation, will be publicly released for the community's benefit. For example, you can see the 2024 usage report [here](https://2024.vllm.ai).", "file_path": "usage/usage_stats.md"}
{"id": "38639859f1ff9a03686ba8e5b4b950db68f1c46f4ba65a544cdb4a2fd19eeea3", "heading": "Usage Stats Collection/What data is collected?", "level": 2, "text": "## What data is collected?  \nThe list of data collected by the latest version of vLLM can be found here: <gh-file:vllm/usage/usage_lib.py>  \nHere is an example as of v0.4.0:  \n??? console \"Output\"  \n```json\n{\n\"uuid\": \"fbe880e9-084d-4cab-a395-8984c50f1109\",\n\"provider\": \"GCP\",\n\"num_cpu\": 24,\n\"cpu_type\": \"Intel(R) Xeon(R) CPU @ 2.20GHz\",\n\"cpu_family_model_stepping\": \"6,85,7\",\n\"total_memory\": 101261135872,\n\"architecture\": \"x86_64\",\n\"platform\": \"Linux-5.10.0-28-cloud-amd64-x86_64-with-glibc2.31\",\n\"gpu_count\": 2,\n\"gpu_type\": \"NVIDIA L4\",\n\"gpu_memory_per_device\": 23580639232,\n\"model_architecture\": \"OPTForCausalLM\",\n\"vllm_version\": \"0.3.2+cu123\",\n\"context\": \"LLM_CLASS\",\n\"log_time\": 1711663373492490000,\n\"source\": \"production\",\n\"dtype\": \"torch.float16\",\n\"tensor_parallel_size\": 1,\n\"block_size\": 16,\n\"gpu_memory_utilization\": 0.9,\n\"quantization\": null,\n\"kv_cache_dtype\": \"auto\",\n\"enable_lora\": false,\n\"enable_prefix_caching\": false,\n\"enforce_eager\": false,\n\"disable_custom_all_reduce\": true\n}\n```", "file_path": "usage/usage_stats.md"}
{"id": "38639859f1ff9a03686ba8e5b4b950db68f1c46f4ba65a544cdb4a2fd19eeea3", "heading": "Usage Stats Collection/What data is collected?", "level": 2, "text": "\"disable_custom_all_reduce\": true\n}\n```  \nYou can preview the collected data by running the following command:  \n```bash\ntail ~/.config/vllm/usage_stats.json\n```", "file_path": "usage/usage_stats.md"}
{"id": "38639859f1ff9a03686ba8e5b4b950db68f1c46f4ba65a544cdb4a2fd19eeea3", "heading": "Usage Stats Collection/Opting out", "level": 2, "text": "## Opting out  \nYou can opt out of usage stats collection by setting the `VLLM_NO_USAGE_STATS` or `DO_NOT_TRACK` environment variable, or by creating a `~/.config/vllm/do_not_track` file:  \n```bash\n# Any of the following methods can disable usage stats collection\nexport VLLM_NO_USAGE_STATS=1\nexport DO_NOT_TRACK=1\nmkdir -p ~/.config/vllm && touch ~/.config/vllm/do_not_track\n```", "file_path": "usage/usage_stats.md"}
{"id": "5e706ba96ba173b96ab5f039ffe10a482bed43bc60d451104c38d947feb401a3", "heading": "vLLM V1", "level": 1, "text": "# vLLM V1  \n!!! announcement  \nWe have started the process of deprecating V0. Please read [RFC #18571](gh-issue:18571) for more details.  \nV1 is now enabled by default for all supported use cases, and we will gradually enable it for every use case we plan to support. Please share any feedback on [GitHub](https://github.com/vllm-project/vllm) or in the [vLLM Slack](https://inviter.co/vllm-slack).  \nTo disable V1, please set the environment variable as: `VLLM_USE_V1=0`, and send us a GitHub issue sharing the reason!", "file_path": "usage/v1_guide.md"}
{"id": "5e706ba96ba173b96ab5f039ffe10a482bed43bc60d451104c38d947feb401a3", "heading": "vLLM V1/Why vLLM V1?", "level": 2, "text": "## Why vLLM V1?  \nvLLM V0 successfully supported a wide range of models and hardware, but as new features were developed independently, the system grew increasingly complex. This complexity made it harder to integrate new capabilities and introduced technical debt, revealing the need for a more streamlined and unified design.  \nBuilding on V0â€™s success, vLLM V1 retains the stable and proven components from V0\n(such as the models, GPU kernels, and utilities). At the same time, it significantly\nre-architects the core systems, covering the scheduler, KV cache manager, worker,\nsampler, and API server, to provide a cohesive, maintainable framework that better\naccommodates continued growth and innovation.  \nSpecifically, V1 aims to:  \n- Provide a **simple, modular, and easy-to-hack codebase**.\n- Ensure **high performance** with near-zero CPU overhead.\n- **Combine key optimizations** into a unified architecture.\n- Require **zero configs** by enabling features/optimizations by default.", "file_path": "usage/v1_guide.md"}
{"id": "5e706ba96ba173b96ab5f039ffe10a482bed43bc60d451104c38d947feb401a3", "heading": "vLLM V1/Why vLLM V1?", "level": 2, "text": "We see significant performance improvements from upgrading to V1 core engine, in\nparticular for long context scenarios. Please see performance benchmark (To be\nadded).  \nFor more details, check out the vLLM V1 blog post [vLLM V1: A Major\nUpgrade to vLLMâ€™s Core Architecture](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html) (published Jan 27, 2025).  \nThis living user guide outlines a few known **important changes and limitations** introduced by vLLM V1. The team has been working actively to bring V1 as the default engine, therefore this guide will be updated constantly as more features get supported on vLLM V1.", "file_path": "usage/v1_guide.md"}
{"id": "5e706ba96ba173b96ab5f039ffe10a482bed43bc60d451104c38d947feb401a3", "heading": "vLLM V1/Current Status", "level": 2, "text": "## Current Status  \nFor each item, our progress towards V1 support falls into one of the following states:  \n- **ðŸš€ Optimized**: Nearly fully optimized, with no further work currently planned.\n- **ðŸŸ¢ Functional**: Fully operational, with ongoing optimizations.\n- **ðŸš§ WIP**: Under active development.\n- **ðŸŸ¡ Planned**: Scheduled for future implementation (some may have open PRs/RFCs).\n- **ðŸŸ  Delayed**: Temporarily dropped in V1 but planned to be re-introduced later.\n- **ðŸ”´ Deprecated**: Not planned for V1 unless there is strong demand.  \n!!! note\nvLLM V1â€™s unified scheduler treats both prompt and output tokens the same\nway by using a simple dictionary (e.g., `{request_id: num_tokens}`) to dynamically\nallocate a fixed token budget per request, enabling features like chunked prefills,\nprefix caching, and speculative decoding without a strict separation between prefill\nand decode phases.  \nThe V1 scheduler supports multiple scheduling policies, including First-Come,", "file_path": "usage/v1_guide.md"}
{"id": "5e706ba96ba173b96ab5f039ffe10a482bed43bc60d451104c38d947feb401a3", "heading": "vLLM V1/Current Status", "level": 2, "text": "First-Served (FCFS) and priority-based scheduling (where requests are processed\nbased on assigned priority, with FCFS as a tie-breaker), configurable via the\n`--scheduling-policy` argument.", "file_path": "usage/v1_guide.md"}
{"id": "5e706ba96ba173b96ab5f039ffe10a482bed43bc60d451104c38d947feb401a3", "heading": "vLLM V1/Current Status/Hardware", "level": 3, "text": "### Hardware  \n| Hardware   | Status                                        |\n|------------|-----------------------------------------------|\n| **NVIDIA** | <nobr>ðŸš€</nobr>                               |\n| **AMD**    | <nobr>ðŸŸ¢</nobr>                               |\n| **INTEL GPU**    | <nobr>ðŸŸ¢</nobr>                               |\n| **TPU**    | <nobr>ðŸŸ¢</nobr>                               |\n| **CPU**    | <nobr>ðŸŸ¢ (x86\\_64/aarch64) ðŸŸ¡ (MacOS) </nobr> |  \n!!! note  \nMore hardware platforms may be supported via plugins, e.g.:  \n- [vllm-ascend](https://github.com/vllm-project/vllm-ascend)\n- [vllm-spyre](https://github.com/vllm-project/vllm-spyre)\n- [vllm-gaudi](https://github.com/vllm-project/vllm-gaudi)\n- [vllm-openvino](https://github.com/vllm-project/vllm-openvino)  \nPlease check their corresponding repositories for more details.", "file_path": "usage/v1_guide.md"}
{"id": "5e706ba96ba173b96ab5f039ffe10a482bed43bc60d451104c38d947feb401a3", "heading": "vLLM V1/Current Status/Models", "level": 3, "text": "### Models  \n| Model Type                  | Status                                                                             |\n|-----------------------------|------------------------------------------------------------------------------------|\n| **Decoder-only Models**     | <nobr>ðŸš€ Optimized</nobr>                                                          |\n| **Encoder-Decoder Models**  | <nobr>ðŸŸ¢ Whisper only</nobr>                                                       |\n| **Embedding Models**        | <nobr>ðŸŸ¢ Functional</nobr>                                                         |\n| **Mamba Models**            | <nobr>ðŸŸ¢ (Mamba-2), ðŸŸ¢ (Mamba-1)</nobr>                                            |\n| **Multimodal Models**       | <nobr>ðŸŸ¢ Functional</nobr>                                                         |  \nSee below for the status of models that are not yet supported or have more features planned in V1.  \n#### Embedding Models  \nThe initial basic support is now functional.", "file_path": "usage/v1_guide.md"}
{"id": "5e706ba96ba173b96ab5f039ffe10a482bed43bc60d451104c38d947feb401a3", "heading": "vLLM V1/Current Status/Models", "level": 3, "text": "The initial basic support is now functional.  \nLater, we will consider using [hidden states processor](gh-issue:12249),\nwhich is based on [global logits processor](gh-pr:13360)\nto enable simultaneous generation and embedding using the same engine instance in V1.  \n#### Mamba Models  \nModels using selective state-space mechanisms instead of standard transformer attention are supported.\nModels that use Mamba-2 and Mamba-1 layers (e.g., `Mamba2ForCausalLM`, `MambaForCausalLM`,`FalconMambaForCausalLM`) are supported.  \nHybrid models that combine Mamba-2 and Mamba-1 layers with standard attention layers are also supported (e.g., `BambaForCausalLM`,\n`Zamba2ForCausalLM`, `NemotronHForCausalLM`, `FalconH1ForCausalLM` and `GraniteMoeHybridForCausalLM`, `JambaForCausalLM`, `Plamo2ForCausalLM`).  \nHybrid models with mechanisms different to Mamba are also supported (e.g, `MiniMaxText01ForCausalLM`, `MiniMaxM1ForCausalLM`, `Lfm2ForCausalLM`).", "file_path": "usage/v1_guide.md"}
{"id": "5e706ba96ba173b96ab5f039ffe10a482bed43bc60d451104c38d947feb401a3", "heading": "vLLM V1/Current Status/Models", "level": 3, "text": "Please note that prefix caching is not yet supported for any of the above models.  \n#### Encoder-Decoder Models  \nWhisper is supported. Other models requiring cross-attention between separate\nencoder and decoder (e.g., `BartForConditionalGeneration`,\n`MllamaForConditionalGeneration`) are not supported.", "file_path": "usage/v1_guide.md"}
{"id": "5e706ba96ba173b96ab5f039ffe10a482bed43bc60d451104c38d947feb401a3", "heading": "vLLM V1/Current Status/Features", "level": 3, "text": "### Features  \n| Feature                                     | Status                                                                            |\n|---------------------------------------------|-----------------------------------------------------------------------------------|\n| **Prefix Caching**                          | <nobr>ðŸš€ Optimized</nobr>                                                         |\n| **Chunked Prefill**                         | <nobr>ðŸš€ Optimized</nobr>                                                         |\n| **LoRA**                                    | <nobr>ðŸš€ Optimized</nobr>                                                         |\n| **Logprobs Calculation**                    | <nobr>ðŸŸ¢ Functional</nobr>                                                        |\n| **FP8 KV Cache**                            | <nobr>ðŸŸ¢ Functional on Hopper devices (<gh-pr:15191>)</nobr>|", "file_path": "usage/v1_guide.md"}
{"id": "5e706ba96ba173b96ab5f039ffe10a482bed43bc60d451104c38d947feb401a3", "heading": "vLLM V1/Current Status/Features", "level": 3, "text": "| **Spec Decode**                             | <nobr>ðŸš€ Optimized</nobr>                                                         |\n| **Prompt Logprobs with Prefix Caching**     | <nobr>ðŸŸ¡ Planned ([RFC #13414](gh-issue:13414))</nobr>|\n| **Structured Output Alternative Backends**  | <nobr>ðŸŸ¢ Functional</nobr>                                                        |\n| **Request-level Structured Output Backend** | <nobr>ðŸ”´ Deprecated</nobr>                                                        |\n| **best_of**                                 | <nobr>ðŸ”´ Deprecated ([RFC #13361](gh-issue:13361))</nobr>|\n| **Per-Request Logits Processors**           | <nobr>ðŸ”´ Deprecated ([RFC #13360](gh-pr:13360))</nobr> |\n| **GPU <> CPU KV Cache Swapping**            | <nobr>ðŸ”´ Deprecated</nobr>                                                        |  \n!!! note  \nvLLM V1â€™s unified scheduler treats both prompt and output tokens the same\nway by using a simple dictionary (e.g., `{request_id: num_tokens}`) to dynamically", "file_path": "usage/v1_guide.md"}
{"id": "5e706ba96ba173b96ab5f039ffe10a482bed43bc60d451104c38d947feb401a3", "heading": "vLLM V1/Current Status/Features", "level": 3, "text": "allocate a fixed token budget per request, enabling features like chunked prefills,\nprefix caching, and speculative decoding without a strict separation between prefill\nand decode phases.  \n#### Semantic Changes to Logprobs  \nvLLM V1 supports logprobs and prompt logprobs. However, there are some important semantic\ndifferences compared to V0:  \n##### Logprobs Calculation  \nBy default, logprobs in V1 are now returned immediately once computed from the modelâ€™s raw output (i.e.\nbefore applying any logits post-processing such as temperature scaling or penalty\nadjustments). As a result, the returned logprobs do not reflect the final adjusted\nprobabilities used during sampling.  \nYou can adjust this behavior by setting the `--logprobs-mode` flag.\nFour modes are supported: `raw_logprobs` (default), `processed_logprobs`, `raw_logits`, `processed_logits`.\nRaw means the values before applying any logit processors, like bad words.", "file_path": "usage/v1_guide.md"}
{"id": "5e706ba96ba173b96ab5f039ffe10a482bed43bc60d451104c38d947feb401a3", "heading": "vLLM V1/Current Status/Features", "level": 3, "text": "Processed means the values after applying all processors, including temperature and top_k/top_p.  \n##### Prompt Logprobs with Prefix Caching  \nLogprobs are not cached. For a request requiring prompt logprobs, the engine will ignore the prefix cache and recompute the prefill of full prompt to generate the logprobs.  \n#### Deprecated Features  \nAs part of the major architectural rework in vLLM V1, several legacy features have been deprecated.  \n##### Sampling features  \n- **best_of**: This feature has been deprecated due to limited usage. See details at [RFC #13361](gh-issue:13361).\n- **Per-Request Logits Processors**: In V0, users could pass custom\nprocessing functions to adjust logits on a per-request basis. In vLLM V1, this\nfeature has been deprecated. Instead, the design is moving toward supporting **global logits\nprocessors**, a feature the team is actively working on for future releases. See details at [RFC #13360](gh-pr:13360).  \n##### KV Cache features", "file_path": "usage/v1_guide.md"}
{"id": "5e706ba96ba173b96ab5f039ffe10a482bed43bc60d451104c38d947feb401a3", "heading": "vLLM V1/Current Status/Features", "level": 3, "text": "##### KV Cache features  \n- **GPU <> CPU KV Cache Swapping**: with the new simplified core architecture, vLLM V1 no longer requires KV cache swapping\nto handle request preemptions.  \n##### Structured Output features  \n- **Request-level Structured Output Backend**: Deprecated, alternative backends (outlines, guidance) with fallbacks is supported now.", "file_path": "usage/v1_guide.md"}
