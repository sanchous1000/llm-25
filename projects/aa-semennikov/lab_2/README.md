# Лабораторная работа №2

## 1. Описание задания

В рамках лабораторной работы была реализована система RAG (Retrieval-Augmented Generation) для работы с README-файлами из репозиториев GitHub. Система позволяет:

- Загружать README-файлы из репозиториев GitHub
- Парсить Markdown-файлы с сохранением структуры заголовков
- Разбивать документы на семантически осмысленные чанки
- Создавать векторные представления (эмбеддинги) текстовых фрагментов
- Сохранять эмбеддинги в векторную базу данных Qdrant
- Выполнять семантический поиск по документации
- Генерировать ответы на вопросы с использованием найденного контекста
- Оценивать качество retrieval-системы с помощью стандартных метрик

Работа включает пять основных компонентов:
1. **Загрузка данных** (`source/pull_readmies.py`) - загрузка README-файлов из репозиториев GitHub
2. **Построение индекса** (`source/build_index.py`) - разбиение документов на чанки и создание эмбеддингов
3. **Загрузка в векторное хранилище** (`source/load_to_vector_store.py`) - загрузка эмбеддингов в Qdrant
4. **RAG-пайплайн** (`source/rag_pipeline.py`) - генерация ответов с использованием найденного контекста
5. **Оценка качества** (`source/run_evaluation.py`) - вычисление метрик Recall@k, Precision@k и MRR

## 2. Использованные фреймворки и модели

### LLM и модели эмбеддингов:
- **bge-base-en-v1.5** - модель для создания эмбеддингов текстов (через sentence-transformers)
- **llama3.1:8b** - языковая модель для генерации ответов (через Ollama)

### Фреймворки и библиотеки:
- **Ollama** - локальный сервер для запуска LLM
- **Qdrant** - векторная база данных для хранения и поиска эмбеддингов, **qdrant-client** - Python-клиент для работы с БД
- **sentence-transformers** - библиотека для создания эмбеддингов
- **transformers** - библиотека для работы с моделями машинного обучения, оттуда же **AutoTokenizer** для загрузки токенизатора от модели для векторизации

## 3. Результаты работы

### Архитектура системы
Система реализует классический пайплайн RAG:

1. **Подготовка данных**: README-файлы загружаются из репозиториев GitHub и разбиваются на чанки с учетом иерархии заголовков (H1-H4). Каждый чанк содержит метаданные: путь к файлу, заголовок секции, уровень вложенности, информация о репозитории.

2. **Векторизация**: Текстовые чанки преобразуются в эмбеддинги с помощью модели `bge-base-en-v1.5` и сохраняются в Qdrant с метаданными. Система поддерживает версионирование данных - каждая конфигурация создает отдельную версию индекса.

3. **Retrieval**: При запросе создается эмбеддинг запроса, выполняется поиск по косинусному расстоянию с использованием HNSW индекса, возвращаются top-k наиболее релевантных документов.
4. **Generation**: Найденные документы используются как контекст для LLM (llama3.1:8b через Ollama), которая генерирует ответ на основе предоставленной информации.

### Особенности реализации

1. **Умное разбиение на чанки**: используется кастомный `MarkdownSplitter`, который:
   - Разбивает документы по заголовкам с учетом иерархии
   - Разбивает секции на чанки фиксированного размера (в токенах) с перекрытием
   - Сохраняет информацию о заголовках в метаданных каждого чанка
2. **Сохранение структуры**: каждый чанк содержит информацию о пути в иерархии документа и метаданные из frontmatter (источник, репозиторий, описание).
3. **Версионирование**: система автоматически создает версионные директории для каждой конфигурации, что позволяет экспериментировать с разными параметрами без потери данных.

### Метрики retrieval

### Конфигурация 1 (базовая):
- Chunk size: 512
- Chunk overlap: 50
- HNSW M: 16
- HNSW ef_construction: 100
- HNSW ef_search: 100

**Время выполнения:**
- build_index: 289.18 секунд (4.82 минут)
- load_to_vector_store: 1.87 секунд
- run_evaluation: 1.41 секунд

**Метрики:**
- Recall@5: 0.62
- Precision@5: 0.30
- Recall@10: 0.75
- Precision@10: 0.19
- MRR: 0.85

### Конфигурация 2 (большие чанки):
- Chunk size: 1000
- Chunk overlap: 100
- HNSW M: 16
- HNSW ef_construction: 100
- HNSW ef_search: 100

**Время выполнения:**
- build_index: 201.08 секунд (3.35 минут)
- load_to_vector_store: 1.51 секунд
- run_evaluation: 1.35 секунд

**Метрики:**
- Recall@5: 0.62
- Precision@5: 0.30
- Recall@10: 0.73
- Precision@10: 0.18
- MRR: 0.90

### Конфигурация 3 (улучшенный HNSW):
- Chunk size: 512
- Chunk overlap: 50
- HNSW M: 32
- HNSW ef_construction: 200
- HNSW ef_search: 200

**Время выполнения:**
- build_index: 241.74 секунд (4.03 минут)
- load_to_vector_store: 1.76 секунд
- run_evaluation: 1.40 секунд

**Метрики:**
- Recall@5: 0.62
- Precision@5: 0.30
- Recall@10: 0.73
- Precision@10: 0.18
- MRR: 0.90

## 4. Выводы

Была создана RAG-система для работы с документацией, способная находить релевантную информацию и генерировать ответы. Были реализованы стандартные метрики для оценки качества retrieval. Экспериментальным путем была выбрана лучшая конфигурация (№2), метрики которой однако не столь отличались от других конфигураций: высокий MRR (0.9), достаточно низкие Recall и Precision - (0.62, 0.3) @5 и (0.73, 0.18) @10, что свидетельствует о небольшом количестве релевантных документов для каждого запроса, которые тем не менее попадают в топ выдачи.


## 5. Инструкция по запуску

### Предварительные требования

1. Установить и запустить **Ollama**:
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama serve

ollama pull llama3.1:8b
```

2. Установить и запустить **Qdrant**:
```bash
cd qdrant
docker-compose up -d
```

3. Установить зависимости Python:
```bash
pip install -r requirements.txt
```

### Конфигурация

Параметры системы настраиваются в файле `config.yaml`:
- **chunking**: размер чанков, перекрытие, уровни заголовков
- **embeddings**: модель эмбеддингов, размер батча
- **qdrant**: параметры подключения и HNSW индекса
- **rag**: параметры LLM и поиска

### Пошаговая инструкция

#### Шаг 1: Загрузка README-файлов из репозиториев GitHub
```bash
python source/pull_readmies.py
```

#### Шаг 2: Построение индекса - разбиение на чанки и создание эмбеддингов
```bash
python source/build_index.py
```

#### Шаг 3: Загрузка эмбеддингов в Qdrant
```bash
python source/load_to_vector_store.py
```

Или для пересоздания коллекции:
```bash
python source/load_to_vector_store.py --rebuild
```

#### Шаг 4: RAG-запрос к LLM - генерация ответа на вопрос с использованием найденного контекста:

```bash
python source/rag_pipeline.py "What is backpropagation?"
```

#### Шаг 5: Оценка качества retrieval
```bash
python source/run_evaluation.py
```