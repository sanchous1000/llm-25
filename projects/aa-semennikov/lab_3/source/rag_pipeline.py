import os
import sys
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import click
import time
import uuid
sys.path.insert(0, str(Path(__file__).parent.parent))
from source.utils import load_config
from source.embeddings import DenseEmbedder
from qdrant_client import QdrantClient
from openai import OpenAI
from dotenv import load_dotenv
from langfuse import Langfuse, get_client
load_dotenv()


class RAGPipeline:
    
    def __init__(self, config_path = './data/config.yaml'):
        self.config = load_config(config_path)
        qdrant_config = self.config.get('qdrant', {})
        self.host = qdrant_config.get('host', 'localhost')
        self.port = qdrant_config.get('port', 6333)
        self.collection_name = qdrant_config.get('collection_name', 'documents')
        self.ef_search = qdrant_config.get('hnsw', {}).get('ef_search', 100)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã RAG
        rag_config = self.config.get('rag', {})
        self.top_k = rag_config.get('top_k', 5)
        self.llm_model = rag_config.get('llm_model', 'llama3.1:8b')
        self.temperature = rag_config.get('temperature', 0.7)
        self.max_tokens = rag_config.get('max_tokens', 512)
        
        # Ollama
        ollama_config = rag_config.get('ollama', {})
        self.ollama_base_url = ollama_config.get('base_url', 'http://localhost:11434/v1')
        self.ollama_api_key = ollama_config.get('api_key', 'pass')
        
        # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ QDrant
        self.client = QdrantClient(host=self.host, port=self.port)
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–µ—Ä
        self.embedder = DenseEmbedder(self.config['embeddings']['dense'])
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º LLM –∫–ª–∏–µ–Ω—Ç (Ollama —á–µ—Ä–µ–∑ OpenAI API)
        self.llm_client = OpenAI(
            base_url=self.ollama_base_url,
            api_key=self.ollama_api_key
        )
        
        langfuse_secret_key = os.getenv("LANGFUSE_SECRET_KEY")
        langfuse_public_key = os.getenv("LANGFUSE_PUBLIC_KEY")
        langfuse_host = os.getenv("LANGFUSE_BASE_URL")
        
        self.langfuse = Langfuse(
            secret_key=langfuse_secret_key,
            public_key=langfuse_public_key,
            base_url=langfuse_host
        )

    
    def search_relevant_chunks(self, query, top_k = None, trace = None):
        if top_k is None:
            top_k = self.top_k
        
        if trace is not None:
            span_context = trace.start_as_current_observation(
                as_type="span",
                name="search_relevant_chunks",
                input={"query": query, "top_k": top_k},
                metadata={"collection_name": self.collection_name}
            )
        else:
            from contextlib import nullcontext
            span_context = nullcontext()
        
        with span_context as span:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            model_name = self.config['embeddings']['dense']['model'].lower()
            
            if 'bge' in model_name:
                query_text = f"Represent this sentence for searching relevant passages: {query}"
            elif 'e5' in model_name:
                query_text = f"query: {query}"
            else:
                query_text = query
            
            embeddings_result = self.embedder.embed_texts([query_text])
            query_vector = embeddings_result['dense'][0].tolist()
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –≤ QDrant
            results = self.client.query_points(
                collection_name=self.collection_name,
                query=query_vector,
                limit=top_k,
                with_payload=True
            ).points
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            chunks = []
            for i, result in enumerate(results):
                payload = result.payload
                chunks.append({
                    'rank': i + 1,
                    'score': result.score,
                    'document': payload.get('document', 'unknown'),
                    'chunk_id': payload.get('chunk_id', 'unknown'),
                    'text': payload.get('text', ''),
                    'metadata': payload.get('metadata', {})
                })
            
            # –û–±–Ω–æ–≤–ª—è–µ–º span —Å –≤—ã—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–≤–∫–ª—é—á–∞—è –ø–æ–ª–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
            if trace is not None and span is not None:
                span.update(
                    output={
                        "num_chunks": len(chunks),
                        "chunks": [
                            {
                                "rank": c["rank"],
                                "score": c["score"],
                                "document": c["document"],
                                "chunk_id": c["chunk_id"],
                                "text": c["text"],
                                "metadata": c["metadata"]
                            }
                            for c in chunks
                        ]
                    }
                )
            
            return chunks


    def build_prompt(self, query, chunks, trace = None):
        if trace is not None:
            span_context = trace.start_as_current_observation(
                as_type="span",
                name="build_prompt",
                input={"query": query, "num_chunks": len(chunks)},
                metadata={
                    "chunk_documents": [c["document"] for c in chunks],
                    "chunk_scores": [c["score"] for c in chunks]
                }
            )
        else:
            from contextlib import nullcontext
            span_context = nullcontext()
        
        with span_context as span:
            # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏
            system_instruction = """–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞:
1. –í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –∏–∑—É—á–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–∫–æ–Ω—Ç–µ–∫—Å—Ç).
2. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –æ–ø–∏—Ä–∞—è—Å—å –¢–û–õ–¨–ö–û –Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
3. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º.
4. –í –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —É–∫–∞–∂–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: –∏–∑ –∫–∞–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤–∑—è—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è.
5. –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏ –ø–æ–Ω—è—Ç–Ω—ã–º."""

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –∏–∑ —á–∞–Ω–∫–æ–≤
            contexts = []
            for chunk in chunks:
                doc_name = chunk['document']
                text = chunk['text']
                metadata = chunk['metadata']
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                extra_info = []
                if 'page' in metadata:
                    extra_info.append(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞: {metadata['page']}")
                if 'slide' in metadata:
                    extra_info.append(f"–°–ª–∞–π–¥: {metadata['slide']}")
                if 'section' in metadata:
                    extra_info.append(f"–†–∞–∑–¥–µ–ª: {metadata['section']}")
                
                extra_str = ", ".join(extra_info) if extra_info else ""
                context_header = f"[–î–æ–∫—É–º–µ–Ω—Ç: {doc_name}"
                if extra_str:
                    context_header += f", {extra_str}"
                context_header += "]"
                contexts.append(f"{context_header}\n{text}")
            
            context_block = "\n\n---\n\n".join(contexts)
            # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            prompt = f"""{system_instruction}

=== –ö–û–ù–¢–ï–ö–°–¢ –ò–ó –î–û–ö–£–ú–ï–ù–¢–û–í ===

{context_block}

=== –í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø ===

{query}

=== –û–¢–í–ï–¢ ===

"""
            
            # –û–±–Ω–æ–≤–ª—è–µ–º span —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–æ–º–ø—Ç–µ
            if trace is not None and span is not None:
                span.update(
                    output={
                        "prompt_length": len(prompt),
                        "context_length": len(context_block),
                        "num_contexts": len(contexts)
                    },
                    metadata={
                        "system_instruction_length": len(system_instruction)
                    }
                )
            
            return prompt

    
    def generate_answer(self, prompt, trace = None):  
        # –°–æ–∑–¥–∞–µ–º generation –¥–ª—è LLM –≤—ã–∑–æ–≤–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –º–µ–Ω–µ–¥–∂–µ—Ä–æ–º (–µ—Å–ª–∏ trace –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω)
        if trace is not None:
            generation_context = trace.start_as_current_observation(
                as_type="generation",
                name="llm_generation",
                model=self.llm_model,
                model_parameters={
                    "temperature": self.temperature,
                    "max_tokens": self.max_tokens
                },
                input=prompt
            )
        else:
            from contextlib import nullcontext
            generation_context = nullcontext()
        
        with generation_context as generation:
            try:
                response = self.llm_client.chat.completions.create(
                    model=self.llm_model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=self.temperature,
                    max_tokens=self.max_tokens
                )
                
                usage = response.usage
                answer_text = response.choices[0].message.content.strip()
                
                # –û–±–Ω–æ–≤–ª—è–µ–º generation —Å –≤—ã—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏, usage –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
                if trace is not None and generation is not None:
                    generation.update(
                        output=answer_text,
                        usage={
                            "prompt_tokens": usage.prompt_tokens,
                            "completion_tokens": usage.completion_tokens,
                            "total_tokens": usage.total_tokens
                        },
                        metadata={
                            "model": self.llm_model
                        }
                    )
                
                return {
                    'success': True,
                    'answer': answer_text,
                    'model': self.llm_model,
                    'tokens': {
                        'prompt': usage.prompt_tokens,
                        'completion': usage.completion_tokens,
                        'total': usage.total_tokens
                    },
                }
                
            except Exception as e:
                # –û–±–Ω–æ–≤–ª—è–µ–º generation —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± –æ—à–∏–±–∫–µ
                if trace is not None and generation is not None:
                    generation.update(
                        output=None,
                        level="ERROR",
                        status_message=str(e)
                    )
                
                return {
                    'success': False,
                    'error': str(e),
                    'answer': None
                }


    
    def format_citations(self, chunks):
        citations = []
        
        for i, chunk in enumerate(chunks, 1):
            doc_name = chunk['document']
            metadata = chunk['metadata']
            text_snippet = chunk['text'][:200] + "..." if len(chunk['text']) > 200 else chunk['text']
            citation = f"{i}. **{doc_name}**"
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            extras = []
            if 'page' in metadata:
                extras.append(f"—Å—Ç—Ä–∞–Ω–∏—Ü–∞ {metadata['page']}")
            if 'slide' in metadata:
                extras.append(f"—Å–ª–∞–π–¥ {metadata['slide']}")
            if 'section' in metadata:
                extras.append(f"—Ä–∞–∑–¥–µ–ª '{metadata['section']}'")
            
            if extras:
                citation += f" ({', '.join(extras)})"
            
            citation += f"\n   –§—Ä–∞–≥–º–µ–Ω—Ç: \"{text_snippet}\""
            citation += f"\n   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {chunk['score']:.3f}"
            citations.append(citation)
        
        return "\n\n".join(citations)
    
    def answer_question(self, query, top_k = None, session_id = None):
        with self.langfuse.start_as_current_observation(
            as_type="span",
            name="rag_query",
            input={"query": query, "top_k": top_k},
            metadata={
                "model": self.llm_model,
                "temperature": self.temperature,
                "max_tokens": self.max_tokens
            }
        ) as trace:
            # –ü—Ä–∏–≤—è–∑—ã–≤–∞–µ–º trace –∫ —Å–µ—Å—Å–∏–∏
            if session_id:
                trace.update_trace(session_id=session_id)
            
            # 1. –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
            chunks = self.search_relevant_chunks(query, top_k, trace)
            
            if not chunks:
                result = {
                    'query': query,
                    'answer': "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å.",
                    'sources': [],
                    'success': False
                }
                trace.update(output=result, level="WARNING")
                return result
            
            # 2. –°–±–æ—Ä–∫–∞ –ø—Ä–æ–º–ø—Ç–∞
            prompt = self.build_prompt(query, chunks, trace)
            # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            llm_result = self.generate_answer(prompt, trace)
            
            if not llm_result['success']:
                result = {
                    'query': query,
                    'answer': f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {llm_result.get('error', 'Unknown error')}",
                    'sources': [],
                    'success': False
                }
                trace.update(output=result, level="ERROR")
                return result
            
            # 4. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–∏—Ç–∞—Ç
            citations = self.format_citations(chunks)
            # 5. –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = {
                'query': query,
                'answer': llm_result['answer'],
                'sources': citations,
                'metadata': {
                    'num_sources': len(chunks),
                    'model': llm_result['model'],
                    'tokens': llm_result['tokens'],
                    'timestamp': datetime.now().isoformat()
                },
                'success': True
            }
            
            # –ó–∞–≤–µ—Ä—à–∞–µ–º trace —Å —Ñ–∏–Ω–∞–ª—å–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
            trace.update(
                output={
                    "answer": result['answer'],
                    "num_sources": result['metadata']['num_sources'],
                    "tokens": result['metadata']['tokens']
                },
                metadata={
                    "timestamp": result['metadata']['timestamp']
                }
            )
            
            return result


def print_result(result):
    """–ö—Ä–∞—Å–∏–≤–æ –≤—ã–≤–æ–¥–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç RAG –ø–∞–π–ø–ª–∞–π–Ω–∞."""
    print("\n" + "="*80)
    print(f"–í–û–ü–†–û–°: {result['query']}")
    print("="*80)
    
    if not result['success']:
        print(f"\n‚ùå {result['answer']}")
        return
    
    print(f"\nüìù –û–¢–í–ï–¢:\n")
    print(result['answer'])
    print(f"\n\nüìö –ò–°–¢–û–ß–ù–ò–ö–ò ({result['metadata']['num_sources']}):\n")
    print(result['sources'])
    print("\n" + "-"*80)
    metadata = result['metadata']
    print(f"–ú–æ–¥–µ–ª—å: {metadata['model']}")

@click.command()
@click.argument('question', required=True)
@click.option('--top_k', '-k', type=int, default=None, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞')
@click.option('--config', '-c', default='./data/config.yaml', help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏')
@click.option('--output', '-o', type=click.Path(), help='–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ JSON —Ñ–∞–π–ª')
@click.option('--session_id', '-s', type=str, default=None, help='–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–µ—Å—Å–∏–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≤ Langfuse (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –Ω–æ–≤—ã–π)')
def main(question, top_k, config, output, session_id):
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º session_id –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ CLI, –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
    if session_id is None:
        session_id = str(uuid.uuid4())
        print(f"üîë –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è —Å–µ—Å—Å–∏—è: {session_id}")
    else:
        print(f"üîë –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–µ—Å—Å–∏—è: {session_id}")
    
    rag = RAGPipeline(config_path=config)
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞
    result = rag.answer_question(question, top_k, session_id=session_id)
    print_result(result)

if __name__ == "__main__":
    main()