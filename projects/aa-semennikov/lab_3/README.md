# Лабораторная работа №3
## Описание задания

В рамках лабораторной работы был реализован мониторинг и трейсинг RAG-системы с использованием платформы **Langfuse**. 

Работа включает следующие основные компоненты:
1. **Создание датасета** (`create_dataset.py`) - преобразование ground truth данных в датасет Langfuse для экспериментов
2. **Оценка качества** (`langfuse_eval.py`) - запуск экспериментов с автоматическим вычислением метрик
3. **RAG-пайплайн** (`rag_pipeline.py`) - полный цикл обработки запросов с трейсингом
4. **Построение индекса** (`build_index.py`, `load_to_vector_store.py`) - подготовка векторной базы данных

## Использованные технологии

- **Платформы и сервисы**: `Langfuse`, `Ollama`, `Qdrant`
- **LLM**: `qwen2.5:0.5b`, `llama3.1:8b`
- **Библиотеки**: `langfuse`, `qdrant-client`, `openai`, `sentence-transformers`, `transformers`

## Результаты работы

### Архитектура системы мониторинга

Система реализует полный цикл мониторинга LLM-приложений:

1. **Создание датасета**: Ground truth данные, собранные в ЛР№2, преобразуются в структурированный датасет Langfuse, где каждый элемент содержит вопрос (input) и список релевантных ID документов (expected_output).

2. **Трейсинг запросов**: Все запросы к LLM отслеживаются через Langfuse с сохранением:
   - Входных данных (промпты, параметры)
   - Выходных данных (ответы модели)
   - Метаданных (модель, сессия)
   - Временных меток

3. **Детальная трассировка RAG**: трассируется поиск релевантных чанков в векторной базе данных, сборка промпта с контекстом и генерация ответа языковой моделью

4. **Эксперименты**: Автоматическая оценка качества retrieval на датасете с вычислением метрик

### Особенности реализации

Каждый этап RAG-пайплайна отслеживается как отдельная observation с типом:
- `generation` - для операций генерации (эмбеддинги, ответы LLM)
- `span` - для промежуточных операций (поиск, сборка промпта)

Все observations связаны через `trace_id`, что позволяет видеть полный путь выполнения запроса.

### Метрики оценки качества

Для оценки качества retrieval используются следующие метрики:
- **Precision@K** - точность извлечения на K документах
- **Recall@K** - полнота извлечения на K документах
- **MRR** - средняя обратная позиция первого релевантного документа

Все метрики автоматически вычисляются для каждого запроса в эксперименте и сохраняются в Langfuse.

### Сравнение результатов для разных параметров retrieval

Были проведены эксперименты с различными значениями параметра `top_k` (количество извлекаемых документов) для оценки влияния на качество и производительность системы.

**Таблица результатов экспериментов:**

| Параметр | MRR | Precision@3 | Recall@3 | Latency (среднее) |
|----------|-----|-------------|----------|-------------------|
| top_k=3  | 0.8667 | 0.4000 | 0.5000 | 6 мин 49 сек |
| top_k=5  | 0.8667 | 0.2800 | 0.6000 | 12 мин 9 сек |

**Анализ результатов:**

1. **MRR (Mean Reciprocal Rank)**: Остается неизменным (0.8667) для обоих значений top_k, что говорит о том, что первый релевантный документ находится на одинаковой позиции независимо от количества извлекаемых документов.

2. **Precision@3**:
   - При top_k=3: 0.4000 (40% извлеченных документов релевантны)
   - При top_k=5: 0.2800 (28% извлеченных документов релевантны)
   - Снижение precision ожидаемо, так как увеличение количества извлекаемых документов приводит к добавлению менее релевантных результатов.

3. **Recall@3**:
   - При top_k=3: 0.5000 (найдено 50% релевантных документов)
   - При top_k=5: 0.6000 (найдено 60% релевантных документов)
   - Увеличение recall на 10% показывает, что дополнительные документы помогают найти больше релевантной информации.

4. **Latency (задержка)**:
   - При top_k=3: 6 мин 49 сек
   - При top_k=5: 12 мин 9 сек
   - Увеличение времени обработки почти в 2 раза связано с необходимостью обработки большего объема контекста в LLM.

**Выводы по экспериментам:**

- **Компромисс precision-recall**: Система демонстрирует классический trade-off между точностью и полнотой. Top_k=3 обеспечивает более точные результаты, а top_k=5 — более полный охват релевантных документов.
- **Производительность**: Удвоение времени обработки при top_k=5 делает этот вариант менее предпочтительным для интерактивных приложений.
- **Рекомендации**: Для большинства сценариев оптимальным является **top_k=3**, так как обеспечивает хороший баланс между качеством (MRR=0.8667, recall=0.5) и скоростью работы.

**Возможности улучшения RAG-системы:**

1. **Оптимизация retrieval**:
   - Реализация гибридного поиска (dense + sparse embeddings) для улучшения recall
   - Использование reranking моделей для повышения precision
   - Настройка порогов релевантности для фильтрации слабо релевантных документов

2. **Улучшение качества эмбеддингов**:
   - Применение domain-specific моделей эмбеддингов
   - Fine-tuning модели эмбеддингов на специфичных данных проекта
   - Использование более мощных моделей (например, с большей размерностью векторов)

3. **Оптимизация промптов**:
   - A/B тестирование различных форматов системных инструкций
   - Адаптивное формирование контекста в зависимости от типа вопроса
   - Добавление примеров (few-shot learning) для улучшения качества ответов

4. **Повышение производительности**:
   - Кэширование эмбеддингов популярных запросов
   - Параллельная обработка запросов
   - Использование более быстрых LLM для генерации или оптимизация параметров inference

5. **Расширение функциональности**:
   - Добавление поддержки multimodal данных (изображения, таблицы)
   - Реализация conversational RAG с учетом истории диалога
   - Внедрение механизма обратной связи для continuous learning

## Выводы

Реализована полная интеграция платформы мониторинга Langfuse с RAG-системой, что позволяет:

1. **Отслеживать все этапы работы**: от создания эмбеддинга запроса до генерации финального ответа
2. **Оценивать качество retrieval**: автоматическое вычисление метрик Precision, Recall и MRR для каждого запроса
3. **Проводить эксперименты**: систематическая оценка качества на датасетах с сохранением истории
4. **Анализировать производительность**: выявление узких мест через детальный трейсинг каждого этапа
5. **Централизованное хранение**: все запросы, ответы и метрики доступны для поиска и анализа

RAG-пайплайн разбит на логические этапы (embed_query → search_results → create_prompt → generate_LLM), что обеспечивает прозрачность работы системы и упрощает отладку.

**Результаты экспериментов** показали, что система демонстрирует высокое качество ранжирования (MRR=0.8667) и позволяет гибко настраивать баланс между точностью и полнотой результатов через параметр top_k. Оптимальным для большинства сценариев является значение top_k=3, обеспечивающее хорошее соотношение качества (precision=0.40, recall=0.50) и производительности (6 мин 49 сек на запрос).

## Инструкция по запуску

### Предварительные требования

1. **Установить Langfuse** (локально или использовать облачную версию):
```bash
git clone https://github.com/langfuse/langfuse.git
cd langfuse/
docker compose up
```

2. **Создать файл `.env`** в корне проекта с ключами доступа для LangFuse и Qdrant:
```bash
LANGFUSE_PUBLIC_KEY=pk-...
LANGFUSE_SECRET_KEY=sk-...
LANGFUSE_BASE_URL=...

QDRANT_HOST=...
QDRANT_PORT=...
```

3. **Установить зависимости**:

```bash
pip install -r requirements.txt
```

4. **Запустить Qdrant** (если используется локально):

```bash
cd qdrant
docker compose up -d
```

### Пошаговая инструкция

#### Шаг 1: Подготовка данных и индексация

1. Собрать корпус документов (README файлы из репозиториев):

```bash
python source/pull_readmies.py
```

2. Построить индекс и загрузить в Qdrant:

```bash
python source/build_index.py
python source/load_to_vector_store.py
```

#### Шаг 2: Создание датасета в Langfuse

```bash
python source/create_dataset.py
```

#### Шаг 3: RAG-запрос с трейсингом

```bash
python source/rag_pipeline.py "Как установить и использовать nanoGPT?" --top_k 5
```

**Результат**: В Langfuse создается trace с четырьмя этапами:
1. `search_relevant_chunks` - поиск релевантных чанков в Qdrant
2. `build_prompt` - сборка промпта с контекстом
3. `llm_generation` - генерация ответа языковой моделью
4. `rag_query` - общий span, объединяющий все этапы

#### Шаг 4: Оценка качества через эксперименты

```bash
python source/langfuse_eval.py -e "experiment_1" -d "dataset_name" -k 5
```

Или с автоматическим созданием датасета:

```bash
python source/langfuse_eval.py -e "experiment_1" -d "dataset_name" -k 5 --create-dataset
```