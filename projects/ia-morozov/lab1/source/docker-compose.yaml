x-vllm-base: &vllm_base
    image: vllm/vllm-openai:latest
    environment:
      - HF_HOME=/models_cache
      - HF_TOKEN=${HF_TOKEN:-}
      - https_proxy=${https_proxy:-}
    volumes:
      - ./models_cache:/models_cache
      - ./default.json:/default.json
    ports:
      - "8000:8000"
    runtime: nvidia

services:
  qwen:
    <<: *vllm_base
    command: >
      --model Qwen/Qwen2.5-7B-Instruct
      --served-model-name qwen2.5
      --gpu-memory-utilization 0.9
      --generation-config vllm 
      --download-dir /models_cache 
      --dtype bfloat16
  ministral:
    <<: *vllm_base
    command: >
      --model mistralai/Ministral-8B-Instruct-2410
      --served-model-name ministral
      --gpu-memory-utilization 0.9
      --generation-config vllm 
      --download-dir /models_cache 
      --dtype bfloat16
  yandex:
    <<: *vllm_base
    command: >
      --model yandex/YandexGPT-5-Lite-8B-instruct
      --served-model-name yandex
      --gpu-memory-utilization 0.9 
      --download-dir /models_cache
      --generation-config vllm
      --dtype bfloat16

