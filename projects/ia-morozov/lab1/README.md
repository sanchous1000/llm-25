# Лабораторная работа №1: Развёртывание и эмпирический анализ LLM

## Описание задания

В рамках данной лабораторной работы было проведено исследование трех LLM из различных семейств: **Qwen**, **Gemma** и **Ministral**. Модели были развернуты с использованием фреймворка **vLLM**.

Для эмпирического анализа были подготовлены три стандартизированных промпта:
1.  **Генерация (P1)**:

```
[
    {
        "role": "system",
        "content": "Твоя задача быть вежливым ассистентом компании AZOT, твое имя Боб, укажи что ты ИИ ассистент",
    },
    {
        "role": "user",
        "content": "Напиши письмо нашему клиенту Стиву, о том, что его доставка AZOT, а именно заказ номер 123321"
        "задерживается в связи с (придумай необычную причину) на 2 года. К сожалению он не сможет вернуть заказ, так как (придумай почему)."
        "Но скажи не расстраиваться и предложи купон на скидку 100 руб на заказ от 10 тыс. руб.",
    },
]
```

2.  **Классификация (P2)**:

```
[
    {
        "role": "system",
        "content": "Ты менеджер в системе поддержки службы доставки. Классифицируй сообщение от клиента по списку меток: ['Оплата', 'Доставка', 'Возврат', 'Аккаунт', 'Скидки', 'Прочее']."
        "Вернуть необходимо одну метку в формате JSON, например: '{\"class\": \"Оплата\"}'",
    },
    {
        "role": "user",
        "content": "Здравствуйте. Вчера я делал заказ, где указывал оплату карточкой. Помимо того, что все привезли холодным, еще и помятым."
        "Что случилось? Почему так плохо стало? Я в тот же день просил промокод, но ответа не получил. Пишу во второй раз.",
    },
]
```

3.  **Извлечение (P3)**:

```
[ 
    {
        "role": "system",
        "content": "Твоя задача извлекать информацию для карточки товара из его описания. Отвечай в формате: - **Атрибут1**: Значение1\n - **Атрибут2**: Значение2",
    },
    {
        "role": "user",
        "content": "Новый смартфон iPhone 16 Pro Max оснащен дисплеем Super Retina XDR диагональю 6,9 дюйма с частотой обновления 120 Гц и разрешением 2796×1290 пикселей. Корпус выполнен из титана пятого класса и доступен в четырех цветах: серебристый, графитовый, синий и бежевый. Процессор A18 Pro с 6‑ядерным GPU обеспечивает прирост производительности на 20% по сравнению с предыдущим поколением. Объем оперативной памяти составляет 8 ГБ, встроенной памяти — 256, 512 ГБ или 1 ТБ. Основная камера на 48 Мп поддерживает съемку видео в 4K при 60 кадрах в секунду, фронтальная — 12 Мп с улучшенной стабилизацией. Аккумулятор ёмкостью 4500 мА·ч обеспечивает до 29 часов воспроизведения видео. Поддерживается быстрая зарядка мощностью до 30 Вт и беспроводная — до 15 Вт через MagSafe. Смартфон защищен по стандарту IP68 и способен выдерживать погружение в воду на глубину до 6 метров в течение 30 минут.Рекомендованная цена на старте продаж — от 1399 долларов США. Старт продаж в России запланирован на 27 сентября 2025 года.",
    },
]
```

Каждая модель тестировалась в двух конфигурациях: с базовыми гиперпараметрами (все равны 1.0) и с настроенными - `tuned`:
- `temperature`: **0.5**
- `top_p`: **0.6**
- `repetition_penalty`: **1.1**

Целью такой настройки было преобразовать поведение модели из творческого генератора в более сфокусированного и точного ассистента.

## Использованные технологии и модели

- **LLM**:
  - `Qwen/Qwen2.5-7B-Instruct`
  - `google/gemma-3-4b-it`
  - `mistralai/Ministral-8B-Instruct-2410`
- **Развертывание**:
  - `Docker` на вм с NVIDIA A30
  - `vLLM`
- **Тестирование**:
  - `Python`
  - `Библиотека OpenAI`

## Результаты работы

![График](source/assets/output.png)

### Метрики

1.  **TPS**: Скорость генерации стала меньше (хотя можно сказать, что это на уровне погрешности).

    P.S. Хотя если подумать, то presence_penalty 1.1 заставила модели подбирать токены, возможно скорость генерации и уменьшилась.
2.  **Количество токенов**: Применение настроенных параметров по-разному повлияло на модели:
    - **Gemma** малость сократила объем ответа.
    - **Qwen** продемонстрировала значительный рост количества токенов.
    - **Ministral** не показала существенных изменений в объеме генерации.

### Ответы

#### P1 - Генерация
- В базовой конфигурации **Qwen** и **Ministral** сгенерировали ответы с артефактами (включение китайских символов), что указывает на недостаточную стабильность. Настроенные параметры полностью устранили эту проблему.
- **Gemma** в обеих конфигурациях сгенерировала хороший текст на русском языке. Но сам ответ особо не поменялся.
#### P2 - Классификация
- Все модели справились с задачей в обеих конфигурациях, вернув корректный JSON. Метка была выбрана неверно, во всех случаях 'Доставка', хотя подразумевалось 'Скидка'. Изменение параметров не повлияло на результат. 

    P.S. Gemma все время добавляла обоснование, хотя ее никто не просил.

#### P3 - Извлечение
- Все модели продемонстрировали схожее поведение. С настроенными параметрами улучшилось качество извлечения информации, представляя данные в более гранулярном и структурированном виде по сравнению с базовой конфигурацией.

Таблица ответов получилась большой, поэтому поместил ее [сюда](source/assets/table.md)

## Выводы

Настройка `temperature`, `top_p` и `repetition_penalty` является эффективным инструментом для повышения стабильности и предсказуемости моделей. Она позволяет устранить артефакты генерации (как в случае с Qwen и Ministral) и повысить качество структурированных ответов.

## Инструкция по запуску

Нужен установленный Docker и uv.

P.S. Для Gemma обязательно нужно задать `HF_TOKEN` env переменную.

1. Запустить модель через команду `docker comppose up qwen|ministral|gemma`
2. Запустить файл main.py через команду `uv run main.py`. После выполнения в папке `results` появится результат в формате `.csv`
3. Остановить модель `docker compose down`
4. Повторить шаги 1-3 для каждой из моделей.
5. Зайти в `main.ipynb`, выполнить все ячейки.

