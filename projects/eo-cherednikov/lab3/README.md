# Отчет по лабораторной работе №3

## 1. Описание задания

В рамках лабораторной работы была реализована система мониторинга и трейсинга LLM-приложений с использованием платформы **Langfuse**. Работа направлена на:

- Интеграцию Langfuse для отслеживания работы RAG-системы
- Создание датасетов для оценки качества системы
- Трейсинг отдельных запросов к LLM (запросы к Ollama)
- Детальный трейсинг RAG-запросов с разбиением на этапы
- Автоматическую оценку качества retrieval с использованием экспериментов Langfuse

Работа включает четыре основных компонента:
1. **Создание датасета** (`create_dataset.py`) - преобразование ground truth данных в датасет Langfuse для экспериментов
2. **Оценка качества** (`evaluate.py`) - запуск экспериментов с автоматическим вычислением метрик
3. **Трейсинг Ollama** (`trace_ollama.py`) - отслеживание простых запросов к языковой модели
4. **Трейсинг RAG-запросов** (`trace_request.py`) - детальная трассировка всех этапов RAG-пайплайна

## 2. Использованные технологии и модели

### Платформы и сервисы:
- **Langfuse** - платформа для мониторинга, трейсинга и оценки LLM-приложений
- **Ollama** - локальный сервер для запуска LLM 
- **Qdrant** - векторная база данных, **qdrant-client** - клиент для работы

### LLM и модели:
- **nomic-embed-text** - модель для создания эмбеддингов
- **qwen3:1.7b** - языковая модель для генерации ответов


## 3. Результаты работы

### Архитектура системы мониторинга

Система реализует полный цикл мониторинга LLM-приложений:

1. **Создание датасета**: Ground truth данные, собранные в ЛР№2, преобразуются в структурированный датасет Langfuse, где каждый элемент содержит вопрос (input) и список релевантных ID документов (expected_output).

2. **Трейсинг запросов**: Все запросы к LLM отслеживаются через Langfuse с сохранением:
   - Входных данных (промпты, параметры)
   - Выходных данных (ответы модели)
   - Метаданных (модель, пользователь, сессия)
   - Временных меток

3. **Детальная трассировка RAG**: RAG-пайплайн разбит на отдельные наблюдаемые этапы:
   - **embed_query** - создание эмбеддинга запроса
   - **search_results** - поиск в векторной БД
   - **create_prompt** - сборка промпта с контекстом
   - **generate_LLM** - генерация ответа языковой моделью

4. **Эксперименты**: Автоматическая оценка качества retrieval на датасете с вычислением метрик (Recall, Precision, MRR).

### Особенности реализации

Каждый этап RAG-пайплайна отслеживается как отдельная observation с типом:
- `generation` - для операций генерации (эмбеддинги, ответы LLM)
- `span` - для промежуточных операций (поиск, сборка промпта)

Все observations связаны через `trace_id`, что позволяет видеть полный путь выполнения запроса.
Также каждый запрос обогащается метаданными:
- `user_id` - идентификатор пользователя
- `session_id` - идентификатор сессии
- `metadata` - дополнительные данные (например, окружение)

## 4. Выводы

Реализована полная интеграция платформы мониторинга с существующей RAG-системой, что позволяет отслеживать все аспекты работы приложения. RAG-пайплайн разбит на логические этапы, каждый из которых отслеживается отдельно. Это позволяет выявлять узкие места в производительности, анализировать качество работы каждого компонента и отлаживать проблемы на конкретных этапах. Все запросы и ответы сохраняются в централизованном хранилище с возможностью поиска, фильтрации и анализа.

## 5. Инструкция по запуску

### Предварительные требования

1. **Установить Langfuse** (локально или использовать облачную версию):
```bash
git clone https://github.com/langfuse/langfuse.git
cd langfuse/
docker compose up
```

2. **Создать файл `.env`** в корне проекта с ключами доступа для LangFuse и Qdrant:
```bash
LANGFUSE_PUBLIC_KEY=pk-...
LANGFUSE_SECRET_KEY=sk-...
LANGFUSE_BASE_URL=https://cloud.langfuse.com

QDRANT_HOST=
QDRANT_PORT=
```

3. **Установить зависимости**:
```bash
pip install langfuse python-dotenv
```


### Пошаговая инструкция

#### Шаг 1: Создание датасета в Langfuse
```bash
python scripts/prepare_data.py --collection vllm_docs
```

#### Шаг 2: Трейсинг простого запроса к Ollama
```bash
python scripts/trace_ollama.py --query "What is machine learning?" --model qwen3:1.7b
```

#### Шаг 3: Трейсинг RAG-запроса
```bash
python scripts/trace_request.py 
  --question "How to install vLLM?" 
  --embed_model nomic-embed-text 
  --llm_model qwen3:1.7b 
  --collection vllm_docs 
  --top_k 5
```

**Результат**: В Langfuse создается trace с четырьмя observations:
1. `embed_query` - создание эмбеддинга запроса
2. `search_results` - поиск в Qdrant
3. `create_prompt` - сборка промпта с контекстом
4. `generate_LLM` - генерация ответа

#### Шаг 4: Оценка качества через эксперименты
```bash
python scripts/evaluate.py
```