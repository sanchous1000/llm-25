# Лабораторная работа №1

## Описание задания
Сгенерировать на 3 моделях из разных семейства по три промпта в двух режимах: базовый и с тюнингом минимум двух результатов

## Использованные технологии и модели
- LLM: Ollama, mistral, qwen
- Библиотеки: requests, time, json, os collections, statistics

## Результаты работы
Были получены и сохранены 18 ответов, также был написан отчёт о результатах работы

## Выводы
1. Указание малого максимального количества токенов вынудило qwen писать рекомендации для написания письма, вместо написания полноценного, 
llama же просто обрезало письмо не дописывая его, соблюдая ограничение на количество токенов, тоже самое касается и mistral.
2. Среднее время ответов для qwen ~20 секунд, для llama ~ 10 секунд и для mistral ~ 5 секунд
3. Также при изменении температуры на значение 0,2 все модели стали как будто более уверенными в ответах, при изменении параметров top_p и repeat_penalty трудно выяснить результат, надо было брать по два гиперпараметра, а не 4))  
4. В базовом режиме qwen как будто более творческий и развёрнутый чем llama и mistral, то есть у него выше температура и максимальное количество токенов, в то же время у mistral как будто большее ограничение по токенам и меньшая температура, чем у llama, 
также у mistral как будто хуже работает токенизация русских текстов, чем у LLama, так как слова дробятся на большее количество токенов, что приводит к более мелкому ответу в плане слов, но большему по токенам.

## Инструкция по запуску
1. Запустить скрипт: `python LLM.py`
2. Запустить скрипт: `python count_result_metrics.py`