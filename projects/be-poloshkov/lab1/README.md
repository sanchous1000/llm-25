# Лабораторная работа №1

## Описание задания

Развертывание и эмпирический анализ трех LLM из разных семейств. Тестирование моделей на трех типах задач (генерация, классификация, извлечение информации) в двух режимах: с базовыми параметрами и с измененными гиперпараметрами.

## Использованные технологии и модели

**LLM:**
- Qwen 2.5 3B (`qwen2.5:3b`)
- Llama 3.2 3B (`llama3.2:3b`)
- Mistral 7B (`mistral:7b`)

**Инструменты:**
- Ollama (локальный запуск моделей с OpenAI-совместимым API)
- Python 3.10+
- openai (Python SDK для работы с API)

## Тестовые промпты

### P1: Генерация текста
Написание вежливого письма коллеге с просьбой перенести встречу из-за срочной командировки.

### P2: Классификация
Определение категории обращения клиента из набора: `["Billing", "Tech support", "Sales"]`.
Тестовое обращение касалось двойного списания средств (корректный ответ: Billing).

### P3: Извлечение информации
Извлечение структурированных данных из описания товара в формате JSON.

## Гиперпараметры

**Базовый режим:** параметры по умолчанию Ollama (temperature ~0.8, top_p ~0.9).

**Режим с тюнингом:**
- `temperature`: 0.3
- `top_p`: 0.85

## Результаты экспериментов и сравнительный анализ

### Качество генерации текста на русском языке

| Модель | Качество русского | Проблемы |
|--------|-------------------|----------|
| Qwen 2.5 3B | Хорошее | Редкие галлюцинации (китайский) |
| Llama 3.2 3B | Низкое | Постоянный code-switching |
| Mistral 7B | Высокое | Минимальные |

### Точность классификации

| Модель | Base    | Tuned |
|--------|---------|-------|
| Qwen 2.5 3B | Верно   | Верно |
| Llama 3.2 3B | Неверно | Верно |
| Mistral 7B | Верно   | Верно |

### Качество структурированного вывода (JSON)

| Модель | Base | Tuned |
|--------|------|-------|
| Qwen 2.5 3B | С ошибками и комментариями | Валидный, с комментариями |
| Llama 3.2 3B | Галлюцинации, избыточность | Ошибки в данных (null) |
| Mistral 7B | Чистый JSON | Чистый JSON |

### Скорость генерации

| Модель | Base (среднее tok/s) | Tuned (среднее tok/s) | Прирост |
|--------|----------------------|-----------------------|---------|
| Qwen 2.5 3B | 23.63 | 33.77 | +43% |
| Llama 3.2 3B | 26.26 | 36.34 | +38% |
| Mistral 7B | 13.32 | 23.59 | +77% |

## Влияние гиперпараметров

### Temperature (0.8 → 0.3)

Снижение temperature уменьшает случайность при выборе следующего токена:

1. Меньше галлюцинаций - Qwen перестал вставлять иероглифы
2. Улучшение точность - llama начала правильно классифицировать 
3. Ответы более предсказуемы

### Top-p (0.9 → 0.85)

Сужение пула токенов-кандидатов:

1. Сокращение избыточности
2. Модели реже добавляют свои дополнительные размышления
3. Быстрее генерация

### Комбинированный эффект

Совместное применение низкой temperature и top_p:
- Увеличение скорости генерации
- Повышение качества на задачах с однозначным ответом (классификация)
- Более лаконичный вывод без потери информативности
- Устранение языковых артефактов (галлюцинации, code-switching частично)

## Выводы

1. **Mistral 7B** — победитель, минимум ошибок, хорошее качество

2. **Qwen 2.5 3B** — неплохое качество и скорость генерации, есть галлюцинации 

3. **Llama 3.2 3B** — серьезные проблемы с точностью и русским языком.

Влияние тюнинга:

- Снижение temperature до 0.2-0.4 устраняет галлюцинации
- Top-p 0.8-0.9 убирает избыточность

## Инструкция по запуску

1. Установить Ollama: https://ollama.ai

2. Загрузить модели:
```bash
ollama pull qwen2.5:3b
ollama pull llama3.2:3b
ollama pull mistral:7b
```

3. Убедиться, что Ollama запущен (по умолчанию на порту 11434).

4. Установить зависимости:
```bash
pip install -r source/requirements.txt
```

5. Запустить скрипт:
```bash
python source/main.py
```

6. Результаты будут сохранены в `results/results.json`.
