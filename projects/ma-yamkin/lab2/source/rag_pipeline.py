import json
import argparse
import numpy as np
from pathlib import Path
from sentence_transformers import SentenceTransformer
import faiss
from openai import OpenAI

from utils import load_config

OLLAMA_BASE_URL = "http://localhost:11434/v1"
API_KEY = "pass"

MODES = {
    "basic": {"temperature": 0.8, "max_tokens": 128, "repeat_penalty": 1.1},
    "tuned": {"temperature": 0.3, "max_tokens": 256, "repeat_penalty": 1.3}
}

client = OpenAI(base_url=OLLAMA_BASE_URL, api_key=API_KEY)


def query_model_with_metrics(model: str, prompt: str, params: dict) -> dict:
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            # temperature=params["temperature"],
            # max_tokens=params["max_tokens"],
            # extra_body={"repeat_penalty": params["repeat_penalty"]}
        )

        answer = response.choices[0].message.content.strip()

        return {
            "answer": answer,
        }
    except Exception as e:
        return {
            "answer": f"[–û–®–ò–ë–ö–ê: {str(e)}]",
        }


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', type=str, required=True)

    args = parser.parse_args()

    config = load_config()

    config_hash = '5790b8cf'
    artifacts_dir = Path(f"../artifacts/index_{config_hash}")
    eval_dir = Path(f"../eval")
    results_path = eval_dir / "results.json"

    if not artifacts_dir.exists():
        raise RuntimeError(
            f"–ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω: {artifacts_dir}. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ build_index.py –∏ load_to_vector_store.py")

    print("–ó–∞–≥—Ä—É–∑–∫–∞ FAISS-–∏–Ω–¥–µ–∫—Å–∞...")
    index = faiss.read_index(str(artifacts_dir / "faiss.index"))
    with open(artifacts_dir / "metadata.json", encoding="utf-8") as f:
        chunks = json.load(f)

    embedder = SentenceTransformer(config["embedding"]["model"])
    top_k = config["retrieval"]["top_k"]

    if args.mode == 'test':
        with open(eval_dir / "questions.json", encoding="utf-8") as f:
            questions = json.load(f)
    else:
        questions = [{'question': str(input())}]

    results = []

    for q in questions:
        question = q['question']

        query_vec = embedder.encode(question)
        faiss.normalize_L2(query_vec.reshape(1, -1))

        distances, indices = index.search(query_vec.reshape(1, -1).astype(np.float32), k=top_k)

        contexts = []
        sources = []
        for idx in indices[0]:
            chunk = chunks[idx]
            text = chunk["text"]
            arxiv_id = chunk.get("arxiv_id", "unknown")
            page = chunk.get("page", "unknown")

            contexts.append(text)
            sources.append((arxiv_id, page))

        if not contexts:
            answer_text = "–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤."
            print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.")
        else:
            context_block = "\n\n".join(
                f"[–ò—Å—Ç–æ—á–Ω–∏–∫: arXiv:{arxiv_id}, —Å—Ç—Ä. {page}]\n{ctx}"
                for (arxiv_id, page), ctx in zip(sources, contexts)
            )

            prompt = f"""–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è –¢–û–õ–¨–ö–û –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—É—é –Ω–∏–∂–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.
                –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π. –ï—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ ‚Äî –Ω–∞–ø–∏—à–∏ "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç".

                –í–æ–ø—Ä–æ—Å: {question}

                –ö–æ–Ω—Ç–µ–∫—Å—Ç:
                {context_block}

                –û—Ç–≤–µ—Ç:
            """

            answer = query_model_with_metrics("mistral:7b-instruct-v0.3-q4_0", prompt, MODES['basic'])
            answer_text = answer['answer']

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ–ø—Ä–æ—Å –∏ –æ—Ç–≤–µ—Ç
        results.append({
            "question": question,
            "answer": answer_text,
            "sources": [{"arxiv_id": arxiv_id, "page": page} for arxiv_id, page in sources]
        })

        print("\n" + "=" * 60)
        print("–û—Ç–≤–µ—Ç:")
        print(answer_text)
        print("\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏:")
        for (arxiv_id, page) in sources:
            print(f"- arXiv:{arxiv_id}, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON
    eval_dir.mkdir(exist_ok=True)
    with open(results_path, "w", encoding="utf-8") as f_out:
        json.dump(results, f_out, ensure_ascii=False, indent=2)

    print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {results_path}")


if __name__ == "__main__":
    main()