# Лабораторная работа №2

## Описание задания

В рамках первой лабораторной работы необходимо спарсить источники данных для системы RAG, после чего разбить текст по чанкам и создать векторную базу данных. После чего необходимо создать пайплайн для векторной базы данных.

## Использованные технологии и модели

- LLM: qwen2.5:3b.
- Библиотеки: sentence_transformers, numpy, langchain_text_splitters, faiss, xml, urrlib, re, fitz, openai.

## Результаты работы

В рамках лабораторной работы были написаны:
1. Парсеры пдф
2. Функция для разбиения пдф на чанки
3. Функция для создания векторной базы данных
4. RAG пайплайн
5. Функция для оценки релевантности извлеченных чанков

***Таблица со сравнение метрик:***

| Файл | recall (avg) | precision (avg) | mrr (avg) |
|------|--------------|------------------|-----------|
| metrics_7e74c146.json | 0.700 | 0.140 | 0.600 |
| metrics_5790b8cf.json | 1.000 | 0.200 | 0.715 |
| metrics_d44dbaee.json | 0.700 | 0.140 | 0.650 |

***Таблица с ответами модели на вопросы:***

| Вопрос | Ответ |
|--------|-------|
| What is the main contribution of the proposed method in addressing data scarcity for Named Entity Recognition (NER) in low-resource settings? | The main contribution of the proposed method in addressing data scarcity for Named Entity Recognition (NER) in low-resource settings is introducing a strategic data augmentation technique that preserves entity information while paraphrasing the surrounding context, thereby expanding the training data without compromising semantic relationships. |
| What advantages does the proposed PANER framework offer over existing zero-shot NER models like GNER and SLIMER in terms of computational efficiency and out-of-domain performance? | The proposed PANER framework offers several advantages over existing zero-shot NER models like GNER and SLIMER in terms of computational efficiency and out-of-domain performance. First, theframework achieves comparable performance with state-of-the-art zero-shot NER models while requiring significantly fewer computational resources. Second, the framework shows consistent improvements in F1 scores (up to 17 points) over baseline versions when using the paraphrasing augmentation technique. This indicates that PANER is effective at leveraging limited available data and can offer a promising solution for organizations with restricted access to annotated datasets and compute power. In addition, the framework demonstrates improved performance in domain-specific settings through strategic paraphrasing of the surrounding context which preserves entity information and expands training data without compromising semantic relationships. Therefore, it is beneficial in low-resource situations where generalization to out-of-domain entities is crucial. |
| What is the primary purpose of the MT-Video-Bench benchmark introduced in the paper, and how does it differ from previous evaluation datasets for multimodal large language models (MLLMs)? | The primary purpose of the MT-Video-Bench benchmark introduced in the paper is to comprehensively evaluate the "Perceptivity" and "Interactivity" of Multimodal Large Language Models (MLLMs) in multi-turn video-grounded dialogues. It differs from previous evaluation datasets for multimodal large language models as it primarily focuses on evaluating the multi-turn dialogue capabilities, while overlooking the video understanding capabilities that were the focus of other benchmarks. MT-Video-Bench is designed to mimic real-world interactive scenarios, emphasizing contextual coherence, cross-scene video comprehension, and adaptive interactivity, and it includes 987 meticulously curated multi-turn dialogues with 5,805 QA pairs. |
| How does the number of video frames affect model performance across different capabilities in the MT-Video-Bench evaluation, and what does this reveal about the role of visual context in multi-turn video dialogue reasoning? | In the MT-Video-Bench evaluation, increasing the number of video frames may have varying effects on model performance depending on the resolution. Within a certain range, higher resolution can enhance the model's ability to capture visual details; however, excessive resolution might lead to a decline in performance due to the increased number of input tokens that may exceed the modelвЂ™s optimal processing capacity.<br><br>Regarding the role of visual context in multi-turn video dialogue reasoning as revealed by the study:<br>- Model performance generally decreases as video length increases, suggesting that longer videos pose greater challenges for capturing and reasoning over multi-turn dialogues' content.<br>- More turns within the dialogue tend to improve model performance, but this improvement is dependent on the conversation's total number of turns and the efficiency with which models integrate contextual information for coherent dialogue states (e.g., larger models may handle long conversations more efficiently).<br>- Contextual information plays a crucial role in model performance, as providing meticulously curated golden dialogue history yields higher results than self-predicted or no context at all.<br>This insight underscores the need for models that can reason over long contexts while engaging in adaptive and coherent conversations involving video content and multi-turn dialogues. |
| What distinguishes AcademicEval from related frameworks like ResearchTown and WildLong in terms of its design goals, data sourcing, and approach to mitigating label leakage in long-context LLM evaluation? | What distinguishes AcademicEval from related frameworks like ResearchTown and WildLong is its design goals, data sourcing, and approach to mitigating label leakage in long-context LLM evaluation.<br><br>1. Design Goals: Unlike ResearchTown which focuses on simulating collaborative behavior and ensuring realistic outputs under controlled settings, AcademicEval is a live benchmark designed specifically for evaluating the performance of Long Language Models (LLMs) on hierarchical writing tasks in authentic academic papers. It aims to measure LLMs' long-context reasoning and generation abilities on real-world academic tasks, unlike WildLong which focuses on data synthesis for instruction tuning.<br>2. Data Sourcing: AcademicEval leverages the ArXiv repository as its main data source, whereas ResearchTown does not explicitly mention where it sources its data from. WildLong also uses user queries to extract meta-information and build co-occurrence graphs but does not focus on authentic academic papers like AcademicEval.<br>3. Approach to Mitigating Label Leakage: AcademicEval is designed as a live-updating benchmark that continuously incorporates newly published ArXiv papers, making the evaluation set evolve over time and reducing the likelihood of performance being dominated by pre-encoded knowledge. This mitigation strategy contrasts with the approaches used in ResearchTown and WildLong, which do not provide explicit information on how they handle label leakage issues during training or testing of LLMs. |
| How do human-judged preferences for academic writing tasks (e.g., Title, Abstract, Introduction, Related Work) differ from automatic metrics like BERTScore and ROUGE-L, and what does this reveal about the strengths of RALM versus non-RALM models across different tasks? | Human-judged preferences for academic writing tasks differ from automatic metrics like BERTScore and ROUGE-L in several ways. Human judgment considers broader qualitative aspects beyond lexical or semantic overlap, focusing more on higher-level writing quality such as style, coherence, feasibility, and academic style. On the other hand, automaticmetrics like BERTScore and ROUGE-L primarily reward overlap between the generated and reference texts.<br><br>The experiment shows that model performance depends on task nature and qualitative dimensions. Some models (like Hermes) perform better in short tasks where strong style and concision are valued, while others (like Mixtral) perform better at longer lengths when given correlated contexts. However, RALM models (Gemma and LLaMA) are not consistently preferred across all tasks, suggesting that aggressive retrieval may not always align with judged quality in certain contexts such as title writing.<br><br>The study also reveals the complementarity of these two evaluations: automatic metrics reward overlap, while human judgment emphasizes higher-level writing quality. This suggests that integrating both types of evaluation can lead to more comprehensive and accurate assessments of machine-generated academic writing. |
| What is the primary advantage of the proposed multi-threading kernel for Spiking Neural Networks (SNNs) in neuromorphic edge applications, and how does it improve upon previous implementations in terms of performance, energy efficiency, and core utilization? | The primary advantage of the proposed multi-threading kernel for Spiking Neural Networks (SNNs) is its ability to improve performance, energy efficiency, and core utilization in neuromorphic edge applications. It provides speed-up gains over single thread processing by a factor of four on moderately sized SNNs and 1.7X on a Synfire network, and it load-balances all cores available on multi-core processors such as ARM, which are used in today's mobile devices. This results in an energy efficiency improvement of up to 70% compared to static core assignment. The kernel enables the development of edge applications that have low Size, Weight, and Power (SWaP), and can prototype the integration of neuromorphic chips. These advantages are achieved through multi-threading, dynamic load balancing, performance monitoring, and the creation of a synthetic load network named Chainfire. |
| How does the multi-threading kernel in CARLsim achieve performance gains on neuromorphic edge applications, and what are the observed speed-up factors for the Chainfire and Synfire networks when scaling across multiple CPU cores? | The multi-threading kernel in CARLsim achieves performance gains on neuromorphic edge applications by scaling efficiently over available cores of modern CPUs, such as ARM Cortex-76 in Raspberry Pi 5, addressing the performance limitations of PThreads in CARLsim and other SNN simulators. The kernel includes multi-threading for SNNs that scale independently of network partitioning, dynamic load balancing to avoid synchronization bottlenecks, a performance monitor with ms precision for SNNs, and a synthetic load network named Chainfire.<br><br>The observed speed-up factors for the Chainfire network when scaling across multiple CPU cores are not explicitly stated in the provided information, so it's impossible to provide exact speed-up factors for this network; however, there are performance comparisons shown for both Chainfire and Synfire networks on Intel i9 and several ARM Cortex processors.<br><br>The specific speed-up factors observed for the Synfire network when scaling across multiple CPU cores can be seen in the figure (b) of the provided document, which shows a speed-up factor of approximately 7 for going from 1 to 8 cores on an Intel i9-11900K @ 3.5 GHz.<br><br>Keep in mind that this analysis is based solely on the information provided and does not make any assumptions or extrapolate beyond what's presented. |
| How does the proposed approach in this study enhance the predictability of Sea Ice Concentration (SIC) and Sea Ice Velocity (SIV) compared to prior deep learning methods, and what role does the physics-informed training scheme play in improving the HIS-Unet framework? | The proposed approach in this study enhances the predictability of Sea Ice Concentration (SIC) and Sea Ice Velocity (SIV) compared to prior deep learning methods because it integrates physical knowledge into the model through a physics-informed neural network (PINN) approach. This is done by designing physics loss functions that regulate SIV and SIC values, modifying the output layer to guarantee physically valid SIC values, and adding an additional activation function to the output layer in the HIS-Unet framework. The physics-informed training scheme plays a critical role in improving the performance of the HIS-Unet. However, the specific improvement it provides is not specified in the context text you've provided. |
| How do the Weighting Attention Modules (WAMs) facilitate information sharing between the Sea Ice Velocity (SIV) and Sea Ice Concentration (SIC) branches in the proposed multi-task U-Net architecture, and what role do channel and spatial attention mechanisms play within each WAM? | The Weighting Attention Modules (WAMs) facilitate information sharing between the Sea Ice Velocity (SIV) and Sea Ice Concentration (SIC) branches in the proposed multi-task U-Net architecture by multiplying linear weights to the input SIV and SIC feature maps, determining the shared information (Оѕin,share), then passing this shared information through channel attention and spatial attention modules. The channel attention mechanism highlights the meaningful channels within the shared information, while the spatial attention mechanism emphasizes informative parts of the shared data. This process is described in more detail in Koo and Rahnemoonfar (2024). |


## Выводы

RAG пайплайн отрабатывает корректно, извлекаются релевантные чанки. Также наиболее корректные параметры для RAG:   

- splitter: "markdown_recursive" (рекурсивное разбиение чанков)
- chunk_size: 512
- chunk_overlap: 8
- include_headers_in_chunk: true

## Инструкция по запуску
1. Установить зависимости: `pip install -r requirements.txt`
2. Запустить скрипт: `python source/fetch_arxiv_pdfs.py`
3. Запустить скрипт: `python source/parser.py`
4. Запустить скрипт: `python source/build_index.py`
5. Запустить скрипт: `python source/load_to_vector_store.py`
6. Запустить скрипт: `python source/evaluate_retrieval.py` (для оценки метрик)
7. Запустить скрипт: `python source/rag_pipeline.py --mode 'test''` чтобы протестировать на подготовленных заранее вопросах или `python source/rag_pipeline.py --mode 'ask''` чтобы задать вопрос в RAG