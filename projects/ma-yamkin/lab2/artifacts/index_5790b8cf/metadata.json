[
  {
    "text": "PANER: A Paraphrase-Augmented Framework for\nLow-Resource Named Entity Recognition\n1st Nanda Kumar Rengarajan\nConcordia University\nnanda.kumark@mail.concordia.ca\n2nd Jun Yan\nConcordia University\njun.yan@concordia.ca\n3rd Chun Wang\nConcordia University\nchun.wang@concordia.ca\nAbstract—Named Entity Recognition (NER) is a critical task\nthat requires substantial annotated data, making it challenging in\nlow-resource scenarios where label acquisition is expensive. While\nzero-shot and instruction-tuned approaches have made progress,\nthey often fail to generalize to domain-specific entities and do not\neffectively utilize limited available data. We present a lightweight\nfew-shot NER framework that addresses these challenges through\ntwo key innovations: (1) a new instruction tuning template with a\nsimplified output format that combines principles from prior IT\napproaches to leverage the large context window of recent state-\nof-the-art LLMs; (2) introducing a strategic data augmentation\ntechnique that preserves entity information while paraphrasing\nthe surrounding context, thereby expanding our training data\nwithout compromising semantic relationships. Experiments on\nbenchmark datasets show that our method achieves performance\ncomparable to state-of-the-art models on few-shot and zero-\nshot tasks, with our few-shot approach attaining an average F1\nscore of 80.1 on the CrossNER datasets. Models trained with\nour paraphrasing approach show consistent improvements in\nF1 scores of up to 17 points over baseline versions, offering\na promising solution for groups with limited NER training data\nand compute power.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_001.md",
    "arxiv_id": "2510.17720v1",
    "page": "1",
    "title": "unknown",
    "id": "2510.17720v1_page_001"
  },
  {
    "text": "Learning, Large Language Models (LLMs), Instruction Tuning,\nData Augmentation.\nI. INTRODUCTION\nNamed Entity Recognition (NER) is a foundational task\nin Natural Language Processing (NLP), enabling applications\nlike information extraction, question answering, and event\ndetection [1]. Traditional NER systems rely on supervised\nlearning, requiring extensive annotated data for specific do-\nmains and predefined entity types. This dependency on large,\nlabelled datasets limits their adaptability to new domains and\nentity categories. Recent breakthroughs in Large Language\nModels (LLMs) have enabled more flexible NER approaches\nthrough instruction tuning, demonstrating promising zero-\nshot and few-shot capabilities without extensive labelled data.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_001.md",
    "arxiv_id": "2510.17720v1",
    "page": "1",
    "title": "unknown",
    "id": "2510.17720v1_page_001"
  },
  {
    "text": "strong generalization across diverse entity types. However,\nthese methods often underperform in specialized domains and\nface practical constraints, either requiring substantial compu-\ntational resources or suffering from slow inference times [4].",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_001.md",
    "arxiv_id": "2510.17720v1",
    "page": "1",
    "title": "unknown",
    "id": "2510.17720v1_page_001"
  },
  {
    "text": "have introduced key innovations that inspire our approach.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_001.md",
    "arxiv_id": "2510.17720v1",
    "page": "1",
    "title": "unknown",
    "id": "2510.17720v1_page_001"
  },
  {
    "text": "prompts, incorporating definitions and annotation guidelines to\nimprove performance on unseen entities. GNER [5] highlights\nFig. 1. Illustration of paraphrasing-based data augmentation process.\nthe importance of negative instances, improving contextual un-\nderstanding and entity boundary delineation by including non-\nentity text. We combine the strengths of both approaches—the\nnegative instance inclusion suggested by GNER [5] and the\nguideline-centric philosophy of SLIMER [4] — to create an\ninstruction-tuning template that clearly defines entity bound-\naries and types. Our framework adopts a simplified ”word/tag”\noutput format, reducing complexity, particularly in low-data\nscenarios. This integration balances robust entity representa-\ntion with efficient domain adaptation.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_001.md",
    "arxiv_id": "2510.17720v1",
    "page": "1",
    "title": "unknown",
    "id": "2510.17720v1_page_001"
  },
  {
    "text": "by modern instruction-tuned models. Specifically, we leverage\nthe 128k context windows of Qwen-2.5-Instruct (7B) [6] and\nLLAMA-3.1-Instruct (8B) [7], as well as the 32k context\nwindow of Falcon3-Instruct (10B) [8], enabling our framework\nto process longer and more complex inputs efficiently. These\nextended context lengths, combined with enriched instruction\ntuning, provide a foundation for robust and scalable NER\narXiv:2510.17720v1  [cs.CL]  20 Oct 2025",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_001.md",
    "arxiv_id": "2510.17720v1",
    "page": "1",
    "title": "unknown",
    "id": "2510.17720v1_page_001"
  },
  {
    "text": "performance across diverse and challenging tasks.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_002.md",
    "arxiv_id": "2510.17720v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17720v1_page_002"
  },
  {
    "text": "dressing limited training data in NER. Traditional methods fo-\ncus on generating augmented samples through back-translation\n[9], and entity-controlled generation using question-answering\ntechniques [10]. MELM [28] introduced a data augmentation\nstrategy that injects entity labels into the training context,\nreducing token-label misalignment and improving entity di-\nversity, particularly in low-resource and multilingual NER\nsettings.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_002.md",
    "arxiv_id": "2510.17720v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17720v1_page_002"
  },
  {
    "text": "text surrounding entities. This preserves semantic relationships\nwhile expanding the linguistic variety, as illustrated in Figure\n1. LLM-DA [11] has already demonstrated the effectiveness of\nlarge language models in generating diverse training examples\nby employing context-level rewriting strategies. Our approach\nbuilds on this idea by also working on sentences with multiple\nentities, ensuring paraphrased variants remain semantically\nconsistent and merging entity representations to refine NER\nmodel predictions. Overall, this technique improves model\nadaptability to domain-specific entities and helps bridge the\nperformance gap in low-resource settings.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_002.md",
    "arxiv_id": "2510.17720v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17720v1_page_002"
  },
  {
    "text": "NER [12] and MIT [13], demonstrate that our zero-shot\nframework achieves comparable performance to state-of-the-\nart zero-shot models while requiring fewer computational re-\nsources. Models using the paraphrasing augmentation consis-\ntently outperform baseline versions (without augmented data),\nvalidating the data strategy. Our key contributions include:\n• An instruction tuning template that combines negative\ninstance style annotation with definition- and guideline-\nbased schema.\n• A controlled paraphrasing technique for generating high-\nquality samples combined with an easily replicable low-\nresource training method for domain-specific NER.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_002.md",
    "arxiv_id": "2510.17720v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17720v1_page_002"
  },
  {
    "text": "II reviews related work in Named Entity Recognition (NER),\nwith a focus on zero-shot and few-shot learning using Large\nLanguage Models (LLMs). Section III describes the proposed\nmethodology, detailing the paraphrasing-based data augmen-\ntation framework and the implementation details, including\noptimization strategies and validation techniques. Section IV\ntalks about the instruction tuning approach and template-used.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_002.md",
    "arxiv_id": "2510.17720v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17720v1_page_002"
  },
  {
    "text": "selection, baseline models, and evaluation metrics. Section VI\nreports the experimental results and provides a comparative\nanalysis of model performance. Section VII discusses key\nfindings, limitations, and implications for future research.\nII. RELATED WORK\nNamed Entity Recognition (NER) has traditionally been\napproached as a sequence labelling task, where models are\ntrained to assign BIO (Beginning, Inside, Outside) tags to input\ntokens [1]. While supervised approaches using BERT-based\narchitectures have shown strong performance, they remain\nconstrained by their reliance on predetermined label sets and\ndomain-specific training data [14].",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_002.md",
    "arxiv_id": "2510.17720v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17720v1_page_002"
  },
  {
    "text": "introduced new paradigms for addressing NER challenges,\nparticularly in zero-shot and few-shot scenarios. Early work\nby [15] demonstrated LLMs’ capacity for multi-task learning\nthrough natural language instructions, laying the groundwork\nfor what would become known as in-context learning and\nprompt engineering. However, initial attempts to apply LLMs\nto Information Extraction tasks, including NER, revealed\nsignificant limitations compared to traditional supervised ap-\nproaches [2], [16].",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_002.md",
    "arxiv_id": "2510.17720v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17720v1_page_002"
  },
  {
    "text": "fer limitations across domains. Recent research has explored\nvarious instruction-tuning strategies to enhance LLMs’ perfor-\nmance on NER tasks. Notable approaches include InstructUIE\n[2], which utilizes a T5-11B architecture fine-tuned on infor-\nmation extraction datasets, and UniNER [3], which employs a\nconversational template with LLAMA. These methods have\nshown promise but often require substantial computational\nresources for training and inference. GoLLIE [17] introduced\nan innovative approach by incorporating annotation guidelines\nsuch as Python docstrings, marking the first attempt to encode\nlabelling criteria within the prompt structure explicitly. While\ninstruction tuning has emerged as a leading method for gen-\neralization to unseen tasks [18], [19] current approaches face\nseveral limitations.\n1) Many methods rely solely on label names in prompts\nwithout considering domain-specific definitions or com-\nplex label semantics [20].\n2) The computational requirements of these models often\nmake them impractical for organizations with limited\nresources. Additionally, the challenge of effectively uti-\nlizing available domain data remains largely unaddressed\nin current instruction-tuning frameworks.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_002.md",
    "arxiv_id": "2510.17720v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17720v1_page_002"
  },
  {
    "text": "performance trade-off in NER systems. GNER [5] intro-\nduced a modified BIO-like generation approach that improves\nboundary detection while addressing classification indecision.\nGLiNER [21] demonstrated that smaller, non-instruction-tuned\narchitectures could achieve competitive performance in both\nsupervised and zero-shot settings, suggesting the potential for\nmore resource-efficient approaches.\nIII. PROPOSED METHOD\nOur data augmentation step, illustrated in Figure 1, was\ndeveloped to address a fundamental challenge in Named Entity\nRecognition (NER) - the scarcity of annotated training data.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_002.md",
    "arxiv_id": "2510.17720v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17720v1_page_002"
  },
  {
    "text": "paraphrases, using the prompt shown in Fig. 2. We chose this\nmodel for its ability to match the performance of larger models\nlike LLAMA 3.1-405B while operating with substantially\nfewer parameters (70B vs 405B), making it more practical\nfor real-world applications [23].",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_002.md",
    "arxiv_id": "2510.17720v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17720v1_page_002"
  },
  {
    "text": "Task Description:\nYou are a helpful assistant. I have a sentence with\ncertain entities that I want to preserve in spirit, but\nyou may modify the sentence slightly to add variety.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_003.md",
    "arxiv_id": "2510.17720v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17720v1_page_003"
  },
  {
    "text": "1) Read the Original Sentence provided.\n2) Create 2 new sentences (variants) that:\n• DO NOT MODIFY any word enclosed in\n<<>> tags or move them around (do not\nintroduce any new <<>> tags that weren’t\nin the original).\n• May adjust phrasing, structure, or add con-\ntextual details while maintaining logical co-\nherence and meaning.\n• Minor modifications are allowed, but retain\nthe core entity references and do not trans-\nform them into something else.\n3) Return the output in a valid JSON format with\nthe generated variants.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_003.md",
    "arxiv_id": "2510.17720v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17720v1_page_003"
  },
  {
    "text": "Fig. 2. Prompt used for generating paraphrases.\nA. Paraphrasing Framework\nThe core of our approach involves transforming input\nsentences into masked templates where named entities are\nreplaced with semantic placeholders. For example, given the\ninput sentence “John visited the supermarket on Tuesday,”\nthe system generates a masked version: “<PER>visited the\n<LOC>on Tuesday.” This masking preserves the essential\nstructure of the sentence while marking entity positions for\nconsistent paraphrasing. Using these masked templates, the\nLLAMA 3.3-70B model generates variations that maintain the\noriginal entity relationships while introducing some diversity.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_003.md",
    "arxiv_id": "2510.17720v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17720v1_page_003"
  },
  {
    "text": "• <PER> stopped by <LOC> last Tuesday\n• On Tuesday, <PER> went to <LOC>\n• Tuesday saw <PER> traveling to <LOC>\nThrough experimental evaluation, we determined that gen-\nerating two paraphrased versions per input sentence achieved\nthe optimal balance between diversity and quality. Attempts\nto produce three or more variations frequently resulted in re-\ndundancy or significant deviations from the original meaning.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_003.md",
    "arxiv_id": "2510.17720v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17720v1_page_003"
  },
  {
    "text": "to the specified output formatting requirements (JSON format)\nand occasionally produced sentences with an inconsistent\nnumber of <ENT>tags compared to the input. Based on\nthese observations, we limited the generation to two variants.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_003.md",
    "arxiv_id": "2510.17720v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17720v1_page_003"
  },
  {
    "text": "generate additional variations by adjusting the temperature\nparameter during generation.\nB. Implementation and Optimization\nWe have iterated over the paraphrasing prompt multiple\ntimes and included optimizations to enhance the reliability of\nthe proposed paraphrasing system. One key improvement in-\nvolved the handling of consecutive entity tags. When multiple\nwords belonging to the same entity type appear in sequence\n(for example, a four-word organization name), we consolidate\nthem into a single entity tag rather than using multiple\nconsecutive tags. This simplification reduced the complexity\nof the paraphrasing task and improved the model’s ability to\nmaintain entity consistency.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_003.md",
    "arxiv_id": "2510.17720v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17720v1_page_003"
  },
  {
    "text": "cific tags like <PER>or <LOC>. However, this approach\nproved less effective as the model lacked sufficient context\nabout the type of entity being masked. Specific entity tags\nprovided better guidance for the paraphrasing process, partic-\nularly in domain-specific contexts where entity relationships\nare more nuanced.\nC. Quality Control and Validation\nTo ensure the quality of generated paraphrases, a structured\nvalidation pipeline was developed using the instructor package\nand a locally hosted version of LLAMA. Our system processes\nthe model’s output in JSON format, allowing efficient parsing\nand validation of the generated paraphrases. For each para-\nphrase, we verify that:\n1) The number of entity tags matches the input sentences.\n2) The semantic relationships between entities are pre-\nserved (cosine similarity).",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_003.md",
    "arxiv_id": "2510.17720v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17720v1_page_003"
  },
  {
    "text": "the system either triggers a regeneration with adjusted param-\neters or attempts to map the entities correctly based on their\nposition and context in the original sentence. This validation\nprocess helps maintain the integrity of the augmented dataset\nwhile allowing for natural variations in sentence structure and\nword choice. The complete paraphrasing prompt is shown in\nFig. 2, where we instruct the model to maintain entity refer-\nences while allowing for structural variations and additional\ncontext.\nIV. INSTRUCTION TUNING AND ADAPTATION OF PROMPT\nDESIGN\nIn this work, we revisit and refine the instruction-tuning\nmethodologies outlined in GNER [5], diverging from the\ntraditional BIO tagging schema in favour of a word/tag repre-\nsentation format. The proposed method annotates each word\nwith its corresponding entity tag using a forward slash (/)\nseparator, simplifying the tagging process by removing the\ncomplexity of distinguishing between “B-” and “I-” labels.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_003.md",
    "arxiv_id": "2510.17720v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17720v1_page_003"
  },
  {
    "text": "same tag. Furthermore, we provide detailed definitions and\nguidelines, the same way as SLIMER [4], for each entity type\nto enhance the extraction.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_003.md",
    "arxiv_id": "2510.17720v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17720v1_page_003"
  },
  {
    "text": "porating negative instances, as outlined by GNER [5]. This\ntechnique helps the model differentiate between entity and",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_003.md",
    "arxiv_id": "2510.17720v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17720v1_page_003"
  },
  {
    "text": "Task Description:\nPlease analyze the sentence provided, identifying the\ntype of entity for each word on a token-by-token basis.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_004.md",
    "arxiv_id": "2510.17720v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17720v1_page_004"
  },
  {
    "text": "its corresponding named entity tag, using a forward\nslash / between the word and the tag. Output format is:\nword_1/label_1, word_2/label_2, ...",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_004.md",
    "arxiv_id": "2510.17720v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17720v1_page_004"
  },
  {
    "text": "1) Use O for words that are not part of any named\nentity.\n2) For multi-word entities, label each word with the\nsame entity tag.\nUse the specific entity tags: l1, l2, . . . , lm, and O.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_004.md",
    "arxiv_id": "2510.17720v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17720v1_page_004"
  },
  {
    "text": "GUIDELINES for each entity tag.\n{ l1 : {\nDEFINITION : ,\nGUIDELINES : }\n}\nInput: x1\nx2\n. . .\nxn\nOutput: x1/ˆy1\nx2ˆy2\n. . .\nxn/ˆyn\nFig. 3. Prompt used for Instruction-tuning LLMs.\nnon-entity tokens by utilizing surrounding context, reducing\nreliance on direct memorization of entity names. By preserving\nthis contextual learning mechanism and combining it with our\nsimplified tagging format, we establish a robust framework\nthat enhances performance for entity recognition tasks. Table I\ncompares the result of different formats for instruction tuning.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_004.md",
    "arxiv_id": "2510.17720v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17720v1_page_004"
  },
  {
    "text": "NER [12] Science dataset with 16 entity types in the training\nprompt, including task description, annotations, and guidelines\nfor all NEs, added up to 1700 tokens, well below the context\nlength of the models used in our experiments. This token effi-\nciency allowed us to include comprehensive task instructions\nand entity definitions directly in the input prompts without\nexceeding the model’s context limit. The results demonstrate\nthat this format is not only feasible but also beneficial for\nfew-shot and zero-shot settings.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_004.md",
    "arxiv_id": "2510.17720v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17720v1_page_004"
  },
  {
    "text": "further optimize training, reducing computational and memory\nrequirements while achieving performance comparable to full\nfine-tuning methods. The goal is to achieve comparable or\nsuperior performance using a fraction of the resources, partic-\nularly for low-resource NER datasets.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_004.md",
    "arxiv_id": "2510.17720v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17720v1_page_004"
  },
  {
    "text": "a chunking strategy is used to split inputs into manageable\nsegments while preserving overall context. To ensure reliable\nresponse generation, the context length is limited to 2048\ntokens. As a result, any sequences exceeding this threshold\nare automatically segmented into multiple examples. Fig. 3\nshows the full instruction tuning prompt used.\nV. EXPERIMENT SETUP\nA. Datasets\nWe use PileNER [3] as the main training corpus, with key\npre-processing steps to ensure data quality and consistency.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_004.md",
    "arxiv_id": "2510.17720v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17720v1_page_004"
  },
  {
    "text": "criteria:\n1) Minimum sentence length threshold of 10 words to\nensure sufficient context.\n2) Language filtering to retain only English text.\n3) Entity type filtering to focus on 423 named entities with\nestablished guidelines and annotations, as documented\nby [4]\nThis filtered dataset yielded approximately 23,402 high-\nquality samples. To maintain experimental consistency with\nprior work [5], 10,000 samples are randomly selected from\nthis preprocessed pool as a starting point for our few-shot\ntesting.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_004.md",
    "arxiv_id": "2510.17720v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17720v1_page_004"
  },
  {
    "text": "benchmarks, each chosen to assess different aspects of model\nperformance:\n1) CrossNER [12]: A comprehensive cross-domain dataset\nthat evaluates domain adaptation capabilities across di-\nverse subject areas, including scientific papers, politics,\nmusic, and literature.\n2) MIT [13]: A standard benchmark for assessing out-of-\ndistribution (OOD) performance, particularly valuable\nfor evaluating generalization to novel domains.\n3) BUSTER [25]: A document-level financial domain NER\nbenchmark that presents unique challenges through its\nspecialized entity types and complex document struc-\nture.\n4) CoNLL [26]: CoNLL-2003 shared task dataset is a\nwidely-used benchmark for NER featuring multilingual\nannotated text with general tags like PERSON, LOCA-\nTION, and others.\nB. Baseline Comparisons\nTo evaluate the effectiveness of our approach, we compare it\nagainst several state-of-the-art methods for zero-shot and few-\nshot Named Entity Recognition (NER) and data augmentation.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_004.md",
    "arxiv_id": "2510.17720v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17720v1_page_004"
  },
  {
    "text": "architecture, providing a diverse comparison framework.\n1) GoLLIE [17]: A generative model based on Code-\nLLAMA, designed to leverage annotation guidelines\nformatted in a code-like representation. We use the 7B\nvariant of GoLLIE for comparability with other models\nin this study.\n2) GLiNER-L [21]: An encoder-only model based on De-\nBERTa with 304 million parameters. Despite being the\nsmallest model among the selected baselines, GLiNER-\nL has demonstrated competitive performance in out-of-\ndistribution (OOD) zero-shot NER tasks.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_004.md",
    "arxiv_id": "2510.17720v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17720v1_page_004"
  },
  {
    "text": "TABLE I\nCOMPARISON BETWEEN INSTRUCTION FORMATS\nAI\nLit\nMusic\nPol\nSci\nAvg\nGNER-BIO\n52.1\n51.1\n58.5\n54.1\n43.8\n51.92\nOurs-slash w/o*\n59.1\n67.4\n72.25\n70.8\n66.1\n67.13\nOurs-slash\n63.9\n67.2\n75.3\n67.8\n68.7\n68.58\n*w/o: prompt without guidelines\n3) GNER [5]: A model released in two variants, each\nleveraging a different backbone architecture:\n• GNER-T5: Based on flan-t5-xxl.\n• GNER-LLAMA: Built on the LLAMA-7B architec-\nture. Both versions emphasize the incorporation of\nentity definitions during instruction-tuning.\n4) SLIMER [4]: A model based on the LLAMA-2-7B\nchat architecture, fine-tuned with LoRA [24] for 10\nepochs. SLIMER integrates structured annotation guide-\nlines, making it a strong benchmark for guideline-based\nNER.\n5) DAGA [27]: DAGA utilizes a one-layer LSTM-based\nlanguage model trained on linearized labelled sentences\nfrom CoNLL and other sequence-tagging datasets to\ngenerate synthetic training data for the same.\n6) MELM [28]: a data augmentation framework that en-\nsures label-consistent entity replacements by fine-tuning\nXLM-RoBERTa with masked entity prediction.\nC. Backbone LLMs and Evaluation Framework\nWe used Qwen-2.5-Instruct (7B), LLAMA-3.1-Instruct\n(8B), and Falcon3-Instruct (10B) as our backbone models,\nselected based on their extended context lengths: 128K for\nQwen-2.5 and LLAMA-3.1, and 32K for Falcon3, respectively,\nas well as their state-of-the-art performance on instruction-\nfollowing benchmarks such as MT-Bench [29] and Alpaca WC\n[30].",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_005.md",
    "arxiv_id": "2510.17720v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17720v1_page_005"
  },
  {
    "text": "shot scenarios to assess model performance under varying\nresource constraints. For zero-shot evaluation, we fine-tuned\nthe models above on 23,402 pileNER samples, as described\nin Section V-A. Although we utilize more samples than [5],\nour training setup (described below) is significantly more\nefficient and effectively bridges the performance gap while still\nserving as a cost-effective solution. For few-shot evaluation,\nwe used 10,000 samples from pileNER as our base dataset and\nadded domain-specific examples from the benchmark datasets\n(CrossNER and MIT) as necessary.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_005.md",
    "arxiv_id": "2510.17720v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17720v1_page_005"
  },
  {
    "text": "All models were fine-tuned on the\nModal platform using Axolotl. Fine-tuning was conducted\nwith LoRA [24] settings of r = 8, α = 16, and the AdamW\noptimizer [31] for one epoch. A cosine learning rate schedule\nwas employed, starting with a warm-up phase covering 4% of\nthe training steps and peaking at 2 × 10−5.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_005.md",
    "arxiv_id": "2510.17720v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17720v1_page_005"
  },
  {
    "text": "in %) for CrossNER dataset [12].\nVI. RESULTS\nA. Comparison of Tagging Formats\nWe first validate the effectiveness of our word/tag format by\ncomparing it against the BIO-style output format presented in\n[5]. For this experiment, LLAMA 3.1-8B-Instruct was used\nas the backbone architecture, and we leveraged the same\nPileNER dataset [3] and filtered for sentences with more\nthan 10 words only in English, resulting in approximately\n23,402 samples for training. The model was fine-tuned with\nLoRA with the hyperparameters in Section V-C. To maintain\nconsistency with GNER [5], we evaluated the model on the\nsame five datasets, with 200 random samples per dataset, for\nzero-shot performance analysis. Results reported in Table I\nrepresent averages over three test runs to ensure robustness.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_005.md",
    "arxiv_id": "2510.17720v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17720v1_page_005"
  },
  {
    "text": "different tagging formats to determine the optimal approach.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_005.md",
    "arxiv_id": "2510.17720v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17720v1_page_005"
  },
  {
    "text": "attributed to our use of a larger model, the LLAMA 3.1-8B,\ncompared to their Flan-T5 large model with 780M parameters.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_005.md",
    "arxiv_id": "2510.17720v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17720v1_page_005"
  },
  {
    "text": "single epoch, in contrast to their full fine-tuning over three\nepochs, which may also contribute to the observed differences\nin performance.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_005.md",
    "arxiv_id": "2510.17720v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17720v1_page_005"
  },
  {
    "text": "formats, we employ entity-level F1 scores for both the BIO\nand word/tag formats to ensure fair comparison. While the\nformats differ in presentation, our evaluation criteria remain\nconsistent across both approaches: an entity prediction is\nconsidered correct only when both the entity type and its\ncomplete boundaries match the gold standard annotation. This\nmeans that in both formats, partial entity identification or\nincorrect boundary detection is treated as an error, regardless\nof whether the entity type was correctly identified.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_005.md",
    "arxiv_id": "2510.17720v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17720v1_page_005"
  },
  {
    "text": "when adopting the word/tag format compared to the tradi-\ntional BIO tagging schema. A slight increase in F1 score",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_005.md",
    "arxiv_id": "2510.17720v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17720v1_page_005"
  },
  {
    "text": "TABLE II\nFEW-SHOT F1 (%) SCORES USING AUGMENTED SAMPLES ACROSS DIFFERENT DOMAINS\nModel Family\nof\nOriginal\nSamples\nof\nAugmented\nSamples\nMovie\nRestaurant\nAI\nLiterature\nMusic\nPolitics\nScience\nAverage\nLLAMA-3.1-8B-Instruct\n0\n0\n43.3\n30.3\n59.4\n61.5\n68.2\n62.1\n62.3\n55.3\n100\n0\n45.1\n35.4\n61.0\n67.2\n75.9\n70.4\n70.9\n60.8\n100\n200\n64.2\n39.8\n64.0\n77.0\n80.8\n73.2\n72.9\n67.4\nQwen-2.5-7B-Instruct\n0\n0\n50.3\n33.6\n46.5\n54.9\n53.9\n54.0\n49.1\n48.9\n100\n0\n54.2\n25.6\n57.8\n63.4\n70.4\n64.2\n65.9\n57.4\n100\n200\n65.1\n38.2\n59.5\n73.3\n79.2\n70.2\n75.3\n65.8\nFalcon-3-10B-Instruct\n0\n0\n64.5\n38.1\n63.7\n58.8\n68.6\n61.8\n59.4\n59.3\n100\n0\n63.7\n37.0\n67.8\n67.6\n82.2\n72.0\n77.5\n66.8\n100\n200\n77.5\n42.8\n72.7\n79.0\n85.3\n81.3\n82.3\n74.4\nTABLE III\nCOMPARISON OF F1 (%) SCORES ON CROSSNER FOR SUPERVISED\nTECHNIQUES\nAI\nLit\nMusic\nPol\nSci\nAvg\nBERT\n68.7\n64.9\n68.3\n63.6\n58.8\n64.9\nCDLM\n68.4\n64.3\n63.5\n59.5\n53.7\n61.9\nDAPT\n72.0\n68.8\n75.7\n69.0\n62.6\n69.6\nNER-BERT\n76.1\n72.1\n80.2\n71.9\n63.3\n72.7\nPANER (Ours)\n72.7\n79\n85.3\n81.3\n82.3\n80.1\nTABLE IV\nCOMPARISON OF F1 (%) SCORES ON CROSSNER FOR AUGMENTATION\nCOMPOSITION WITH LLAMA - 3.1-8B-INSTRUCT\nAI\nLit\nMusic\nPol\nSci\nAvg\n100 OG + 200 dup\n60.8\n67.5\n72.7\n68.4\n64.9\n66.8\n100 OG + 200 aug\n63.9\n77.1\n80.2\n71.9\n72.7\n73.2\n300 OG\n67.4\n79.0\n80.1\n73.3\n76.5\n75.3\nis observed when definitions and guidelines are included\nalongside the new format. While this increase may appear\nmarginal, it offers substantial benefits when integrated with our\nparaphrase-augmented synthetic data during few-shot testing\n(as shown in Table I). This improvement underscores the\ncomplementary nature of clear guidelines and the word/tag\nformat in enhancing model accuracy and adaptability.\nB. Performance of Paraphrasing in Few-shot NER\nWe further evaluate the effectiveness of the paraphrasing-\nbased data augmentation approach across multiple domain-\nspecific datasets, including CrossNER [12] and MIT [13]. The\nthree models listed in Section V-C were fine-tuned using 0,\n100, and 300 augmented samples with a fixed set of 10,000\nPileNER [3] samples. This dataset size was chosen to align\nwith the GNER [5] framework.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_006.md",
    "arxiv_id": "2510.17720v1",
    "page": "6",
    "title": "unknown",
    "id": "2510.17720v1_page_006"
  },
  {
    "text": "scores correlating with the increase in augmented samples.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_006.md",
    "arxiv_id": "2510.17720v1",
    "page": "6",
    "title": "unknown",
    "id": "2510.17720v1_page_006"
  },
  {
    "text": "where we observe that the average F1 score of Falcon 3\n[8] increased by 0.14, Qwen 2.5 [6] by 0.17 and LLAMA\nTABLE V\nPERFORMANCE COMPARISON OF DIFFERENT AUGMENTATION METHODS\nON ENGLISH (EN)\n#Gold\nMethod\nF1 Score (in %)\n100\nGold-Only\n50.57\nLabel-wise\n61.34\nMLM-Entity\n61.22\nDAGA\n68.06\nMELM\n75.21\nPANER (Ours)\n80.52\n200\nGold-Only\n74.64\nLabel-wise\n76.82\nMLM-Entity\n79.16\nDAGA\n79.11\nMELM\n82.91\nPANER (Ours)\n85.74\n400\nGold-Only\n81.85\nLabel-wise\n84.62\nMLM-Entity\n83.82\nDAGA\n84.36\nMELM\n85.73\nPANER (Ours)\n88.11\n3 [7] by 0.12, respectively. The average performance across\nthe CrossNER [12] datasets is illustrated in Figure 4. This\nindicates that the diversity introduced through the paraphrasing\nstrategy effectively contributes to model performance.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_006.md",
    "arxiv_id": "2510.17720v1",
    "page": "6",
    "title": "unknown",
    "id": "2510.17720v1_page_006"
  },
  {
    "text": "mented samples achieved superior performance compared to\npreviously reported supervised techniques on the CrossNER\n[12] datasets, further reinforcing our augmentation strategy.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_006.md",
    "arxiv_id": "2510.17720v1",
    "page": "6",
    "title": "unknown",
    "id": "2510.17720v1_page_006"
  },
  {
    "text": "focus on zero-shot performance metrics (reported below), the\nresults surpass few-shot and fine-tuned in which CrossNER\n[12], which utilized supervised training samples. These com-\nparative results, detailed in Table III, demonstrate that our data\naugmentation technique can enhance model performance for\ncases with few examples. Although the performance of other\nInstruction-tuned models with similar augmentation strate-\ngies remains unexplored, the results suggest that combining\nlightweight few-shot learning with intelligent data augmen-\ntation offers a promising direction for domain-specific NER\ntasks.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_006.md",
    "arxiv_id": "2510.17720v1",
    "page": "6",
    "title": "unknown",
    "id": "2510.17720v1_page_006"
  },
  {
    "text": "TABLE VI\nCOMPARISON OF ZERO-SHOT LEARNING PERFORMANCE F1 (%) SCORES\nModel\nBackbone\n#Params\nMovie\nRestaurant\nAI\nLiterature\nMusic\nPolitics\nScience\nAverage\nChatGPT\ngpt-3.5-turbo\n-\n5.3\n32.8\n52.4\n39.8\n66.6\n68.5\n67\n47.5\nInstructUIE\nFlan-T5-xxl\n11B\n63\n21\n49\n47.2\n53.2\n48.2\n49.3\n47.3\nUniNER-type+sup.\nLLAMA-1\n7B\n61.2\n35.2\n62.9\n64.9\n70.6\n66.9\n70.8\n61.8\nGoLLIE\nCode-LLAMA\n7B\n63\n43.4\n59.1\n62.7\n67.8\n57.2\n55.5\n58.4\nGLiNER-L\nDeBERTa-v3\n0.3B\n57.2\n42.9\n57.2\n64.4\n69.6\n72.6\n62.6\n60.9\nGNER-T5\nFlan-T5-xxl\n11B\n62.5\n51\n68.2\n68.7\n81.2\n75.1\n76.7\n69.1\nGNER-LLAMA\nLLAMA-1\n7B\n68.6\n47.5\n63.1\n68.2\n75.7\n69.4\n69.9\n66.1\nSLIMER\nLLAMA-2-chat\n7B\n50.9\n38.2\n50.1\n58.7\n60\n63.9\n56.3\n54\nPANER\nQwen-2.5-Instruct\n7B\n51.5\n37.3\n62\n61.7\n75.9\n69.72\n65.63\n60.5\nPANER\nLLAMA-3.1-Instruct\n8B\n52\n37\n63.9\n67.2\n75.3\n67.8\n68.7\n61.7\nPANER\nFalcon3-Instruct\n10B\n69.4\n43.3\n65.5\n61.3\n75.8\n70.3\n68.3\n64.8\nTABLE VII\nZERO-SHOT RESULT COMPARISON ON BUSTER DATASET F1 (%) SCORES\nModel\nBackbone\n#Params\nPr.\nR\nF1\nGNER-LLAMA\nLLAMA-1\n7B\n14.68\n59.97\n23.58\nGLINER-L\nDeBERTa-v3\n0.3B\n42.55\n19.31\n26.57\nGoLLIE\nCode-LLAMA\n7B\n28.82\n26.63\n27.68\nGNER-T5\nFlan-T5-xxl\n11B\n19.31\n50.15\n27.88\nUniNER-type+sup.\nLLAMA-1\n7B\n31.4\n47.53\n37.82\nSLIMER\nLLAMA-2-chat\n7B\n47.69\n43.09\n45.27\nPANER\nFalcon3-Instruct\n10B\n29.92\n38.38\n33.63\nAdditionally, we compared our paraphrasing-based data\naugmentation approach against existing paraphrasing tech-\nniques, including DAGA [27] and MELM [28], on the CoNLL\n[26] shared task dataset. Our results indicate that leveraging\nLLMs for paraphrasing yields superior performance compared\nto these established techniques, as shown in Table V.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_007.md",
    "arxiv_id": "2510.17720v1",
    "page": "7",
    "title": "unknown",
    "id": "2510.17720v1_page_007"
  },
  {
    "text": "for the CoNLL dataset by using 100, 200, and 400 gold\nsamples, following the setup of [28], and then generating\n200, 400, and 800 augmented samples, respectively. We then\ntrained the LLAMA 3.1-8B model using these configurations.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_007.md",
    "arxiv_id": "2510.17720v1",
    "page": "7",
    "title": "unknown",
    "id": "2510.17720v1_page_007"
  },
  {
    "text": "3× the number of samples compared to the 2× used in our\nmethod. This performance gain can be attributed to our use\nof LLMs for prediction, as they are already familiar with the\nentity types in the CoNLL dataset (PERSON, LOCATION,\nORGANIZATION).\nC. Effectiveness of Paraphrase-Based Augmentation Com-\npared to Data Duplication and In-Domain Expansion\nIn order to understand the isolated effects of our paraphras-\ning method against duplication. We tested the comparative\neffectiveness of our data augmentation approach versus simply\nadding more in-domain samples or duplicating existing data,\nwe conducted an additional experiment to isolate the impact\nof our paraphrasing-based augmentation strategy. This exper-\niment systematically compared three training configurations,\neach with a total of 300 samples but differing in composition:\n1) 100 original in-domain samples augmented with 200\nparaphrased variants,\n2) 300 distinct original in-domain samples, and\n3) 100 original in-domain samples duplicated two times.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_007.md",
    "arxiv_id": "2510.17720v1",
    "page": "7",
    "title": "unknown",
    "id": "2510.17720v1_page_007"
  },
  {
    "text": "key findings. The configuration using 300 distinct original\nsamples achieved the highest average F1 score (75.3%), which\nwas expected given the inherent value of diverse, authen-\ntic samples. However, our hybrid approach combining 100\noriginal samples with 200 paraphrased variants performed\nremarkably well, reaching an F1 score of (73.2%), only 2.1\npercentage points below the all-original configuration. This\nsuggests that our paraphrasing strategy successfully preserves\nthe essential entity relationships while introducing beneficial\nlinguistic variation.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_007.md",
    "arxiv_id": "2510.17720v1",
    "page": "7",
    "title": "unknown",
    "id": "2510.17720v1_page_007"
  },
  {
    "text": "stantially lower performance (66.8%), confirming that mere\nrepetition of training examples provides no meaningful diver-\nsity to enhance model generalization. These findings validate\nour augmentation approach as an effective strategy when\nadditional authentic in-domain samples are unavailable or\nprohibitively expensive to obtain, offering nearly comparable\nperformance to training with three times the amount of original\ndata.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_007.md",
    "arxiv_id": "2510.17720v1",
    "page": "7",
    "title": "unknown",
    "id": "2510.17720v1_page_007"
  },
  {
    "text": "generated paraphrases in enhancing model generalization for\nNER tasks, further validating our approach as a viable alter-\nnative to conventional data augmentation strategies.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_007.md",
    "arxiv_id": "2510.17720v1",
    "page": "7",
    "title": "unknown",
    "id": "2510.17720v1_page_007"
  },
  {
    "text": "D. Performance of Instruction Tuning Template in Zero-shot\nNER\nWhile our primary goal is to improve few-shot Named En-\ntity Recognition (NER), as demonstrated above, the proposed\ninstruction-tuning template also performs competitively with\nstate-of-the-art methods in zero-shot NER. Table VI presents\nthe zero-shot performance compared to existing state-of-the-\nart benchmarks. Notably, the Falcon-3 [8] model achieves an\naverage F1 score of 0.648, close to the GNER-T5 [5] and\nGNER-LLAMA [5] models. This performance was obtained\nvia fine-tuning with LoRA [24], requiring significantly less\ncomputational time and resources: only one epoch was suffi-\ncient to achieve this performance, whereas GNER was fully\nfine-tuned for three epochs.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_008.md",
    "arxiv_id": "2510.17720v1",
    "page": "8",
    "title": "unknown",
    "id": "2510.17720v1_page_008"
  },
  {
    "text": "tuning template is showcased in Table VII, where we com-\npare it against the above-mentioned models on the BUSTER\ndataset. The results demonstrate that the proposed model\nperforms better than both GNER [5] models with an F1 of\n0.336 and achieves performance comparable to SLIMER [4],\nwhich holds the state-of-the-art F1 score of 0.4527.\nVII. CONCLUSIONS\nIn this work, we presented PANER, a paraphrase-augmented\nframework designed to enhance Named Entity Recognition\n(NER) in low-resource settings. The approach integrates in-\nstruction tuning with paraphrase-based data augmentation,\nenabling improved performance while maintaining compu-\ntational efficiency. Experimental results demonstrate that\nPANER achieves competitive performance with state-of-the-\nart zero-shot NER models while requiring significantly fewer\ncomputational resources. The paraphrasing technique consis-\ntently improves entity recognition, particularly in domain-\nspecific and few-shot learning settings. The results indicate\nthat our approach is an effective alternative for organizations\nwith limited access to annotated datasets and compute power.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_008.md",
    "arxiv_id": "2510.17720v1",
    "page": "8",
    "title": "unknown",
    "id": "2510.17720v1_page_008"
  },
  {
    "text": "paraphrasing improves model generalization, the quality of\ngenerated variations can vary depending on the complexity\nof the input sentences. Additionally, the approach to include\nguidelines and annotations for all entity types does not benefit\ncases where the entity is negatively affected by the guidelines.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_008.md",
    "arxiv_id": "2510.17720v1",
    "page": "8",
    "title": "unknown",
    "id": "2510.17720v1_page_008"
  },
  {
    "text": "how guidelines and annotations are helping each entity, and\nthe results show some entities do not require or benefit from\nthe presence of guidelines and annotations. Since we process\nentire sentences and extract all entities in a single request, it\nis difficult to selectively include or exclude guidelines based\non specific entity types, which could impact performance for\ncertain categories.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_008.md",
    "arxiv_id": "2510.17720v1",
    "page": "8",
    "title": "unknown",
    "id": "2510.17720v1_page_008"
  },
  {
    "text": "our\nparaphrasing-based\naugmentation\napproach\ndemonstrates promising results, we acknowledge a few lim-\nitations. First, our strict entity preservation constraints, though\neffective for maintaining semantic relationships, may restrict\nthe diversity of the generated samples. During our analysis,\nwe observed that augmented sentences often exhibit limited\nstructural variation when multiple entities appear in close\nproximity, as the model prioritizes preserving entity positions\nover introducing novel sentence constructions. This trade-off\nbetween entity integrity and linguistic diversity represents an\ninherent tension in our current implementation. Additionally,\nthe paraphrasing approach occasionally struggles with domain-\nspecific terminology and complex syntactic structures, result-\ning in approximately 15% of initially generated paraphrases\nfailing our validation checks and requiring regeneration.\nVIII. FUTURE WORK\nFuture work will explore more flexible entity augmentation\nstrategies that preserve semantic relationships while allowing\ncontrolled entity variations (such as replacing entities with\nsemantically equivalent alternatives within the same type) and\nadaptive paraphrasing approaches that adjust constraint strict-\nness based on sentence complexity and domain characteristics.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_008.md",
    "arxiv_id": "2510.17720v1",
    "page": "8",
    "title": "unknown",
    "id": "2510.17720v1_page_008"
  },
  {
    "text": "that combine paraphrasing with other techniques to further\nenhance sample diversity while maintaining the crucial entity\nrelationships that drive NER performance.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_008.md",
    "arxiv_id": "2510.17720v1",
    "page": "8",
    "title": "unknown",
    "id": "2510.17720v1_page_008"
  },
  {
    "text": "guideline inclusion, where entity-specific constraints could be\ndynamically applied during instruction tuning. Further, while\nour approach has demonstrated strong performance in English-\nlanguage datasets, its multilingual effectiveness remains unex-\nplored. A critical next step is studying how paraphrase-based\naugmentation can be effectively applied to other languages.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_008.md",
    "arxiv_id": "2510.17720v1",
    "page": "8",
    "title": "unknown",
    "id": "2510.17720v1_page_008"
  },
  {
    "text": "adaptable, and an efficient framework for real-world, multilin-\ngual applications.\nREFERENCES\n[1] J. Li, A. Sun, J. Han, and C. Li, “A Survey on Deep Learning for Named\nEntity Recognition,” IEEE Trans. Knowl. Data Eng., vol. 34, no. 1, pp.\n50–70, Jan. 2022, doi: 10.1109/TKDE.2020.2981314.\n[2] X. Wang et al., “InstructUIE: Multi-task Instruction Tuning for Unified\nInformation Extraction,” Apr. 17, 2023, arXiv: arXiv:2304.08085. doi:\n10.48550/arXiv.2304.08085.\n[3] W. Zhou, S. Zhang, Y. Gu, M. Chen, and H. Poon, “UniversalNER:\nTargeted Distillation from Large Language Models for Open Named\nEntity Recognition,” Aug. 06, 2023, arXiv: arXiv:2308.03279.\n[4] A. Zamai, A. Zugarini, L. Rigutini, M. Ernandes, and M. Maggini,\n“Show Less, Instruct More: Enriching Prompts with Definitions and\nGuidelines for Zero-Shot NER,” Jul. 02, 2024, arXiv: arXiv:2407.01272.\n[5] Y. Ding, J. Li, P. Wang, Z. Tang, B. Yan, and M. Zhang, “Rethinking\nNegative Instances for Generative Named Entity Recognition,” Jun. 18,\n2024, arXiv: arXiv:2402.16602.\n[6] Qwen et al., “Qwen2.5 Technical Report,” Dec. 19, 2024, arXiv:\narXiv:2412.15115. doi: 10.48550/arXiv.2412.15115.\n[7] A. Grattafiori et al., “The Llama 3 Herd of Models,” Nov. 23, 2024,\narXiv: arXiv:2407.21783. doi: 10.48550/arXiv.2407.21783.\n[8] “Welcome to the Falcon 3 Family of Open Models!” Available:\nhttps://huggingface.co/blog/falcon3\n[9] U. Yaseen and S. Langer, “Data Augmentation for Low-Resource Named\nEntity Recognition Using Backtranslation,” presented at the ICON, Aug.\n2021.\n[10] K. Aggarwal, H. Jin, and A. Ahmad, “Entity-Controlled Synthetic Text\nGeneration using Contextual Question and Answering with Pre-trained\nLanguage Models”.\n[11] ] J. Ye et al., “LLM-DA: Data Augmentation via Large Lan-\nguage Models for Few-Shot Named Entity Recognition,” 2024, doi:\n10.48550/ARXIV.2402.14568.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17720v1\\page_008.md",
    "arxiv_id": "2510.17720v1",
    "page": "8",
    "title": "unknown",
    "id": "2510.17720v1_page_008"
  },
  {
    "text": "2025-10-21\nMT-Video-Bench: A Holistic Video Understanding\nBenchmark for Evaluating Multimodal LLMs in\nMulti-Turn Dialogues\nYaning Pan1, Zekun Wang2, Qianqian Xie3, Yongqian Wen3, Yuanxing Zhang2,\nGuohui Zhang4, Haoxuan Hu3, Zhiyu Pan3, Yibing Huang3, Zhidong Gan3,\nYonghong Lin3, An Ping3, Tianhao Peng3, Jiaheng Liu3,†\n1Fudan University,\n2Kuaishou Technology,\n3Nanjing University,\n4University of Science and Technology of China\nynpan24@m.fudan.edu.cn\nliujiaheng@nju.edu.cn\nAbstract\nThe recent development of Multimodal Large Language Models (MLLMs) has significantly ad-\nvanced AI’s ability to understand visual modalities. However, existing evaluation benchmarks\nremain limited to single-turn question answering, overlooking the complexity of multi-turn di-\nalogues in real-world scenarios. To bridge this gap, we introduce MT-Video-Bencha, a holistic\nvideo understanding benchmark for evaluating MLLMs in multi-turn dialogues. Specifically, our\nMT-Video-Bench mainly assesses six core competencies that focus on perceptivity and interactivity,\nencompassing 987 meticulously curated multi-turn dialogues from diverse domains. These capa-\nbilities are rigorously aligned with real-world applications, such as interactive sports analysis and\nmulti-turn video-based intelligent tutoring. With MT-Video-Bench, we extensively evaluate various\nstate-of-the-art open-source and closed-source MLLMs, revealing their significant performance\ndiscrepancies and limitations in handling multi-turn video dialogues. The benchmark will be\npublicly available to foster future research.\nahttps://github.com/NJU-LINK/MT-Video-Bench\n1\nIntroduction\nThe rapid progress of Multimodal Large Language Models (MLLMs) has markedly advanced AI’s\ncapacity to perceive and reason over visual modalities, especially when integrated with natural language.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_001.md",
    "arxiv_id": "2510.17722v1",
    "page": "1",
    "title": "unknown",
    "id": "2510.17722v1_page_001"
  },
  {
    "text": "2.5 (Team, 2025) demonstrate impressive performance in single-turn video question answering and\nlong-form video comprehension (Zhang et al., 2023; Rawal et al., 2024; Sun et al., 2022; Wang et al., 2024a;\nChandrasegaran et al., 2024). Yet, real-world human–AI interaction is rarely confined to single-turn\nqueries. Instead, it typically unfolds as multi-turn dialogues, where users iteratively refine their questions,\nshift topics, and expect contextually coherent responses grounded in video content. This interactive\nsetting poses unique challenges: models must not only recall and integrate prior dialogue history but also\nadapt to conversational dynamics, such as handling topic shifting or gracefully refusing unanswerable\nqueries.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_001.md",
    "arxiv_id": "2510.17722v1",
    "page": "1",
    "title": "unknown",
    "id": "2510.17722v1_page_001"
  },
  {
    "text": "Zhou et al., 2025; Ma et al., 2025) predominantly focus on single-turn evaluation, emphasizing factual\nperception of video content—such as recognizing objects, actions, or temporal relations—while neglecting\ndialogue-level reasoning. A few recent efforts explore long-context or multi-shot video benchmarks, yet\nthey fall short of capturing the interplay between perceptivity (faithfully interpreting multimodal input)\nand interactivity (sustaining natural, user-aware conversations). Consequently, the community lacks\na rigorous and holistic framework to measure how well MLLMs can operate in realistic multi-turn,\nvideo-grounded dialogues.\nTo fill this gap, as shown in Figure 2, we introduce MT-Video-Bench, a holistic benchmark for evaluat-\ning MLLMs in multi-turn video dialogue. MT-Video-Bench systematically targets six core capabilities\nspanning perceptivity (object reference, memory recall, and content summary) and interactivity (an-\nswer refusal, topic shifting, and proactive interaction). The benchmark comprises 987 carefully curated\ndialogues across 135 videos, covering diverse domains such as sports, education, and daily activities.\n† Corresponding Author.\n1\narXiv:2510.17722v1  [cs.CV]  20 Oct 2025",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_001.md",
    "arxiv_id": "2510.17722v1",
    "page": "1",
    "title": "unknown",
    "id": "2510.17722v1_page_001"
  },
  {
    "text": "What happens in the first ...? Describe the sequence...\nIn their cave, Grizzly picks up an empty cardboard  ...\n... What is the single most effective way to prevent\nthe spread of infectious diseases...?\nAccording to public health organizations ...",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_002.md",
    "arxiv_id": "2510.17722v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17722v1_page_002"
  },
  {
    "text": "The bubble proves very effective. In the garbage ...\n...Speaking of birds, what is the 'V' formation ...?\nThe 'V' formation is a specific flight pattern used by...\n...a new character appears. Who is this character...?\nThe character is Charlie, a tall, gray, Bigfoot-like ...",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_002.md",
    "arxiv_id": "2510.17722v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17722v1_page_002"
  },
  {
    "text": "... because he sees it as a way to protect himself\nfrom ...",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_002.md",
    "arxiv_id": "2510.17722v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17722v1_page_002"
  },
  {
    "text": "...\n...",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_002.md",
    "arxiv_id": "2510.17722v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17722v1_page_002"
  },
  {
    "text": "... appears. What is his apparent profession, and ...?\nThe man in the formal black tuxedo is a waiter. In ...",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_002.md",
    "arxiv_id": "2510.17722v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17722v1_page_002"
  },
  {
    "text": "The managing director, Berndt Querfeld, states that ...",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_002.md",
    "arxiv_id": "2510.17722v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17722v1_page_002"
  },
  {
    "text": "Later in the video ..., the waiter is seen serving ...\nA customer ... provided by people like him. What ... ?\nA female customer states that the service provided ...",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_002.md",
    "arxiv_id": "2510.17722v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17722v1_page_002"
  },
  {
    "text": "... because they embody the long-standing,\ntraditional ...",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_002.md",
    "arxiv_id": "2510.17722v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17722v1_page_002"
  },
  {
    "text": "The video presents historical evidence through a ...",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_002.md",
    "arxiv_id": "2510.17722v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17722v1_page_002"
  },
  {
    "text": "...\n...",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_002.md",
    "arxiv_id": "2510.17722v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17722v1_page_002"
  },
  {
    "text": "(Topic Shifting)\n(Topic Shifting)\n(Object Reference)\n(Object Reference)\nFigure 1: Illustration of multi-turn dialogues under single-scene and cross-scene settings. The evaluated\nquestions corresponding to tasks are marked with underlining, and the scenes involved in the entire\nmulti-turn dialogues are marked with blue dotted boxes.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_002.md",
    "arxiv_id": "2510.17722v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17722v1_page_002"
  },
  {
    "text": "dencies, and interactive adaptability, thereby aligning closely with real-world application demands.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_002.md",
    "arxiv_id": "2510.17722v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17722v1_page_002"
  },
  {
    "text": "source models, highlighting the current limitations and performance discrepancies in different abilities.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_002.md",
    "arxiv_id": "2510.17722v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17722v1_page_002"
  },
  {
    "text": "• The perceptual and interactive capabilities of MLLMs in multi-turn dialogues still have significant\nroom for improvement. On MT-Video-Bench, even the strongest closed-source model Gemini 2.5 Pro\nachieves only 68.45% overall accuracy, while most open-sourced MLLMs exhibit accuracies below 50%,\nexcept for the Qwen2.5-VL and InternVL3.5 series.\n• Performance is imbalanced across different tasks and scene types. MLLMs generally perform better on\nperceptual subtasks (e.g., Object Reference) than on interactive ones (e.g., Proactive Interaction), with a\nsubstantial gap between closed- and open-source models. Moreover, all models tend to perform worse\nin cross-scene settings compared to single-scene tasks.\n• Model scaling is beneficial but not sufficient. Larger models consistently outperform smaller counter-\nparts across most subtasks, yet scaling alone does not ensure consistent improvements. For example,\nin the InternVL 3.5 series, enabling the Thinking mode allows smaller models to achieve performance\ncomparable to that of larger models, which demonstrates the significant benefit of the reasoning\nprocess in enhancing model performance.\nTo summarize, the contributions of this paper are as follows: We identify the critical gap in evaluating\nmulti-turn video-grounded dialogues and propose the MT-Video-Bench, the first holistic benchmark that\noperationalizes this evaluation via six well-defined capabilities across 987 dialogues and 5,805 QA pairs.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_002.md",
    "arxiv_id": "2510.17722v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17722v1_page_002"
  },
  {
    "text": "directions for improvement of handling and reasoning over multi-turn dialogues, offering a roadmap for\nfuture research and development.\n2\nRelated Work\nMultimodal LLMs. MLLMs have become a central research focus in advancing general-purpose intelli-\ngence. By jointly modeling textual and visual modalities, these models are able to capture cross-modal\ndependencies and enhance semantic reasoning (Zhu et al., 2023; Ma et al., 2024; Zhang et al., 2024a; Wang\net al., 2025b; 2024c). Recent advances have further extended MLLMs to the video domain, enabling\n2",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_002.md",
    "arxiv_id": "2510.17722v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17722v1_page_002"
  },
  {
    "text": "Table 1: Comparison with other benchmarks. Avg. Q/V: the average number of QA pairs per video.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_003.md",
    "arxiv_id": "2510.17722v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17722v1_page_003"
  },
  {
    "text": "covers more than 4 scenes.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_003.md",
    "arxiv_id": "2510.17722v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17722v1_page_003"
  },
  {
    "text": "#QAs\nAvg. Q/V\nLong\nDialogue\n#Turns\nCross-Scene\nAnnotation\nMVBench (Li et al., 2024a)\n4,000\n1.00\n%\n%\n1.00\n-\nAuto\nLongVideoBench (Wu et al., 2024a)\n6,678\n1.77\n%\n%\n1.00\n-\nManual\nVideo-MME (Fu et al., 2025)\n2,700\n3.00\n\"\n%\n1.00\n-\nManual\nLVBENCH (Wang et al., 2024b)\n1,549\n15.04\n\"\n%\n1.00\n-\nManual\nMLVU (Zhou et al., 2025)\n3,102\n1.79\n\"\n%\n1.00\n-\nManual\nVideo-MMLU (Song et al., 2025)\n15,746\n14.78\n%\n%\n1.00\n-\nAuto&Manual\nScaleLong (Ma et al., 2025)\n1,747\n6.49\n\"\n%\n1.00\n-\nManual\nSVBench (Yang et al., 2025)\n7,374\n36.87\n%\n\"\n4.29\n%\nAuto&Manual\nMT-Video-Bench (Ours)\n5,805\n43.00\n\"\n\"\n5.88\n\"\nAuto&Manual\nvideo understanding, which subsequently supports dialogue(Li et al., 2023; Cheng et al., 2024; Maaz\net al., 2023). For example, Qwen2.5-VL (Bai et al., 2025) employs a dynamic-resolution Vision Trans-\nformer with MRoPE for spatiotemporal alignment, and connects an MLP merger to the Qwen2.5 LLM\ndecoder. InternVL3.5 (Wang et al., 2025a) integrates InternViT as the vision encoder with a ViT-MLP-LLM\nparadigm, and further adopts Visual Resolution Router (ViR) with Visual Consistency Learning (ViCO)\nfor cross-modal alignment.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_003.md",
    "arxiv_id": "2510.17722v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17722v1_page_003"
  },
  {
    "text": "marks(Wang et al., 2023; Wu et al., 2024b; Xiao et al., 2021; Li et al., 2025). For example, MVBench (Li\net al., 2024a) focuses on concise video QA tasks to evaluate multimodal understanding abilities, while\nMLVU (Zhou et al., 2025) and LVBENCH (Wang et al., 2024b) provide a comprehensive analysis for\nMLLMs’ long-video understanding performance. MMBench-Video (Fang et al., 2024) is a long-form,\nmulti-shot benchmark that evaluates fine-grained abilities of MLLMs, including temporal reasoning,\nperception, and general reasoning in video understanding. SVBench (Yang et al., 2025) is a benchmark\nfor temporal multi-turn dialogues in streaming videos, designed to assess the capabilities of streaming\nvideo understanding of MLLMs. However, prior benchmarks primarily focus on evaluating the video\nunderstanding capabilities of models, overlooking the multi-turn dialogue capabilities, which require not\nonly the ability to recall contextual information but also to engage in coherent, interactive communication\nwith users across multiple turns.\n3\nMT-Video-Bench\n3.1\nOverview\nMT-Video-Bench is designed to comprehensively evaluate the “Perceptivity” and “Interactivity” of\nMLLMs in multi-turn video-grounded dialogues. Different from conventional video understanding\nbenchmarks that primarily focus on single-turn question answering, MT-Video-Bench is specifically\ndesigned to mimic real-world interactive scenarios, emphasizing contextual coherence, cross-scene video\ncomprehension, and adaptive interactivity.\nMT-Video-Bench systematically evaluates six core capabilities of MLLMs through 987 meticulously\ncurated multi-turn dialogues with 5,805 QA pairs. Each conversation requires not only accurate video\nperception but also contextual reasoning within or across video scenes, with representative examples\nshown in Figure 1.\nA comprehensive comparison between our MT-Video-Bench and other related benchmarks is provided\nin Table 1. MT-Video-Bench presents the following critical values: (1) supports multi-turn dialogues\nthat evaluate contextual coherence and long-range dependency, (2) supports cross-scene reasoning that\nrequires integrating information across different video clips, and (3) provides a fine-grained assessment\nof perceptivity and interactivity through six tasks.\n3.2\nEvaluation Tasks\nPerceptivity assesses the model’s foundational ability to perceive and integrate information from both the\nvisual video content and the multi-turn conversational context. This capability is essential for accurately\nunderstanding user queries and generating contextually grounded responses throughout the dialogue. It\n3",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_003.md",
    "arxiv_id": "2510.17722v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17722v1_page_003"
  },
  {
    "text": "High similarity\nto the left frame\nVideo Datasets\n2. Extract Frames\n3. Object Extraction\n4. Object Memory Bank\n5. Relevant Scene Merging\n6. MTQA Generation\n7. Human Verification\nFilter\nCriteria\n•\nSharpness\n•\nSimilariy\nObject Detection & Caption\nID: ID 1\nInformation: ......",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_004.md",
    "arxiv_id": "2510.17722v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17722v1_page_004"
  },
  {
    "text": "•\nSingle Scene\n•\nCross Scene\n...\n...\n...\n...\n1. Scene Splitting\nLow Sharpness\nPerson Caption\nAnimal Caption\n...\n...\nTime\nID: ID 2\nInformation: ......\n......\n•\nMTQA Accuracy Checking\n•\nTask Quality Verification\n•\nContextual Coherence\n•\n......",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_004.md",
    "arxiv_id": "2510.17722v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17722v1_page_004"
  },
  {
    "text": "6 Core Abilities\n•\nPerceptivity\n•\nInteractivity\n...\n...",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_004.md",
    "arxiv_id": "2510.17722v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17722v1_page_004"
  },
  {
    "text": "Scene j\nScene k\nObject-Based Scene Merging\nFigure 2: An overview of the semi-automatic data construction process of MT-Video-Bench.\nincludes:\n• Object Reference (OR) evaluates the model’s ability to resolve references and pronouns in the user’s\ninput, ensuring that entities mentioned implicitly are correctly mapped to the appropriate objects,\ncharacters, or concepts.\n• Memory Recall (MR) measures the model’s capacity to retrieve, retain, and integrate relevant informa-\ntion from prior conversational turns or long-term history, enabling coherent reasoning and continuity\nacross interactions.\n• Content Summary (CS) assesses the model’s effectiveness in condensing conversational and video\ncontent into succinct yet comprehensive summaries, while preserving essential details, coherent\nstructure, and semantic fidelity.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_004.md",
    "arxiv_id": "2510.17722v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17722v1_page_004"
  },
  {
    "text": "based on the video content. It focuses on appropriately refusing unanswerable questions, smoothly\nadapting to topic changes, and proactively maintaining engagement. It includes:\n• Answer Refusal (AR) tests the ability to recognize unanswerable queries based on available evidence\nand explicitly decline or indicate insufficiency without hallucination.\n• Topic Shifting (TS) evaluates how effectively the model can track and adapt to user-initiated changes in\nconversational focus or subject matter, while maintaining coherence, fluency, and relevance throughout\nthe dialogue.\n• Proactive Interaction (PI) probes the model’s capacity to sustain or restore engagement through\nclarifications, elaborations, or novel insights when signs of disinterest or disengagement are detected,\nthereby fostering renewed interest and continuation of the dialogue.\n3.3\nData Collection\nAs shown in Figure 2, the data collection process for MT-Video-Bench involves both automated con-\nstruction and human verification. We first acquire videos from online platforms and split them into\nsingle-scene segments. Next, we retrieve and merge relevant scenes by extracting frames, performing\nobject detection, and constructing an object memory bank. Multi-turn dialogues are then generated\nautomatically for diverse evaluation tasks. Finally, human annotators are involved to ensure the accuracy\nand quality of the generated dialogues.\nVideo Collection and Single-Scene Splitting. The data collection process begins with the manual acqui-\nsition of 135 videos from various online platforms, such as YouTube, within the past year. Subsequently,\nwe employ PySceneDetect1 to divide the videos into shorter clips. Recognizing that these clips are often\ntoo brief to represent complete scenes, we then use the Gemini 2.5 Flash model (Team, 2025) to generate\ndescriptive captions for each clip. Finally, the caption-based clip merging method is iteratively applied\ntwice to combine related clips into a coherent, single-scene video, ensuring a seamless and contextually\naccurate representation of the scene. These refined single-scene videos serve as the core visual content for\nthe subsequent task of generating single-scene, multi-turn dialogues.\n1https://github.com/Breakthrough/PySceneDetect\n4",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_004.md",
    "arxiv_id": "2510.17722v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17722v1_page_004"
  },
  {
    "text": "retrieval and merging of relevant scenes from disparate video segments, which serves as a critical step\nin creating coherent interactions that span across multiple visual contexts. Firstly, frames are extracted\nfrom the video at 2 FPS and then filtered based on two criteria: sharpness and similarity to the previous\nselected frame. The sharpness of each frame is evaluated by the Laplace Operator to ensure that only\nclear, visually significant frames are retained, improving the overall quality of the selected frames. To\navoid redundancy, frames with high similarity to the preceding selected frame are discarded. Specifically,\na histogram-based image similarity calculation method is used to compare consecutive frames, excluding\nthose with a similarity score above 0.9. This approach ensures that the selected frames are distinct and\ncapture key moments in the video.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_005.md",
    "arxiv_id": "2510.17722v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17722v1_page_005"
  },
  {
    "text": "and each detected object is then annotated with a caption generated by the Gemini 2.5 Flash (Team, 2025),\nproviding detailed descriptions for each object. As the video progresses, a dynamic object memory bank\nis maintained, continuously expanded based on object captions and visual similarities. This memory\nbank associates unique object IDs with their corresponding attributes, enabling the identification of the\nsame objects across frames. To merge relevant scenes, a retrieval step across scenes is performed to select\nvideo segments that share common objects or themes, which are then merged to ensure continuity both\nthematically and contextually.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_005.md",
    "arxiv_id": "2510.17722v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17722v1_page_005"
  },
  {
    "text": "the generation of both single-scene and cross-scene multi-turn dialogues, based on the six evaluation\ntasks defined earlier. For each video, we generate multiple multi-turn dialogues, each corresponding to\ndifferent scenes. To determine the most appropriate task for each scene, we prompt MLLMs to evaluate\nthe scene’s capabilities, scoring them on a scale from 1 to 6. Only those tasks that receive a score of 5 or 6\nare selected for dialogue generation. For multi-turn dialogues spanning multiple scenes, we specifically\nadopt an object-centered approach for cross-scene question design since objects often serve as the central\nelement around which events unfold. This approach emphasizes the continuity and relationships of\nobjects across scenes, enabling the generation of dialogues that are both contextually consistent and\nthematically coherent.\n3.4\nQuality Control\nFollowing automated data collection, we employ the following two-stage human verification process to\nenhance dataset quality.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_005.md",
    "arxiv_id": "2510.17722v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17722v1_page_005"
  },
  {
    "text": "context-dependent, which can be answered solely based on dialogue history; and (2) video-dependent,\nwhich require direct grounding in the video content. We observe that in some generated dialogues,\nearlier QA pairs embedded excessive background hints, enabling models to answer subsequent questions\nwithout relying on the video. This led to an overrepresentation of context-dependent items, thereby\nweakening the evaluation of video understanding. To mitigate this, we systematically removed such\ncases to ensure that the majority of questions required genuine video-based reasoning.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_005.md",
    "arxiv_id": "2510.17722v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17722v1_page_005"
  },
  {
    "text": "secondary review from a human perspective. We verify whether each question and answer pair was\nfactually aligned with the video and free from ambiguities. Beyond factual correctness, we also examine\nwhether each question is properly aligned with its intended ability dimension. For example, answer\nrefusal questions must explicitly test whether a model can recognize “events absent from the video,”\nwhile object reference questions must involve pronoun disambiguation. Any misaligned samples are\ndiscarded. Finally, we filter out overly simple questions, as they can be trivially solved by most models\nand fail to highlight multi-turn reasoning and video comprehension capacities.\n3.5\nDataset Statistics\nFigure 3 presents the statistics of MT-Video-Bench. It covers a broad range of topics across five main cate-\ngories: Movie, TV, Sports, Knowledge, and Life Record, each with multiple sub-topics, ensuring a diverse\nand balanced data distribution. With a total of 987 multi-turn dialogues, the data distribution across the\nsix primary tasks in MT-Video-Bench is relatively balanced, as shown in Figure 3 (b). Furthermore, our\ndataset features videos of varying lengths, with most being under 15 minutes and a small proportion\nexceeding 15 minutes, thereby ensuring coverage of both short and long videos. The number of dialogue\nturns typically ranges from 5 to 8, with an average of 5.88 turns per dialogue.\n5",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_005.md",
    "arxiv_id": "2510.17722v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17722v1_page_005"
  },
  {
    "text": "(a) Video Categories\n(b) Task Distribution\n(c) Video Duration Distribution\n(d) Dialogue Turn Distribution\nFigure 3: Overview of MT-Video-Bench. (a) Video Categories. MT-Video-Bench includes videos spanning\n5 major categories, ensuring diverse topical coverage. (b) Task Distribution. MT-Video-Bench consists\nof a total of 6 tasks with a relatively balanced distribution. (c) Video Duration Distribution. MT-Video-\nBench includes both long and short videos. (d) Dialogue Turn Distribution. Multi-turn dialogues in\nMT-Video-Bench involve 5 to 8 rounds.\n3.6\nEvaluation Method\nIn multi-turn dialogues, each new turn depends on the interactions between users and assistants in\nprevious turns. This dynamic is particularly crucial in tasks that involve high interactivity, such as\nproactive interactions. Therefore, we follow the multi-turn dialogue evaluation setup used in LLMs (Bai\net al., 2024), leveraging our meticulously curated dataset as the golden context for dialogue history, rather\nthan relying on self-predicted context from MLLMs.\nFor evaluation, we first use Gemini 2.5 Flash (Team, 2025) to construct a checklist for each QA pair.\nSpecifically, each checklist consists of five yes/no questions designed to assess the accuracy of the model’s\nresponses and its performance on specific tasks. Then, manual validation is employed to filter out\nunqualified checklists. After filtering, each QA pair has an average of 3.29 questions in the final checklists,\nwith 62.35% answered as yes and 37.65% as no. During the evaluation process, Gemini 2.5 Flash (Team,\n2025) is used to answer each checklist question based on the model-generated answers. The evaluation\nmetric is calculated as the accuracy (ACC), based on the proportion of correct answers across all checklists.\n4\nExperiments\n4.1\nExperimental Settings\nFor closed-source models, we evaluate Gemini 2.5 Pro (Team, 2025), Gemini 2.5 Flash (Team, 2025),\nand Doubao-Seed-1.6-vision (Seed, 2025). For open-source models, we select 18 representative MLLMs,\nincluding Qwen2.5 VL series (Bai et al., 2025), InternVL3.5 series (Wang et al., 2025c), LLaVA-Onevision\nseries (Li et al., 2024b), InterVideo2.5 series (Wang et al., 2025d), LLaVA-Video series (Zhang et al., 2024b),\nLLaVA-NeXT-Video series (Zhang et al., 2024c), VideoChat-Flash series (Li et al., 2024c), VideoLlama3\nseries (Zhang et al., 2025a) and MiniCPM series (Yao et al., 2024).\nEvaluation. For each model, we adopt a uniform sampling strategy to process video frames, setting the\nnumber of frames to 32. Each video is resized that the longer side is limited to 720 pixels and the other\nside is scaled proportionally. More details are described in Appendix B.1. For the prompts, we provide\nthe evaluation prompts of six tasks of MT-Video-Bench in B.2.\n4.2\nMain Results\nAs shown in Table 2, we provide the performance results of different MLLMs on our MT-Video-Bench,\nand we have the following insightful and interesting observations:\n• MT-Video-Bench is very challenging. Even the best-performing closed-source model, Gemini 2.5 Pro,\nonly achieves 68.45% overall accuracy, which is inferior to the performance of human experts a lot.\n• Among all evaluated models, Gemini 2.5 Pro consistently ranks first in both overall accuracy and every\nindividual subtask. While closed-source systems still dominate overall performance, some open-source\n6",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_006.md",
    "arxiv_id": "2510.17722v1",
    "page": "6",
    "title": "unknown",
    "id": "2510.17722v1_page_006"
  },
  {
    "text": "Summary. AR: Answer Refusal. TS: Topic Shifting. PI: Proactive Interaction. The best performance and\nthe second best performance are highlighted in green and blue, respectively.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_007.md",
    "arxiv_id": "2510.17722v1",
    "page": "7",
    "title": "unknown",
    "id": "2510.17722v1_page_007"
  },
  {
    "text": "Overall\nPerceptivity\nInteractivity\nOR\nMR\nCS\nAR\nTS\nPI\nClosed-Sourced Models\nGemini 2.5 Pro (Team, 2025)\n68.45\n66.13\n67.80\n80.49\n67.50\n73.67\n55.12\nGemini 2.5 Flash (Team, 2025)\n63.30\n63.44\n63.41\n73.48\n64.32\n68.12\n47.04\nDoubao-Seed-1.6-vision (Seed, 2025)\n58.55\n66.19\n60.85\n68.95\n43.84\n65.99\n45.50\nOpen-Sourced Models\nModel Size > 8B\nQwen2.5-VL-72B (Bai et al., 2025)\n58.48\n60.60\n56.40\n74.20\n57.07\n64.27\n38.35\nInternVL3.5-38B (Think) (Wang et al., 2025c)\n58.11\n60.87\n60.36\n69.90\n46.86\n65.17\n45.51\nQwen2.5-VL-32B (Bai et al., 2025)\n57.88\n60.20\n59.63\n74.88\n50.71\n63.41\n38.47\nInternVL3.5-38B (No Think) (Wang et al., 2025c)\n50.04\n52.51\n46.37\n61.86\n44.24\n58.78\n36.46\n4B < Model Size ≤8B\nInternVL3.5-8B (Think) (Wang et al., 2025c)\n56.29\n57.81\n54.82\n73.18\n47.62\n62.50\n41.84\nQwen2.5-VL-7B (Bai et al., 2025)\n53.12\n56.18\n49.99\n67.21\n52.20\n57.20\n35.92\nInternVL3.5-8B (No Think) (Wang et al., 2025c)\n49.35\n51.71\n46.95\n61.50\n40.83\n57.23\n37.85\nLLaVA-Video-7B (Zhang et al., 2025b)\n49.17\n53.85\n43.57\n63.64\n41.32\n56.67\n35.98\nMiniCPM-o (Yao et al., 2024)\n48.41\n55.06\n43.27\n61.59\n34.58\n57.53\n38.43\nMiniCPM-V4.5 (Yao et al., 2024)\n47.06\n51.57\n43.08\n56.17\n38.46\n52.58\n40.47\nInternVideo2.5-8B (Wang et al., 2025e)\n47.04\n44.87\n43.49\n60.33\n45.23\n54.81\n33.50\nVideoLLaMA3-7B (Bai et al., 2025)\n46.06\n52.06\n42.40\n55.74\n45.23\n48.25\n32.69\nLLaVA-OneVision-7B (Li et al., 2024d)\n45.75\n50.01\n43.36\n59.34\n32.79\n55.44\n33.56\nVideoChat-Flash-7B (Li et al., 2024e)\n41.11\n47.92\n39.33\n51.14\n28.02\n48.27\n32.01\nLLaVA-NeXT-Video-7B (Zhang et al., 2024d)\n38.04\n43.05\n36.04\n48.58\n27.60\n42.94\n30.00\nModel Size ≤4B\nInternVL3.5-4B (Think) (Wang et al., 2025c)\n52.25\n54.94\n53.78\n67.50\n37.74\n54.67\n44.89\nQwen2.5-VL-3B (Bai et al., 2025)\n48.07\n50.64\n43.54\n65.82\n46.80\n50.33\n31.30\nInternVL3.5-4B (No Think) (Wang et al., 2025c)\n45.90\n46.03\n46.19\n61.30\n30.41\n55.72\n35.74\nmodels demonstrate competitive results in specific dimensions. For example, Qwen2.5-VL-72B shows\nstrong ability in MR, narrowing the gap with Gemini 2.5 Pro. However, on interaction-related subtasks\nsuch as AR, the performance difference between open-source and closed-source models remains\nsubstantial.\n• Results vary significantly across different dimensions, and models generally perform better on\nperception-related subtasks, where large-scale models generally achieve stronger scores, sometimes\nexceeding 60. For example, the average score of OR is 54.55, while for PI is 38.60.\n• Larger models tend to achieve higher accuracy. For instance, within the Qwen2.5-VL series, the 72B\nand 32B models significantly outperform the 7B and 3B variants across nearly all subtasks. Similarly,\nlarger InternVL3.5 models achieve better results than their smaller counterparts. However, sometimes\nsmall MLLMs can lead to higher scores. For instance, the AR scores for Qwen2.5-VL-7B, Qwen2.5-\nVL-32B, and InternVideo2.5-8B are 52.20, 50.71, and 45.23, respectively. In addition, enabling thinking\nmode within the same model variant leads to significant performance improvements, suggesting that\ninference strategies, beyond model size, can substantially affect benchmark outcomes.\n4.3\nFurther Analysis\n4.3.1\nPerformance comparison between single scene and cross scene\nBased on the selected three models in Figure 4, we summarize the following conclusions: (1) Across\nalmost all abilities, model performance under the cross-scene setting is worse than under the single-scene\nsetting. (2) Regardless of the setting, Gemini 2.5 Pro consistently outperforms Qwen2.5-VL-7B and\nInternVL3.5-8B across all abilities, particularly in Content Summary and Memory Recall, while also\nsustaining relatively high performance under the cross-scene condition. In comparison, InternVL3.5-8B\nperforms comparably to Gemini 2.5 Pro in the single-scene setting but suffers from substantial degradation\nin the cross-scene setting. Meanwhile, Qwen2.5-VL-7B shows severe performance drops in Proactive\n7",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_007.md",
    "arxiv_id": "2510.17722v1",
    "page": "7",
    "title": "unknown",
    "id": "2510.17722v1_page_007"
  },
  {
    "text": "(a) Qwen2.5-VL-7B\n(c) Gemini 2.5 Pro\n(b) InternVL3.5-8B (Think)\nFigure 4: Performance comparison of Qwen2.5-VL-7B, InternVL3.5-8B (Think), and Gemini 2.5 Pro across\nvarious tasks under single-scene and cross-scene settings.\nInteraction and Memory Recall under cross-scene evaluation.\n4.3.2\nPerformance of different video lengths\nTo study the impact of video length on model performance, videos are grouped into different length\nranges. From Figure 5 (a), we find that: (1) Model performance generally decreases as video length\nincreases, suggesting that longer videos pose greater challenges for capturing and reasoning over multi-\nturn dialogue content. (2) Higher-capacity models, such as Gemini 2.5 Pro, tend to achieve higher overall\nscores across all video lengths compared to smaller models like Qwen2.5VL-7B. However, all models\nexhibit noticeable performance drops for very long videos. (3) The performance gap between models is\nmore pronounced for shorter videos, while for longer videos, the performance difference narrows.\n(a) Performance of different video lengths\n(c) Effect of dialogue context\n(b) Performance across dialogue turns\nFigure 5: Performance of different video lengths, dialogue turns, and settings of dialogue context.\n4.3.3\nModel performance across dialogue turns\nTo evaluate the impact of dialogue length on model performance, we conduct experiments with dialogues\nof varying total turn numbers with Gemini-2.5-Pro and Qwen2.5VL-7B. Several key observations can be\ndrawn from the results shown in Figure 5 (b): Model performance tends to improve as the total number of\nturns increases, although the degree and stability of this improvement vary across models. This suggests\nthat dialogue length plays a dual role in multi-turn video understanding: offering more contextual cues\nbeneficial for reasoning while increasing the burden of sustaining coherent dialogue states. One possible\nreason for this pattern is that larger models are generally able to integrate contextual information more\nefficiently, leveraging additional turns to further improve. Smaller models, on the other hand, tend to\nrely more heavily on the accumulation of dialogue context across multiple turns.\n4.3.4\nEffect of dialogue context\nTo investigate the role of contextual information, we design three experimental settings:\nWithout Context. The model answers each question solely based on the video.\nWith Self-predicted Context. The model is provided with its own generated dialogue history.\nWith Golden Context. The model is provided with our meticulously curated golden dialogue history.\nAs shown in Figure 5 (c), the golden context yields the highest performance across all abilities. However,\n8",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_008.md",
    "arxiv_id": "2510.17722v1",
    "page": "8",
    "title": "unknown",
    "id": "2510.17722v1_page_008"
  },
  {
    "text": "the accuracy of the context is more critical than its mere presence. Self-predicted context does not always\nlead to performance gains and remains roughly on par with the no-context setting, as the model-generated\ndialogue history may contain factual errors or semantic drift. These inconsistencies may accumulate over\nmultiple rounds, causing the model to be misled by “incorrect memories” in subsequent responses.\n4.3.5\nEffect of different numbers of frames\nFigure 6: Ablation results of frames on different abil-\nities. (a) Results of Object Reference, Memory Recall,\nContent Summary, and Proactive Interaction; (b) Re-\nsults of Answer Refusal and Topic Shifting.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_009.md",
    "arxiv_id": "2510.17722v1",
    "page": "9",
    "title": "unknown",
    "id": "2510.17722v1_page_009"
  },
  {
    "text": "according to the number of frames, with the resolu-\ntion fixed at 720p and the number of frames vary-\ning from 4 to 64. Several distinct trends emerge\nfrom the results:\n(1) Topic Shifting. The performance on topic shift-\ning remains largely unaffected by the number of\nframes. This suggests that the ability to adapt to\nunexpected user queries and maintain coherent re-\nsponses is primarily dependent on dialogue-level\nreasoning rather than fine-grained visual informa-\ntion.\n(2) Anwser Refusal. Models perform better on\nanswer refusal cases when fewer frames are pro-\nvided. With limited visual evidence, the model becomes more cautious in generating answers and is less\nlikely to hallucinate unsupported content, while when more frames are provided, the model may overfit\nto irrelevant visual cues and produce unwarranted responses, leading to decreased performance on this\nability.\n(3) Long Context Benefits. For the other four abilities, as shown in Figure 6 (a), models’ performance\nconsistently improves with more frames, because longer visual evidence provides richer contextual\nsignals, which support more accurate reasoning.\n4.3.6\nEffect of different resolutions\nFigure 7: Ablation results of resolutions\non different abilities.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_009.md",
    "arxiv_id": "2510.17722v1",
    "page": "9",
    "title": "unknown",
    "id": "2510.17722v1_page_009"
  },
  {
    "text": "performance, we evaluate the performance of Qwen2.5-VL-\n7B under different resolutions, with the number of frames\nfixed at 32. The input video frames are set to 120p, 240p,\n480p, 720p, and 960p.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_009.md",
    "arxiv_id": "2510.17722v1",
    "page": "9",
    "title": "unknown",
    "id": "2510.17722v1_page_009"
  },
  {
    "text": "prove from 120p to 720p, while a slight decline is observed\nwhen the resolution further increases to 960p. This suggests\nthat, within a certain range, higher resolution indeed en-\nhances the model’s ability to capture visual details, but exces-\nsive resolution may lead to a decline in performance, primar-\nily due to the increased number of input tokens that exceeds\nthe model’s optimal processing capacity.\n5\nConclusion\nIn this paper, we presented MT-Video-Bench, a holistic benchmark for evaluating MLLMs in multi-turn\nvideo dialogues. Unlike prior video understanding benchmarks that primarily focus on single-turn\nfactual perception, MT-Video-Bench jointly assesses perceptivity and interactivity through six carefully\ndefined capabilities, covering tasks such as memory recall, topic shifting, and proactive interaction. Our\nevaluation of 20 state-of-the-art models provides insightful findings, and we hope our MT-Video-Bench\ncan establish a rigorous foundation for future research, highlighting the need for models that can reason\nover long contexts while engaging in natural, adaptive conversations.\n9",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17722v1\\page_009.md",
    "arxiv_id": "2510.17722v1",
    "page": "9",
    "title": "unknown",
    "id": "2510.17722v1_page_009"
  },
  {
    "text": "Haozhen Zhang∗\nhaozhenz@illinois.edu, wazhz14@gmail.com\nUniversity of Illinois at Urbana-Champaign\nTao Feng\ntaofeng2@illinois.edu\nUniversity of Illinois at Urbana-Champaign\nPengrui Han\nphan12@illinois.edu\nUniversity of Illinois at Urbana-Champaign\nJiaxuan You\njiaxuan@illinois.edu\nUniversity of Illinois at Urbana-Champaign\nAbstract\nLarge Language Models (LLMs) have recently achieved remarkable performance in long-\ncontext understanding. However, current long-context LLM benchmarks are limited by rigid\ncontext length, labor-intensive annotation, and the pressing challenge of label leakage issues\nduring LLM training. Therefore, we propose AcademicEval, a live benchmark for evalu-\nating LLMs over long-context generation tasks. AcademicEval adopts papers on arXiv to\nintroduce several academic writing tasks with long-context inputs, i.e., Title, Abstract,\nIntroduction, and Related Work, which cover a wide range of abstraction levels and\nrequire no manual labeling. Moreover, AcademicEval integrates high-quality and expert-\ncurated few-shot demonstrations from a collected co-author graph to enable flexible context\nlength. Especially, AcademicEval features an efficient live evaluation, ensuring no label\nleakage. We conduct a holistic evaluation on AcademicEval, and the results illustrate that\nLLMs perform poorly on tasks with hierarchical abstraction levels and tend to struggle with\nlong few-shot demonstrations, highlighting the challenge of our benchmark. Through exper-\nimental analysis, we also reveal some insights for enhancing LLMs’ long-context modeling\ncapabilities. Code is available at https://github.com/ulab-uiuc/AcademicEval.\n1\nIntroduction\nLarge Language Models (LLMs) have recently achieved tremendous success in natural language processing\n(NLP) tasks (Achiam et al., 2023; AI@Meta, 2024).",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_001.md",
    "arxiv_id": "2510.17725v1",
    "page": "1",
    "title": "unknown",
    "id": "2510.17725v1_page_001"
  },
  {
    "text": "show a sharp decline in performance, which poses a pressing challenge to LLMs in understanding and\ncapturing key information in long texts (Li et al., 2024; Liu et al., 2024). Therefore, several long-context LLM\nbenchmarks are spawned to evaluate LLMs in various settings, including question answering, summarizing,\nand reasoning (Shaham et al., 2023; An et al., 2023; Dong et al., 2023; Bai et al., 2023b; Li et al., 2023;\nZhang et al., 2024b). Despite their success, these benchmarks still suffer from concerns of rigid context\nlength, saturated performance, and being leaked in LLM training.\nWe envision that the next-generation long-context LLM benchmarks should ideally possess three key features.\n(1) Flexible and potentially unlimited context length: existing benchmarks fix the context for each long-\ncontext problem; ideally, the format and length of the context could be flexibly set based on the LLM’s\ncapability, especially given the release of long-context LLMs (Reid et al., 2024) and their capabilities in\ningesting multi-modal information, e.g., graphs (Dong et al., 2024). (2) High-quality labels derived from\nreal-world data, minimizing human labeling efforts: existing long-context benchmarks often require human\nlabeling (Bai et al., 2023b; An et al., 2023; Li et al., 2023; Dong et al., 2023; Zhang et al., 2024b), which\n∗Work done as an intern at University of Illinois at Urbana-Champaign\n1\narXiv:2510.17725v1  [cs.CL]  20 Oct 2025",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_001.md",
    "arxiv_id": "2510.17725v1",
    "page": "1",
    "title": "unknown",
    "id": "2510.17725v1_page_001"
  },
  {
    "text": "average input length, whether the annotation is human-assisted, whether there are tasks with hierarchical\nabstraction levels, whether it contains few-shot demonstrations, and whether the benchmark is lively updated,\nrespectively.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_002.md",
    "arxiv_id": "2510.17725v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17725v1_page_002"
  },
  {
    "text": "Avg Len\nAutomatic\nAnnotation\nHierarchical\nAbstraction\nFew-shot\nDemos\nLive\nUpdate\nZeroSCROLLS (Shaham et al., 2023)\n∼10K\n✓\n✗\n✗\n✗\nL-Eval (An et al., 2023)\n∼8K\n✗\n✗\n✗\n✗\nBAMBOO (Dong et al., 2023)\n∼16K\n✗\n✗\n✗\n✗\nLongBench (Bai et al., 2023b)\n∼8K\n✗\n✗\n✓\n✗\nLooGLE (Li et al., 2023)\n∼20K\n✗\n✗\n✗\n✗\n∞Bench (Zhang et al., 2024b)\n∼200K\n✗\n✗\n✗\n✗\nAcademicEval (ours)\nFlexible\n✓\n✓\n✓\n✓\nis costly and limits the size of the benchmarks to about 2000 samples (Xu et al., 2023) (3) Live updates\nto mitigate information leakage during LLM pretraining and fine-tuning: benchmark data contamination in\nLLM has gradually become a severe issue (Sainz et al., 2023; Ye et al., 2024; Zhu et al., 2024b;a; Xu et al.,\n2024); we argue that holding out future data as the val/test set is one of the most effective approaches for\nopen benchmarks.\nBased on these principles, we propose AcademicEval, a live benchmark to evaluate LLMs over long-context\ngeneration tasks. AcademicEval adopts arXiv as its data source and features a suite of academic writing\ntasks on each paper without labor-intensive annotation: Title, Abstract, Introduction, and Related\nWork, each of which has long-context input and hierarchical abstraction levels. In particular, we construct\na co-author graph via the arXiv API to conveniently obtain co-author papers as high-quality and expert-\ncurated few-shot demonstrations, which also possess AcademicEval flexible context length. Furthermore,\nAcademicEval introduces efficient live evaluation based on the co-author graph, which utilizes the lat-\nest papers on arXiv to update the benchmark data periodically and ensures no label leakage. Moreover,\nAcademicEval provides in-context few-shot demonstrations for each sample, which is neglected by most ex-\nisting long-context LLM benchmarks (Liu et al., 2024; Li et al., 2024). In our experiments, we evaluate three\ncategories of baselines on AcademicEval: standard LLMs, long-context LLMs, and retrieval-augmented\nlanguage models (RALM). Under automatic metrics (BERTScore and ROUGE-L), RALM often attains the\nstrongest results by concentrating salient evidence into shorter retrieved chunks, while long-context LLMs\nand strong standard models remain competitive in several settings. However, an LLM-as-a-Judge evaluation,\nwhich assesses novelty, feasibility, consistency, factuality, and academic style, reveals a more nuanced picture:\nretrieval is not always preferred (e.g., for Title/Abstract), whereas it is highly beneficial for Related\nWork. Across both evaluations, performance commonly degrades as the input length grows, and correlated\nfew-shot demonstrations from the co-author graph can provide modest gains for specific model–task pairs.\nOverall, the results indicate that AcademicEval is a challenging benchmark that exposes complementary\nfacets of long-context modeling: overlap-oriented automatic metrics and higher-level judged quality.\nWe illustrate the comparison with existing long-context LLM benchmarks in Table 1. Our contributions are\nsummarized as follows:\n• We propose a live benchmark, AcademicEval, to evaluate LLMs over long-context generation\ntasks. AcademicEval features four academic writing tasks with hierarchical abstraction levels and\nrequires no manual annotation.\n• We construct a co-author graph via the arXiv API and draw on the co-author papers as informative\nfew-shot demonstrations, making the context length of AcademicEval flexible and scalable. Es-\n2",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_002.md",
    "arxiv_id": "2510.17725v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17725v1_page_002"
  },
  {
    "text": "pecially, AcademicEval conducts periodic data updates on the co-author graph to enable efficient\nlive evaluation, which ensures no label leakage and fair evaluation.\n• We conduct comprehensive experiments on AcademicEval, and the results demonstrate its chal-\nlenges and yield potential insights for improving LLMs in long-context modeling.\n2\nRelated Work\nLong-context Modeling and LLM Benchmarks. LLMs are known to be powerful in language modeling\ntasks (Achiam et al., 2023; AI@Meta, 2024). However, when it comes to long-context inputs, LLMs show a\nsharp decline in performance, posing a pressing challenge when benchmarking their long-context modeling\ncapabilities (Liu et al., 2024; Li et al., 2024; 2025). Currently, there are two mainstream technologies for\nlong-context modeling tasks: retrieval-augmented language models (RALM)(Ram et al., 2023; Yu et al.,\n2023; Trivedi et al., 2022; Jiang et al., 2023; Asai et al., 2023; Zhang et al., 2024a; Feng et al., 2024) and\nlong-context LLMs (Bai et al., 2023a; Jiang et al., 2024; Teknium et al.).\nRALM equips LLMs with a\nretriever (Robertson et al., 2009; Ramos et al., 2003; Karpukhin et al., 2020; Izacard et al., 2021) to perform\ninformation retrieval on short text chunks, which are then fed to LLMs together with the input query to\ngenerate the final output. As a retrieval system, RALM is usually evaluated over retrieval-based benchmarks,\nincluding STARK (Wu et al., 2024), RGB (Chen et al., 2024), ARES (Saad-Falcon et al., 2023), etc. In\ncomparison, long-context LLMs expand their context window length to accommodate longer inputs and are\nbenchmarked over various tasks, which include long-context QA, summarization, conversations, reasoning,\netc (Shaham et al., 2023; An et al., 2023; Dong et al., 2023; Bai et al., 2023b; Li et al., 2023; Zhang et al.,\n2024b; Li et al., 2025).",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_003.md",
    "arxiv_id": "2510.17725v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17725v1_page_003"
  },
  {
    "text": "proximity to our setting but target different goals. ResearchTown is a multi-agent simulation framework\nthat models the dynamics of a research community via message-passing on an agent–data graph, simulating\nactivities such as paper and review writing. Its focus lies in simulating collaborative behavior and ensuring\nthe realism of outputs under controlled settings. In contrast, AcademicEval is a live, real-world benchmark\ngrounded in authentic academic papers, designed to evaluate LLMs on hierarchical writing tasks (Title,\nAbstract, Introduction, and Related Work) under evolving and leakage-resistant conditions. While\nboth leverage graph structures, ResearchTown uses them for interaction simulation, whereas AcademicEval\nemploys a co-author graph for retrieving high-quality few-shot demonstrations, supporting scalable context\nlengths, and enabling periodic data updates.\nSimilarly, WildLong introduces a scalable framework for synthesizing realistic long-context instruction data.\nIt extracts meta-information from user queries, builds co-occurrence graphs, and employs adaptive generation\nto create 150K instruction–response pairs for complex multi-document reasoning tasks. While WildLong\nfocuses on data synthesis for instruction tuning, AcademicEval focuses on evaluation, providing a live,\nautomatically updated benchmark that measures LLMs’ long-context reasoning and generation abilities\non real-world academic tasks.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_003.md",
    "arxiv_id": "2510.17725v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17725v1_page_003"
  },
  {
    "text": "contribute to synthetic data generation and simulation, whereas AcademicEval provides a robust evaluation\nframework for real-world, graph-enabled long-context reasoning.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_003.md",
    "arxiv_id": "2510.17725v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17725v1_page_003"
  },
  {
    "text": "must attempt to avoid during data collection. However, recent research (Xu et al., 2024; Zhu et al., 2024b;a;\nYe et al., 2024) point out that most LLM benchmarks are composed of statically collected data, which may\nbe inevitably included in the large amount of training data of LLMs, causing label leakage.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_003.md",
    "arxiv_id": "2510.17725v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17725v1_page_003"
  },
  {
    "text": "some works attempt to measure or detect the extent of label leakage in LLM benchmarks. Benbench (Xu\net al., 2024) leverages perplexity and N-gram accuracy to quantify potential label leakage, while PAC (Ye\net al., 2024) detects contaminated data by comparing the polarized distance of samples before and after\naugmentation. Even though these approaches propose to measure or detect label leakage, there is little work\non mitigating and solving this issue (Zhu et al., 2024b). Dynabench (Kiela et al., 2021) and Dynaboard (Ma\net al., 2021) feature dynamic human-in-the-loop dataset creation while avoiding leakage, which is very labor-\nintensive. DyVal (Zhu et al., 2024b) leverages pre-set constraints and directed acyclic graphs (DAG) to\ndynamically generate test cases with diverse complexities, reducing the risk of label leakage. FreshBench (Zhu\n3",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_003.md",
    "arxiv_id": "2510.17725v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17725v1_page_003"
  },
  {
    "text": "et al., 2024a) and StackMIA (Ye et al., 2024) collect the latest data from public websites periodically and\nsimply rely on the chronological split to build a dynamic benchmark.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_004.md",
    "arxiv_id": "2510.17725v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17725v1_page_004"
  },
  {
    "text": "marization capability (Liu et al., 2024). Existing works include (1) query-based summarization tasks, focus-\ning on the capability of models to position and capture local key information in long texts given a specific\nquery (Litvak & Vanetik, 2017; Wang et al., 2022); (2) single-document or multi-document summarization\ntasks concentrate on evaluating the ability of models to understand long texts holistically (Cohan et al.,\n2018; Meng et al., 2021; Huang et al., 2021; Kryściński et al., 2021; Cachola et al., 2020). These long-context\nsummarization benchmarks suffer from the above-mentioned limitations, including requiring human-assisted\nlabeling and concerns about data leakage; moreover, these summarization tasks focus on one-level summa-\nrization, failing to consider the summarizations at different abstraction levels.\n3\nAcademicEval Benchmark\nIn this section, we propose AcademicEval (Figure 1) for live evaluation over long-context generation tasks\nwith hierarchical abstraction levels.\nWe first describe data collection and preprocessing in Section 3.1.\nThen, in Section 3.2, four academic writing tasks with diverse abstraction levels are introduced, and we\nalso integrate few-shot demonstrations to make the context length flexible and scalable. Finally, Section 3.3\nelucidates the live evaluation with periodic data updates.\n3.1\nData Curation\nCo-author Graph Construction via arXiv. As a public paper preprint platform, arXiv1 has always been\nfavored by researchers. It archives a huge amount of papers and updates the latest ones daily, which serves\nas an excellent data source and also lays the foundation for the live update of our benchmark. Thanks to the\narXiv API2, paper files can be obtained in batch without much manual effort. We first collect and construct\na co-author graph (i.e., edges are established between two co-author nodes) using the arXiv API through\nbreadth-first search (BFS), where the features of each author node include the published first-author papers.\nBy making the co-author graph the carrier of papers, we can form an interconnected whole of scattered\narticles, which provides valuable structural information to be exploited for our benchmark. Furthermore, we\ncan enable efficient live updates on the co-author graph, which will be introduced in Section 3.3.\nAcademic Data Gathering and Preprocessing.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_004.md",
    "arxiv_id": "2510.17725v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17725v1_page_004"
  },
  {
    "text": "authors who have not published independent first-author papers (i.e., appear only as co-authors in the author\nlist) and then prune it to obtain the maximum connected component. For each paper (i.e., node features),\nwe collect essential metadata via the arXiv API, including author information, publication timestamp, etc.,\nand download the PDF file simultaneously, which further goes through a series of pipelines to split and\nextract the text of several sections in it. In detail, we leverage PyMuPDF3 to detect section headings (e.g.,\n\"Introduction\") and extract the paper content by sections. Especially for the \"Related Work\" section, we\nextract each cited paper’s abstract and title via the arXiv API to form an additional citation corpus. All\nthese processed data constitute the node feature of each author node. We will further describe in Section 3.2\nhow to use these data to design long-context academic writing tasks.\n3.2\nBenchmarking LLMs over Long-context Generation Tasks with Hierarchical Abstraction\nTask Description. Employing machine learning approaches to automate academic writing has always been\na research hotspot with significant practical application value (Chen et al., 2022; 2021). Therefore, inspired\nby the leave-one-out validation, we introduce four academic writing tasks with ultra-long context to evaluate\nthe generation capability of LLMs under different abstraction levels, as shown below:\n1https://arxiv.org/\n2https://info.arxiv.org/help/api/index.html\n3https://github.com/pymupdf/PyMuPDF\n4",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_004.md",
    "arxiv_id": "2510.17725v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17725v1_page_004"
  },
  {
    "text": "...\n...\n...",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_005.md",
    "arxiv_id": "2510.17725v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17725v1_page_005"
  },
  {
    "text": "Co-author\nCo-author Graph\nFew-shotDemonstrations Selection\nAuthorNode\nIntegration\nTITLE\nABSTRACT\n...\n...\nMAIN BODY\n+\nINTRODUCTION\nRELATED WORK\nCITATION CORPUS\nPaper Sample\nFigure 1: AcademicEval Benchmark. We construct a co-author graph via arXiv and conduct a chrono-\nlogical split on all paper samples (training, validation, and test samples are represented by red, orange, and\ngreen, respectively). Each paper sample is preprocessed into separate sections and can be integrated with\nfew-shot demonstrations from co-author papers.\n• Title Writing.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_005.md",
    "arxiv_id": "2510.17725v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17725v1_page_005"
  },
  {
    "text": "prompt as inputs, and then asks LLMs to output a predicted title.\n• Abstract Writing. Similar to the above, this task takes a paper’s main body (with the \"Conclu-\nsion\" section removed) and title, along with a specific task prompt as inputs, and then asks LLMs\nto output a predicted abstract.\n• Introduction Writing. This task takes a paper’s main body (with the \"Introduction\" section\nremoved), title, and abstract, along with a specific task prompt as inputs, and then asks LLMs to\noutput a predicted introduction.\n• Related Work Writing. This task takes a paper’s main body (with the \"Related Work\" section\nremoved), title, abstract, and citation corpus (introduced in Section 3.1), along with a specific task\nprompt as inputs, and then asks LLMs to output a predicted related work.\nBased on the above task descriptions, we can generate four basic benchmark settings with different abstrac-\ntion levels, namely Title-10K, Abs-9K, Intro-8K and Related-34K, with suffixes indicating their input\ncontext length4. Intuitively, the paper content itself can be considered as a kind of original, expert-curated,\nand high-quality labeled data without manual annotation. Therefore, for evaluation, we directly adopt the\ncorresponding paper section as the ground truth for each benchmark setting, minimizing human labeling\nefforts.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_005.md",
    "arxiv_id": "2510.17725v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17725v1_page_005"
  },
  {
    "text": "benchmarks and the general effectiveness of in-context learning in LLMs (Dong et al., 2022; Wei et al.,\n2022a;b; Kojima et al., 2022), we propose to integrate long few-shot demonstrations to enable flexible and\nscalable context length, and we have two selection options for each sample in the above four basic benchmark\n4We use BERT (Devlin et al., 2018) tokenizer by default to count the number of input tokens (output tokens are not\nincluded).\n5",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_005.md",
    "arxiv_id": "2510.17725v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17725v1_page_005"
  },
  {
    "text": "four settings of different context length for each task. For each setting, we list their Comp. Rate, Samples\nof Each, Chronological Split, and Timespan of Test Data.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_006.md",
    "arxiv_id": "2510.17725v1",
    "page": "6",
    "title": "unknown",
    "id": "2510.17725v1_page_006"
  },
  {
    "text": "Comp. Rate\n(In-Len. / Out-Len.)\n#Samples\nof Each.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_006.md",
    "arxiv_id": "2510.17725v1",
    "page": "6",
    "title": "unknown",
    "id": "2510.17725v1_page_006"
  },
  {
    "text": "(Train-Val-Test)\nTimespan\nof Test Data\nTitle Writing\nTitle-10K\n587\n5098\n72%-19%-9%\n2024.06-\n2024.07\nTitle-30K\n1773\nTitle-31K-G\n1807\nTitle-50K-M\n2968\nAbstract Writing\nAbs-9K\n36\n5098\n72%-19%-9%\n2024.06-\n2024.07\nAbs-28K\n108\nAbs-29K-G\n112\nAbs-48K-M\n185\nIntroduction Writing\nIntro-8K\n6\n4665\n71%-20%-9%\n2024.06-\n2024.07\nIntro-28K\n21\nIntro-28K-G\n22\nIntro-48K-M\n37\nRelated Work Writing\nRelated-34K\n34\n2240\n72%-20%-8%\n2024.06-\n2024.07\nRelated-53K\n53\nRelated-53K-G\n53\nRelated-72K-M\n72\nNote: We use the BERT tokenizer by default to count the number of tokens.\nsettings: (1) Randomly select papers under the same category. According to the paper categories provided by\nthe arXiv API, we can randomly select several non-duplicate papers under the same category. (2) Randomly\nSelect co-author papers. The motivation is straightforward: the similarity of research directions between\nco-author papers is more fine-grained. Thanks to the co-author graph, it is convenient to obtain the co-\nauthor papers of each original paper sample. These selected papers serve as few-shot demonstrations and\nare utilized as input-output pairs to enrich the input context of the original samples, providing potentially\ninsightful and relevant content while enabling flexible and scalable context length.\nConsequently, we have completed the construction of benchmark settings, and the data statistics in the\ninitial collection round are shown in Table 2.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_006.md",
    "arxiv_id": "2510.17725v1",
    "page": "6",
    "title": "unknown",
    "id": "2510.17725v1_page_006"
  },
  {
    "text": "abstraction levels, and each task features four settings with diverse input context lengths, some of which\nare obtained by integrating few-shot demonstrations.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_006.md",
    "arxiv_id": "2510.17725v1",
    "page": "6",
    "title": "unknown",
    "id": "2510.17725v1_page_006"
  },
  {
    "text": "of a single paper sample. Title-30K and Title-31K-G are obtained by integrating with two few-shot\ndemonstrations from random papers and co-author papers, respectively, while Title-50K-M is obtained by\nusing both of the above integration options. Actually, we can scale context length by increasing the number\nof few-shot demonstrations to provide more informative references, enhancing task performance.\nFurthermore, we present the text compression rate (defined as the number of input tokens divided by the\nnumber of output tokens) for each benchmark setting in Table 2 to illustrate the diverse abstraction levels in\nAcademicEval. Across the four tasks, a higher compression rate means a higher level of text abstraction\n6",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_006.md",
    "arxiv_id": "2510.17725v1",
    "page": "6",
    "title": "unknown",
    "id": "2510.17725v1_page_006"
  },
  {
    "text": "...",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_007.md",
    "arxiv_id": "2510.17725v1",
    "page": "7",
    "title": "unknown",
    "id": "2510.17725v1_page_007"
  },
  {
    "text": "NEW\nNEW\nNEW\n1. Node Update\n2. Node & Edge Update\n3. Graph Pruning\nNEW\nNEW\nNEW\nFigure 2: Live Evaluation of AcademicEval Benchmark. To support continual benchmarking, Aca-\ndemicEval incrementally updates the co-author graph using daily arXiv data. The procedure includes: (1)\nNode Update – augmenting node features for authors with newly published first-author papers; (2) Node\nand Edge Update – identifying and prioritizing new co-authors via BFS to expand the graph with recent\npublications; and (3) Graph Pruning – removing outdated papers and inactive authors to maintain graph\nconnectivity and efficiency.\nin this task. Among several settings within each task, a higher compression rate makes it tougher to exploit\ninformation holistically but more likely to produce better outputs (since more references are integrated).\nThese different tasks and settings increase the diversity of the AcademicEval benchmark.\nAs for data splitting, we perform a chronological split in AcademicEval, which means that the test set\nalways contains the latest papers collected in each collection round, ensuring no label leakage. Note that\nTable 2 shows only the data collected in the initial round, which will be updated periodically as described\nin the next section.\n3.3\nLive Evaluation with Periodic Data Updates on the Co-author Graph\nThe daily updates of arXiv provide the basis for the live evaluation of AcademicEval: we can periodically\nupdate the benchmark with the latest papers on arXiv. By setting a reasonable update cycle (e.g., monthly\nor quarterly), we can ensure that the data in the benchmark is not contaminated so that it can be used to\nevaluate LLMs fairly in a live manner. Therefore, we proposed an efficient incremental update procedure on\nthe co-author graph:\n(1) Node Update. For each author on the co-author graph, check whether the author has a newly published\nfirst-author paper through the arXiv API. If so, add it to the corresponding node feature on the co-author\ngraph.\n(2) Node and Edge Update. During the traversal of Node Update, each author’s new co-authors are\nadded to a candidate list, and the number of new papers (including first-author and non-first-author papers)\nwhen searching for the author is used as the priority of the co-authors (co-authors of active authors tend\nto be active as well, and we can efficiently collect the latest papers from active authors). Then, we use the\nprioritized candidate list to conduct BFS to update nodes and edges until a specific number of incremental\nupdate papers is met.\n(3) Graph Pruning. As the benchmark is updated, we will remove some outdated papers and inactive\nauthors (defined as those who have not published new first-author or non-first-author papers for a long time)\nfrom the co-author graph.\nIn this way, the latest papers can be obtained sufficiently and efficiently while ensuring connectivity and a\nsmaller graph size.\n7",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_007.md",
    "arxiv_id": "2510.17725v1",
    "page": "7",
    "title": "unknown",
    "id": "2510.17725v1_page_007"
  },
  {
    "text": "4\nExperiments\n4.1\nBaselines\nWe adopt the following three types of baselines to conduct a holistic evaluation of AcademicEval.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_008.md",
    "arxiv_id": "2510.17725v1",
    "page": "8",
    "title": "unknown",
    "id": "2510.17725v1_page_008"
  },
  {
    "text": "LLMs.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_008.md",
    "arxiv_id": "2510.17725v1",
    "page": "8",
    "title": "unknown",
    "id": "2510.17725v1_page_008"
  },
  {
    "text": "(70B) (AI@Meta, 2024) as standard LLM baselines, each with a context length of 8K.\nLong-context LLMs.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_008.md",
    "arxiv_id": "2510.17725v1",
    "page": "8",
    "title": "unknown",
    "id": "2510.17725v1_page_008"
  },
  {
    "text": "(46.7B) (Jiang et al., 2024), and Nous Hermes 2 - Mixtral 8x7B-DPO (46.7B) (Teknium et al.) as long-context\nLLM baselines, each with a context length of 32K.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_008.md",
    "arxiv_id": "2510.17725v1",
    "page": "8",
    "title": "unknown",
    "id": "2510.17725v1_page_008"
  },
  {
    "text": "(1)\nBM25 (Robertson et al., 2009): This is a widely used retrieval model that ranks documents based on\nthe frequency of query terms in each document. (2) TF-IDF (Ramos et al., 2003): It scores documents\nby multiplying the term frequency of each query term by the inverse document frequency. Second, we also\nconsider three dense retrievers: (3) DPR (Karpukhin et al., 2020): It uses a bi-encoder to retrieve relevant\ndocuments based on dense embeddings. (4) Contriever (Izacard et al., 2021): It leverages unsupervised\ncontrastive learning to learn high-quality dense representations. (5) Dragon (Lin et al., 2023): It enhances\nretriever training by employing data augmentation, including query and label augmentation.\n4.2\nSettings\nAPI Access. In this paper, we conduct a comprehensive evaluation over AcademicEval benchmark using\nthe LLM API provided by together.ai5. For each API call, we fix the temperature parameter to 0 (i.e.,\ngreedy decoding).",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_008.md",
    "arxiv_id": "2510.17725v1",
    "page": "8",
    "title": "unknown",
    "id": "2510.17725v1_page_008"
  },
  {
    "text": "AcademicEval. However, since the tokenizer of each LLM is usually different, it will cause some inputs\nto exceed the context length limit of the LLM. Therefore, for the evaluation of each LLM, we additionally\ndownload its tokenizer configuration file from the official website at Hugging Face, which is utilized to ensure\ncorrect and accurate truncation of input tokens.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_008.md",
    "arxiv_id": "2510.17725v1",
    "page": "8",
    "title": "unknown",
    "id": "2510.17725v1_page_008"
  },
  {
    "text": "short.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_008.md",
    "arxiv_id": "2510.17725v1",
    "page": "8",
    "title": "unknown",
    "id": "2510.17725v1_page_008"
  },
  {
    "text": "evaluation metric score (although we have given LLM instructions not to generate irrelevant information).\nTherefore, for the Title Writing task, we additionally refine the LLM responses, for example, removing\nirrelevant information such as “here is the title”. For other tasks, since LLM’s responses are relatively long,\noccasional small amounts of irrelevant information will not have a significant impact on the evaluation, so\nwe do not perform any refinement on LLM’s responses in this case.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_008.md",
    "arxiv_id": "2510.17725v1",
    "page": "8",
    "title": "unknown",
    "id": "2510.17725v1_page_008"
  },
  {
    "text": "of RALM (such as Target Content and Reference Content introduced in Section D). For text split, we use\nthe RecursiveCharacterTextSplitter from LangChain6 and set chunk size and chunk overlap to 512 and 64,\nrespectively. For each retrieval, we recall up to 12 text chunks (limited by the context length of standard\nLLMs) based on text similarity (semantic similarity based on inner product for dense retrievers or similarity\nbased on word frequency for sparse retrievers).\n8",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_008.md",
    "arxiv_id": "2510.17725v1",
    "page": "8",
    "title": "unknown",
    "id": "2510.17725v1_page_008"
  },
  {
    "text": "Table 3: Main Results on AcademicEval w.r.t. BERTScore.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_009.md",
    "arxiv_id": "2510.17725v1",
    "page": "9",
    "title": "unknown",
    "id": "2510.17725v1_page_009"
  },
  {
    "text": "Standard LLMs\nLong-context LLMs\nRALM\nGemma\nLLaMA\nQwen\nMixtral\nHermes\nGemma†\nLLaMA†\n#Params.\n7B\n70B\n72B\n8x7B\n8x7B\n7B\n70B\nContext Length\n8K\n8K\n32K\n32K\n32K\n8K\n8K\nSetting: Title Writing\nTitle-10K\n66.1\n74.1\n73.9\n73.4\n74.2\n65.8\n73.9\nTitle-30K\n-\n-\n73.0\n72.9\n73.4\n65.7\n73.9\nTitle-31K-G\n-\n-\n72.8\n72.8\n73.3\n65.7\n73.8\nSetting: Abstract Writing\nAbs-9K\n59.9\n62.4\n62.5\n61.4\n62.2\n60.3\n61.5\nAbs-28K\n-\n-\n61.3\n61.2\n62.6\n60.1\n61.4\nAbs-29K-G\n-\n-\n61.3\n61.4\n62.5\n60.2\n61.3\nSetting: Introduction Writing\nIntro-8K\n54.8\n55.8\n55.4\n54.6\n55.2\n55.0\n55.2\nIntro-28K\n-\n-\n54.8\n54.0\n54.8\n55.0\n55.2\nIntro-28K-G\n-\n-\n54.9\n54.1\n54.7\n55.0\n55.3\nSetting: Related Work Writing\nRelated-34K\n52.0\n56.2\n58.5\n55.3\n57.8\n52.4\n54.7\nRelated-53K\n-\n-\n-\n-\n-\n52.4\n54.7\nRelated-53K-G\n-\n-\n-\n-\n-\n52.4\n54.8\nBold indicates the highest score in each row.\n† denotes augmentation with a retriever (Default: Contriever).\n“-” means that the context length is too long to be fed into LLMs.\n4.3\nAutomatic Metric Evaluation\n4.3.1\nEvaluation Setup\nFor automatic evaluation metrics, we adopt (1) BERTScore7 (Zhang et al., 2019): This metric leverages\nBERT-based embedding to measure semantic similarity between predicted and reference texts. (2) ROUGE-\nL (Lin, 2004): This metric evaluates the longest common subsequence between the generated and reference\ntexts, providing a measure of similarity in terms of sequential matching. For both metrics, higher scores\nindicate a better match between the predicted and the reference text.\n4.3.2\nResult Analysis\nWe conduct comprehensive experiments on the four academic writing tasks, and the results w.r.t. BERTScore\nand RougeL are presented in Table 3 and 4, respectively. Note that we do not conduct experiments on -M\nsettings because its context length is too long for most of our selected baselines.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_009.md",
    "arxiv_id": "2510.17725v1",
    "page": "9",
    "title": "unknown",
    "id": "2510.17725v1_page_009"
  },
  {
    "text": "LLMs over long-context generation tasks with different abstraction levels. From Table 3 and 4, we can clearly\nobserve that it provides different difficulties for LLMs to perform well from Title Writing to Related\nWork Writing tasks, and the results of all baselines on these four tasks have a relatively obvious trend.\n5https://www.together.ai/\n6https://www.langchain.com/\n7We use deberta-xlarge-mnli (He et al., 2021) instead of the default roberta-large (Liu et al., 2019) as the backbone model\nto have the best correlation with human evaluation.\n9",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_009.md",
    "arxiv_id": "2510.17725v1",
    "page": "9",
    "title": "unknown",
    "id": "2510.17725v1_page_009"
  },
  {
    "text": "Table 4: Main Results on AcademicEval w.r.t. RougeL.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_010.md",
    "arxiv_id": "2510.17725v1",
    "page": "10",
    "title": "unknown",
    "id": "2510.17725v1_page_010"
  },
  {
    "text": "Standard LLMs\nLong-context LLMs\nRALM\nGemma\nLLaMA\nQwen\nMixtral\nHermes\nGemma†\nLLaMA†\n#Params.\n7B\n70B\n72B\n8x7B\n8x7B\n7B\n70B\nContext Length\n8K\n8K\n32K\n32K\n32K\n8K\n8K\nSetting: Title Writing\nTitle-10K\n44.5\n47.1\n44.2\n45.2\n46.2\n42.7\n47.3\nTitle-30K\n-\n-\n44.5\n44.6\n45.9\n42.6\n47.3\nTitle-31K-G\n-\n-\n44.2\n44.4\n45.3\n42.5\n47.0\nSetting: Abstract Writing\nAbs-9K\n22.4\n25.0\n24.3\n24.1\n26.1\n23.4\n24.2\nAbs-28K\n-\n-\n23.3\n24.7\n26.6\n23.1\n24.1\nAbs-29K-G\n-\n-\n23.3\n24.9\n26.6\n23.2\n24.0\nSetting: Introduction Writing\nIntro-8K\n14.9\n18.1\n16.2\n17.2\n17.8\n15.4\n17.9\nIntro-28K\n-\n-\n16.3\n17.5\n17.5\n15.3\n17.8\nIntro-28K-G\n-\n-\n16.3\n17.5\n17.5\n15.4\n17.8\nSetting: Related Work Writing\nRelated-34K\n13.5\n14.9\n16.0\n13.4\n15.1\n14.1\n15.3\nRelated-53K\n-\n-\n-\n-\n-\n14.0\n15.3\nRelated-53K-G\n-\n-\n-\n-\n-\n14.0\n15.2\nBold indicates the highest score in each row.\n† denotes augmentation with a retriever (Default: Contriever).\n“-” means that the context length is too long to be fed into LLMs.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_010.md",
    "arxiv_id": "2510.17725v1",
    "page": "10",
    "title": "unknown",
    "id": "2510.17725v1_page_010"
  },
  {
    "text": "which may indicate that the Title Writing task is easier than the Abstract Writing task. Since a\ntitle only has a few words, LLMs only need to generate a roughly related theme to achieve a high semantic\nsimilarity, while an abstract requires a more detailed description to achieve it.\nBaseline Performance Comparison.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_010.md",
    "arxiv_id": "2510.17725v1",
    "page": "10",
    "title": "unknown",
    "id": "2510.17725v1_page_010"
  },
  {
    "text": "tains the highest scores in multiple settings (e.g., Title-30K/31K-G, Intro-28K/28K-G, and Related-\n53K/53K-G), despite using an 8K input window. Standard LLMs remain competitive and long-context\nLLMs (e.g., Qwen, Hermes) lead in some settings (e.g., Related-34K on BERTScore).",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_010.md",
    "arxiv_id": "2510.17725v1",
    "page": "10",
    "title": "unknown",
    "id": "2510.17725v1_page_010"
  },
  {
    "text": "the shortcomings of long-context LLMs’ generation capabilities, which are well revealed by AcademicE-\nval. Among long-context LLMs, Hermes performs best overall, but is still slightly inferior to RALM with\nLLaMA. This shows that although the current long-context LLMs have a longer context window size, they\nstill have great deficiencies in processing long text information. Overall, RALM often has an edge under au-\ntomatic metrics, likely because retrieval concentrates salient content into shorter chunks, thereby maximizing\noverlap-oriented scores.\nImpact of Context Length.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_010.md",
    "arxiv_id": "2510.17725v1",
    "page": "10",
    "title": "unknown",
    "id": "2510.17725v1_page_010"
  },
  {
    "text": "settings and both metrics, with baselines often performing worse as the context length increases, though\nthe extent is model- and task-dependent. For example, the Title Writing task shows a noticeable drop\nin scores as the context length extends from 10K to 31K tokens. This trend is also apparent in Abstract\nWriting and Introduction Writing, where longer contexts correlate with decreased model performance,\nshowing that our benchmark challenges LLMs in effectively processing ultra-long inputs.\n10",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_010.md",
    "arxiv_id": "2510.17725v1",
    "page": "10",
    "title": "unknown",
    "id": "2510.17725v1_page_010"
  },
  {
    "text": "Table 5: Additional results on AcademicEval w.r.t. LLM-as-a-Judge win rate (%).",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_011.md",
    "arxiv_id": "2510.17725v1",
    "page": "11",
    "title": "unknown",
    "id": "2510.17725v1_page_011"
  },
  {
    "text": "Standard LLMs\nLong-context LLMs\nRALM\nGemma\nLLaMA\nQwen\nMixtral\nHermes\nGemma†\nLLaMA†\n#Params.\n7B\n70B\n72B\n8x7B\n8x7B\n7B\n70B\nContext Length\n8K\n8K\n32K\n32K\n32K\n8K\n8K\nSetting: Title Writing\nTitle-10K\n45.7\n42.7\n63.2\n43.1\n72.0\n50.0\n43.9\nTitle-30K\n-\n-\n54.5\n45.6\n62.5\n47.5\n44.9\nTitle-31K-G\n-\n-\n52.4\n62.7\n45.4\n47.7\n43.7\nSetting: Abstract Writing\nAbs-9K\n12.0\n55.5\n77.0\n70.0\n61.1\n14.3\n43.2\nAbs-28K\n-\n-\n72.7\n66.1\n41.2\n12.7\n42.0\nAbs-29K-G\n-\n-\n71.0\n65.9\n40.7\n12.0\n43.9\nSetting: Introduction Writing\nIntro-8K\n34.6\n63.2\n79.3\n61.5\n58.0\n48.8\n64.1\nIntro-28K\n-\n-\n70.3\n60.1\n56.9\n46.5\n62.9\nIntro-28K-G\n-\n-\n70.9\n61.9\n59.3\n48.2\n63.9\nSetting: Related Work Writing\nRelated-34K\n55.9\n91.9\n91.2\n65.6\n88.6\n71.3\n89.8\nRelated-53K\n-\n-\n-\n-\n-\n72.5\n90.7\nRelated-53K-G\n-\n-\n-\n-\n-\n71.7\n90.2\nBold indicates the highest score in each row.\n† denotes augmentation with a retriever (Default: Contriever).\n“-” means that the context length is too long to be fed into LLMs.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_011.md",
    "arxiv_id": "2510.17725v1",
    "page": "11",
    "title": "unknown",
    "id": "2510.17725v1_page_011"
  },
  {
    "text": "shot demonstrations yields mixed effects: in several settings it is neutral or slightly negative under automatic\nmetrics, yet correlated demonstrations can produce small but consistent gains for certain model–task pairs.\nThis shows that current LLMs cannot exploit long few-shot demonstrations to benefit the target tasks well,\nemphasizing the importance of evaluating long in-context learning in LLM benchmarks. In addition, we can\nalso find that few-shot demonstrations from co-author papers generally have a more positive impact on task\nperformance than randomly selected ones.\n4.4\nLLM-as-a-Judge Evaluation\n4.4.1\nEvaluation Setup\nTo complement automatic metrics, we further incorporate an LLM-as-a-Judge evaluation to capture\nhigher-level qualitative aspects beyond semantic overlap. Specifically, we employ the open-source Mixtral-\n8x22B-Instruct-v0.1 (Jiang et al., 2024) to assess five dimensions of generation quality: (1) Novelty — the\ndegree to which the content introduces new and meaningful ideas; (2) Feasibility — the plausibility and\npracticality of the described methods or claims; (3) Consistency — the internal logical coherence of the out-\nput; (4) Factuality — the correctness of factual statements; and (5) Academic Style — the alignment with\nconventions of scholarly writing, enabling a more nuanced evaluation of LLM outputs. For each task, we\nreport the win rate (%), i.e., the percentage of cases where the generated text is preferred over the reference\naccording to the LLM judge. The detailed LLM-as-a-Judge prompt can be found in Appendix D.\n11",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_011.md",
    "arxiv_id": "2510.17725v1",
    "page": "11",
    "title": "unknown",
    "id": "2510.17725v1_page_011"
  },
  {
    "text": "BERT Score\nRouge L\n50\n52\n54\n56\n58\n60\n62\n64\nBERT Score\nContriever\nDPR\nDragon\nBM25\nTF-IDF\n15\n17\n19\n21\n23\n25\nRouge L\nGemma Instruct (7B)\nBERT Score\nRouge L\n50\n52\n54\n56\n58\n60\n62\n64\nBERT Score\nContriever\nDPR\nDragon\nBM25\nTF-IDF\n15\n17\n19\n21\n23\n25\nRouge L\nLLaMA-3 Chat (70B)\nFigure 3: Analysis of RALM on Abs-9K. The left figure shows results with Gemma Instruct (7B), while\nthe right one shows results with LLaMA-3 Chat (70B).\n4.4.2\nResult Analysis\nWe report results for the overall preference in Table 5. Compared to BERTScore and ROUGE-L, the LLM-\nas-a-Judge evaluation reveals partially different patterns, reflecting that it targets broader qualitative aspects\nbeyond lexical or semantic overlap.\nTitle Writing. Under Title-10K, Hermes achieves the highest win rate (72.0), while Mixtral becomes the\ntop model under the correlated setting Title-31K-G (62.7). This suggests that (1) short, highly abstract\noutputs benefit from strong style and concision (favoring Hermes at 10K); and (2) correlated contexts can\nhelp certain models (e.g., Mixtral) at longer lengths when the judge considers qualities beyond surface\nsimilarity. Notably, RALM variants (Gemma†, LLaMA†) are not consistently preferred in the title task,\nindicating that aggressive retrieval does not always align with judged title quality.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_012.md",
    "arxiv_id": "2510.17725v1",
    "page": "12",
    "title": "unknown",
    "id": "2510.17725v1_page_012"
  },
  {
    "text": "lengths (Abs-28K: 72.7; Abs-29K-G: 71.0). Mixtral follows closely, while RALM variants trail in preference.\nThis contrasts with automatic metrics where RALM often ranks highly, suggesting that, for abstracts, the\njudge values holistic qualities (coherence, feasibility, academic style) over high overlap with references.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_012.md",
    "arxiv_id": "2510.17725v1",
    "page": "12",
    "title": "unknown",
    "id": "2510.17725v1_page_012"
  },
  {
    "text": "70.9), and Hermes improves slightly with correlated contexts (56.9 →59.3). LLaMA† remains competitive\nbut is not top-ranked. Overall, correlated few-shot demonstrations offer modest gains for some models, sup-\nporting that graph-informed contexts can help introductions when evaluated on broader quality dimensions.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_012.md",
    "arxiv_id": "2510.17725v1",
    "page": "12",
    "title": "unknown",
    "id": "2510.17725v1_page_012"
  },
  {
    "text": "preferences at longer lengths (Related-53K: 90.7; Related-53K-G: 90.2), aligning with the intuition that\nretrieval is particularly beneficial for Related Work, where judged quality rewards appropriate citations,\nprior studies, and domain-specific terminology.\nTakeaways. (1) The judge-based preferences are not dominated by RALM across all tasks; instead, pref-\nerences depend on task nature and qualitative dimensions. (2) Correlated contexts can yield improvements\nin several settings (e.g., Mixtral on Title-31K-G, Hermes on Intro-28K-G), though gains are model-\ndependent. (3) The divergence from automatic metrics underscores their complementarity: automatic met-\nrics reward overlap, whereas the judge emphasizes higher-level writing quality.\n4.5\nDiscussion\nAdditional Analysis on RALM. We conduct extensive experiments on RALM on the Abs-9K setting\nusing standard LLMs Gemma Instruct (7B) and LLaMA-3 Chat (70B), and the results are presented in\nFigure 3. We can find that the performance of dense retrievers consistently outperforms sparse retrievers,\n12",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_012.md",
    "arxiv_id": "2510.17725v1",
    "page": "12",
    "title": "unknown",
    "id": "2510.17725v1_page_012"
  },
  {
    "text": "reliance on external context rather than in-weights memorization.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_013.md",
    "arxiv_id": "2510.17725v1",
    "page": "13",
    "title": "unknown",
    "id": "2510.17725v1_page_013"
  },
  {
    "text": "Model\nBERTScore\nROUGE-L\nDefault (Abs-9K)\nLLaMA\n62.4\n25.0\nTitle-only (Abs-9K)\nLLaMA\n57.4\n18.8\nDefault (Abs-9K)\nHermes\n62.2\n26.1\nTitle-only (Abs-9K)\nHermes\n56.7\n19.3\namong which contriever achieves the best results. This is because the summary generation task emphasizes\nsemantic similarity, which can be well measured by the similarity of dense embeddings. However, the sparse\nretrievers perform text chunk recall based on sparse embeddings, and the results are significantly worse than\nthose of the dense retrievers.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_013.md",
    "arxiv_id": "2510.17725v1",
    "page": "13",
    "title": "unknown",
    "id": "2510.17725v1_page_013"
  },
  {
    "text": "longer contexts (e.g., 9K→30K) invites further examination of its underlying causes. While our analysis\nattributes this plateau partly to the limited ability of current models to utilize ultra-long inputs through\nin-context learning (ICL), another plausible factor lies in in-weights learning (IWL) (Chan et al.,\n2024). That is, certain academic knowledge may already be internalized during pretraining. In such cases,\nadding more context brings diminishing informational returns even when the benchmark itself remains well-\nconstructed.\nTo\nbetter\nunderstand\nthis\nphenomenon,\nwe\nanalyze\nboth\nstructural\nand\nempirical\nevi-\ndence.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_013.md",
    "arxiv_id": "2510.17725v1",
    "page": "13",
    "title": "unknown",
    "id": "2510.17725v1_page_013"
  },
  {
    "text": "AcademicEval\norganizes\ntasks\nacross\nhierarchical\nabstraction\nlevels\n(Title→Abstract→Introduction→Related Work), where deeper contextual reasoning becomes\nincreasingly essential. Plateaus may thus occur when longer inputs introduce redundancy rather than new\ncues, suggesting ICL saturation instead of pure memorization. Empirically, we conduct a Title-only ablation\non the Abstract Writing task under Abs-9K, where most contextual information is removed except for\nthe paper title. As shown in Table 6, the BERTScore and ROUGE-L of both LLaMA and Hermes drop\nsharply (by 5–7 points), confirming that model performance depends strongly on the provided context and\nis not solved by IWL alone.\nOverall, our evidence indicates that the observed plateau on AcademicEval is primarily driven by imperfect\nlong-context utilization (ICL limitation), rather than by IWL. This reading is supported by the Title-only\nablation, where removing most contextual information yields substantial drops in both BERTScore and\nROUGE-L, indicating strong dependence on the provided context.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_013.md",
    "arxiv_id": "2510.17725v1",
    "page": "13",
    "title": "unknown",
    "id": "2510.17725v1_page_013"
  },
  {
    "text": "designed as a live-updating benchmark that continuously incorporates newly published arXiv papers via\nthe co-author graph, the evaluation set evolves over time and reduces the likelihood that performance is\ndominated by pre-encoded (in-weights) knowledge. While diminishing informational returns can occur when\nadditional tokens introduce redundancy, AcademicEval serves as a diagnostic lens showing that the plateau\nchiefly reflects current models’ limited ability to exploit ultra-long inputs under realistic, evolving conditions.\nThis discussion also aims to inspire further reflection in the long-context benchmarking community on how\ndataset design and periodic updates can better disentangle ICL and IWL effects, while underscoring the\ncontinued importance of mitigating data leakage in future benchmark construction.\n5\nConclusion\nIn this paper, we propose AcademicEval, a live long-context LLM benchmark for evaluating long-context\ngeneration tasks with hierarchical abstraction levels.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_013.md",
    "arxiv_id": "2510.17725v1",
    "page": "13",
    "title": "unknown",
    "id": "2510.17725v1_page_013"
  },
  {
    "text": "and introduces several long-context academic writing tasks without manual annotation since the papers on\narXiv can be regarded as original, high-quality, and expert-curated labels. Moreover, we integrate few-shot\ndemonstrations from a collected co-author graph to make the context length of our benchmark flexible and\nscalable. An efficient live evaluation is also designed to make AcademicEval immune to the label leakage\n13",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17725v1\\page_013.md",
    "arxiv_id": "2510.17725v1",
    "page": "13",
    "title": "unknown",
    "id": "2510.17725v1_page_013"
  },
  {
    "text": "A Multi-threading Kernel for Enabling\nNeuromorphic Edge Applications\nLars Niedermeier∗‡, Vyom Shah†, and Jeffrey L. Krichmar†‡\n∗Niedermeier Consulting, Zurich, ZH, Switzerland\n†Department of Computer Science, University of California, Irvine, CA, USA\n‡Department of Cognitive Sciences, University of California, Irvine, CA, USA\nCorrespondence Email: lars@niedermeier-consulting.ch\nAbstract—Spiking Neural Networks (SNNs) have sparse, event-\ndriven processing that can leverage neuromorphic applications.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_001.md",
    "arxiv_id": "2510.17745v1",
    "page": "1",
    "title": "unknown",
    "id": "2510.17745v1_page_001"
  },
  {
    "text": "neuromorphic applications running at the edge, meaning they\nprocess sensory input directly and without any up-link to or\ndependency on a cloud service. The kernel shows speed-up gains\nover single thread processing by a factor of four on moderately\nsized SNNs and 1.7X on a Synfire network. Furthermore, it\nload-balances all cores available on multi-core processors, such\nas ARM, which run today’s mobile devices and is up to 70%\nmore energy efficient compared to statical core assignment. The\npresent work can enable the development of edge applications\nthat have low Size, Weight, and Power (SWaP), and can prototype\nthe integration of neuromorphic chips.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_001.md",
    "arxiv_id": "2510.17745v1",
    "page": "1",
    "title": "unknown",
    "id": "2510.17745v1_page_001"
  },
  {
    "text": "Spiking Neural Networks\nI. INTRODUCTION\nSpiking Neural Networks (SNNs) mimic natural nervous\nsystems with elements that replicate the sparse, all-or-none\nneural activity. This makes them a good fit to take advantage of\nlow Size, Weight, and Power (SWaP) computing systems. The\ndownside of SNNs are the higher computational costs for the\nnumerical solution of the differential equations that determine\nthe neuron’s state. Software packages such as CARLsim have\nevolved over the years to provide a mature framework for\ncomputational neuroscientists, embedded systems engineers,\nand roboticists [1].",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_001.md",
    "arxiv_id": "2510.17745v1",
    "page": "1",
    "title": "unknown",
    "id": "2510.17745v1_page_001"
  },
  {
    "text": "applications, methods are needed to quantitatively measure\nperformance. Recently, a multilayered recurrent SNN bench-\nmark, called a Synfire chain, was used to measure the energy\nefficiency of the SpiNNaker2 neuromorphic chip [2]. The\nSynfire network, which consisted of leaky-integrated-and-fire\n(LIF) spiking neurons, is used to measure the dynamic voltage\nand frequency scaling (DVFS) developed for SpiNNaker [3].",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_001.md",
    "arxiv_id": "2510.17745v1",
    "page": "1",
    "title": "unknown",
    "id": "2510.17745v1_page_001"
  },
  {
    "text": "with CARLsim and the Izhikevich neuron model [4], [5].",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_001.md",
    "arxiv_id": "2510.17745v1",
    "page": "1",
    "title": "unknown",
    "id": "2510.17745v1_page_001"
  },
  {
    "text": "range of dynamics and can mimic a variety of neuron types\nfound in the brain. Fig. 1 presents the Synfire network de-\nveloped to benchmark CARLsim. The excitatory groups E\nconsist of regular spiking (RS) neurons found as pyramidal\ncells in the cortex, the inhibitory groups I of fast spiking (FS)\ninterneurons. The network is segmented in four partitions that\nare assigned to a dedicated core. We follow [6] in connecting\nthe last segment partition3 to the first paritition0 in contrast\nto the original referenced Synfire network is a strict feed-\nforward-inhibition (FFI) network [7].\n(a) Synfire network as CARLsim kernel benchmark.\n(b) Spikewave propagation with correlated inhibition.\n(c) PThreads kernel (CARLsim 4).\n(d) New multi-threading kernel.\nFig. 1.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_001.md",
    "arxiv_id": "2510.17745v1",
    "page": "1",
    "title": "unknown",
    "id": "2510.17745v1_page_001"
  },
  {
    "text": "the stimulus. (a) Izhikevich neurons in four partitions recurrently linked.\n(b) Neural activity visualized in CARLsim spike monitor. (c) Paritions with\nfixed core affinity in pthreads kernel of CARLsim4. (d) The new kernel is\nmore energy efficient by assigning cores dynamically to free up SoC compute\nresources.\narXiv:2510.17745v1  [cs.NE]  20 Oct 2025",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_001.md",
    "arxiv_id": "2510.17745v1",
    "page": "1",
    "title": "unknown",
    "id": "2510.17745v1_page_001"
  },
  {
    "text": "require parallel processing on GPUs, multi-core CPUs, or\nneuromorphic hardware. For instance, small networks up to\na few hundred neurons run most efficiently in a single thread\non a CPU. The efficient simulation of large-scale networks\nwith millions of neurons, as typically used in computational\nneuroscience [8], are the core feature of CARLsim as its\nPThreads based kernel scales over multiple GPUs [9]. In the\ncase of mid-size networks (e.g., 103 neurons), which are often\nused for edge applications, the overhead for PThreads has poor\nperformance compared to an execution on a single thread.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_002.md",
    "arxiv_id": "2510.17745v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17745v1_page_002"
  },
  {
    "text": "that scales efficiently over the available cores of modern CPUs\nused in SoCs such as ARM Cortex-76 in Raspberry Pi 5. This\naddresses the performance limitation of PThreads in CARLsim\nand other SNN simulators. All code and models are open-\nsource and available on [10]. The main contributions of this\nwork are:\n1) Multi-threading for SNNs. A multi-threading kernel\nthat scales independently of the partitioning of the\nnetwork.\n2) Dynamic load balancing. A load-balancing algorithm\nthat dynamically allocates computation to cores and\navoids synchronization bottlenecks of idle threads.\n3) Performance monitoring. An SNN performance moni-\ntor with ms precision.\n4) Synthetic load network. A synthetic load network\nnamed Chainfire that produces and measures neural and\nsynaptic activity.\n5) SNN Benchmarks. Concrete benchmark results on Intel\nand ARM processors.\n6) Edge processing. SNNs that fulfill real-time require-\nments on off-the shelf mobile processors, without the\nneed of specialized neuromorphic hardware.\nII. METHODS\nA. Spiking neuron model\nCARLsim efficiently implements spiking neural networks\nsuch as LIF and the Izhikevich neuron model with 4 and 9\nparameters [1]. In contrast to [2], we implemented the Synfire\nnetwork utilizing Izhikevich 4-parameter model described by\nthe following equations [4].\n˙v\n=\n0.04v2 + 5v + 140 −u + I\n(1)\n˙u\n=\na(bv −u)\n(2)\nif v ≥30\n(\nv = c\nu = u + d\n(3)\nThe present simulations use Forward Euler for numerical\nintegration. More complicated neuron models with multiple\ncompartments may require other numerical methods, such as\nRunge-Kutta, to handle instabilities. The CARLsim kernel is\ndesigned to handle these cases.\nB. Synfire network architecture\nWe utilize the Synfire chain network as a benchmark to mea-\nsure and compare performance of CARLsim and potentially\nother neuromorphic chips. We follow H¨oppner et. al. in their\napproach for SpiNNaker [2] and built a network with the same\nstructure and sizing, see Fig. 1. The excitatory groups E has\n200 regular spiking (RS) neurons (a = 0.02, b = 0.2, c =\n−65, d = 8), and the inhibitory groups consist of 50 fast\nspiking (FS) neurons (a = 0.1, b = 0.2, c = −65, d = 2).",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_002.md",
    "arxiv_id": "2510.17745v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17745v1_page_002"
  },
  {
    "text": "the CARLsim GitHub repository [10]. Using the CARLsim\nmulti-thread kernel, we replicated Synfire results for current\nbased (CUBA) and conductances based (COBA) synapses. See\nFig. 3db and GitHub repository [11] for the benchmark runs\nand Subsection II-B.\nC. Chainfire network architecture\nIn addition to using the Synfire chain SNN for benchmark-\ning, we created a Chainfire network, which could more readily\ncontrol parameters that affect CPU loads. Fig. 2 shows the\nsynthetic load network Chainfire with the minimal amount\nof synapses that allows to produce and measure arbitrary\nloads, specifically targeted at the compute-intensive status\nupdates of neurons, defined by equations 1 - 3. The network\nis designed to generate loads similar to the Synfire network\nand therefore has four clusters, that are marked in blue. This\nallows validation, tuning, scaling of the parallelization, and\nprovides the maximum of performance improvement possible\nby this kernel.\nFig. 2. Chainfire SNN for validation, tuning, and scaling of the parallelization.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_002.md",
    "arxiv_id": "2510.17745v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17745v1_page_002"
  },
  {
    "text": "loads on the multi-threading kernel. The green squares depict synchronization\nneurons that dictate the fan-in and fan-out between neuron groups.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_002.md",
    "arxiv_id": "2510.17745v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17745v1_page_002"
  },
  {
    "text": "specific delays and weights (e.g. dexc = 5 ms, wexc = 0.432),\nconfigured to propagate the spike wave with minimal delay\nand without noise. The synchronization neurons, indicated\nby a green frames, are the also excitatory RS neurons and\nconsolidate the fan-in for predecessor group or integrate the\nfan-out.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_002.md",
    "arxiv_id": "2510.17745v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17745v1_page_002"
  },
  {
    "text": "following parameters: N total number of neurons per cluster,\nd ms of delay for pre-synaptic to post-synaptic neuron in the\nparallel chains that are sequential connected, and the span\nin ms that determine the length of the chain, e.g. N = 100,\nd = 20ms, span = 100ms results in the shown network groups\nwith 100 neurons, structured in 4 rows (parallel chains) and\nfive columns.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_002.md",
    "arxiv_id": "2510.17745v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17745v1_page_002"
  },
  {
    "text": "that a spike activates the input layer (first column) at the same\ntime. The spike-wave travels through the group and activates",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_002.md",
    "arxiv_id": "2510.17745v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17745v1_page_002"
  },
  {
    "text": "each neuron of the adjacent column. The output layer (last\ncolumn) transmits the spikes to the fan-out which has reduced\nweights, so that the synchronization neurons produce a single\nspike. This in turn feeds back to the first column of the\nChainfire.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_003.md",
    "arxiv_id": "2510.17745v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17745v1_page_003"
  },
  {
    "text": "delays enforce a travel time of the spike-wave of around\n500 ms (400 ms for the cluster plus inter-neurons, and some\nprocessing time around 2 ms for the RS neurons). The spike-\nwave travels through the network without interference to the\nlast synchronization neuron, before the next stimuli is induced.\nD. Benchmark processors\nSNNs running at the edge can take advantage of off-the-\nshelf multi-core processors. The CARLsim multi-threaded\nkernel utilizes available cores of modern energy efficient\nCPUs, such as the ARM Cortex-76 in Raspberry Pi 5. These\nprocessors are typically built in System-on-chips (SoCs), and\nare used to operate devices at the edge. For a moderate sized\nSNN to run efficiently at the edge, care must be taken to ensure\nall cores have fully balanced loads. Algorithm 1 outlines\nparallelization used in the multi-threaded kernel.\nAlgorithm 1: Multi-threading kernel for neuron state updates.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_003.md",
    "arxiv_id": "2510.17745v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17745v1_page_003"
  },
  {
    "text": "steps per ms S, core threads T\nOutput: spikes\n1 while step ∈S do\n/* numerical integration */\n2\nwhile group ∈partion do\n/* initialize shared */\n3\nwhile neuron ∈group do\n/* in parallel T */\n4\nIsum = CUBA/COBA;\n/* synaptic fan-in */\n5\nv, u = EULER/RK4(dudt, dvdt, a, b, c, d, Isum)\n6\nif v > 30 then\n/* Izhikevich */\n7\nspike = true;\n8\nruntimeData.recovery[lNId] = u;\n/* update */\nE. Load balancing by dynamic core assignment (DCA)\nOne reason SNNs are energy efficient is that their activity\nis sparse with a firing rate of a few Hz with occasional spike\nbursts. CARLsim utilizes the intrinsic Dynamic Voltage and\nFrequency Scaling (DVFS) of modern CPUs, which provide\nadvanced energy policies. Intel no longer recommends direct\nmanipulation of the P-states to manipulate DVFS and rec-\nommends only to use power profiles for the sake of system\nstability.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_003.md",
    "arxiv_id": "2510.17745v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17745v1_page_003"
  },
  {
    "text": "glected. Consequently, our load-balancer allocates only the\nminimum cores necessary to fulfill the performance criteria,\nfor instance real-time, meaning 1 ms in the SNN corresponds\nto 1 ms wall clock time.\nIII. RESULTS\nWe provide benchmark results for the Intel i9 with 8 cores\nand several ARM Cortex processors with 4 cores. All results\ncan be reproduced with the open source of the CARLsim\nrepository. Furthermore, we provide supplemental material\nsuch as videos and log of the run on GitHub [11].\nA. Performance gain by multi-threading on Chainfire\nWe measure Chainfire performance on a release build and\na version with debugging enabled. Fig. 3a and Tables I and\nII present the performance of a Chainfire network with 2000\nneurons run on a Intel i9 desktop processor. The simulation\nmodel time was 10s. The overall spike count is 20, 040 and\napplied as a measure to determine that the neural activity is\nsame for all (parallel) runs. The texecution (treal) is the wall\nclock time, which is implemented by the C++ standard library\n(STL) std::chrono::steady clock. Performance is measured by\na speed factor, defined as tmodel/texecution. For example, if\nthe simulation of the SNN with eight threads runs in 2.28s,\nit has a speed factor = 4.4. If the single threaded run takes\n8.88s, it a the speed factor = 1.1. The resulting performance\ngain is then 4.4/1.1 = 4.0.\nTABLE I\nPERFORMANCE IMPROVEMENTS CHAINFIRE (RELEASE BUILD)\nThreads\nExecution Time\nSpeed Factor\nPerformance Gain\n1\n8.88 s\n1.1 x\n2\n5.20 s\n1.9 x\n1.7 x\n4\n3.09 s\n3.2 x\n2.9 x\n8\n2.38 s\n4.2 x\n3.8 x\n16\n2.28 s\n4.4 x\n4.0 x\n32\n5.87 s\n1.7 x\n1.5 x\nTABLE II\nPERFORMANCE IMPROVEMENTS CHAINFIRE (DEBUG BUILD)\nThreads\nExecution Time\nSpeed Factor\nPerformance Gain\n1\n14.82 s\n67.5%\n2\n8.49 s\n1.2 x\n1.8 x\n4\n4.93 s\n2.0 x\n3.0 x\n8\n3.96 s\n2.5 x\n3.7 x\n16\n3.72 s\n2.7 x\n4.0 x\n32\n5.87 s\n1.7 x\n1.9 x\nB. Performance gain by multi-threading on Synfire\nFig. 3b and Table III present the performance improvement\non the Synfire chain network with 1,200 neurons and 77K\nsynapses run on a Intel i9 desktop processor. As expected, the\nperformance gain is lower than on the synthetic SNN as the\noptimization for the multi-threading kernel aimed primarily on\nthe compute intense numerical integration of the differential\nequations for the neuron state. On the other hand, the network\nis rather small and 70% performance gain is actually above\nexpectations. Future work will aim to parallelize the synaptic\nprocessing.\nTABLE III\nPERFORMANCE IMPROVEMENTS SYNFIRE (RELEASE BUILD)\nThreads\nExecution Time\nSpeed Factor\nPerformance Gain\n1\n0.43 s\n7 x\n2\n0.31 s\n9.7 x\n1.4 x\n4\n0.26 s\n12.0 x\n1.7 x\n8\n0.26 s\n12.0 x\n1.7 x\n16\n0.29 s\n10.0 x\n1.4 x\n32\n0.62 s\n4.8 x\n0.7 x",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_003.md",
    "arxiv_id": "2510.17745v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17745v1_page_003"
  },
  {
    "text": "(a) Chainfire on Intel i9\n(b) Synfire on Intel i9\n(c) Synfire on ARM Cortex\n(d) DCA\nFig. 3. Performance improvements by multi-threading kernel over allocated cores. (a) Chainfire (2k neurons) on 11th Gen Intel Core i9-11900K @ 3.5 GHz\n(8 cores, 16 logical). (b) Synfire (1.2k neurons, 77k synapses) on Intel i9. (c) Synfire on several ARM Cortex processors. (d) Dynamic core allocation saves\nup to 70% energy as overhead for over-allocated cores thread synchornization is avoided. The performance monitor Intel PCM shows this at ms precision at\ntimeline of the SNN.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_004.md",
    "arxiv_id": "2510.17745v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17745v1_page_004"
  },
  {
    "text": "same Synfire network on several ARM processors that can\nbe used at the edge. The kernel has a similar structural\nperformance scaling on ARM as on Intel, as long as the cores\nare used on Intel. Because two Intel logical processors share\na core, the performance degrades, which is indicated by the\ndashed line in Fig. 3b.\nC. Energy savings by DCA\nFig. 3d shows that DCA reduces the core threads as long as\nthe real-time criteria are met (first 250 ms). When the system\nload demands, DCA allocates additional core threads (e.g. at\n750 ms and 1800 ms), respectively frees them, when no longer\nneeded (e.g. at 750 ms and 1800 ms).\nIV. CONCLUSION\nThe present work introduces a multi-threading kernel for\nSNNs. It especially optimizes moderately sized SNNs de-\nployed on the multicore processors used for mobile devices.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_004.md",
    "arxiv_id": "2510.17745v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17745v1_page_004"
  },
  {
    "text": "processing across SoC devices. We show impressive perfor-\nmance gains on a network architecture, Synfire chain, which\nis commonly used to test SNNs. We also introduce a Chainfire\nnetwork to further evaluate performance.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_004.md",
    "arxiv_id": "2510.17745v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17745v1_page_004"
  },
  {
    "text": "derstand the performance relevant internal workings of the\nformer multi-threading kernel. It also enabled to validate and\nquantitative measure the optimization, see Fig. 3a. Without\nit, optimization information is hidden behind noise. There are\ngreat tools available for performance profiling. However, to\nidentify bottlenecks, it was essential to produce specific loads,\nfor instance the status update only without interference of the\nsynaptic processing.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_004.md",
    "arxiv_id": "2510.17745v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17745v1_page_004"
  },
  {
    "text": "supports mid-size SNNs on modern multicore processors such\nas the ARM Cortex family. Compared to neuromorphic chips\nsuch as Intel’s Loihi or Brainchip’s Akida, which are highly\nspecialized on neural processing and usually require a host\nsystem to run the base application, CARLsim utilizes the\nexisting compute capacity of the mobile processor. Depending\non the use case, similar energy efficiency on these specialized\nneuromorphic chips might be achieved with CARLsim on\nmobile processors. The new power saving policies in CARL-\nsim may make the intrinsic energy efficiency of SNNs even\nmore attractive for edge applications. It makes neuromorphic\napplications possible without additional hardware costs. Fur-\nthermore, an SNN executed by software is much more flexible\nin regards of changes and use case adaptation.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_004.md",
    "arxiv_id": "2510.17745v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17745v1_page_004"
  },
  {
    "text": "neuromorphic applications that can be deployed on millions\nof mobile devices, such as wearables like the Samsung Watch\nUltra 2025 or embedded devices based on SoCs such as the\nRaspberry Pi 5.\nREFERENCES\n[1] L. Niedermeier, K. Chen, J. Xing, A. Das, J. Kopsick, E. Scott,\nN. Sutton, K. Weber, N. Dutt, and J. L. Krichmar, “Carlsim 6: An\nopen source library for large-scale, biologically detailed spiking neural\nnetwork simulation,” in 2022 International Joint Conference on Neural\nNetworks (IJCNN).\nIEEE, 2022, pp. 1–10.\n[2] S. H¨oppner, Y. Yan, A. Dixius, S. Scholze, J. Partzsch, M. Stolba,\nF. Kelber, B. Vogginger, F. Neum¨arker, G. Ellguth, S. Hartmann,\nS. Schiefer, T. Hocker, D. Walter, G. Liu, J. Garside, S. Furber,\nand C. Mayr, “The spinnaker 2 processing element architecture for\nhybrid digital neuromorphic computing,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2103.08392\n[3] S. H¨oppner, Y. Yan, B. Vogginger, A. Dixius, J. Partzsch, F. Neum¨arker,\nS. Hartmann, S. Schiefer, S. Scholze, G. Ellguth et al., “Dynamic voltage\nand frequency scaling for neuromorphic many-core systems,” in 2017\nIEEE International Symposium on Circuits and Systems (ISCAS). IEEE,\n2017, pp. 1–4.\n[4] E. M. Izhikevich, “Simple model of spiking neurons,” IEEE Trans.\nNeural Netw., vol. 14, no. 6, pp. 1569–1572, 2003.\n[5] ——, “Which model to use for cortical spiking neurons?” IEEE Trans.\nNeural Netw., vol. 15, no. 5, pp. 1063–1070, Sep. 2004.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17745v1\\page_004.md",
    "arxiv_id": "2510.17745v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17745v1_page_004"
  },
  {
    "text": "Arctic Ocean using Physics-informed Neural Network\nYounghyun Kooa,b,c, Maryam Rahnemoonfara,b\naDepartment of Computer Science and Engineering, Lehigh University, 27 Memorial Dr\nW, Bethlehem, 18015, PA, USA\nbDepartment of Civil and Environmental Engineering, Lehigh University, 27 Memorial\nDr W, Bethlehem, 18015, PA, USA\ncNational Snow and Ice Data Center, Cooperative Institute for Research in\nEnvironmental Sciences, University of Colorado Boulder, 1540 30th\nSt, Boulder, 80303, CO, USA\nAbstract\nAs an increasing amount of remote sensing data becomes available in the\nArctic Ocean, data-driven machine learning (ML) techniques are becom-\ning widely used to predict sea ice velocity (SIV) and sea ice concentration\n(SIC). However, fully data-driven ML models have limitations in generaliz-\nability and physical consistency due to their excessive reliance on the quan-\ntity and quality of training data. In particular, as Arctic sea ice entered a\nnew phase with thinner ice and accelerated melting, there is a possibility\nthat an ML model trained with historical sea ice data cannot fully represent\nthe dynamically changing sea ice conditions in the future.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_001.md",
    "arxiv_id": "2510.17756v1",
    "page": "1",
    "title": "unknown",
    "id": "2510.17756v1_page_001"
  },
  {
    "text": "we develop physics-informed neural network (PINN) strategies to integrate\nphysical knowledge of sea ice into the ML model. Based on the Hierarchi-\ncal Information-sharing U-net (HIS-Unet) architecture, we incorporate the\nphysics loss function and the activation function to produce physically plau-\nsible SIV and SIC outputs. Our PINN model outperforms the fully data-\narXiv:2510.17756v1  [cs.LG]  20 Oct 2025",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_001.md",
    "arxiv_id": "2510.17756v1",
    "page": "1",
    "title": "unknown",
    "id": "2510.17756v1_page_001"
  },
  {
    "text": "driven model in the daily predictions of SIV and SIC, even when trained\nwith a small number of samples. The PINN approach particularly improves\nSIC predictions in melting and early freezing seasons and near fast-moving\nice regions.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_002.md",
    "arxiv_id": "2510.17756v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17756v1_page_002"
  },
  {
    "text": "Arctic Ocean, Convolutional neural network (CNN),\nHierarchical Information-sharing U-net (HIS-Unet), Physics loss function,\nActivation function\n1. Introduction\nDramatic declines in the Arctic sea ice extent (SIE) and sea ice thickness\n(SIT) have been observed for the last few decades along with a warming\nclimate resulting from the anthropogenic CO2 emissions (Notz and Stroeve,\n2016). Arctic SIE has been reduced by more than 50,000 km2/year, and at\nthe same time, Arctic SIT has decreased by more than 2 m in average, with\nmore than a 50 % SIT reduction in thick multi-year ice (Kwok, 2018). With\nsuch dramatic changes in SIE and SIT, the thermodynamic and dynamic\nconditions of Arctic sea ice have entered a new phase (Wang et al., 2022;\nStroeve and Notz, 2018).",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_002.md",
    "arxiv_id": "2510.17756v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17756v1_page_002"
  },
  {
    "text": "physical conditions of the atmosphere and ocean (e.g., heat exchange, salinity,\netc.) (Haumann et al., 2016; Hirst, 1999; McPhee, 1992; Budikova, 2009),\nunderstanding sea ice dynamics is essential for understanding the future of\nthe Arctic Ocean and global climate.\nA traditional way to predict the sea ice dynamics in the Arctic Ocean is\nusing numerical models based on physics laws and equations that describe\nsea ice in terms of mass and momentum balances, as well as its interaction\n2",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_002.md",
    "arxiv_id": "2510.17756v1",
    "page": "2",
    "title": "unknown",
    "id": "2510.17756v1_page_002"
  },
  {
    "text": "with the atmosphere and ocean (Stark et al., 2008; Fritzner et al., 2019;\nHibler, 1979; Parkinson and Washington, 1979). Although physics models\nhave successfully explained general sea ice behaviors by assuming sea ice as\nviscous-plastic or elastic-viscous-plastic material (Hunke and Dukowicz, 1997;\nHibler, 1979), several challenges make it difficult to improve their fidelity\nand efficiency. First, since countless atmospheric, oceanic, and even biologi-\ncal variables are involved in sea ice dynamics, established physics equations\nhave limitations in explaining every detailed mechanism of sea ice dynamics.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_003.md",
    "arxiv_id": "2510.17756v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17756v1_page_003"
  },
  {
    "text": "physics models may not agree perfectly with the real observations. Second,\nuncertainties in the complicated parameterization of sea ice physical condi-\ntions, including initial and boundary conditions, can propagate through the\nmodel predictions (Blockley et al., 2020; Hunke et al., 2010). Since real obser-\nvations are not always available, physics models are often based on physical\nassumptions. Considering that numerical models are sensitive to initial con-\nditions and physical assumptions, they can result in significant uncertainties\nand discrepancies with real observations (Blanchard-Wrigglesworth et al.,\n2015). Third, solving coupled and non-linear partial differential equations\n(PDEs) is computationally intensive and expensive. Although adding more\ncomplexity to a model or increasing model resolution can enhance the model’s\naccuracy, it exponentially increases computational costs (Hunke et al., 2010;\nLi et al., 2023). Hence, this computational demand inhibits improving the\nmodel performance for higher spatial and temporal resolutions.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_003.md",
    "arxiv_id": "2510.17756v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17756v1_page_003"
  },
  {
    "text": "ice behaviors with fewer complexities and a lower computational cost than\n3",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_003.md",
    "arxiv_id": "2510.17756v1",
    "page": "3",
    "title": "unknown",
    "id": "2510.17756v1_page_003"
  },
  {
    "text": "traditional numerical models.\nML approaches have several benefits com-\npared to numerical models. First, complex PDEs or parameterizations are\nnot required for ML models because they need only observational data to be\ntrained. ML models can learn complex non-linear patterns or relationships\nbetween multiple atmospheric and oceanic variables directly from observa-\ntional data without any explicit knowledge of physical processes. Second,\nML models can be easily updated and trained continuously with new ob-\nservational data, and this data integration can make the model prediction\nmore accurate and reliable. Finally, ML models require less computational\ncost than numerical models. While numerical models often require intensive\ncomputing resources (e.g., high-performance computing) to solve complex\nPDEs, ML models can be implemented relatively fast by leveraging the par-\nallel processing ability of graph processing units (GPUs).",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_004.md",
    "arxiv_id": "2510.17756v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17756v1_page_004"
  },
  {
    "text": "developed to predict sea ice concentration (SIC) and sea ice velocity (SIV),\nmost of which rely on convolutional neural networks (CNN) (Ren and Li,\n2021; Liu et al., 2021; Yan and Huang, 2018; Hoffman et al., 2023; Petrou\nand Tian, 2019). As a deep learning algorithm specialized for image data,\nCNN propagates the input data through convolutional layers with multiple\nfilters and extracts spatial patterns and features. Based on the ability of\nCNN to learn complex features in image datasets, CNNs have succeeded in\nvarious applications in environmental modeling (Wang et al., 2024; Jiang\net al., 2023; Sadeghi et al., 2020). Since the SIC and SIV data for the entire\nArctic Ocean are often provided from remote sensing data sources as grid\nformats, CNN has advantages in exploiting the spatial variability in those\n4",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_004.md",
    "arxiv_id": "2510.17756v1",
    "page": "4",
    "title": "unknown",
    "id": "2510.17756v1_page_004"
  },
  {
    "text": "sea ice data.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_005.md",
    "arxiv_id": "2510.17756v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17756v1_page_005"
  },
  {
    "text": "on the quantity and quality of training datasets, which leads to limitations in\nfurther improving the model performance. First and foremost, data-driven\nML models require a large amount of data to ensure generalizability.\nIf\ntraining samples are insufficient or their distribution is biased, the model can\nbe overfitted and lack generalizability to out-of-training cases. Second, ML\nmodels are considered black-box models and lack inherent interpretability in\nhow the final predictions are derived. This lack of inherent interpretability\nmakes it difficult to understand why the model predicts physically invalid\nvalues about sea ice conditions (e.g., negative SIC or SIC > 100%).",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_005.md",
    "arxiv_id": "2510.17756v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17756v1_page_005"
  },
  {
    "text": "improve the model’s generalizability, it is helpful to guide the learning pro-\ncess to agree with fundamental physical laws and domain knowledge. By\nintegrating physics knowledge and constraints into ML training, the ML\nmodels can yield physically consistent predictions even in the presence of\nimperfect data, such as missing, noise, or outliers data (Karniadakis et al.,\n2021; Raissi et al., 2019; Maier et al., 2023). This integration of ML and\nphysics knowledge is often referred to as physics-informed machine learning\n(PIML), and has been proposed for various dynamic systems (Karniadakis\net al., 2021; Maier et al., 2023). Recently, many studies have attempted this\nPIML framework for various applications including, but not limited to, flow\ndynamics, material sciences, molecular simulations, chemistry (Karniadakis\net al., 2021). In particular, PIML based on well-known flow laws such as\nStokes equation has shown great success in modeling the dynamics of ice\n5",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_005.md",
    "arxiv_id": "2510.17756v1",
    "page": "5",
    "title": "unknown",
    "id": "2510.17756v1_page_005"
  },
  {
    "text": "sheets and glaciers (i.e., land ice) (Teisberg et al., 2021; Riel et al., 2021; Riel\nand Minchew, 2023; Iwasaki and Lai, 2023; Jouvet and Cordonnier, 2023;\nHe et al., 2023). In sea ice application, Liu et al. (2024) proposed dual-task\nneural network architecture and incorporated a loss function based on sea ice\ncontrol equation. However, it remains unclear how this PIML concept and\nphysics-informed loss function can contribute to future sea ice predictions\nunder rapidly changing climate conditions.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_006.md",
    "arxiv_id": "2510.17756v1",
    "page": "6",
    "title": "unknown",
    "id": "2510.17756v1_page_006"
  },
  {
    "text": "predict sea ice dynamics by integrating physical knowledge into the model\ntraining. In order to enforce the deep learning model (neural network) to con-\nverge into physically valid SIV and SIC values, we explicitly adopt a physics-\ninformed neural network (PINN) approach based on loss function. We em-\nbed this physics-informed learning strategy in the hierarchical information-\nsharing U-net (HIS-Unet), a CNN model for SIV and SIC predictions (Koo\nand Rahnemoonfar, 2024). The main contributions of this research consist\nof the following.\n• We design physics loss functions and combine them with the data loss\nfunction to regulate physical validity of SIC and SIV.\n• We modify the output layer to guarantee the physically valid SIC val-\nues.\n• Our extensive experiments show that the physics-informed deep learn-\ning model can improve the performance in SIC and SIV predictions\neven with a small number of training samples.\n• We explore the spatiotemporal variability in the improved performance\n6",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_006.md",
    "arxiv_id": "2510.17756v1",
    "page": "6",
    "title": "unknown",
    "id": "2510.17756v1_page_006"
  },
  {
    "text": "of our physics-informed learning strategies compared to fully data-\ndriven deep learning models that do not embed physics.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_007.md",
    "arxiv_id": "2510.17756v1",
    "page": "7",
    "title": "unknown",
    "id": "2510.17756v1_page_007"
  },
  {
    "text": "brief review of the physical background of sea ice modeling, machine learn-\ning for sea ice prediction, and PIML. Section 3 explains details of remote\nsensing and meteorological data used in this study, and section 4 presents\nthe architecture of our CNN model and physics-informed learning strategies.\nThe performance and implication of our PINN are discussed in Section 5.\n2. Background\n2.1. Physics of sea ice dynamics\nIn physical sea ice models, the volume and area of sea ice are determined\nby thermodynamic evolution and dynamic motion field. The spatiotemporal\nchanges in SIT (h) and SIC (A) can be expressed by the following equations\nfor mass conservation (Hibler, 1979; Holland and Kwok, 2012; Flato, 2004):\n∂h\n∂t + ∇· (uh) = Sh\n(1)\n∂A\n∂t + ∇· (uA) = SA\n(2)\nwhere u is the ice motion vector, Sh and SA are the changes in SIT and SIC\ndriven by thermodynamic sources (e.g., freezing or melting), respectively.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_007.md",
    "arxiv_id": "2510.17756v1",
    "page": "7",
    "title": "unknown",
    "id": "2510.17756v1_page_007"
  },
  {
    "text": "ious numerous physical sea ice models to explain the balance of horizontal\n7",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_007.md",
    "arxiv_id": "2510.17756v1",
    "page": "7",
    "title": "unknown",
    "id": "2510.17756v1_page_007"
  },
  {
    "text": "forces on sea ice based on the most common assumption of elastic-viscous-\nplastic (EVP) properties of sea ice (Hibler, 1979):\nmDu\nDt = −mfk × u + τai + τwi + F −mg∇H\n(3)\nwhere D/Dt = ∂/∂t+u·∇is the substantial time derivative, m is the ice mass\nper unit area, k is a unit vector normal to the surface, u is the ice velocity, f is\nthe Coriolis parameter, τai and τwi are the forces due to air and water stresses,\nH is the elevation of the sea surface, g is the gravity acceleration, and F is\nthe force due to variations in internal ice stress. Following this equation,\nmany previous studies described SIV as interacting with wind and ocean\nforcings (Rampal et al., 2016; Wang et al., 2014; Timmermann et al., 2009;\nSalas Mélia, 2002). Particularly, wind velocity has been treated as a major\nvariable in SIV, contributing to up to 70 % of the sea ice velocity variances\n(Thorndike and Colony, 1982) depending on season or region. Nevertheless,\npredicting sea ice dynamics based on physical models is still challenging due\nto the intrinsic complexity and dependency of physical models on numerous\nparameterizations.\n2.2. Neural network for sea ice prediction\nConvolutional neural networks (CNNs) have been used as the most pop-\nular and efficient deep learning network for modeling SIC and SIV. First, in\nterms of SIC, Andersson et al. (2021) proposed a deep-learning sea ice fore-\ncasting system named IceNet to forecast monthly SIC for the next six months.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_008.md",
    "arxiv_id": "2510.17756v1",
    "page": "8",
    "title": "unknown",
    "id": "2510.17756v1_page_008"
  },
  {
    "text": "daily SIC forecasts in several subsections of the Arctic Ocean, including the\nBarents and Kara Seas, Labrador Sea, and Laptev Sea. Kim et al. (2020) also\n8",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_008.md",
    "arxiv_id": "2510.17756v1",
    "page": "8",
    "title": "unknown",
    "id": "2510.17756v1_page_008"
  },
  {
    "text": "used CNN to predict monthly SIC from satellite-based SIC observations, sea\nsurface temperature, air temperature, albedo, and wind velocity data from\nthe previous months. Similarly, Fritzner et al. (2020) proposed a CNN that\npredicts weekly SIC from SIC, sea surface temperature, and air temperature\nfrom the previous six days. CNN models proposed by Ren and Li (2021) and\nRen et al. (2022) used the sequences of satellite-derived SIC observations\nto make daily SIC predictions. In some studies, long short-term memory\n(LSTM), an advanced recurrent neural network (RNN), has been modified\nand inserted into CNN architecture to obtain better performance in both spa-\ntial and temporal SIC predictions (Liu et al., 2021). CNN and LSTM have\nalso been used for SIV prediction in several studies. Zhai and Bitz (2021)\nand Hoffman et al. (2023) used CNN to predict daily SIV, and their model\noutperformed other statistical and physical models. Petrou and Tian (2019)\nshowed that adding LSTM units to convolutional layers improves the per-\nformance of a model that predicts SIV. In this study, we adopt a multi-task\nneural network framework named HIS-Unet (Koo and Rahnemoonfar, 2024)\nand improve the predictability of SIV and SIC of this model by incorporating\nphysics-informed training scheme.\n2.3. Physics-informed machine learning (PIML)\nHistorically, PIML has been achieved by integrating physics knowledge\ninto ML training in the form of: (i) data, (ii) model architecture, and (iii)\noptimization (Hao et al., 2023; Karniadakis et al., 2021). First, one of the\nstraightforward methods to incorporate physics knowledge into ML is to gen-\nerate training data from the desired physics knowledge.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_009.md",
    "arxiv_id": "2510.17756v1",
    "page": "9",
    "title": "unknown",
    "id": "2510.17756v1_page_009"
  },
  {
    "text": "models that are trained with sufficient simulation data governed by certain\n9",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_009.md",
    "arxiv_id": "2510.17756v1",
    "page": "9",
    "title": "unknown",
    "id": "2510.17756v1_page_009"
  },
  {
    "text": "underlying physics laws can accurately represent these physics laws. Given\nthat enough high-quality and labeled data is not always available for real-\nworld tasks, synthetic data constructed with physical simulations can provide\na large amount of data with high quality. However, this approach can have\nlimitations in perfectly reflecting real world because real-world data have\ndifferent distributions from simulation data. In addition, the cost of data\nacquisition can be a critical issue, as simulation data is often generated via\nexpensive experiments or large-scale simulations.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_010.md",
    "arxiv_id": "2510.17756v1",
    "page": "10",
    "title": "unknown",
    "id": "2510.17756v1_page_010"
  },
  {
    "text": "tures that implicitly embed any prior knowledge and inductive biases associ-\nated with a given task (e.g., symmetry, conservation laws, etc.) (Karniadakis\net al., 2021). CNN is a popular example of this neural network architecture\nthat achieves extensive applicability for image recognition by respecting in-\nvariance and symmetry groups in natural images (Karniadakis et al., 2021;\nMallat, 2016). Another example of this category is equivariant networks,\nwhich embed the dynamic changes in spatial coordinates to preserve the\nequivariance of data points to rotation and translation (Satorras et al., 2022;\nSchütt et al., 2017). Some neural network architectures also used Lagrangian\nand Hamiltonian mechanics to enforce the energy conservation property of\nthe networks (Hao et al., 2023). Furthermore, in solving PDEs, there have\nbeen several attempts to modify neural network architecture to satisfy the\nrequired initial conditions (Hao et al., 2023). Although such a model ar-\nchitecture approach can be effective with relatively simple and well-defined\nphysics or symmetry groups, this approach has limitations in extending to\nhighly complex problems.\n10",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_010.md",
    "arxiv_id": "2510.17756v1",
    "page": "10",
    "title": "unknown",
    "id": "2510.17756v1_page_010"
  },
  {
    "text": "learning process by imposing constraints of prior physics knowledge into the\nform of loss functions.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_011.md",
    "arxiv_id": "2510.17756v1",
    "page": "11",
    "title": "unknown",
    "id": "2510.17756v1_page_011"
  },
  {
    "text": "instead of enforcing a specific condition directly, this approach can indirectly\nreshape the target spaces of NN output to converge to physically plausible\nsolutions (Karniadakis et al., 2021; Hao et al., 2023); in this study, we will\nrefer to this approach specifically as PINN. This approach can be regarded\nas a case of multi-task learning, balancing the loss functions for two tasks\nof fitting both the observation data and physical constraints. Although such\na soft constraint approach is the most common and flexible way for PIML,\nbalancing these two tasks can be challenging because they can counteract\nthe convergence to each other’s solution.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_011.md",
    "arxiv_id": "2510.17756v1",
    "page": "11",
    "title": "unknown",
    "id": "2510.17756v1_page_011"
  },
  {
    "text": "dict ice flow, and most previous studies employed the optimization approach\nby adding physics loss functions or regularization terms (i.e., PINN). Teis-\nberg et al. (2021) developed a PINN to predict ice thickness and velocity by\nadding a physics loss function based on the mass conservation of ice sheets.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_011.md",
    "arxiv_id": "2510.17756v1",
    "page": "11",
    "title": "unknown",
    "id": "2510.17756v1_page_011"
  },
  {
    "text": "erning equations of ice flow to their PINN framework to infer spatially and\ntemporally varying basal drag in ice sheets. Riel and Minchew (2023) pro-\nposed a PINN framework to predict the distribution of ice rigidity by adding\nthe loss functions of ice flow and Kullback-Leibler divergence. Iwasaki and\nLai (2023) proposed a physics loss function to fit the model results with\nphysics laws regarding ice flows, and this physics loss function contributed\nto improving the model performance when the data was contaminated by\n11",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_011.md",
    "arxiv_id": "2510.17756v1",
    "page": "11",
    "title": "unknown",
    "id": "2510.17756v1_page_011"
  },
  {
    "text": "noise. Jouvet and Cordonnier (2023) optimized their PINN by minimizing\nthe energy associated with high-order ice-flow equations. Cheng et al. (2024)\nemployed a PINN to infer basal sliding while filling gaps in sparsely observed\nice thickness data. In the regime of sea ice, there have been several attempts\nto integrate physics knowledge into machine learning by training the model\nwith the data retrieved from physical models (Palerme and Muller, 2021;\nPalerme et al., 2024). Liu et al. (2024) used a dual-task CNN architecture\nand physics-informed loss function to enforce dynamic constraints of SIC and\nSIV. Despite such developments of PINN for cryosphere, it is still necessary\nto assess how PINN and physical constraints can help future predictions of\nsea ice conditions. Therefore, this study explicitly applies the concept of\nPINN to sea ice prediction and explore the how weights to physics-informed\nloss function and representability of training samples contribute to the model\npredictability for future unseen sea ice conditions.\n3. Data\nIn this study, we use daily SIV and SIC data collected from satellite\nobservation from 2009 to 2022. We use the SIV and SIC from previous three\ndays (inputs of the model) to predict the next day’s SIV and SIC (output of\nthe model) (Table 1). Besides satellite observations of SIV and SIC, we also\nuse wind velocity and air temperature from reanalysis as additional input\nvariables for the model. Additionally, we add X and Y coordinates as inputs\nto represent regional variability. The input datasets and their sources are\nsummarized in Table 1. This section presents the details of these datasets.\n12",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_012.md",
    "arxiv_id": "2510.17756v1",
    "page": "12",
    "title": "unknown",
    "id": "2510.17756v1_page_012"
  },
  {
    "text": "Dataset\nName\nSpatial\nresolution\nSea ice velocity\n(u and v)\nNSIDC Polar Pathfinder Daily 25 km EASE-\nGrid Sea Ice Motion Vectors (Tschudi et al.,\n2020)\n25 km\nSea Ice Concen-\ntration\nNOAA/NSIDC Climate Data Record of Pas-\nsive Microwave Sea Ice Concentration (Meier\net al., 2021)\n25 km\n10 m wind veloc-\nity (u and v)\nECMWF Reanalysis v5 (ERA5) hourly data\non single levels (Hersbach et al., 2020)\n0.25◦\n2 m air tempera-\nture\nECMWF Reanalysis v5 (ERA5) hourly data\non single levels (Hersbach et al., 2020)\n0.25◦\n13",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_013.md",
    "arxiv_id": "2510.17756v1",
    "page": "13",
    "title": "unknown",
    "id": "2510.17756v1_page_013"
  },
  {
    "text": "3.1. Sea ice velocity\nWe use the NSIDC Polar Pathfinder Daily 25 km EASE-Grid Sea Ice\nMotion Vectors version 4 (Tschudi et al., 2019, 2020) as the input and output\nSIV. This product derives daily SIV from three primary types of sources: (1)\ngridded satellite images, (2) wind reanalysis data, and (3) buoy position data\nfrom the International Arctic Buoy Program (Tschudi et al., 2019, 2020). The\nu component (along-x) and v component (along-y) of SIV are independently\nderived from each of these sources and optimally interpolated onto a 25 km\nEqual-Area Scalable Earth (EASE) grid. When SIV is derived from satellite\nimages, a correlation coefficient is calculated between a small target area in a\none-day image and a searching area in the next-day image. Then, the location\nin the next-day image with the highest correlation coefficient is determined as\nthe displacement of ice (Tschudi et al., 2019). The mean difference between\nthe Polar Pathfinder and buoy measurements of SIV is approximately 0.1\nkm/day and 0.3 km/day for u and v components, respectively (Tschudi et al.,\n2019). We note that the SIV of this product is valid over short distances\naway from the ice edge in areas where ice conditions are relatively stable,\nstationary, homogenous, and isotropic (Tschudi et al., 2020). In this study,\nwe exclude SIV values close to the coastlines within 50 km (or 2 pixels) of\ndistance.\n3.2. Sea ice concentration\nFor the SIC data, we use NOAA/NSIDC Climate Data Record of Passive\nMicrowave Sea Ice Concentration version 4 data (Meier et al., 2021). This\ndata set provides a Climate Data Record (CDR) of SIC (i.e., the areal fraction\nof ice within a grid cell) from passive microwave (PMW) data. The CDR\n14",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_014.md",
    "arxiv_id": "2510.17756v1",
    "page": "14",
    "title": "unknown",
    "id": "2510.17756v1_page_014"
  },
  {
    "text": "algorithm output is the combination of SIC estimations from two algorithms:\nthe NASA Team algorithm (Cavalieri et al., 1984) and NASA Bootstrap\nalgorithm (Comiso, 1986). These empirical algorithms estimate SIC from\nthe PMW brightness temperatures at different frequencies and polarizations:\nvertical and horizontal polarizations at 19 GHz, 22 GHz, and 37 GHz. Several\nassessments showed that the error of this SIC estimation is approximately\n5 % within the consolidated ice pack during cold winter conditions (Meier,\n2005; Comiso et al., 1997; Ivanova et al., 2015). However, in the summer\nseason, the error can rise to more than 20 % due to surface melt and the\npresence of melt ponds (Kern et al., 2020). Due to the data quality issue\nnear coastal areas, we use the SIC data more than 50 km from the coastline.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_015.md",
    "arxiv_id": "2510.17756v1",
    "page": "15",
    "title": "unknown",
    "id": "2510.17756v1_page_015"
  },
  {
    "text": "the NSIDC Sea Ice Polar Stereographic grid of the SIC product into the\nEASE grid of the SIV product using bilinear interpolation.\n3.3. ERA5 climate reanalysis\nAs shown in Eq.\n3, sea ice dynamics are largely associated with at-\nmospheric and oceanic circulation. Thus, we use the wind speed and air\ntemperature as the input variables of the ML model.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_015.md",
    "arxiv_id": "2510.17756v1",
    "page": "15",
    "title": "unknown",
    "id": "2510.17756v1_page_015"
  },
  {
    "text": "generation ECMWF (European Centre for Medium-Range Weather Fore-\ncasts) atmospheric reanalysis (ERA5) as the data sources for wind velocity\nand air temperature. ERA5 provides hourly estimates of atmospheric, land,\nand oceanic climate variables, covering the period from January 1940 to the\npresent (Hersbach et al., 2020). We obtain the daily average wind velocity\n(zonal and meridional components) at 10 m height and 2 m air temperature\nfrom this hourly data. To co-locate this data with the SIV and SIC data, we\n15",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_015.md",
    "arxiv_id": "2510.17756v1",
    "page": "15",
    "title": "unknown",
    "id": "2510.17756v1_page_015"
  },
  {
    "text": "reproject the raw ERA5 latitude-longitude grid onto the 25 km EASE grid\nusing bilinear interpolation.\n4. Methods\nIn this study, we use a hierarchical information-sharing U-net (HIS-Unet)\nbased on Koo and Rahnemoonfar (2024) as a backbone deep learning model\n(Fig. 1) to make daily predictions of SIC and SIV from inputs of SIC, SIV,\nwind, and atmospheric temperature from the previous three days. As a fully\nconvolutional network, the HIS-Unet is designed to predict SIC and SIV\nsimultaneously. By sharing SIC and SIV information during the propagation\nprocesses, the HIS-Unet achieves better fidelity than other neural networks\nand statistical approaches in predicting both SIC and SIV. In particular, this\ninformation sharing is proven to be useful for predicting sea ice conditions\nwhere and when SIV has impacts on SIC changes (Koo and Rahnemoonfar,\n2024).",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_016.md",
    "arxiv_id": "2510.17756v1",
    "page": "16",
    "title": "unknown",
    "id": "2510.17756v1_page_016"
  },
  {
    "text": "information (e.g., Eq. 2), we use the multi-task prediction of SIV and SIC\nby the HIS-Unet to facilitate integrating the physics knowledge of sea ice\ndynamics. To embed fundamental knowledge of sea ice dynamics into the\nHIS-Unet architecture, we (1) introduce physics loss functions in the training\nphase and (2) add an additional activation function to the output layer.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_016.md",
    "arxiv_id": "2510.17756v1",
    "page": "16",
    "title": "unknown",
    "id": "2510.17756v1_page_016"
  },
  {
    "text": "informed training strategies used in this study.\n4.1. Hierarchical information-sharing U-net (HIS-Unet)\nThe HIS-Unet architecture consists of two separate task branches, each\nfor predicting SIV and SIC (Fig. 1). The task branches are connected with\n16",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_016.md",
    "arxiv_id": "2510.17756v1",
    "page": "16",
    "title": "unknown",
    "id": "2510.17756v1_page_016"
  },
  {
    "text": "weighting attention modules (WAMS) that allow information sharing be-\ntween the SIV and SIC branches. SIC and SIV branches have U-net struc-\ntures (Ronneberger et al., 2015) with a series of encoders (contracting path)\nand decoders (expansive path). Each encoder path consists of repeated two\n3 × 3 convolutions and a 2 × 2 max pooling operation with stride 2 for\ndownsampling and doubling the number of features. The decoders conduct\n2 × 2 up-convolution and half the number of feature channels, followed by a\nconcatenation with the cropped feature map and two 3×3 convolutions (Ron-\nneberger et al., 2015). The hyperbolic tangent (tanh) activation function is\napplied after each convolutional layer:\nTanh(x) = ex −e−x\nex + e−x\n(4)\nThe separated U-net structures of SIV and SIC branches output SIV or\nSIC, respectively, but they share and transfer their information through six\nweighting attention modules (WAMs) during the propagation process. Six\nWAMs are inserted into 3 encoder steps and 3 decoder steps between SIC\nand SIV branches (Fig. 1).",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_017.md",
    "arxiv_id": "2510.17756v1",
    "page": "17",
    "title": "unknown",
    "id": "2510.17756v1_page_017"
  },
  {
    "text": "and calculates the weighted sum of them (Fig. 1b). Letting a WAM receive\nthe SIV feature map (ξin,SIV ; height H, width W, channels C) and SIC\nfeature map (ξin,SIC; H × W × C), the input shared information (ξin,share)\nis determined by multiplying linear weights, Ain,SIV and Ain,SIC, to ξin,SIV\nand ξin,SIC, respectively (Fig. 1b). Then, this shared information ξin,share\npasses through sequentially arranged channel attention (Fig. 1c) and spatial\nattention modules (Fig. 1d). The channel attention highlights what channel\nis meaningful, and the spatial attention highlights where an informative part\n17",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_017.md",
    "arxiv_id": "2510.17756v1",
    "page": "17",
    "title": "unknown",
    "id": "2510.17756v1_page_017"
  },
  {
    "text": "(a) Architecture of Hierarchical information-sharing U-net (HIS-Unet); (b)\nWeighting attention module (WAM) in HIS-Unet; (c) Channel attention module and (d)\nSpatial attention module in a WAM (Koo and Rahnemoonfar, 2024).\n18",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_018.md",
    "arxiv_id": "2510.17756v1",
    "page": "18",
    "title": "unknown",
    "id": "2510.17756v1_page_018"
  },
  {
    "text": "is spatially located (Woo et al., 2018). These channel and spatial attention\nmodules are also applied to the input SIV and SIC feature maps (ξin,SIV and\nξin,SIC) (Fig. 1b). Then, the attention shared information is sent to the SIV\nand SIC branches after multiplying output weights (Aout,SIV and Aout,SIC)\nand adding attention SIV and SIC information, respectively. More details\nof how the WAMs enable sharing and highlighting SIC and SIV information\nare described in Koo and Rahnemoonfar (2024).\n4.2. Physics-informed training\nIn order to make the HIS-Unet incorporate physics knowledge of sea ice,\nwe embed two physics-informed regularizations: (1) include physics loss func-\ntions along with the data loss function and (2) insert the sigmoid activation\nfunction to the SIC branch to guarantee valid SIC values.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_019.md",
    "arxiv_id": "2510.17756v1",
    "page": "19",
    "title": "unknown",
    "id": "2510.17756v1_page_019"
  },
  {
    "text": "objective loss function (Ldata). The MSE data loss term (Ldata) is calculated\nby the following equation:\nLdata =\nX\n(|up −uo|2 + |vp −vo|2 + |Ap −Ao|2)\n(5)\nwhere u and v denote x-component and y-component of SIV, respectively, A\ndenotes SIC, and the subscript o means observation and p means prediction\nby HIS-Unet. In addition to this MSE data loss function, we design physics\nloss functions that apply physical constraints: (i) Lsat constraining valid SIV\nvalues and (ii) Ltherm constraining thermodynamically valid ice growth.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_019.md",
    "arxiv_id": "2510.17756v1",
    "page": "19",
    "title": "unknown",
    "id": "2510.17756v1_page_019"
  },
  {
    "text": "the PMW-derived SIV can be only defined where sea ice presents with > 15\n19",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_019.md",
    "arxiv_id": "2510.17756v1",
    "page": "19",
    "title": "unknown",
    "id": "2510.17756v1_page_019"
  },
  {
    "text": "% of SIC (Tschudi et al., 2019), SIV should be zero where SIC is less than\n15 %. Therefore, we design the first physics loss term as follows:\nLsat =  \n|u2\np + v2\np|,\nif Ap < 0.15\n0,\nif Ap ≥0.15\n(6)\nThe next physics loss term (Ltherm) is based on Eq.2, which explains the\nthermodynamic and dynamic SIC changes. The thermodynamic SIC changes\n(SA), the right term in Eq. 2, can be calculated by temporal changes of SIC\nand the combination of advection and divergence of SIC. In the case of daily\nSIC prediction, the daily SA can be assumed not to exceed (-1, 1). That\nis, we assume that the thermodynamic freezing and melting of sea ice are\nunlikely to saturate SIC from 0 % to 100 % or remove the entire sea ice from\n100 % to 0 % within a day in most cases. Thus, based on this assumption,\nwe design the second physics loss term as follows:\nLtherm = ReLU(|∂Ap\n∂t + ∇· (upAp)| −1)\n(7)\nwhere the time derivative term (∂Ap\n∂t ) is derived by subtracting the previous-\nday SIC from the output SIC, and the spatial derivative term (∇· (upAp))\nis derived by calculating the spatial gradients in output SIV and SIC grids.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_020.md",
    "arxiv_id": "2510.17756v1",
    "page": "20",
    "title": "unknown",
    "id": "2510.17756v1_page_020"
  },
  {
    "text": "dynamic ice growth prediction exceeds 1.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_020.md",
    "arxiv_id": "2510.17756v1",
    "page": "20",
    "title": "unknown",
    "id": "2510.17756v1_page_020"
  },
  {
    "text": "loss functions (L) are defined as follows:\nLphy = λsatLsat + λthermLtherm\n(8)\nL = Ldata + Lphy\n(9)\n20",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_020.md",
    "arxiv_id": "2510.17756v1",
    "page": "20",
    "title": "unknown",
    "id": "2510.17756v1_page_020"
  },
  {
    "text": "where λsat and λtherm are the relative weights of Lsat and Ltherm, respectively,\nto the data loss term. In this study, we conduct experiments with different\nλsat and λtherm of 0, 0.2, 1.0, and 5.0 and examine how this weight changes\nthe model performance.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_021.md",
    "arxiv_id": "2510.17756v1",
    "page": "21",
    "title": "unknown",
    "id": "2510.17756v1_page_021"
  },
  {
    "text": "value range of 0 to 1 (0 to 100 %) by adding the sigmoid activation function\nto the last output layer of the SIC branch:\nSigmoid(x) =\n1\n1 + e−x\n(10)\nThis can constrain the valid range of SIC outputs, even for out-of-training\nsamples.\n4.3. Traininig strategy\nIn our HIS-Unet, we use the previous 3 days of SIV (x- and y-components),\nSIC, air temperature, and wind velocity (x- and y- components) as the inputs\nto predict the next day’s SIV and SIC. Consequently, the input layer has 18\nchannels of 256×256 grid size. All input values are normalized to -1 to 1\nbased on the nominal maximum and minimum values that each variable can\nhave. All the data is collected for 14 years from 2009 to 2022; the first seven\nyears of data (2009-2015) are utilized as training data, and the remaining\nseven years of data (2016-2022) are utilized as test data. To examine how\nthe model performance is changed by the number of training data, we train\nthe model with three different training sample sizes: (1) using all 2009-2015\ndata as training samples (i.e., 100 % sampling), (2) randomly selecting 50 %\nof 2009-2015 data as training samples (i.e., 50 % sampling), and (3) randomly\nselecting 20 % of 2009-2015 data as training samples (i.e., 20 % sampling).\n21",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_021.md",
    "arxiv_id": "2510.17756v1",
    "page": "21",
    "title": "unknown",
    "id": "2510.17756v1_page_021"
  },
  {
    "text": "for 50 % sampling, and 436 for the 20 % sampling. The HIS-Unet models\ntrained with three different sample sizes are applied to the 2016-2022 test\ndata. All models are optimized by the Adam stochastic gradient descent\nalgorithm (Kingma and Ba, 2017) with 100 epochs and a 0.001 learning rate.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_022.md",
    "arxiv_id": "2510.17756v1",
    "page": "22",
    "title": "unknown",
    "id": "2510.17756v1_page_022"
  },
  {
    "text": "memory.\n4.4. Model performance\nThe model performance is assessed by the root mean square error (RMSE):\nRMSE(ˆy, y) =\nsPN\ni=1(ˆyi −yi)2\nN\n(11)\nwhere ˆy denotes predicted values, y denotes true values, N is the number\nof data points.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_022.md",
    "arxiv_id": "2510.17756v1",
    "page": "22",
    "title": "unknown",
    "id": "2510.17756v1_page_022"
  },
  {
    "text": "y-component SIV, and the average of x- and y-component RMSEs is deter-\nmined as the SIV RMSE to embrace the magnitude and angle error of SIV.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_022.md",
    "arxiv_id": "2510.17756v1",
    "page": "22",
    "title": "unknown",
    "id": "2510.17756v1_page_022"
  },
  {
    "text": "training sample cases and four different λsat and λtherm (i.e., 0, 0.2, 1.0, and\n5.0). We use the HIS-Unet without any physics-informed regularization (i.e.,\ntrained only with the data loss Ldata and without the sigmoid activation\nfunction to the SIC branch) as the baseline model, and this purely data-\ndriven model is notated as No-Phy. Meanwhile, we call the physics-informed\nHIS-Unet simply PINN. We compare the RMSE differences between PINN\nand No-Phy for different training sample sizes (20 %, 50 %, and 100 %).",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_022.md",
    "arxiv_id": "2510.17756v1",
    "page": "22",
    "title": "unknown",
    "id": "2510.17756v1_page_022"
  },
  {
    "text": "region in seven test years (2019-2022). To assess the statistical significance\n22",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_022.md",
    "arxiv_id": "2510.17756v1",
    "page": "22",
    "title": "unknown",
    "id": "2510.17756v1_page_022"
  },
  {
    "text": "of RMSE improvement by PINN, we conduct paired t-tests between RM-\nSEs of PINN and RMSEs of No-Phy. If the p-value from the t-test is below\n0.05, we determine that the RMSE difference between PINN and No-Phy is\nsignificant.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_023.md",
    "arxiv_id": "2510.17756v1",
    "page": "23",
    "title": "unknown",
    "id": "2510.17756v1_page_023"
  },
  {
    "text": "correlation coefficient (ACC) to evaluate the predictive performance of PINN\nand No-Phy.\nMAE(ˆy, y) =\nPN\ni=1 |ˆyi −yi|\nN\n(12)\nACC(ˆy, y) =\nPN\ni=1(ˆyi −¯ˆiy)(yi −¯yi)\nqPN\ni=1(ˆyi −¯ˆiy)2 PN\ni=1(yi −¯yi)2\n(13)\nLower RMSE and MAE values indicate higher prediction accuracy, while a\nhigher ACC (i.e., approaching 1) reflects stronger agreement between pre-\ndicted and observed anomalies, indicating better predictive skill. We note\nthat the performance of No-Phy model trained with full training samples can\nbe found in Koo and Rahnemoonfar (2024) in detail. The No-Phy HIS-Unet\nshowed better performance than another simple statistical model (e.g., lin-\near regression), physical model (Hybrid Coordinate Ocean Model; HyCOM),\nsimple convolutional network, and independent-branch U-nets.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_023.md",
    "arxiv_id": "2510.17756v1",
    "page": "23",
    "title": "unknown",
    "id": "2510.17756v1_page_023"
  },
  {
    "text": "HIS-Unet performance has already been assessed in the previous study, this\nstudy is focused on assessing the effect of using a physics-informed learning\nstrategy rather than assessing the performance of the HIS-Unet architecture\nitself.\n23",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_023.md",
    "arxiv_id": "2510.17756v1",
    "page": "23",
    "title": "unknown",
    "id": "2510.17756v1_page_023"
  },
  {
    "text": "5. Results and discussion\n5.1. Performance of PINN\nWe evaluate the predictive performance of No-Phy and PINNs with dif-\nferent λsat and λtherm settings, and check whether PINNs can improve the\nmodel performance compared to No-Phy. Additionally, we investigate how\nthis improvement pattern varies by different training sample sizes. Figure 2\nshows the SIV and SIC RMSE changes by PINNs as functions of different\ntraining sample sizes, λsat, and λtherm. Table 2 shows the RMSE, MAE, and\nACC of No-Phy baseline and PINN with 0.2 λtherm and 0.2 λsat. According\nto Table 2, the errors of both SIV and SIC predictions decrease with more\ntraining samples.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_024.md",
    "arxiv_id": "2510.17756v1",
    "page": "24",
    "title": "unknown",
    "id": "2510.17756v1_page_024"
  },
  {
    "text": "λtherm and λsat, improves the model fidelity relative to the No-Phy baseline\nmodel (Fig. 2a). The reduction in SIV RMSE by the physics loss term is\nobserved in all sample sizes, but this effect is more significant in the 20 %\nsample size: SIV RMSE decreases by up to 0.10 km/day when λsat is set to\n0.2. When 50 % and 100 % of training samples are used, the SIV RMSE\ndecreases by up to 0.02 km/day and 0.05 km/day, respectively. We also find\nimprovements of SIV MAE and ACC by PINN in all training sample cases\n(Table 2).",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_024.md",
    "arxiv_id": "2510.17756v1",
    "page": "24",
    "title": "unknown",
    "id": "2510.17756v1_page_024"
  },
  {
    "text": "than the No-Phy baseline model (Fig. 2), and the improvement of the PINN\nis more significant for smaller sample sizes. When 20 % of training samples\nare used, the PINNs show approximately 1.1 % lower SIC RMSE than the\nNo-Phy model. When the training samples are set to 50 % and 100 %, the\n24",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_024.md",
    "arxiv_id": "2510.17756v1",
    "page": "24",
    "title": "unknown",
    "id": "2510.17756v1_page_024"
  },
  {
    "text": "(20, 50, and 100 %) and combinations of λsat and λtherm (0, 0.2, 1.0, and 5.0), relative\nto the SIV RMSE of the No-Phy model. (b) The reduction of SIC RMSE by PINNs with\ndifferent training sample sizes and combinations of λsat and λtherm, relative to the SIC\nRMSE of the No-Phy model. Cross markers indicate statistically significant reduction.\n25",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_025.md",
    "arxiv_id": "2510.17756v1",
    "page": "25",
    "title": "unknown",
    "id": "2510.17756v1_page_025"
  },
  {
    "text": "and No-Phy with different training sample sizes. The best accuracy is highlighted in bold.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_026.md",
    "arxiv_id": "2510.17756v1",
    "page": "26",
    "title": "unknown",
    "id": "2510.17756v1_page_026"
  },
  {
    "text": "20 %\n50 %\n100 %\nSIV\nModel\nRMSE\n(km/d)\nMAE\n(km/d)\nACC\nRMSE\n(km/d)\nMAE\n(km/d)\nACC\nRMSE\n(km/d)\nMAE\n(km/d)\nACC\nPINN\n2.778\n1.954\n0.853\n2.732\n1.915\n0.859\n2.629\n1.844\n0.867\nNo-Phy\n2.873\n2.053\n0.843\n2.759\n1.936\n0.856\n2.684\n1.880\n0.863\nSIC\nModel\nRMSE\n(%)\nMAE\n(%)\nACC\nRMSE\n(%)\nMAE\n(%)\nACC\nRMSE\n(%)\nMAE\n(%)\nACC\nPINN\n6.687\n3.441\n0.975\n6.222\n3.118\n0.978\n5.854\n2.917\n0.981\nNo-Phy\n7.393\n4.335\n0.969\n6.611\n3.533\n0.975\n6.197\n3.274\n0.978\nSIC RMSEs are approximately 0.5 % and 0.3 % lower for the PINNs than the\nNo-Phy models, respectively. Besides the RMSE comparison, the MAE and\nACC are also improved by PINN (Table 2). These results suggest that the\nincorporation of physics-informed architecture and optimization is beneficial\nfor SIC predictability under limited training sample scenarios. However, the\nweights to the physics loss function are not so significant for the SIC accuracy:\nno consistent trend in SIC RMSE is observed with increasing λtherm or λsat\nvalues.\n5.2. Temporal characteristics of model performance\nFigure 3 shows the monthly RMSE of SIV prediction for seven test years\n(2016-2022) with different sample sizes. For the PINN model, we set λsat =\n0.2 and λtherm = 0.2 based on the optimal configuration identified in Figure\n2. In general, SIV prediction shows relatively lower errors in summer (June-\nAugust) due to the slow drift speed during these months (Fig. A.7c). The\nimprovement of SIV RMSE by PINN (i.e., RMSE difference between PINN\nand No-Phy model) is evident in most months when the sample size is 20\n26",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_026.md",
    "arxiv_id": "2510.17756v1",
    "page": "26",
    "title": "unknown",
    "id": "2510.17756v1_page_026"
  },
  {
    "text": "for seven test years (2016-2022). On the top panel, the solid lines indicate monthly SIV\nRMSEs of PINN models, and the dashed lines indicate No-Phy models. The bottom panel\nshows the differences between each PINN model and the No-Phy model.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_027.md",
    "arxiv_id": "2510.17756v1",
    "page": "27",
    "title": "unknown",
    "id": "2510.17756v1_page_027"
  },
  {
    "text": "significant decreases in RMSE by PINN are highlighted as circles in the bottom panel.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_027.md",
    "arxiv_id": "2510.17756v1",
    "page": "27",
    "title": "unknown",
    "id": "2510.17756v1_page_027"
  },
  {
    "text": "RMSE results from PINN and No-Phy. The shaded areas in the bottom panels show the\n25-75 % quantile of RMSE difference.\n%, and the RMSE reduction is relatively greater during the winter months\n(January to April) (Fig.\n3).",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_027.md",
    "arxiv_id": "2510.17756v1",
    "page": "27",
    "title": "unknown",
    "id": "2510.17756v1_page_027"
  },
  {
    "text": "significant temporal trend in the SIV performance improvement is found.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_027.md",
    "arxiv_id": "2510.17756v1",
    "page": "27",
    "title": "unknown",
    "id": "2510.17756v1_page_027"
  },
  {
    "text": "to the SIV results, SIC exhibits relatively higher errors during the summer\nmonths from June to July. While SIC remains consistently high from January\nto May, it decreases rapidly in these months (Fig. A.7d); the model could\nhave difficulty capturing this trend. The most interesting finding is that SIC\nRMSE is highly dependent on training sample size: SIC RMSE decreases with\na greater sample size. As a result, a more remarkable improvement in model\naccuracy by PINN is observed with fewer sample sizes. When the models are\ntrained with 20 % of training samples, PINN reduces the SIC RMSE by >\n27",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_027.md",
    "arxiv_id": "2510.17756v1",
    "page": "27",
    "title": "unknown",
    "id": "2510.17756v1_page_027"
  },
  {
    "text": "for seven test years (2016-2022). On the top panel, the solid lines indicate monthly SIC\nRMSEs of PINN models, and the dashed lines indicate No-Phy models. The bottom panel\nshows the differences between each PINN model and the No-Phy model.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_028.md",
    "arxiv_id": "2510.17756v1",
    "page": "28",
    "title": "unknown",
    "id": "2510.17756v1_page_028"
  },
  {
    "text": "significant decreases in RMSE by PINN are highlighted as circles in the bottom panel.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_028.md",
    "arxiv_id": "2510.17756v1",
    "page": "28",
    "title": "unknown",
    "id": "2510.17756v1_page_028"
  },
  {
    "text": "RMSE results from PINN and No-Phy. The shaded areas in the bottom panels show the\n25-75 % quantile of RMSE difference.\n0.5 % in all months, and such RMSE reduction is more significant in winter\nmonths from January to March.\n5.3. Spatial characteristics of model performance\nIn this section, we examine where the PINN approach is more or less\neffective and discuss the implications of such spatial patterns. Herein, we\ndiscuss only the PINN with λsat = 0.2 and λtherm = 0.2 configuration because\n(1) this PINN setting shows overall best RMSE in all sampling sizes and (2)\nthe spatial patterns of PINN improvement appear similar for all different λsat\nand λtherm values. The spatial distributions of SIV RMSE for PINN and No-\nPhy models with different sample sizes are shown in Figure 5. As previously\ndiscussed in Figures 2 and 3, the most substantial RMSE improvement occurs\n28",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_028.md",
    "arxiv_id": "2510.17756v1",
    "page": "28",
    "title": "unknown",
    "id": "2510.17756v1_page_028"
  },
  {
    "text": "with 20 % of training samples (Fig. 5h). When 20 % of training samples\nare used, the SIV RMSE improvement by PINN reaches up to 10 % near\nthe central Arctic and northern Canadian Archipelago (Fig. 5g). When 50\n% of training samples are used, RMSE reductions are observed across the\ncentral Arctic, but it is not statistically significant, with less than 5 % SIV\nimprovement in most regions (Fig. 5h). Similarly, when 100 % samples are\nused to train the model, <5 % of SIV RMSE reduction is observed across\nthe Arctic Ocean except central Arctic (Fig. 5i).",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_029.md",
    "arxiv_id": "2510.17756v1",
    "page": "29",
    "title": "unknown",
    "id": "2510.17756v1_page_029"
  },
  {
    "text": "different pattern from SIV. Above all, SIC RMSE improvement is observed\nacross the Arctic Ocean. The PINN approach plays a particularly important\nrole in improving SIC RMSE when only 20 % of training samples are used.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_029.md",
    "arxiv_id": "2510.17756v1",
    "page": "29",
    "title": "unknown",
    "id": "2510.17756v1_page_029"
  },
  {
    "text": "SIC RMSE of the No-Phy model exceeds 4 % across the Arctic Ocean (Fig.\n6a). However, by adopting PINN, SIC RMSE decreases significantly by up\nto 50 % in the central Arctic (Fig. 6g). The improvement of SIC RMSE\nby PINN is attenuated when more training samples are used. Although SIC\nRMSE improvement is not significant with 50 % and 100 % sample exper-\niments, the effectiveness of PINN still appears widespread over the central\nArctic with up to ∼20 % (Fig. 6h and i).\n5.4. Implications to future Arctic sea ice prediction\nGiven that many studies have shown the Arctic sea ice entered a new\nstate in the 2010s with a dramatic loss of sea ice cover (Stroeve and Notz,\n2018; Meier et al., 2014; Guemas et al., 2016; Döscher et al., 2014), historical\nsatellite-based sea ice observations spanning over more than 40 years since\n29",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_029.md",
    "arxiv_id": "2510.17756v1",
    "page": "29",
    "title": "unknown",
    "id": "2510.17756v1_page_029"
  },
  {
    "text": "% training samples, and (c) 100 % training samples. The SIV RMSE map of the PINN\nmodel (λsat = 0.2 and λtherm = 0.2) for (d) 20 % training samples, (e) 50 % training\nsamples, and (f) 100 % training samples. The relative SIV RMSE difference map between\nPINN and No-Phy model for (g) 20 % training samples, (h) 50 % training samples, and (i)\n100 % training samples. On the bottom panels of the RMSE differences, the reddish color\nindicates the improvement in SIV RMSE by PINN over the No-Phy model (i.e., reduction\nin RMSE).\n30",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_030.md",
    "arxiv_id": "2510.17756v1",
    "page": "30",
    "title": "unknown",
    "id": "2510.17756v1_page_030"
  },
  {
    "text": "% training samples, and (c) 100 % training samples. The SIC RMSE map of the PINN\nmodel (λsat = 0.2 and λtherm = 0.2) for (d) 20 % training samples, (e) 50 % training\nsamples, and (f) 100 % training samples. The relative SIC RMSE difference map between\nPINN and No-Phy model for (g) 20 % training samples, (h) 50 % training samples, and (i)\n100 % training samples. On the bottom panels of the RMSE differences, the reddish color\nindicates the improvement in SIC RMSE by PINN over the No-Phy model (i.e., reduction\nin RMSE).\n31",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_031.md",
    "arxiv_id": "2510.17756v1",
    "page": "31",
    "title": "unknown",
    "id": "2510.17756v1_page_031"
  },
  {
    "text": "the 1980s may no longer fully represent the recent and future non-stationary\nsea ice state in the Arctic. Moreover, considering that the Arctic Ocean is\nprojected to be nearly ice-free during summer as early as 2030-2040 (Masson-\nnet et al., 2012; Overland and Wang, 2013; Årthun et al., 2021), the future\nsea ice dynamics in the Arctic Ocean will likely be different from those in\nhistorical observations. Hence, this raises concerns about the reliability of\npurely data-driven deep learning models trained solely on historical records,\nas such models may struggle to generalize to recent and future non-stationary\nsea ice conditions under rapid climate change. However, our PINN strategy\nprovides a more robust alternative to fully data-driven learning by integrat-\ning physical principles directly into the learning process, thereby reducing\nreliance on historical data alone and enhancing model generalizability in a\nnon-stationary climate regime.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_032.md",
    "arxiv_id": "2510.17756v1",
    "page": "32",
    "title": "unknown",
    "id": "2510.17756v1_page_032"
  },
  {
    "text": "them with 2016-2022 data. Compared to the training seven years, the test\nseven years are characterized as fast-moving sea ice and lower SIC (Fig. A.7).",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_032.md",
    "arxiv_id": "2510.17756v1",
    "page": "32",
    "title": "unknown",
    "id": "2510.17756v1_page_032"
  },
  {
    "text": "in both SIV and SIC RMSEs compared to the fully data-driven No-Phy\nmodel. We highlight that our PINN models maintain consistent performance\nregardless of the number of training samples, whereas the performance of\nNo-Phy is dependent on the number of training samples (Figs. 5 and 6).",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_032.md",
    "arxiv_id": "2510.17756v1",
    "page": "32",
    "title": "unknown",
    "id": "2510.17756v1_page_032"
  },
  {
    "text": "predictive performance and generalizability even when the training data do\nnot fully represent unseen sea ice conditions. Moreover, considering that the\nsignificant improvements by PINN are observed near multi-year ice (MYI)\n32",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_032.md",
    "arxiv_id": "2510.17756v1",
    "page": "32",
    "title": "unknown",
    "id": "2510.17756v1_page_032"
  },
  {
    "text": "regions in the central Arctic and Canadian Archipelago (Figs. 5 and 6), our\nfindings suggest that PINNs can contribute to obtaining consistent model\nperformance despite the ongoing reduction of MYI coverage.\n6. Conclusion\nIn this study, we propose a physics-informed learning strategy to im-\nprove the fidelity of the existing deep learning model for the prediction of\ndaily sea ice velocity (SIV) and sea ice concentration (SIC) retrieved from\nspaceborne remote sensing data. Our physics-informed learning strategy is\nachieved through two pathways: (1) design additional physics-informed loss\nfunctions that regularize SIV values based on SIC values and regularize SIC\nvalues based on the assumption of daily thermodynamic ice growth and melt-\ning; (2) insert sigmoid activation function to restrict the output SIC values\ninto the range of 0 to 1. We implement extensive experiments by employ-\ning the Hierarchical information-sharing U-net (HIS-Unet), which predicts\nSIV and SIC through a series of information sharing between SIV and SIC\nbranches. In order to investigate the impact of training samples on the model\nrobustness, we train the physics-informed neural network (PINN) and normal\ndata-driven neural network (No-Phy) separately with three different train-\ning sample ratios (100 %, 50 %, and 20 %) and four different weights to the\nphysics-informed loss terms (0.0, 0.2, 1.0, and 0.5).",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_033.md",
    "arxiv_id": "2510.17756v1",
    "page": "33",
    "title": "unknown",
    "id": "2510.17756v1_page_033"
  },
  {
    "text": "both SIC and SIV predictions. In particular, the improvement of SIC pre-\ndiction by PINN is more obvious than SIV with a small number of samples.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_033.md",
    "arxiv_id": "2510.17756v1",
    "page": "33",
    "title": "unknown",
    "id": "2510.17756v1_page_033"
  },
  {
    "text": "33",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_033.md",
    "arxiv_id": "2510.17756v1",
    "page": "33",
    "title": "unknown",
    "id": "2510.17756v1_page_033"
  },
  {
    "text": "even in the case of a lack of sufficient training datasets.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_034.md",
    "arxiv_id": "2510.17756v1",
    "page": "34",
    "title": "unknown",
    "id": "2510.17756v1_page_034"
  },
  {
    "text": "physics-informed learning strategy can contribute to improving the perfor-\nmance of deep learning models for sea ice prediction, even if the past sea ice\ndata cannot fully represent the future non-stationary sea ice conditions af-\nfected by rapid climate change. Additionally, this strategy can also be easily\napplied for a multi-day forecast of sea ice conditions combined with recurrent\nnetwork architectures, such as LSTM.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_034.md",
    "arxiv_id": "2510.17756v1",
    "page": "34",
    "title": "unknown",
    "id": "2510.17756v1_page_034"
  },
  {
    "text": "The codes that were used for the prediction of sea ice concentration and\nvelocity using Python language (version 3.11) based on PyTorch library can\nbe found in Github: https://github.com/BinaLab/PINN_seaice. This repos-\nitory was created by Younghyun Koo (e-mail: kooala317@gmail.com) in 2023\nand contains program codes (111 MB) and training data (3.41 GB). The au-\nthors’ experimental environment was as follows:\n• OS: Windows 11 Pro\n• CPU: Intel(R) Core(TM) i7-11700F\n• RAM: 16.0 GB\n• GPU: NVIDIA GeForce RTX 3070\nThe sea ice velocity and concentration data can be downloaded free of charge\nfrom NSIDC (Tschudi et al., 2019; Meier et al., 2021), and the ERA5 weather\ndata can be downloaded free of charge from the Copernicus Climate Data\nStore (Hersbach et al., 2020).\n34",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_034.md",
    "arxiv_id": "2510.17756v1",
    "page": "34",
    "title": "unknown",
    "id": "2510.17756v1_page_034"
  },
  {
    "text": "CRediT authorship contribution statement\nYounghyun Koo: Software, Validation, Formal analysis, Data cura-\ntion, Visualization, Writing - Original Draft. Maryam Rahnemoonfar:\nConceptualization, Methodology, Writing - Review & Editing, Supervision,\nFunding acquisition.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_035.md",
    "arxiv_id": "2510.17756v1",
    "page": "35",
    "title": "unknown",
    "id": "2510.17756v1_page_035"
  },
  {
    "text": "This work is supported by the U.S. National Science Foundation (NSF)\nBIGDATA (IIS-1838230, IIS-2308649) and NSF Leadership Class Computing\n(OAC-2139536) awards.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_035.md",
    "arxiv_id": "2510.17756v1",
    "page": "35",
    "title": "unknown",
    "id": "2510.17756v1_page_035"
  },
  {
    "text": "Figures A.7a and b show the average SIV and SIC, respectively, over\nthe Arctic Ocean in 2022. The Fram Strait in eastern Greenland shows the\nfastest ice movement. Figures A.7c and d show the monthly mean SIV and\nSIC, respectively, in 2022. SIV decreased from January to June and increased\nagain thereafter. SIC was consistent to 90 % from January to May, decreased\nto ∼70 % until September, and increased again from October.",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_035.md",
    "arxiv_id": "2510.17756v1",
    "page": "35",
    "title": "unknown",
    "id": "2510.17756v1_page_035"
  },
  {
    "text": "Andersson, T.R., Hosking, J.S., Pérez-Ortiz, M., Paige, B., Elliott, A., Rus-\nsell, C., Law, S., Jones, D.C., Wilkinson, J., Phillips, T., Byrne, J.,\nTietsche, S., Sarojini, B.B., Blanchard-Wrigglesworth, E., Aksenov, Y.,\nDownie, R., Shuckburgh, E., 2021. Seasonal arctic sea ice forecasting with\nprobabilistic deep learning. Nature Communications 12.\n35",
    "source": "D:\\Магистратура\\Учеба\\LLM\\llm-25\\projects\\ma-yamkin\\lab2\\data\\processed\\arxiv\\2510.17756v1\\page_035.md",
    "arxiv_id": "2510.17756v1",
    "page": "35",
    "title": "unknown",
    "id": "2510.17756v1_page_035"
  }
]