---
source: "..\data\raw\arxiv\2510.17722v1.pdf"
arxiv_id: "2510.17722v1"
page: 5
total_pages: 9
date_converted: "2025-11-05"
---

## Cross-Scene Video Merging. The generation of cross-scene, multi-turn dialogues necessitates the
retrieval and merging of relevant scenes from disparate video segments, which serves as a critical step
in creating coherent interactions that span across multiple visual contexts. Firstly, frames are extracted
from the video at 2 FPS and then filtered based on two criteria: sharpness and similarity to the previous
selected frame. The sharpness of each frame is evaluated by the Laplace Operator to ensure that only
clear, visually significant frames are retained, improving the overall quality of the selected frames. To
avoid redundancy, frames with high similarity to the preceding selected frame are discarded. Specifically,
a histogram-based image similarity calculation method is used to compare consecutive frames, excluding
those with a similarity score above 0.9. This approach ensures that the selected frames are distinct and
capture key moments in the video.
## Following frame selection, object detection is performed using YOLOv11 (Khanam and Hussain, 2024),
and each detected object is then annotated with a caption generated by the Gemini 2.5 Flash (Team, 2025),
providing detailed descriptions for each object. As the video progresses, a dynamic object memory bank
is maintained, continuously expanded based on object captions and visual similarities. This memory
bank associates unique object IDs with their corresponding attributes, enabling the identification of the
same objects across frames. To merge relevant scenes, a retrieval step across scenes is performed to select
video segments that share common objects or themes, which are then merged to ensure continuity both
thematically and contextually.
## Multi-Turn Dialogues Generation. This process employs the Gemini 2.5 Pro (Team, 2025) to automate
the generation of both single-scene and cross-scene multi-turn dialogues, based on the six evaluation
tasks defined earlier. For each video, we generate multiple multi-turn dialogues, each corresponding to
different scenes. To determine the most appropriate task for each scene, we prompt MLLMs to evaluate
the scene’s capabilities, scoring them on a scale from 1 to 6. Only those tasks that receive a score of 5 or 6
are selected for dialogue generation. For multi-turn dialogues spanning multiple scenes, we specifically
adopt an object-centered approach for cross-scene question design since objects often serve as the central
element around which events unfold. This approach emphasizes the continuity and relationships of
objects across scenes, enabling the generation of dialogues that are both contextually consistent and
thematically coherent.
3.4
Quality Control
Following automated data collection, we employ the following two-stage human verification process to
enhance dataset quality.
## Stage 1: Eliminating information leakage. We categorize all benchmark questions into two types: (1)
context-dependent, which can be answered solely based on dialogue history; and (2) video-dependent,
which require direct grounding in the video content. We observe that in some generated dialogues,
earlier QA pairs embedded excessive background hints, enabling models to answer subsequent questions
without relying on the video. This led to an overrepresentation of context-dependent items, thereby
weakening the evaluation of video understanding. To mitigate this, we systematically removed such
cases to ensure that the majority of questions required genuine video-based reasoning.
## Stage 2: Human verification and validation. After the first filtering, human annotators conducted a
secondary review from a human perspective. We verify whether each question and answer pair was
factually aligned with the video and free from ambiguities. Beyond factual correctness, we also examine
whether each question is properly aligned with its intended ability dimension. For example, answer
refusal questions must explicitly test whether a model can recognize “events absent from the video,”
while object reference questions must involve pronoun disambiguation. Any misaligned samples are
discarded. Finally, we filter out overly simple questions, as they can be trivially solved by most models
and fail to highlight multi-turn reasoning and video comprehension capacities.
3.5
Dataset Statistics
Figure 3 presents the statistics of MT-Video-Bench. It covers a broad range of topics across five main cate-
gories: Movie, TV, Sports, Knowledge, and Life Record, each with multiple sub-topics, ensuring a diverse
and balanced data distribution. With a total of 987 multi-turn dialogues, the data distribution across the
six primary tasks in MT-Video-Bench is relatively balanced, as shown in Figure 3 (b). Furthermore, our
dataset features videos of varying lengths, with most being under 15 minutes and a small proportion
exceeding 15 minutes, thereby ensuring coverage of both short and long videos. The number of dialogue
turns typically ranges from 5 to 8, with an average of 5.88 turns per dialogue.
5
