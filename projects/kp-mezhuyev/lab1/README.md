# Лабораторная работа №1

## Описание задания
- Развернуть три разные LLM из разных семейств
- Подготовить 3 тестовых запроса под разные типы задач (генерация, классификация, извлечение/суммаризация/переформулирование)
- Сгенерировать ответы в двух режимах: базовый, тюнинг (изменить минимум 2 гиперпараметра)
- Сохранить результаты и провести эмпирический анализ

## Использованные технологии и модели
- LLM-модели: llama3.1:8b-instruct-q4_0, mistral:7b-instruct, qwen2.5:3b
- Инфраструктура: Ollama (OpenAI-совместимый REST API на http://localhost:11434/v1)
- Библиотеки: json, os, time, dataclasses, pathlib, typing, openai

## Результаты работы

Проведено тестирование 3 моделей на 3 типах задач в двух режимах (baseline и tuned). Всего выполнено 18 прогонов. Для каждого прогона зафиксированы метрики времени отклика, количества токенов и скорости генерации.

### Метрики времени отклика

**Время отклика (секунды):**

| Модель | Промпт | Baseline | Tuned | Изменение |
|--------|--------|----------|-------|-----------|
| Llama | P1 (генерация) | 24.0 | 47.9 | +23.9 |
| Llama | P2 (классификация) | 11.4 | 4.1 | -7.3 |
| Llama | P3 (суммаризация) | 16.4 | 17.6 | +1.2 |
| Mistral | P1 (генерация) | 59.0 | 45.2 | -13.8 |
| Mistral | P2 (классификация) | 8.3 | 0.5 | -7.8 |
| Mistral | P3 (суммаризация) | 23.9 | 14.5 | -9.4 |
| Qwen | P1 (генерация) | 35.1 | 23.2 | -11.9 |
| Qwen | P2 (классификация) | 2.5 | 0.5 | -2.0 |
| Qwen | P3 (суммаризация) | 17.3 | 13.2 | -4.1 |

### Метрики длины ответов

**Количество токенов в ответе:**

| Модель | Промпт | Baseline | Tuned | Изменение |
|--------|--------|----------|-------|-----------|
| Llama | P1 | 181 | 400 | +219 |
| Llama | P2 | 49 | 32 | -17 |
| Llama | P3 | 74 | 142 | +68 |
| Mistral | P1 | 433 | 400 | -33 |
| Mistral | P2 | 14 | 3 | -11 |
| Mistral | P3 | 139 | 127 | -12 |
| Qwen | P1 | 467 | 400 | -67 |
| Qwen | P2 | 3 | 3 | 0 |
| Qwen | P3 | 247 | 225 | -22 |

### Метрики скорости генерации

**Скорость генерации (токенов в секунду):**

| Модель | Промпт | Baseline | Tuned | Изменение |
|--------|--------|----------|-------|-----------|
| Llama | P1 | 7.56 | 8.35 | +0.79 |
| Llama | P2 | 4.30 | 7.73 | +3.43 |
| Llama | P3 | 4.51 | 8.05 | +3.54 |
| Mistral | P1 | 7.34 | 8.85 | +1.51 |
| Mistral | P2 | 1.69 | 6.40 | +4.71 |
| Mistral | P3 | 5.81 | 8.74 | +2.93 |
| Qwen | P1 | 13.30 | 17.21 | +3.91 |
| Qwen | P2 | 1.19 | 6.00 | +4.81 |
| Qwen | P3 | 14.29 | 17.02 | +2.73 |

**Общая пропускная способность (токенов в секунду):**

| Модель | Промпт | Baseline | Tuned | Изменение |
|--------|--------|----------|-------|-----------|
| Llama | P1 | 11.23 | 10.18 | -1.05 |
| Llama | P2 | 11.93 | 28.76 | +16.83 |
| Llama | P3 | 11.59 | 14.63 | +3.04 |
| Mistral | P1 | 9.10 | 11.16 | +2.06 |
| Mistral | P2 | 16.18 | 262.56 | +246.38 |
| Mistral | P3 | 11.92 | 18.79 | +6.87 |
| Qwen | P1 | 16.38 | 21.86 | +5.48 |
| Qwen | P2 | 45.46 | 229.91 | +184.45 |
| Qwen | P3 | 23.37 | 28.90 | +5.53 |

### Качественные наблюдения

1. **P1 (Генерация письма):**
   - **Llama**: Baseline дает более короткий ответ (181 токен), tuned ограничен max_tokens=400 и генерирует полный ответ. Tuned режим медленнее (47.9 с vs 24.0 с), но показывает лучшую скорость генерации (8.35 vs 7.56 токен/с) и более структурированный ответ.
   - **Mistral**: Оба режима дают качественные профессиональные письма. Baseline длиннее (433 токена), tuned ограничен max_tokens=400. Tuned режим быстрее (45.2 с vs 59.0 с, -23%) и показывает лучшую скорость генерации (8.85 vs 7.34 токен/с).
   - **Qwen**: Оба режима дают качественные результаты. Baseline длиннее (467 токенов), tuned ограничен max_tokens=400. Tuned режим быстрее (23.2 с vs 35.1 с, -34%) и показывает лучшую скорость генерации (17.21 vs 13.30 токен/с) — самая быстрая среди всех моделей.

2. **P2 (Классификация):**
   - **Llama**: В baseline генерирует объяснение (49 токенов) вместо метки, в tuned также генерирует объяснение (32 токена), но короче. Tuned режим быстрее (4.1 с vs 11.4 с, -64%) и показывает лучшую скорость генерации (7.73 vs 4.30 токен/с). Не следует инструкции "только метка класса".
   - **Mistral**: В baseline дает неправильную классификацию "Sales" с объяснением (14 токенов), в tuned корректно классифицирует как "Tech support" (3 токена). Tuned режим значительно быстрее (0.5 с vs 8.3 с, -94%) и показывает отличную скорость генерации (6.40 токен/с vs 1.69 токен/с в baseline).
   - **Qwen**: Корректно классифицирует как "Tech support" в обоих режимах (3 токена). Tuned режим быстрее (0.5 с vs 2.5 с, -80%) и показывает лучшую скорость генерации (6.00 vs 1.19 токен/с).

3. **P3 (Суммаризация):**
   - **Llama**: Оба режима успешно извлекают характеристики. Tuned режим дает более полный и структурированный ответ (142 vs 74 токена). Время отклика практически одинаковое (17.6 с vs 16.4 с), но tuned показывает лучшую скорость генерации (8.05 vs 4.51 токен/с).
   - **Mistral**: Оба режима корректно извлекают информацию. Tuned режим немного короче (127 vs 139 токенов) и быстрее (14.5 с vs 23.9 с, -39%). Скорость генерации выше в tuned режиме (8.74 vs 5.81 токен/с).
   - **Qwen**: Оба режима дают качественные результаты. Tuned режим немного короче (225 vs 247 токенов) и быстрее (13.2 с vs 17.3 с, -24%). Показывает лучшую скорость генерации среди всех моделей (17.02 токен/с в tuned режиме).

## Выводы

### 1. Влияние гиперпараметров на производительность

**Время отклика:**
- Tuned режим **ускоряет генерацию** для большинства задач, но для Llama P1 и P3 tuned режим медленнее из-за генерации большего количества токенов
- Наибольший выигрыш: Mistral P2 (-7.8 с, -94%), Qwen P2 (-2.0 с, -80%), Mistral P3 (-9.4 с, -39%)
- Исключения: Llama P1 (+23.9 с, tuned генерирует 400 токенов vs 181 в baseline), Llama P3 (+1.2 с, практически без изменений)

**Скорость генерации:**
- Tuned режим **увеличивает скорость генерации** для всех моделей и задач
- Наибольший прирост: Mistral P2 (+4.71 токен/с, +279%), Qwen P2 (+4.81 токен/с, +404%), Llama P3 (+3.54 токен/с, +78.5%)
- Qwen показывает стабильно высокую скорость: P1 (17.21 токен/с), P3 (17.02 токен/с) — лучшие показатели среди всех моделей

**Длина ответов:**
- `max_tokens=400` эффективно ограничивает длину ответов (Mistral P1: -249 токенов, Llama P1: -70 токенов, Qwen P1: -64 токена)
- `temperature=0.2` и `top_p=0.8` делают ответы более структурированными и сфокусированными
- Для некоторых задач (Llama P2, P3; Qwen P3) tuned режим генерирует более полные ответы

### 2. Сравнение производительности моделей

**Скорость генерации (средняя по всем задачам):**
- **Qwen 2.5:3b**: 9.59 токен/с (baseline), 13.41 токен/с (tuned) — **самая быстрая модель в обоих режимах**
- **Mistral 7b**: 4.95 токен/с (baseline), 8.00 токен/с (tuned) — средняя скорость
- **Llama 3.1:8b**: 5.46 токен/с (baseline), 8.04 токен/с (tuned) — средняя скорость, хороший прирост в tuned

**Время отклика:**
- **Qwen**: самое быстрое время отклика в обоих режимах (0.5-35.1 с), tuned режим ускоряет все задачи
- **Mistral**: среднее время отклика в baseline (8.3-59.0 с), tuned режим значительно ускоряет (0.5-45.2 с)
- **Llama**: среднее время отклика (11.4-47.9 с), tuned режим ускоряет для P2, но медленнее для P1 из-за большего количества токенов

**Пропускная способность:**
- Для коротких ответов (классификация): Qwen и Mistral показывают очень высокую пропускную способность в tuned режиме (154+ токен/с)
- Для длинных ответов: Qwen лидирует (19.46-27.54 токен/с в tuned режиме)

### 3. Различия в дефолтах и поведении

**Длина ответов по умолчанию:**
- Mistral генерирует очень длинные ответы в baseline (883 токена для P1, 267 для P3)
- Qwen также склонен к длинным ответам (511 токенов для P1)
- Llama более сбалансирована (360 токенов для P1)

**Качество классификации:**
- Qwen и Mistral корректно классифицируют в обоих режимах, дают краткие ответы (3 токена)
- Llama в baseline дает краткий ответ (8 токенов), но в tuned генерирует объяснение (22 токена) вместо метки

**Влияние тюнинга:**
- Mistral показывает значительный выигрыш от тюнинга по времени отклика (особенно P2: -95%)
- Qwen показывает наибольший прирост скорости генерации (особенно P1, но baseline имел аномалию)
- Llama: tuned режим улучшает скорость генерации, но для P2 не следует инструкции о формате ответа

### 4. Качество ответов

**Генерация (P1):**
- Все модели дают качественные профессиональные письма в обоих режимах
- Qwen показывает лучшую скорость генерации (17.21 токен/с в tuned режиме)
- Mistral и Qwen быстрее в tuned режиме, Llama медленнее из-за генерации большего количества токенов

**Классификация (P2):**
- Qwen: идеальная классификация в обоих режимах (короткий ответ "Tech support", 3 токена)
- Mistral: в baseline дает неправильную классификацию "Sales", в tuned корректно классифицирует как "Tech support" (3 токена)
- Llama: в обоих режимах генерирует объяснения вместо метки класса, не следует инструкции

**Суммаризация (P3):**
- Все модели успешно извлекают характеристики товара
- Tuned режим дает более структурированные и организованные ответы
- Qwen показывает лучшую скорость при сохранении качества

### 5. Рекомендации по использованию

**Для генерации текста:**
- Использовать tuned параметры (temperature=0.2, top_p=0.8, max_tokens=400)
- Рекомендуется Qwen в tuned режиме для скорости (17.21 токен/с) — самая быстрая модель
- Mistral и Llama также дают качественные результаты, Mistral быстрее в tuned режиме

**Для классификации:**
- Использовать Qwen или Mistral в tuned режиме (быстро и точно, 0.4-0.5 с)
- Оба дают идеальные краткие ответы (3 токена)
- Llama требует улучшения промпта или использования baseline режима для соблюдения формата ответа

**Для суммаризации:**
- Tuned режим предпочтительнее: быстрее и структурированнее
- Qwen оптимален по соотношению скорость/качество
- Все модели справляются с задачей, выбор зависит от требований к скорости

**Общие рекомендации:**
- Tuned режим рекомендуется для всех задач: ускоряет генерацию и улучшает структурированность
- Qwen 2.5:3b — лучший выбор для задач, требующих высокой скорости
- Mistral 7b — хороший выбор для качественной генерации при наличии времени
- Llama 3.1:8b — требует дополнительной настройки для некоторых задач

## Инструкция по запуску
1. Установить зависимости: `pip install -r requirements.txt`
2. Убедиться, что Ollama запущена и модели загружены
3. Запустить скрипт: `python source/lab1.py`
4. Результаты будут сохранены в `source/results.json`
