"""
RAG-–ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ LLM.
–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞–Ω–∏–µ 6: —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–≤–∏–∂–∫–∞ –æ–±—â–µ–Ω–∏—è (RAG-–ø–∞–π–ø–ª–∞–π–Ω).
"""

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import json
from pathlib import Path
from typing import List, Dict, Any, Optional
import argparse
from openai import OpenAI

from evaluate_retrieval import RetrievalEvaluator
from utils import get_device


class RAGPipeline:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞."""
    
    def __init__(self,
                 faiss_index_dir: str,
                 chunks_dir: str,
                 dense_model_name: str,
                 llm_client: OpenAI,
                 llm_model: str = "qwen2.5:7b",
                 search_type: str = "hybrid",
                 top_k: int = 5,
                 device: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞.
        
        Args:
            faiss_index_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏–Ω–¥–µ–∫—Å–æ–º Faiss
            chunks_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —á–∞–Ω–∫–∞–º–∏
            dense_model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è dense —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            llm_client: –ö–ª–∏–µ–Ω—Ç OpenAI –¥–ª—è –≤—ã–∑–æ–≤–∞ LLM
            llm_model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ LLM
            search_type: –¢–∏–ø –ø–æ–∏—Å–∫–∞ (dense, sparse, hybrid)
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        """
        self.llm_client = llm_client
        self.llm_model = llm_model
        self.search_type = search_type
        self.top_k = top_k
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Ü–µ–Ω—â–∏–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞
        self.evaluator = RetrievalEvaluator(
            faiss_index_dir,
            chunks_dir,
            dense_model_name,
            device=device
        )
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        self.system_prompt = """–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.
–ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞.
–ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º.
–í—Å–µ–≥–¥–∞ —É–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Å–≤–æ–µ–º –æ—Ç–≤–µ—Ç–µ, —Å—Å—ã–ª–∞—è—Å—å –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.
–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, —á–µ—Ç–∫–æ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ."""
    
    def retrieve_context(self, query: str) -> List[Dict[str, Any]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞.
        
        Args:
            query: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if self.search_type == "dense":
            results = self.evaluator.search_dense(query, k=self.top_k)
        elif self.search_type == "sparse":
            results = self.evaluator.search_sparse(query, k=self.top_k)
        elif self.search_type == "hybrid":
            results = self.evaluator.search_hybrid(query, k=self.top_k)
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –ø–æ–∏—Å–∫–∞: {self.search_type}")
        
        return results
    
    def format_context(self, chunks: List[Dict[str, Any]]) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —á–∞–Ω–∫–∏ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞.
        
        Args:
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        """
        context_parts = []
        
        for i, result in enumerate(chunks, start=1):
            chunk = result.get("chunk", {})
            text = chunk.get("text", "")
            metadata = chunk.get("metadata", {})
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            source_file = metadata.get("source_file", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫")
            repository = metadata.get("repository", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π")
            repository_url = metadata.get("repository_url", "")
            header = metadata.get("Header 1", "")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context_header = f"[–ö–æ–Ω—Ç–µ–∫—Å—Ç {i}]"
            if header:
                context_header += f" –†–∞–∑–¥–µ–ª: {header}"
            if repository:
                context_header += f" | –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {repository}"
            if repository_url:
                context_header += f" ({repository_url})"
            
            context_parts.append(f"{context_header}\n{text}\n")
        
        return "\n---\n\n".join(context_parts)
    
    def build_prompt(self, question: str, context: str) -> List[Dict[str, str]]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM.
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è OpenAI API
        """
        user_prompt = f"""–í–æ–ø—Ä–æ—Å: {question}

–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏:
{context}

–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –£–∫–∞–∂–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."""
        
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        return messages
    
    def generate_answer(self, question: str, temperature: float = 0.7, max_tokens: int = 1000) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG.
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –æ—Ç–≤–µ—Ç–æ–º, –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        print(f"üîç –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        retrieved_chunks = self.retrieve_context(question)
        
        if not retrieved_chunks:
            return {
                "answer": "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –≤–∞—à–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.",
                "context": [],
                "sources": []
            }
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(retrieved_chunks)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
        
        context_text = self.format_context(retrieved_chunks)
        
        messages = self.build_prompt(question, context_text)
        
        print(f"ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é Ollama ({self.llm_model})...")
        try:
            response = self.llm_client.chat.completions.create(
                model=self.llm_model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                extra_body={"repeat_penalty": 1.1}
            )
            
            answer = response.choices[0].message.content
            
        except Exception as e:
            return {
                "answer": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}",
                "context": retrieved_chunks,
                "sources": self._extract_sources(retrieved_chunks)
            }
        
        formatted_answer = self._format_answer_with_citations(answer, retrieved_chunks)
        
        return {
            "answer": formatted_answer,
            "raw_answer": answer,
            "context": retrieved_chunks,
            "sources": self._extract_sources(retrieved_chunks)
        }
    
    def _extract_sources(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ —á–∞–Ω–∫–æ–≤."""
        sources = []
        for chunk_result in chunks:
            chunk = chunk_result.get("chunk", {})
            metadata = chunk.get("metadata", {})
            
            source = {
                "repository": metadata.get("repository", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"),
                "repository_url": metadata.get("repository_url", ""),
                "source_file": metadata.get("source_file", ""),
                "header": metadata.get("Header 1", ""),
                "snippet": chunk.get("text", "")[:200] + "..." if len(chunk.get("text", "")) > 200 else chunk.get("text", "")
            }
            sources.append(source)
        
        return sources
    
    def _format_answer_with_citations(self, answer: str, chunks: List[Dict[str, Any]]) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å —Ü–∏—Ç–∞—Ç–∞–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.
        
        Args:
            answer: –û—Ç–≤–µ—Ç –æ—Ç LLM
            chunks: –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç —Å —Ü–∏—Ç–∞—Ç–∞–º–∏
        """
        sources_section = "\n\n---\n\n**–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**\n\n"
        
        for i, chunk_result in enumerate(chunks, start=1):
            chunk = chunk_result.get("chunk", {})
            metadata = chunk.get("metadata", {})
            
            repository = metadata.get("repository", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π")
            repository_url = metadata.get("repository_url", "")
            header = metadata.get("Header 1", "")
            
            source_text = f"{i}. "
            if repository_url:
                source_text += f"[{repository}]({repository_url})"
            else:
                source_text += repository
            
            if header:
                source_text += f" ‚Äî {header}"
            
            sources_section += source_text + "\n"
        
        return answer + sources_section


def create_llm_client() -> OpenAI:
    """
    –°–æ–∑–¥–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç –¥–ª—è Ollama API.
    –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Ollama (–∫–∞–∫ –≤ lab1).
    
    Returns:
        –ö–ª–∏–µ–Ω—Ç OpenAI –¥–ª—è Ollama
    """
    # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º Ollama (–∫–∞–∫ –≤ lab1)
    api_base = os.getenv("LLM_API_BASE", "http://localhost:11434/v1")
    api_key = os.getenv("LLM_API_KEY", "ollama")
    
    client = OpenAI(
        base_url=api_base,
        api_key=api_key
    )
    
    return client


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='RAG-–ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã')
    parser.add_argument('--faiss-index-dir', type=str, default='faiss_index',
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏–Ω–¥–µ–∫—Å–æ–º Faiss')
    parser.add_argument('--chunks-dir', type=str, default='chunks',
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —á–∞–Ω–∫–∞–º–∏')
    parser.add_argument('--dense-model', type=str, default='intfloat/multilingual-e5-large',
                       help='–ú–æ–¥–µ–ª—å –¥–ª—è dense —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤')
    parser.add_argument('--llm-model', type=str, default='qwen2.5:7b',
                       help='–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ LLM –∏–∑ Ollama (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é qwen2.5:7b, –∫–∞–∫ –≤ lab1)')
    parser.add_argument('--search-type', type=str, choices=['dense', 'sparse', 'hybrid'],
                       default='hybrid', help='–¢–∏–ø –ø–æ–∏—Å–∫–∞')
    parser.add_argument('--top-k', type=int, default=5,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞')
    parser.add_argument('--device', type=str, default=None,
                       help='–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (mps/cuda/cpu)')
    parser.add_argument('--temperature', type=float, default=0.7,
                       help='–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏')
    parser.add_argument('--max-tokens', type=int, default=1000,
                       help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("RAG-–ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç LLM –¥–ª—è Ollama (–∫–∞–∫ –≤ lab1)
    print(f"\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama –∫–ª–∏–µ–Ω—Ç–∞...")
    llm_client = create_llm_client()
    api_base_used = os.getenv('LLM_API_BASE', 'http://localhost:11434/v1')
    print(f"‚úÖ Ollama –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω (API: {api_base_used})")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG-–ø–∞–π–ø–ª–∞–π–Ω
    print(f"\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞...")
    device = get_device(args.device) if args.device else None
    rag = RAGPipeline(
        faiss_index_dir=args.faiss_index_dir,
        chunks_dir=args.chunks_dir,
        dense_model_name=args.dense_model,
        llm_client=llm_client,
        llm_model=args.llm_model,
        search_type=args.search_type,
        top_k=args.top_k,
        device=device
    )
    print("‚úÖ RAG-–ø–∞–π–ø–ª–∞–π–Ω –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
    print(f"\n{'='*60}")
    print("–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º")
    print("–í–≤–µ–¥–∏—Ç–µ 'exit' –∏–ª–∏ 'quit' –¥–ª—è –≤—ã—Ö–æ–¥–∞")
    print("=" * 60)
    
    while True:
        try:
            question = input("\n‚ùì –í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()
            
            if not question:
                continue
            
            if question.lower() in ['exit', 'quit', '–≤—ã—Ö–æ–¥']:
                print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            
            print()
            result = rag.generate_answer(
                question,
                temperature=args.temperature,
                max_tokens=args.max_tokens
            )
            
            print(f"\n{'='*60}")
            print("–û—Ç–≤–µ—Ç:")
            print("=" * 60)
            print(result["answer"])
            print()
            
        except KeyboardInterrupt:
            print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞: {str(e)}")