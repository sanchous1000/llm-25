# Лабораторная работа №3 — Развертывание Langfuse для логирования LLM-запросов

## Описание задания

Интеграция Langfuse в RAG-приложение для централизованного логирования запросов к LLM, трассировки взаимодействий, создания датасетов и оценки качества через эксперименты.

### Задание 1: Развернуть Langfuse

**Реализованный функционал:**
- Развертывание Langfuse локально с использованием Docker Compose
- Конфигурация всех необходимых сервисов:
  - PostgreSQL для хранения метаданных
  - ClickHouse для хранения событий и метрик
  - Redis для очередей задач
  - MinIO для хранения медиафайлов
  - Langfuse Web UI на порту 3000
  - Langfuse Worker для обработки задач
- Настройка переменных окружения для подключения приложения к Langfuse
- Проверка доступности интерфейса через браузер

**Результаты:**
- Langfuse развернут и доступен по адресу `http://localhost:3000`
- Создан проект в Langfuse с полученными credentials
- Настроены переменные окружения:
  - `LANGFUSE_PUBLIC_KEY`: Публичный ключ для доступа
  - `LANGFUSE_SECRET_KEY`: Секретный ключ для аутентификации
  - `LANGFUSE_HOST`: Адрес сервера Langfuse (по умолчанию `http://localhost:3000`)

**Использование:**
```bash
cd source
docker compose up -d
```

### Задание 2: Интегрировать логирование запросов LLM

**Реализованный функционал:**
- Интеграция Langfuse SDK в RAG-пайплайн (`rag_pipeline.py`)
- Логирование каждого пользовательского запроса к LLM:
  - Входные параметры (промпт, параметры модели: temperature, max_tokens)
  - Модель LLM (qwen3:latest через Ollama API)
  - Системный промпт и пользовательский запрос
- Логирование ответов LLM:
  - Сгенерированный текст
  - Метаданные: количество токенов (prompt_tokens, completion_tokens, total_tokens)
  - Длительность генерации (duration_seconds)
- Сохранение идентификатора пользователя и сессии:
  - Генерация уникального `user_id` для каждого запроса
  - Генерация `session_id` при запуске приложения для отслеживания сессий
  - Связывание всех запросов в рамках одной сессии

**Реализация:**
- Использование `langfuse.start_as_current_observation()` для логирования генераций
- Тип наблюдения: `generation` для LLM-запросов
- Автоматическое сохранение метаданных о токенах и времени выполнения

### Задание 3: Логирование взаимодействия с пользователями и их сохранения

**Реализованный функционал:**
- Полное логирование взаимодействия с пользователями через трассировку Langfuse
- Логирование промежуточных шагов RAG-пайплайна:
  1. **Retrieval (извлечение релевантных документов)**:
     - Тип поиска (dense/sparse/hybrid)
     - Количество извлеченных чанков
     - Метаданные чанков (источник, репозиторий, заголовок)
     - Оценки релевантности (scores)
     - Длительность поиска
  2. **LLM Generation (генерация ответа)**:
     - Промпт с контекстом
     - Параметры модели
     - Сгенерированный ответ
     - Метаданные о токенах
- Использование трассировки для связывания всех шагов:
  - Создание корневого span `rag_pipeline` для всего взаимодействия
  - Вложенные spans для каждого этапа (retrieval, llm_generation)
  - Автоматическая связь всех наблюдений через `start_as_current_observation`
- Сохранение всех метаданных для анализа:
  - Источники документов (repository, repository_url, source_file)
  - Использованные чанки с превью текста
  - Оценка релевантности (scores)
  - Конфигурация поиска (search_type, top_k)
  - Параметры LLM (model, temperature, max_tokens)

**Структура трассировки:**
```
rag_pipeline (span)
├── retrieval (span)
│   └── metadata: search_type, top_k, duration, chunks
└── llm_generation (generation)
    └── metadata: model, tokens, duration
```

**Реализация:**
- Использование контекстных менеджеров для автоматического связывания наблюдений
- Обновление трассировки с финальными результатами
- Сохранение идентификаторов сессии и пользователя на уровне трассировки

### Задание 4: Выбрать датасет и загрузить его в Langfuse

**Выбранный датасет:**
- Источник: Вопросы из Лабораторной работы 2 (`questions.md`)
- Количество элементов: 20 пар вопрос-ответ
- Тип: Собственный набор вопросов-ответов по документации GitHub репозиториев
- Формат: Каждый элемент содержит:
  - `input`: Вопрос пользователя
  - `expected_output`: Релевантные чанки с метаданными (текст, источник, репозиторий)

**Реализованный функционал:**
- Скрипт `create_dataset.py` для создания датасета в Langfuse
- Автоматическое нахождение релевантных чанков для каждого вопроса:
  - Использование RetrievalEvaluator для поиска релевантных документов
  - Поддержка различных типов поиска (dense/sparse/hybrid)
  - Настраиваемое количество релевантных чанков (top_k)
- Создание датасета через Langfuse SDK:
  - Создание датасета с именем и описанием
  - Добавление элементов датасета (Dataset Items)
  - Сохранение метаданных (question_id, num_relevant_chunks)

**Результаты:**
- Датасет создан в Langfuse с именем "answers"
- Всего элементов: 20
- Каждый элемент содержит:
  - `input`: {"question": "текст вопроса"}
  - `expected_output`: {
      "relevant_chunks": [
        {
          "text": "текст чанка",
          "metadata": {...},
          "score": оценка_релевантности
        }
      ],
      "num_chunks": количество_чанков
    }
  - `metadata`: {"question_id": id, "num_relevant_chunks": количество}

**Использование:**
```bash
python create_dataset.py \
    --questions-file questions.md \
    --dataset-name answers \
    --search-type hybrid \
    --top-k 10
```

### Задание 5: Провести оценку и логирование метрик по датасету

**Реализованный функционал:**
- Скрипт `run_experiment.py` для запуска оценки через Langfuse Experiment Run
- Настройка Experiment Run через Langfuse SDK:
  - Загрузка датасета из Langfuse
  - Автоматическое выполнение оценки на всех элементах датасета
  - Создание эксперимента с метаданными конфигурации
- Реализация кастомного `run_evaluator` для RAG-пайплайна:
  - Принимает элемент датасета (`input`, `expected_output`)
  - Выполняет retrieval через RAG-пайплайн
  - Выполняет генерацию ответа LLM
  - Вычисляет retrieval-метрики:
    - **Recall@k**: Доля найденных релевантных чанков среди топ-k результатов
    - **Precision@k**: Точность среди топ-k результатов
    - **MRR**: Mean Reciprocal Rank (средний обратный ранг первого релевантного чанка)
  - Возвращает `list[Evaluation]` с рассчитанными метриками
- Сопоставление извлеченных чанков с эталонными:
  - Функция `find_chunk_index_by_content()` для поиска чанков по тексту и метаданным
  - Функция `get_relevant_indices_from_expected_output()` для извлечения индексов релевантных чанков
  - Поддержка различных значений k для метрик (по умолчанию [5, 10])
- Автоматическое логирование метрик в Langfuse:
  - Все метрики автоматически сохраняются в интерфейсе Langfuse
  - Доступны для анализа и сравнения конфигураций
  - Возможность сравнения метрик по разным экспериментам

**Реализация:**
- Функция `rag_task()`: Выполняет RAG-пайплайн на элементе датасета
- Функция `run_evaluator()`: Вычисляет метрики и возвращает объекты `Evaluation`
- Использование `dataset.run_experiment()` для запуска эксперимента на датасете
- Автоматическая отправка данных через `langfuse_client.flush()`

**Использование:**
```bash
python run_experiment.py \
    --dataset-name answers \
    --search-type hybrid \
    --top-k 5 \
    --k-values 5 10 \
    --device mps
```

**Результаты:**
- Эксперимент создан в Langfuse и связан с датасетом
- Все метрики логируются и доступны в интерфейсе
- Возможность сравнения разных конфигураций (разные search_type, top_k)
- Анализ качества работы RAG-системы через интерфейс Langfuse

## Использованные технологии и библиотеки

- **Python 3**
- **Docker Compose**: Для развертывания Langfuse локально
- **Langfuse**: Платформа для логирования и оценки LLM-приложений
- **langfuse SDK**: Python SDK для интеграции с Langfuse
- **PostgreSQL**: База данных для метаданных Langfuse
- **ClickHouse**: Хранилище для событий и метрик
- **Redis**: Очередь задач для обработки
- **MinIO**: Хранилище для медиафайлов
- **openai**: Для работы с LLM через Ollama API
- **sentence-transformers**: Для построения эмбеддингов (используется из lab2)
- **faiss**: Для векторного поиска (используется из lab2)

## Структура проекта

```
lab3/
├── source/
│   ├── docker-compose.yml        # Задание 1: Конфигурация Docker Compose для Langfuse
│   ├── rag_pipeline.py           # Задание 2-3: RAG-пайплайн с логированием
│   ├── create_dataset.py         # Задание 4: Создание датасета в Langfuse
│   ├── run_experiment.py         # Задание 5: Запуск оценки через Experiment Run
│   ├── questions.md              # Вопросы для датасета (из lab2)
│   ├── requirements.txt          # Зависимости проекта
│   ├── config/
│   │   └── config.json           # Конфигурация (из lab2)
│   ├── utils/                    # Общие утилиты (из lab2)
│   │   ├── __init__.py
│   │   ├── utils.py
│   │   ├── data_loader.py
│   │   └── metrics.py
│   ├── evaluate_retrieval.py     # Оценщик retrieval (из lab2)
│   ├── load_to_vector_store.py   # Загрузка в Faiss (из lab2)
│   └── evaluation_results.json   # Результаты оценки (из lab2)
├── task3.md                      # Задание лабораторной работы
└── README.md                     # Этот файл
```

## Результаты работы

### Развертывание Langfuse

- **Статус**: Успешно развернут локально через Docker Compose
- **Доступность**: Интерфейс доступен по адресу `http://localhost:3000`
- **Компоненты**:
  - Langfuse Web UI (порт 3000)
  - Langfuse Worker (порт 3030)
  - PostgreSQL (порт 5432)
  - ClickHouse (порт 8123)
  - Redis (порт 6379)
  - MinIO (порт 9090)

### Интеграция логирования

- **Логирование LLM-запросов**: Реализовано полное логирование всех запросов и ответов
- **Трассировка взаимодействий**: Все шаги RAG-пайплайна логируются с метаданными
- **Отслеживание сессий**: Реализована поддержка session_id и user_id
- **Метаданные**: Сохраняются все необходимые данные для анализа:
  - Параметры запросов
  - Результаты поиска
  - Метаданные чанков
  - Статистика токенов
  - Время выполнения

### Датасет в Langfuse

- **Название датасета**: "answers"
- **Количество элементов**: 20
- **Источник**: Вопросы из Лабораторной работы 2
- **Формат**: Каждый элемент содержит вопрос и релевантные чанки
- **Метод создания**: Автоматический поиск релевантных чанков через RetrievalEvaluator

### Оценка через Experiment Run

- **Реализованные метрики**:
  - Recall@5, Recall@10
  - Precision@5, Precision@10
  - MRR (Mean Reciprocal Rank)
- **Интеграция**: Все метрики автоматически логируются в Langfuse
- **Сравнение конфигураций**: Возможность сравнения разных параметров поиска
- **Анализ**: Результаты доступны в интерфейсе Langfuse для детального анализа

## Инструкция по запуску

### 1. Установка зависимостей

```bash
cd projects/ie-danilenko/lab3/source
pip install -r requirements.txt
```

### 2. Настройка переменных окружения

Создайте файл `.env` в директории `source/`:

```bash
# Langfuse credentials (получите после создания проекта в Langfuse)
LANGFUSE_PUBLIC_KEY=your_public_key
LANGFUSE_SECRET_KEY=your_secret_key
LANGFUSE_HOST=http://localhost:3000

# LLM API (Ollama)
LLM_API_BASE=http://localhost:11434/v1
LLM_API_KEY=ollama
```

### 3. Развертывание Langfuse (Задание 1)

```bash
cd source
docker compose up -d
```

После запуска:
1. Откройте браузер и перейдите на `http://localhost:3000`
2. Создайте аккаунт и проект в Langfuse
3. Получите `LANGFUSE_PUBLIC_KEY` и `LANGFUSE_SECRET_KEY` из настроек проекта
4. Добавьте их в файл `.env`

### 4. Интеграция логирования (Задание 2-3)

Запустите RAG-пайплайн с логированием:

```bash
python rag_pipeline.py \
    --faiss-index-dir faiss_index \
    --chunks-dir chunks \
    --search-type hybrid \
    --top-k 5
```

Все взаимодействия будут автоматически логироваться в Langfuse. Проверьте трассировки в интерфейсе.

### 5. Создание датасета (Задание 4)

```bash
python create_dataset.py \
    --questions-file questions.md \
    --dataset-name answers \
    --faiss-index-dir faiss_index \
    --chunks-dir chunks \
    --search-type hybrid \
    --top-k 10
```

Скрипт:
- Загружает вопросы из `questions.md`
- Находит релевантные чанки для каждого вопроса
- Создает датасет в Langfuse с именем "answers"
- Добавляет все элементы с релевантными чанками

### 6. Запуск оценки (Задание 5)

```bash
python run_experiment.py \
    --dataset-name answers \
    --faiss-index-dir faiss_index \
    --chunks-dir chunks \
    --search-type hybrid \
    --top-k 5 \
    --k-values 5 10 \
    --device mps
```

Опции:
- `--dataset-name`: Название датасета в Langfuse (по умолчанию "answers")
- `--search-type`: Тип поиска (dense/sparse/hybrid, по умолчанию hybrid)
- `--top-k`: Количество релевантных чанков для контекста (по умолчанию 5)
- `--k-values`: Значения k для метрик (по умолчанию [5, 10])
- `--device`: Устройство для вычислений (mps/cuda/cpu)
- `--experiment-name`: Название эксперимента (опционально)
- `--description`: Описание эксперимента (опционально)

Скрипт:
- Загружает датасет из Langfuse
- Выполняет RAG-пайплайн на каждом элементе
- Вычисляет метрики Recall@k, Precision@k, MRR
- Сохраняет результаты в Langfuse через Experiment Run

### 7. Анализ результатов

1. Откройте интерфейс Langfuse: `http://localhost:3000`
2. Перейдите в раздел "Datasets" → "answers"
3. Откройте вкладку "Runs" для просмотра экспериментов
4. Анализируйте метрики и сравнивайте разные конфигурации

## Выводы

1. **Успешно выполнено задание 1**: Langfuse развернут локально через Docker Compose, настроен проект и получены credentials для подключения.

2. **Успешно выполнено задание 2**: Интегрировано логирование запросов LLM в RAG-пайплайн с сохранением всех входных параметров, ответов модели и метаданных (токены, длительность).

3. **Успешно выполнено задание 3**: Реализовано полное логирование взаимодействий с пользователями через трассировку Langfuse, включая промежуточные шаги (retrieval, generation) и все необходимые метаданные для анализа.

4. **Успешно выполнено задание 4**: Создан датасет "answers" в Langfuse из 20 вопросов с автоматическим поиском релевантных чанков для каждого вопроса.

5. **Успешно выполнено задание 5**: Реализована оценка RAG-пайплайна через Langfuse Experiment Run с вычислением метрик Recall@k, Precision@k, MRR и автоматическим логированием результатов в интерфейсе Langfuse для анализа и сравнения конфигураций.
