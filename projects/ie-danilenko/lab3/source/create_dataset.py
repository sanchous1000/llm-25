"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ Langfuse –∏–∑ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞–Ω–∏–µ 4: –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ Langfuse.
"""

import os
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
import argparse
from dotenv import load_dotenv

from langfuse import Langfuse
from evaluate_retrieval import RetrievalEvaluator
from utils import load_questions, get_device

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()


def create_langfuse_client() -> Optional[Langfuse]:
    """
    –°–æ–∑–¥–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç Langfuse –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è.
    
    Returns:
        –ö–ª–∏–µ–Ω—Ç Langfuse –∏–ª–∏ None, –µ—Å–ª–∏ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω
    """
    public_key = os.getenv("LANGFUSE_PUBLIC_KEY")
    secret_key = os.getenv("LANGFUSE_SECRET_KEY")
    host = os.getenv("LANGFUSE_HOST", "http://localhost:3000")
    
    if not public_key or not secret_key:
        print("‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: LANGFUSE_PUBLIC_KEY –∏ LANGFUSE_SECRET_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
        return None
    
    try:
        return Langfuse(
            public_key=public_key,
            secret_key=secret_key,
            host=host
        )
    except Exception as e:
        print(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∫–ª–∏–µ–Ω—Ç Langfuse: {e}")
        return None


def find_relevant_chunks(
    evaluator: RetrievalEvaluator,
    question: str,
    top_k: int = 10,
    search_type: str = "hybrid"
) -> List[Dict[str, Any]]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞.
    
    Args:
        evaluator: –û—Ü–µ–Ω—â–∏–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞
        question: –í–æ–ø—Ä–æ—Å
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        search_type: –¢–∏–ø –ø–æ–∏—Å–∫–∞ (dense, sparse, hybrid)
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    """
    try:
        if search_type == "dense":
            results = evaluator.search_dense(question, k=top_k)
        elif search_type == "sparse":
            results = evaluator.search_sparse(question, k=top_k)
        elif search_type == "hybrid":
            results = evaluator.search_hybrid(question, k=top_k)
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –ø–æ–∏—Å–∫–∞: {search_type}")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        relevant_chunks = []
        for result in results:
            chunk = result.get("chunk", {})
            if chunk:
                relevant_chunks.append({
                    "text": chunk.get("text", ""),
                    "metadata": chunk.get("metadata", {}),
                    "index": result.get("index"),
                    "score": result.get("distance", 0.0)
                })
        
        return relevant_chunks
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ '{question[:50]}...': {e}")
        return []


def create_dataset_in_langfuse(
    langfuse_client: Langfuse,
    dataset_name: str,
    questions: List[Dict[str, Any]],
    relevant_chunks_map: Dict[int, List[Dict[str, Any]]],
    description: Optional[str] = None
) -> None:
    """
    –°–æ–∑–¥–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –≤ Langfuse.
    
    Args:
        langfuse_client: –ö–ª–∏–µ–Ω—Ç Langfuse
        dataset_name: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
        questions: –°–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤
        relevant_chunks_map: –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ —á–∞–Ω–∫–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ (–ø–æ ID)
        description: –û–ø–∏—Å–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
    """
    try:
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        print(f"\nüì¶ –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ '{dataset_name}' –≤ Langfuse...")
        dataset = langfuse_client.create_dataset(
            name=dataset_name,
            description=description or f"–î–∞—Ç–∞—Å–µ—Ç –∏–∑ {len(questions)} –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ RAG-—Å–∏—Å—Ç–µ–º—ã"
        )
        print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç '{dataset_name}' —Å–æ–∑–¥–∞–Ω")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –¥–∞—Ç–∞—Å–µ—Ç–∞
        print(f"\nüìù –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç...")
        for question_data in questions:
            question_id = question_data["id"]
            question_text = question_data["question"]
            relevant_chunks = relevant_chunks_map.get(question_id, [])
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º expected_output —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ —á–∞–Ω–∫–∞–º–∏
            expected_output = {
                "relevant_chunks": [
                    {
                        "text": chunk["text"],
                        "metadata": chunk["metadata"],
                        "score": chunk.get("score", 0.0)
                    }
                    for chunk in relevant_chunks
                ],
                "num_chunks": len(relevant_chunks)
            }
            
            # –°–æ–∑–¥–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞
            try:
                langfuse_client.create_dataset_item(
                    dataset_name=dataset_name,
                    input={"question": question_text},
                    expected_output=expected_output,
                    metadata={
                        "question_id": question_id,
                        "num_relevant_chunks": len(relevant_chunks)
                    }
                )
                print(f"  ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –≤–æ–ø—Ä–æ—Å {question_id}: {question_text[:60]}...")
            except Exception as e:
                print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –≤–æ–ø—Ä–æ—Å–∞ {question_id}: {e}")
        
        print(f"\n‚úÖ –î–∞—Ç–∞—Å–µ—Ç '{dataset_name}' —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω —Å {len(questions)} —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
        raise


def main():
    parser = argparse.ArgumentParser(description='–°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ Langfuse –∏–∑ –≤–æ–ø—Ä–æ—Å–æ–≤')
    parser.add_argument('--questions-file', type=str, default='questions.md',
                       help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: questions.md)')
    parser.add_argument('--faiss-index-dir', type=str, default='faiss_index',
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏–Ω–¥–µ–∫—Å–æ–º Faiss')
    parser.add_argument('--chunks-dir', type=str, default='chunks',
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —á–∞–Ω–∫–∞–º–∏')
    parser.add_argument('--dense-model', type=str, default='intfloat/multilingual-e5-large',
                       help='–ú–æ–¥–µ–ª—å –¥–ª—è dense —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤')
    parser.add_argument('--dataset-name', type=str, default='answers',
                       help='–ù–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ Langfuse')
    parser.add_argument('--search-type', type=str, choices=['dense', 'sparse', 'hybrid'],
                       default='hybrid', help='–¢–∏–ø –ø–æ–∏—Å–∫–∞ –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤')
    parser.add_argument('--top-k', type=int, default=10,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞')
    parser.add_argument('--device', type=str, default=None,
                       help='–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (mps/cuda/cpu)')
    parser.add_argument('--description', type=str, default=None,
                       help='–û–ø–∏—Å–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("–°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ Langfuse")
    print("=" * 60)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª–∏–µ–Ω—Ç–∞ Langfuse
    langfuse_client = create_langfuse_client()
    if not langfuse_client:
        print("‚ùå –û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∫–ª–∏–µ–Ω—Ç Langfuse")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã
    print(f"\nüìñ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ {args.questions_file}...")
    try:
        questions = load_questions(args.questions_file)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {len(questions)}")
    except FileNotFoundError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Ü–µ–Ω—â–∏–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
    print(f"\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω—â–∏–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤...")
    device = get_device(args.device) if args.device else None
    evaluator = RetrievalEvaluator(
        faiss_index_dir=args.faiss_index_dir,
        chunks_dir=args.chunks_dir,
        dense_model_name=args.dense_model,
        device=device
    )
    print("‚úÖ –û—Ü–µ–Ω—â–∏–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    # –ù–∞—Ö–æ–¥–∏–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
    print(f"\nüîç –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞...")
    print(f"   –¢–∏–ø –ø–æ–∏—Å–∫–∞: {args.search_type}, top_k: {args.top_k}")
    relevant_chunks_map = {}
    
    for i, question_data in enumerate(questions, 1):
        question_text = question_data["question"]
        print(f"\n  [{i}/{len(questions)}] –í–æ–ø—Ä–æ—Å: {question_text[:60]}...")
        
        relevant_chunks = find_relevant_chunks(
            evaluator,
            question_text,
            top_k=args.top_k,
            search_type=args.search_type
        )
        
        relevant_chunks_map[question_data["id"]] = relevant_chunks
        print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤: {len(relevant_chunks)}")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –≤ Langfuse
    create_dataset_in_langfuse(
        langfuse_client,
        args.dataset_name,
        questions,
        relevant_chunks_map,
        description=args.description
    )
    
    print("\n" + "=" * 60)
    print("‚úÖ –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üìä –î–∞—Ç–∞—Å–µ—Ç '{args.dataset_name}' –¥–æ—Å—Ç—É–ø–µ–Ω –≤ Langfuse")
    print("=" * 60)


if __name__ == "__main__":
    main()
