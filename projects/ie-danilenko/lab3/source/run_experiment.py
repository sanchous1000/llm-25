"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ—Ü–µ–Ω–∫–∏ RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞ —á–µ—Ä–µ–∑ Langfuse Experiment Run.
–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞–Ω–∏–µ 5: –æ—Ü–µ–Ω–∫–∞ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É.
"""

from typing import List, Dict, Any, Optional
import argparse
from dotenv import load_dotenv

from langfuse import Evaluation

from rag_pipeline import RAGPipeline, create_llm_client, create_langfuse_client
from utils import get_device, load_chunks
from utils.metrics import calculate_recall_at_k, calculate_precision_at_k, calculate_mrr

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()


def find_chunk_index_by_content(
    chunks: List[Dict[str, Any]],
    target_text: str,
    target_metadata: Dict[str, Any]
) -> Optional[int]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç –∏–Ω–¥–µ–∫—Å —á–∞–Ω–∫–∞ –≤ –∫–æ—Ä–ø—É—Å–µ –ø–æ —Ç–µ–∫—Å—Ç—É –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º.
    
    Args:
        chunks: –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤
        target_text: –¢–µ–∫—Å—Ç —Ü–µ–ª–µ–≤–æ–≥–æ —á–∞–Ω–∫–∞
        target_metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ü–µ–ª–µ–≤–æ–≥–æ —á–∞–Ω–∫–∞
    
    Returns:
        –ò–Ω–¥–µ–∫—Å —á–∞–Ω–∫–∞ –∏–ª–∏ None, –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω
    """
    if not target_text:
        return None
    
    target_text_normalized = target_text.strip().lower()
    
    # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
    for idx, chunk in enumerate(chunks):
        chunk_text = chunk.get("text", "").strip().lower()
        if chunk_text == target_text_normalized:
            # –ï—Å–ª–∏ –µ—Å—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏—Ö –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            if target_metadata:
                chunk_metadata = chunk.get("metadata", {})
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                metadata_matches = []
                
                if target_metadata.get("source_file"):
                    metadata_matches.append(
                        target_metadata.get("source_file") == chunk_metadata.get("source_file")
                    )
                
                if target_metadata.get("repository"):
                    metadata_matches.append(
                        target_metadata.get("repository") == chunk_metadata.get("repository")
                    )
                
                # –ï—Å–ª–∏ –µ—Å—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏, –≤—Å–µ –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å
                if metadata_matches and all(metadata_matches):
                    return idx
                elif not metadata_matches:
                    # –ï—Å–ª–∏ –Ω–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ —Ç–µ–∫—Å—Ç—É
                    return idx
            else:
                return idx
    
    # –ï—Å–ª–∏ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—â–µ–º –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º –∏ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é —Ç–µ–∫—Å—Ç–∞
    if target_metadata:
        best_match_idx = None
        best_match_score = 0
        
        for idx, chunk in enumerate(chunks):
            chunk_metadata = chunk.get("metadata", {})
            chunk_text = chunk.get("text", "").strip().lower()
            
            # –í—ã—á–∏—Å–ª—è–µ–º score —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            score = 0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            if target_metadata.get("source_file") == chunk_metadata.get("source_file"):
                score += 3
            if target_metadata.get("repository") == chunk_metadata.get("repository"):
                score += 2
            if target_metadata.get("Header 1") == chunk_metadata.get("Header 1"):
                score += 1
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
            if target_text_normalized in chunk_text or chunk_text in target_text_normalized:
                # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                shorter_len = min(len(target_text_normalized), len(chunk_text))
                longer_len = max(len(target_text_normalized), len(chunk_text))
                if shorter_len > 0:
                    overlap_ratio = shorter_len / longer_len
                    score += overlap_ratio * 2
            
            if score > best_match_score and score >= 3:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
                best_match_score = score
                best_match_idx = idx
        
        return best_match_idx
    
    return None


def get_relevant_indices_from_expected_output(
    expected_output: Dict[str, Any],
    all_chunks: List[Dict[str, Any]]
) -> List[int]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω–¥–µ–∫—Å—ã —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –∏–∑ expected_output.
    
    Args:
        expected_output: –û–∂–∏–¥–∞–µ–º—ã–π –≤—ã–≤–æ–¥ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
        all_chunks: –í—Å–µ —á–∞–Ω–∫–∏ –∏–∑ –∫–æ—Ä–ø—É—Å–∞
    
    Returns:
        –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
    """
    relevant_chunks = expected_output.get("relevant_chunks", [])
    relevant_indices = []
    
    for relevant_chunk in relevant_chunks:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∏–Ω–¥–µ–∫—Å –Ω–∞–ø—Ä—è–º—É—é
        if "index" in relevant_chunk:
            idx = relevant_chunk["index"]
            if isinstance(idx, int) and 0 <= idx < len(all_chunks):
                relevant_indices.append(idx)
                continue
        
        # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å–∞ –Ω–µ—Ç, –∏—â–µ–º –ø–æ —Ç–µ–∫—Å—Ç—É –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
        chunk_text = relevant_chunk.get("text", "")
        chunk_metadata = relevant_chunk.get("metadata", {})
        
        idx = find_chunk_index_by_content(all_chunks, chunk_text, chunk_metadata)
        if idx is not None:
            relevant_indices.append(idx)
    
    return relevant_indices


def rag_task(*, item, rag_pipeline: RAGPipeline, **kwargs) -> Dict[str, Any]:
    """
    –ó–∞–¥–∞—á–∞ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞ –Ω–∞ —ç–ª–µ–º–µ–Ω—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç–∞.
    
    Args:
        item: –≠–ª–µ–º–µ–Ω—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞ (DatasetItemClient) —Å input –∏ expected_output
        rag_pipeline: RAG-–ø–∞–π–ø–ª–∞–π–Ω
        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏: answer, retrieved_chunks, retrieved_indices
    """
    question = item.input["question"]
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º RAG-–ø–∞–π–ø–ª–∞–π–Ω
    result = rag_pipeline.generate_answer(question, session_id=None)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
    retrieved_chunks = result.get("context", [])
    retrieved_indices = [chunk_result.get("index") for chunk_result in retrieved_chunks if chunk_result.get("index") is not None]
    
    return {
        "answer": result.get("answer", ""),
        "raw_answer": result.get("raw_answer", ""),
        "retrieved_chunks": retrieved_chunks,
        "retrieved_indices": retrieved_indices,
        "sources": result.get("sources", [])
    }


def run_evaluator(
    *,
    input: Dict[str, Any],
    output: Dict[str, Any],
    expected_output: Optional[Dict[str, Any]] = None,
    all_chunks: List[Dict[str, Any]] = None,
    k_values: List[int] = [5, 10],
    **kwargs
) -> List[Dict[str, Any]]:
    """
    –û—Ü–µ–Ω—â–∏–∫ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è retrieval-–º–µ—Ç—Ä–∏–∫.
    
    Args:
        input: –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–≤–æ–ø—Ä–æ—Å)
        output: –í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ä–µ–∑—É–ª—å—Ç–∞—Ç RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞)
        expected_output: –û–∂–∏–¥–∞–µ–º—ã–π –≤—ã–≤–æ–¥ (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏)
        all_chunks: –í—Å–µ —á–∞–Ω–∫–∏ –∏–∑ –∫–æ—Ä–ø—É—Å–∞
        k_values: –ó–Ω–∞—á–µ–Ω–∏—è k –¥–ª—è –º–µ—Ç—Ä–∏–∫
        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    
    Returns:
        –°–ø–∏—Å–æ–∫ Evaluation –æ–±—ä–µ–∫—Ç–æ–≤ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
    retrieved_indices = output.get("retrieved_indices", [])
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –∏–∑ expected_output
    relevant_indices = get_relevant_indices_from_expected_output(expected_output, all_chunks)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    evaluations = []
    
    for k in k_values:
        recall = calculate_recall_at_k(retrieved_indices, relevant_indices, k)
        precision = calculate_precision_at_k(retrieved_indices, relevant_indices, k)
        
        evaluations.append(Evaluation(
            name=f"Recall@{k}",
            value=recall,
            comment=f"Recall@{k} = {recall:.4f}"
        ))
        
        evaluations.append(Evaluation(
            name=f"Precision@{k}",
            value=precision,
            comment=f"Precision@{k} = {precision:.4f}"
        ))
    
    mrr = calculate_mrr(retrieved_indices, relevant_indices)
    evaluations.append(Evaluation(
        name="MRR",
        value=mrr,
        comment=f"MRR = {mrr:.4f}"
    ))
    
    return evaluations


def main():
    parser = argparse.ArgumentParser(description='–ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞ —á–µ—Ä–µ–∑ Langfuse Experiment Run')
    parser.add_argument('--dataset-name', type=str, default="answers",
                       help='–ù–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ Langfuse')
    parser.add_argument('--faiss-index-dir', type=str, default='faiss_index',
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏–Ω–¥–µ–∫—Å–æ–º Faiss')
    parser.add_argument('--chunks-dir', type=str, default='chunks',
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —á–∞–Ω–∫–∞–º–∏')
    parser.add_argument('--dense-model', type=str, default='intfloat/multilingual-e5-large',
                       help='–ú–æ–¥–µ–ª—å –¥–ª—è dense —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤')
    parser.add_argument('--llm-model', type=str, default='qwen3:latest',
                       help='–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ LLM –∏–∑ Ollama')
    parser.add_argument('--search-type', type=str, choices=['dense', 'sparse', 'hybrid'],
                       default='hybrid', help='–¢–∏–ø –ø–æ–∏—Å–∫–∞')
    parser.add_argument('--top-k', type=int, default=5,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞')
    parser.add_argument('--device', type=str, default=None,
                       help='–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (mps/cuda/cpu)')
    parser.add_argument('--temperature', type=float, default=0.7,
                       help='–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏')
    parser.add_argument('--max-tokens', type=int, default=1000,
                       help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ')
    parser.add_argument('--k-values', type=int, nargs='+', default=[5, 10],
                       help='–ó–Ω–∞—á–µ–Ω–∏—è k –¥–ª—è –º–µ—Ç—Ä–∏–∫ Recall@k –∏ Precision@k')
    parser.add_argument('--experiment-name', type=str, default=None,
                       help='–ù–∞–∑–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: dataset_name + –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è)')
    parser.add_argument('--description', type=str, default=None,
                       help='–û–ø–∏—Å–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("–ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞ —á–µ—Ä–µ–∑ Langfuse Experiment Run")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç Langfuse
    print(f"\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Langfuse –∫–ª–∏–µ–Ω—Ç–∞...")
    langfuse_client = create_langfuse_client()
    print("‚úÖ Langfuse –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ Langfuse
    print(f"\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ '{args.dataset_name}' –∏–∑ Langfuse...")
    dataset = langfuse_client.get_dataset(args.dataset_name)
    dataset_items = list(dataset.items)
    print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {len(dataset_items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
    print(f"\nüìö –ó–∞–≥—Ä—É–∑–∫–∞ —á–∞–Ω–∫–æ–≤ –∏–∑ {args.chunks_dir}...")
    all_chunks = load_chunks(args.chunks_dir)
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(all_chunks)}")
    
    # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç LLM
    print(f"\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞...")
    llm_client = create_llm_client()
    print("‚úÖ LLM –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG-–ø–∞–π–ø–ª–∞–π–Ω
    print(f"\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞...")
    device = get_device(args.device) if args.device else None
    rag_pipeline = RAGPipeline(
        faiss_index_dir=args.faiss_index_dir,
        chunks_dir=args.chunks_dir,
        dense_model_name=args.dense_model,
        llm_client=llm_client,
        llm_model=args.llm_model,
        search_type=args.search_type,
        top_k=args.top_k,
        device=device,
        langfuse_client=langfuse_client  # –ü–µ—Ä–µ–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    )
    print("‚úÖ RAG-–ø–∞–π–ø–ª–∞–π–Ω –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    if not args.experiment_name:
        experiment_name = f"{args.dataset_name}_eval_{args.search_type}_top{args.top_k}"
    else:
        experiment_name = args.experiment_name
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
    if not args.description:
        description = f"–û—Ü–µ–Ω–∫–∞ RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ '{args.dataset_name}'. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: search_type={args.search_type}, top_k={args.top_k}, llm_model={args.llm_model}"
    else:
        description = args.description
    
    # –°–æ–∑–¥–∞–µ–º –æ–±—ë—Ä—Ç–∫—É –¥–ª—è task —Ñ—É–Ω–∫—Ü–∏–∏
    def task_wrapper(*, item, **kwargs):
        return rag_task(item=item, rag_pipeline=rag_pipeline, **kwargs)
    
    # –°–æ–∑–¥–∞–µ–º –æ–±—ë—Ä—Ç–∫—É –¥–ª—è evaluator —Ñ—É–Ω–∫—Ü–∏–∏
    def evaluator_wrapper(*, input, output, expected_output, **kwargs):
        return run_evaluator(
            input=input,
            output=output,
            expected_output=expected_output,
            all_chunks=all_chunks,
            k_values=args.k_values,
            **kwargs
        )
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ
    print(f"\nüöÄ –ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ '{experiment_name}'...")
    print(f"   –û–ø–∏—Å–∞–Ω–∏–µ: {description}")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(dataset_items)}")
    print(f"   –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: search_type={args.search_type}, top_k={args.top_k}, k_values={args.k_values}")
    
    result = dataset.run_experiment(
        name=experiment_name,
        description=description,
        task=task_wrapper,
        evaluators=[evaluator_wrapper],
        metadata={
            "search_type": args.search_type,
            "top_k": args.top_k,
            "llm_model": args.llm_model,
            "dense_model": args.dense_model,
            "k_values": args.k_values,
            "temperature": args.temperature,
            "max_tokens": args.max_tokens
        }
    )
    
    print(f"\n‚úÖ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print(result.format())
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Å–µ—Ä–≤–µ—Ä Langfuse
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ Langfuse...")
    langfuse_client.flush()
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    
    print(f"\n{'='*60}")
    print("‚úÖ –û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ—Å—Ç—É–ø–Ω—ã –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ Langfuse")
    print("=" * 60)


if __name__ == "__main__":
    main()
