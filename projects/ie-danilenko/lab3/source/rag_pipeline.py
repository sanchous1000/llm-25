"""
RAG-–ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ LLM.
–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞–Ω–∏–µ 6: —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–≤–∏–∂–∫–∞ –æ–±—â–µ–Ω–∏—è (RAG-–ø–∞–π–ø–ª–∞–π–Ω).
"""

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from typing import List, Dict, Any, Optional
import argparse
from openai import OpenAI
import time
import uuid
from dotenv import load_dotenv

from langfuse import Langfuse

from evaluate_retrieval import RetrievalEvaluator
from utils import get_device

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()


class RAGPipeline:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞."""
    
    def __init__(self,
                 faiss_index_dir: str,
                 chunks_dir: str,
                 dense_model_name: str,
                 llm_client: OpenAI,
                 llm_model: str = "qwen2.5:7b",
                 search_type: str = "hybrid",
                 top_k: int = 5,
                 device: Optional[str] = None,
                 langfuse_client: Optional[Langfuse] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞.
        
        Args:
            faiss_index_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏–Ω–¥–µ–∫—Å–æ–º Faiss
            chunks_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —á–∞–Ω–∫–∞–º–∏
            dense_model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è dense —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            llm_client: –ö–ª–∏–µ–Ω—Ç OpenAI –¥–ª—è –≤—ã–∑–æ–≤–∞ LLM
            llm_model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ LLM
            search_type: –¢–∏–ø –ø–æ–∏—Å–∫–∞ (dense, sparse, hybrid)
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
            langfuse_client: –ö–ª–∏–µ–Ω—Ç Langfuse –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        self.llm_client = llm_client
        self.llm_model = llm_model
        self.search_type = search_type
        self.top_k = top_k
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Langfuse –∫–ª–∏–µ–Ω—Ç
        self.langfuse = langfuse_client
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Ü–µ–Ω—â–∏–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞
        self.evaluator = RetrievalEvaluator(
            faiss_index_dir,
            chunks_dir,
            dense_model_name,
            device=device
        )
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        self.system_prompt = """–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.
–ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞.
–ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º.
–í—Å–µ–≥–¥–∞ —É–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Å–≤–æ–µ–º –æ—Ç–≤–µ—Ç–µ, —Å—Å—ã–ª–∞—è—Å—å –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.
–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, —á–µ—Ç–∫–æ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ."""
    
    def retrieve_context(self, query: str) -> List[Dict[str, Any]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞.
        
        Args:
            query: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        start_time = time.time()
        
        try:
            if self.search_type == "dense":
                results = self.evaluator.search_dense(query, k=self.top_k)
            elif self.search_type == "sparse":
                results = self.evaluator.search_sparse(query, k=self.top_k)
            elif self.search_type == "hybrid":
                results = self.evaluator.search_hybrid(query, k=self.top_k)
            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –ø–æ–∏—Å–∫–∞: {self.search_type}")
            
            duration = time.time() - start_time
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            chunks_metadata = []
            for result in results:
                chunk = result.get("chunk", {})
                metadata = chunk.get("metadata", {})
                chunks_metadata.append({
                    "source_file": metadata.get("source_file", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"),
                    "repository": metadata.get("repository", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"),
                    "repository_url": metadata.get("repository_url", ""),
                    "header": metadata.get("Header 1", ""),
                    "score": result.get("score", 0.0),
                    "text_preview": chunk.get("text", "")[:200] + "..." if len(chunk.get("text", "")) > 200 else chunk.get("text", "")
                })
            
            return results, duration, chunks_metadata
        except Exception as e:
            raise
    
    def format_context(self, chunks: List[Dict[str, Any]]) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —á–∞–Ω–∫–∏ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞.
        
        Args:
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        """
        context_parts = []
        
        for i, result in enumerate(chunks, start=1):
            chunk = result.get("chunk", {})
            text = chunk.get("text", "")
            metadata = chunk.get("metadata", {})
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            source_file = metadata.get("source_file", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫")
            repository = metadata.get("repository", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π")
            repository_url = metadata.get("repository_url", "")
            header = metadata.get("Header 1", "")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context_header = f"[–ö–æ–Ω—Ç–µ–∫—Å—Ç {i}]"
            if header:
                context_header += f" –†–∞–∑–¥–µ–ª: {header}"
            if repository:
                context_header += f" | –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {repository}"
            if repository_url:
                context_header += f" ({repository_url})"
            
            context_parts.append(f"{context_header}\n{text}\n")
        
        return "\n---\n\n".join(context_parts)
    
    def build_prompt(self, question: str, context: str) -> List[Dict[str, str]]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM.
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è OpenAI API
        """
        user_prompt = f"""–í–æ–ø—Ä–æ—Å: {question}

–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏:
{context}

–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –£–∫–∞–∂–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."""
        
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        return messages
    
    def generate_answer(self, question: str, temperature: float = 0.7, max_tokens: int = 1000, session_id: Optional[str] = None) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG.
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
            session_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–µ—Å—Å–∏–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π (–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –æ—Ç–≤–µ—Ç–æ–º, –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π user_id –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
        user_id = str(uuid.uuid4())
        
        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫—É –¥–ª—è –≤—Å–µ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
        if self.langfuse:
            with self.langfuse.start_as_current_observation(
                as_type="span",
                name="rag_pipeline",
                input={"question": question},
                metadata={
                    "search_type": self.search_type,
                    "top_k": self.top_k,
                    "llm_model": self.llm_model,
                    "temperature": temperature,
                    "max_tokens": max_tokens
                }
            ) as trace_context:
                trace_params = {}
                if session_id:
                    trace_params["session_id"] = session_id
                trace_params["user_id"] = user_id
                self.langfuse.update_current_trace(**trace_params)
                return self._generate_answer_internal(question, temperature, max_tokens, trace_context)
        else:
            return self._generate_answer_internal(question, temperature, max_tokens, None)
    
    def _generate_answer_internal(self, question: str, temperature: float, max_tokens: int, trace_context) -> Dict[str, Any]:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏."""
        print(f"üîç –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        
        # –õ–æ–≥–∏—Ä—É–µ–º —à–∞–≥ retrieval
        if self.langfuse:
            with self.langfuse.start_as_current_observation(
                as_type="span",
                name="retrieval",
                input={"query": question, "search_type": self.search_type, "top_k": self.top_k}
            ) as retrieval_span:
                retrieved_chunks, retrieval_duration, chunks_metadata = self.retrieve_context(question)
                
                retrieval_span.update(
                    output={
                        "num_chunks": len(retrieved_chunks),
                        "chunks": chunks_metadata
                    },
                    metadata={
                        "search_type": self.search_type,
                        "top_k": self.top_k,
                        "duration_seconds": retrieval_duration
                    }
                )
        else:
            retrieved_chunks, retrieval_duration, chunks_metadata = self.retrieve_context(question)
        
        if not retrieved_chunks:
            result = {
                "answer": "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –≤–∞—à–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.",
                "context": [],
                "sources": []
            }
            if trace_context:
                trace_context.update(output=result, level="WARNING")
            return result
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(retrieved_chunks)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
        
        context_text = self.format_context(retrieved_chunks)
        
        messages = self.build_prompt(question, context_text)
        
        print(f"ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é Ollama ({self.llm_model})...")
        
        # –õ–æ–≥–∏—Ä—É–µ–º LLM –∑–∞–ø—Ä–æ—Å
        start_time = time.time()
        try:
            if self.langfuse:
                with self.langfuse.start_as_current_observation(
                    as_type="generation",
                    name="llm_generation",
                    model=self.llm_model,
                    model_parameters={
                        "temperature": temperature,
                        "max_tokens": max_tokens,
                        "repeat_penalty": 1.1
                    },
                    input=messages
                ) as llm_generation:
                    response = self.llm_client.chat.completions.create(
                        model=self.llm_model,
                        messages=messages,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        extra_body={"repeat_penalty": 1.1}
                    )
                    
                    duration = time.time() - start_time
                    answer = response.choices[0].message.content
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—Ç–≤–µ—Ç–∞
                    usage = response.usage
                    token_metadata = {}
                    if usage:
                        token_metadata = {
                            "prompt_tokens": usage.prompt_tokens,
                            "completion_tokens": usage.completion_tokens,
                            "total_tokens": usage.total_tokens
                        }
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º generation —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
                    llm_generation.update(
                        output=answer,
                        usage_details=token_metadata,
                        metadata={
                            "duration_seconds": duration
                        }
                    )
            else:
                response = self.llm_client.chat.completions.create(
                    model=self.llm_model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    extra_body={"repeat_penalty": 1.1}
                )
                
                duration = time.time() - start_time
                answer = response.choices[0].message.content
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}"
            result = {
                "answer": error_msg,
                "context": retrieved_chunks,
                "sources": self._extract_sources(retrieved_chunks)
            }
            if trace_context:
                trace_context.update(output=result, level="ERROR", status_message=error_msg)
            return result
        
        formatted_answer = self._format_answer_with_citations(answer, retrieved_chunks)
        
        result = {
            "answer": formatted_answer,
            "raw_answer": answer,
            "context": retrieved_chunks,
            "sources": self._extract_sources(retrieved_chunks)
        }
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫—É —Å —Ñ–∏–Ω–∞–ª—å–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
        if trace_context:
            trace_context.update(
                output={
                    "answer": formatted_answer,
                    "num_sources": len(retrieved_chunks),
                    "sources": self._extract_sources(retrieved_chunks)
                }
            )
        
        return result
    
    def _extract_sources(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ —á–∞–Ω–∫–æ–≤."""
        sources = []
        for chunk_result in chunks:
            chunk = chunk_result.get("chunk", {})
            metadata = chunk.get("metadata", {})
            
            source = {
                "repository": metadata.get("repository", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"),
                "repository_url": metadata.get("repository_url", ""),
                "source_file": metadata.get("source_file", ""),
                "header": metadata.get("Header 1", ""),
                "snippet": chunk.get("text", "")[:200] + "..." if len(chunk.get("text", "")) > 200 else chunk.get("text", "")
            }
            sources.append(source)
        
        return sources
    
    def _format_answer_with_citations(self, answer: str, chunks: List[Dict[str, Any]]) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å —Ü–∏—Ç–∞—Ç–∞–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.
        
        Args:
            answer: –û—Ç–≤–µ—Ç –æ—Ç LLM
            chunks: –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç —Å —Ü–∏—Ç–∞—Ç–∞–º–∏
        """
        sources_section = "\n\n---\n\n**–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**\n\n"
        
        for i, chunk_result in enumerate(chunks, start=1):
            chunk = chunk_result.get("chunk", {})
            metadata = chunk.get("metadata", {})
            
            repository = metadata.get("repository", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π")
            repository_url = metadata.get("repository_url", "")
            header = metadata.get("Header 1", "")
            
            source_text = f"{i}. "
            if repository_url:
                source_text += f"[{repository}]({repository_url})"
            else:
                source_text += repository
            
            if header:
                source_text += f" ‚Äî {header}"
            
            sources_section += source_text + "\n"
        
        return answer + sources_section


def create_llm_client() -> OpenAI:
    """
    –°–æ–∑–¥–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç –¥–ª—è Ollama API.
    –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Ollama (–∫–∞–∫ –≤ lab1).
    
    Returns:
        –ö–ª–∏–µ–Ω—Ç OpenAI –¥–ª—è Ollama
    """
    # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º Ollama (–∫–∞–∫ –≤ lab1)
    api_base = os.getenv("LLM_API_BASE", "http://localhost:11434/v1")
    api_key = os.getenv("LLM_API_KEY", "ollama")
    
    client = OpenAI(
        base_url=api_base,
        api_key=api_key
    )
    
    return client


def create_langfuse_client() -> Optional[Langfuse]:
    """
    –°–æ–∑–¥–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç Langfuse –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è.
    
    Returns:
        –ö–ª–∏–µ–Ω—Ç Langfuse –∏–ª–∏ None, –µ—Å–ª–∏ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω
    """
    public_key = os.getenv("LANGFUSE_PUBLIC_KEY")
    secret_key = os.getenv("LANGFUSE_SECRET_KEY")
    host = os.getenv("LANGFUSE_HOST", "http://localhost:3000")
    
    if not public_key or not secret_key:
        return None
    
    try:
        return Langfuse(
            public_key=public_key,
            secret_key=secret_key,
            host=host
        )
    except Exception as e:
        print(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∫–ª–∏–µ–Ω—Ç Langfuse: {e}")
        return None


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='RAG-–ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã')
    parser.add_argument('--faiss-index-dir', type=str, default='faiss_index',
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏–Ω–¥–µ–∫—Å–æ–º Faiss')
    parser.add_argument('--chunks-dir', type=str, default='chunks',
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —á–∞–Ω–∫–∞–º–∏')
    parser.add_argument('--dense-model', type=str, default='intfloat/multilingual-e5-large',
                       help='–ú–æ–¥–µ–ª—å –¥–ª—è dense —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤')
    parser.add_argument('--llm-model', type=str, default='qwen3:latest',
                       help='–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ LLM –∏–∑ Ollama')
    parser.add_argument('--search-type', type=str, choices=['dense', 'sparse', 'hybrid'],
                       default='hybrid', help='–¢–∏–ø –ø–æ–∏—Å–∫–∞')
    parser.add_argument('--top-k', type=int, default=5,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞')
    parser.add_argument('--device', type=str, default=None,
                       help='–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (mps/cuda/cpu)')
    parser.add_argument('--temperature', type=float, default=0.7,
                       help='–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏')
    parser.add_argument('--max-tokens', type=int, default=1000,
                       help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ')
    
    args = parser.parse_args()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π UUID –¥–ª—è —Å–µ—Å—Å–∏–∏
    session_id = str(uuid.uuid4())
    print(f"üìù –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω Session ID –¥–ª—è —ç—Ç–æ–π —Å–µ—Å—Å–∏–∏: {session_id}")
    
    print("=" * 60)
    print("RAG-–ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç LLM –¥–ª—è Ollama (–∫–∞–∫ –≤ lab1)
    print(f"\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama –∫–ª–∏–µ–Ω—Ç–∞...")
    llm_client = create_llm_client()
    api_base_used = os.getenv('LLM_API_BASE', 'http://localhost:11434/v1')
    print(f"‚úÖ Ollama –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω (API: {api_base_used})")
    
    # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç Langfuse
    print(f"\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Langfuse –∫–ª–∏–µ–Ω—Ç–∞...")
    langfuse_client = create_langfuse_client()
    if langfuse_client:
        print("‚úÖ Langfuse –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω - –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∫–ª—é—á–µ–Ω–æ")
    else:
        print("‚ö†Ô∏è  Langfuse –∫–ª–∏–µ–Ω—Ç –Ω–µ —Å–æ–∑–¥–∞–Ω - –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG-–ø–∞–π–ø–ª–∞–π–Ω
    print(f"\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞...")
    device = get_device(args.device) if args.device else None
    rag = RAGPipeline(
        faiss_index_dir=args.faiss_index_dir,
        chunks_dir=args.chunks_dir,
        dense_model_name=args.dense_model,
        llm_client=llm_client,
        llm_model=args.llm_model,
        search_type=args.search_type,
        top_k=args.top_k,
        device=device,
        langfuse_client=langfuse_client
    )
    print("‚úÖ RAG-–ø–∞–π–ø–ª–∞–π–Ω –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
    print(f"\n{'='*60}")
    print("–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º")
    print("–í–≤–µ–¥–∏—Ç–µ 'exit' –∏–ª–∏ 'quit' –¥–ª—è –≤—ã—Ö–æ–¥–∞")
    print(f"Session ID: {session_id}")
    print("=" * 60)
    
    while True:
        try:
            question = input("\n‚ùì –í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()
            
            if not question:
                continue
            
            if question.lower() in ['exit', 'quit', '–≤—ã—Ö–æ–¥']:
                print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            
            print()
            result = rag.generate_answer(
                question,
                temperature=args.temperature,
                max_tokens=args.max_tokens,
                session_id=session_id
            )
            
            print(f"\n{'='*60}")
            print("–û—Ç–≤–µ—Ç:")
            print("=" * 60)
            print(result["answer"])
            print()
            
        except KeyboardInterrupt:
            print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞: {str(e)}")