# Алгоритмы машинного обучения

## Линейная регрессия

### Основы линейной регрессии

Линейная регрессия — это один из самых простых и широко используемых алгоритмов машинного обучения. Она используется для предсказания непрерывных значений на основе линейной зависимости между входными и выходными переменными.

### Математическая модель

Линейная регрессия моделирует зависимость между зависимой переменной y и независимыми переменными x следующим образом:

y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε

где:
- β₀ — свободный член (intercept)
- β₁, β₂, ..., βₙ — коэффициенты регрессии
- ε — ошибка модели

### Метод наименьших квадратов

Метод наименьших квадратов используется для нахождения оптимальных значений коэффициентов. Цель — минимизировать сумму квадратов остатков:

SSE = Σ(yᵢ - ŷᵢ)²

где yᵢ — фактическое значение, ŷᵢ — предсказанное значение.

### Регуляризация

Для предотвращения переобучения используются методы регуляризации:

- Ridge регрессия (L2): добавляет штраф за большие значения коэффициентов
- Lasso регрессия (L1): может обнулять некоторые коэффициенты, выполняя отбор признаков
- Elastic Net: комбинация Ridge и Lasso

## Логистическая регрессия

### Применение

Логистическая регрессия используется для задач бинарной классификации. В отличие от линейной регрессии, она предсказывает вероятность принадлежности к классу.

### Сигмоидная функция

Логистическая регрессия использует сигмоидную функцию для преобразования линейной комбинации признаков в вероятность:

P(y=1|x) = 1 / (1 + e^(-z))

где z = β₀ + β₁x₁ + ... + βₙxₙ

### Функция потерь

Для логистической регрессии используется логистическая функция потерь (log loss):

L = -[y·log(ŷ) + (1-y)·log(1-ŷ)]

## Деревья решений

### Структура дерева решений

Дерево решений — это алгоритм, который использует древовидную структуру для принятия решений. Оно состоит из:
- Корневого узла: начальная точка
- Внутренних узлов: точки принятия решений
- Листовых узлов: конечные результаты

### Критерии разделения

Для построения дерева используются различные критерии:
- Энтропия: мера неопределенности
- Информационный выигрыш: уменьшение энтропии после разделения
- Индекс Джини: мера неоднородности

### Преимущества и недостатки

Преимущества:
- Легко интерпретировать
- Не требует нормализации данных
- Может обрабатывать нелинейные зависимости

Недостатки:
- Склонны к переобучению
- Нестабильны к небольшим изменениям данных

## Случайный лес

### Ансамблевый метод

Случайный лес — это ансамблевый метод, который объединяет множество деревьев решений. Каждое дерево обучается на случайной подвыборке данных и признаков.

### Процесс обучения

1. Создание бутстрап-выборок из обучающих данных
2. Обучение дерева на каждой выборке
3. Использование случайного подмножества признаков для каждого узла
4. Объединение предсказаний всех деревьев (голосование или усреднение)

### Преимущества

- Снижает переобучение по сравнению с одним деревом
- Может обрабатывать большое количество признаков
- Обеспечивает оценку важности признаков
- Устойчив к выбросам

## Метод опорных векторов (SVM)

### Концепция

SVM находит оптимальную разделяющую гиперплоскость, которая максимизирует зазор между классами.

### Ядерный трюк

SVM может работать с нелинейно разделимыми данными с помощью ядерных функций:
- Линейное ядро
- Полиномиальное ядро
- Радиальная базисная функция (RBF)
- Сигмоидное ядро

### Применение

SVM эффективен для:
- Классификации текста
- Распознавания изображений
- Биоинформатики

## K-ближайших соседей (KNN)

### Алгоритм

KNN классифицирует объект на основе классов k ближайших соседей в пространстве признаков.

### Выбор k

Выбор параметра k критичен:
- Малый k: более чувствителен к шуму
- Большой k: более сглаженные границы решений

### Метрики расстояния

- Евклидово расстояние
- Манхэттенское расстояние
- Косинусное расстояние

## Кластеризация

### K-means

K-means — это алгоритм кластеризации, который разделяет данные на k кластеров.

### Процесс

1. Инициализация центроидов
2. Назначение точек ближайшему центроиду
3. Пересчет центроидов
4. Повторение шагов 2-3 до сходимости

### Инициализация

Правильная инициализация важна для качества кластеризации:
- Случайная инициализация
- K-means++: умная инициализация

## Метод главных компонент (PCA)

### Цель

PCA используется для понижения размерности данных, сохраняя максимальную вариацию.

### Процесс

1. Стандартизация данных
2. Вычисление ковариационной матрицы
3. Нахождение собственных векторов и собственных значений
4. Выбор главных компонент
5. Проецирование данных на новое пространство

### Применение

- Визуализация многомерных данных
- Сжатие данных
- Удаление шума

