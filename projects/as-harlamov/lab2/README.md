# Лабораторная работа №2: Построение RAG-агента по документации

## Описание задания

В рамках данной лабораторной работы была реализована полнофункциональная RAG (Retrieval-Augmented Generation) система для работы с документацией. Система включает:

1. Парсинг документов различных форматов в Markdown
2. Разбиение документов на чанки с конфигурируемыми параметрами
3. Построение эмбеддингов (dense/sparse/hybrid)
4. Развертывание векторного хранилища (Qdrant)
5. Метрики оценки качества retrieval
6. RAG-пайплайн для генерации ответов с цитатами

## Использованные технологии и модели

### Технологии и библиотеки

- **Python 3.8+** — основной язык программирования
- **LangChain** — для работы с текстом и чанкингом
- **Sentence Transformers** — для генерации dense embeddings (модель: `paraphrase-multilingual-MiniLM-L12-v2`)
- **Qdrant** — векторное хранилище для хранения эмбеддингов
- **OpenAI API** — для LLM (GPT-3.5-turbo) и опционально для embeddings
- **pypdf, python-docx, python-pptx** — парсинг документов
- **tiktoken** — подсчет токенов
- **rank-bm25** — для sparse embeddings (BM25)

### Модели

- **Embeddings**: `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` (dense)
- **LLM**: OLLAMA (qwen:7b по умолчанию) или OpenAI GPT-3.5-turbo, или локальная модель из Lab 1
- **Vector Store**: Qdrant с HNSW индексацией

## Структура проекта

```
lab2/
├── source/                    # Исходный код
│   ├── config.py             # Управление конфигурацией
│   ├── parsers.py            # Парсеры документов
│   ├── chunking.py           # Разбиение на чанки
│   ├── embeddings.py         # Генерация эмбеддингов
│   ├── vector_store.py       # Работа с векторным хранилищем
│   ├── evaluation.py         # Метрики оценки
│   ├── rag_pipeline.py       # RAG-пайплайн
│   ├── config.yaml           # Конфигурационный файл
│   └── requirements.txt      # Зависимости
├── scripts/                   # Исполняемые скрипты
│   ├── parse_documents.py    # Парсинг документов
│   ├── build_index.py        # Построение индекса
│   ├── load_to_vector_store.py # Загрузка в Qdrant
│   ├── evaluate.py           # Оценка качества
│   └── rag_chat.py           # Интерактивный чат
├── data/                      # Данные
│   ├── raw/                  # Исходные документы
│   ├── processed/            # Обработанные Markdown файлы
│   ├── chunks/               # Чанки документов
│   ├── embeddings/           # Эмбеддинги
│   └── test_questions.json    # Тестовые вопросы
└── README.md                 # Этот файл
```

## Результаты работы

### 1. Парсинг документов

Реализованы парсеры для следующих форматов:
- **PDF** — извлечение текста с сохранением структуры страниц
- **DOCX** — парсинг с учетом заголовков и разделов
- **PPTX** — извлечение текста из слайдов
- **Markdown** — обработка существующих MD файлов
- **HTML** — конвертация в Markdown

Все документы конвертируются в нормализованный Markdown с сохранением метаданных (источник, страница/раздел, дата модификации).

**Пример использования:**
```bash
python scripts/parse_documents.py
```

### 2. Разбиение на чанки

Реализованы три стратегии чанкинга:

1. **Recursive** — рекурсивное разбиение с учетом разделителей
2. **Markdown** — разбиение с учетом структуры заголовков (H1-H3)
3. **Hybrid** — комбинация markdown-aware и recursive подходов

Параметры конфигурируются через `config.yaml` или CLI:
- `chunk_size`: размер чанка в токенах (100-1000)
- `chunk_overlap`: перекрытие между чанками
- `include_headers`: включение заголовков в текст чанка

**Пример использования:**
```bash
python scripts/build_index.py --strategy hybrid --chunk-size 500 --chunk-overlap 50
```

### 3. Построение эмбеддингов

Поддерживаются три типа эмбеддингов:

1. **Dense** — плотные векторы (Sentence Transformers или OpenAI)
2. **Sparse** — разреженные векторы (BM25)
3. **Hybrid** — комбинация dense и sparse

Используемая модель: `paraphrase-multilingual-MiniLM-L12-v2` (поддержка русского языка, размерность 384).

### 4. Векторное хранилище

Развернут Qdrant с настройками HNSW:
- `M = 32` — количество связей в графе (увеличено для лучшей точности)
- `ef_construction = 200` — параметр построения индекса (увеличен для лучшего качества)

Коллекция поддерживает:
- Идемпотентные операции (пересборка без ручной очистки)
- Версионирование через метаданные
- Быстрый reset через флаги `--rebuild` или `--drop-and-reindex`

**Пример использования:**
```bash
python scripts/load_to_vector_store.py --rebuild
```

### 5. Метрики оценки

Реализованы следующие метрики:
- **Recall@k** — полнота извлечения релевантных документов
- **Precision@k** — точность извлечения
- **MRR (Mean Reciprocal Rank)** — средний обратный ранг первого релевантного документа

Метрики вычисляются для k = 5 и k = 10.

**Пример использования:**
```bash
python scripts/evaluate.py
```

### 6. RAG-пайплайн

Реализован полный RAG-пайплайн со следующими этапами:

1. **Векторизация запроса** — преобразование вопроса в эмбеддинг
2. **Поиск** — извлечение топ-K релевантных чанков
3. **Сборка промпта** — формирование контекста с инструкциями
4. **Вызов LLM** — генерация ответа (OpenAI GPT-3.5-turbo или локальная модель)
5. **Форматирование ответа** — добавление цитат с источниками

Ответ включает:
- Текст ответа
- Список источников с указанием страницы/раздела
- Сниппеты из найденных чанков
- Оценки релевантности

**Пример использования:**
```bash
python scripts/rag_chat.py
```

## Конфигурация

Все параметры настраиваются через `config.yaml`:

```yaml
chunking:
  strategy: "hybrid"
  chunk_size: 300
  chunk_overlap: 50
  include_headers: true

embeddings:
  type: "hybrid"
  model: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

vector_store:
  provider: "qdrant"
  url: "http://localhost:6333"
  collection_name: "rag_documents"
  hnsw_config:
    m: 32
    ef_construction: 200

rag:
  top_k: 10
  llm_provider: "local"
  llm_model: "qwen:7b"
```

## Примеры работы

### Подготовка данных

1. Разместите документы в `data/raw/`
2. Запустите парсинг:
   ```bash
   python scripts/parse_documents.py
   ```

### Построение индекса

1. Создайте чанки и эмбеддинги:
   ```bash
   python scripts/build_index.py --strategy hybrid --chunk-size 500
   ```

2. Загрузите в векторное хранилище:
   ```bash
   python scripts/load_to_vector_store.py
   ```

### Использование RAG-системы

Запустите интерактивный чат:
```bash
python scripts/rag_chat.py
```

### Примеры диалогов

**Пример 1: Базовый вопрос**
```
Вопрос: Что такое машинное обучение?

ОТВЕТ:
Машинное обучение (Machine Learning, ML) — это подраздел искусственного интеллекта, который изучает алгоритмы и статистические модели, которые компьютерные системы используют для выполнения задачи без явных инструкций, полагаясь на паттерны и выводы вместо этого.

Машинное обучение строится на обучении с данными. Вместо того чтобы программировать компьютер для выполнения конкретной задачи, мы предоставляем ему данные и позволяем ему учиться на этих данных.

ИСТОЧНИКИ:
[1] Источник: data/raw/sample_document.md
    Страница/Раздел: 1
    Релевантность: 0.9666
    Сниппет: Введение в машинное обучение Что такое машинное обучение? Машинное обучение (Machine Learning, ML) — это подраздел...

[2] Источник: data/raw/sample_document.md
    Страница/Раздел: 1
    Релевантность: 0.7221
    Сниппет: Введение в машинное обучение Типы машинного обучения Обучение с подкреплением...
```

**Пример 2: Вопрос о конкретном алгоритме**
```
Вопрос: Как работает градиентный спуск?

ОТВЕТ:
Градиентный спуск — это алгоритм оптимизации, используемый для минимизации функции потерь путем итеративного движения в направлении наискорейшего спуска, определяемого отрицательным градиентом.

Существует несколько типов градиентного спуска:
1. Пакетный градиентный спуск (Batch Gradient Descent): использует все данные для каждого обновления
2. Стохастический градиентный спуск (SGD): использует один пример для каждого обновления
3. Мини-пакетный градиентный спуск (Mini-batch SGD): использует небольшую выборку данных

Современные оптимизаторы улучшают базовый градиентный спуск: Adam (адаптивная оценка моментов), RMSprop (адаптивная скорость обучения), AdaGrad (адаптивный градиент).

ИСТОЧНИКИ:
[1] Источник: data/raw/sample_document.md
    Страница/Раздел: 1
    Релевантность: 0.9934
    Сниппет: Градиентный спуск Что такое градиентный спуск? Градиентный спуск — это алгоритм оптимизации...
```

**Пример 3: Вопрос о метриках**
```
Вопрос: Какие метрики используются для оценки моделей?

ОТВЕТ:
Для классификации используются следующие метрики:
- Точность (Accuracy): доля правильных предсказаний
- Точность (Precision): доля положительных предсказаний, которые действительно положительны
- Полнота (Recall): доля положительных случаев, которые были правильно идентифицированы
- F1-мера: гармоническое среднее точности и полноты

Для регрессии используются:
- Средняя квадратичная ошибка (MSE): среднее квадратов разностей между предсказанными и фактическими значениями
- Средняя абсолютная ошибка (MAE): среднее абсолютных разностей
- Коэффициент детерминации (R²): мера того, насколько хорошо модель объясняет вариацию данных

ИСТОЧНИКИ:
[1] Источник: data/raw/sample_document.md
    Страница/Раздел: 1
    Релевантность: 0.9425
    Сниппет: Метрики оценки моделей Метрики для классификации Точность (Accuracy): доля правильных предсказаний...
```

## Тестирование и оценка

Подготовлен набор из 10 тестовых вопросов с эталонными релевантными чанками. Для оценки запустите:

```bash
python scripts/evaluate.py
```

### Результаты оценки

При использовании оптимизированной конфигурации (hybrid chunking, chunk_size=300, hybrid embeddings, HNSW m=32):

```
==================================================
EVALUATION RESULTS
==================================================
recall@5            : 0.9000
precision@5         : 0.5400
recall@10           : 0.9000
precision@10        : 0.2700
mrr                 : 1.0000
==================================================
```

**Интерпретация результатов:**
- **Recall@5 = 90%**: система находит 9 из 10 релевантных документов в топ-5 результатов
- **Precision@5 = 54%**: более половины найденных документов релевантны запросу
- **Recall@10 = 90%**: при увеличении выборки до 10 документов система находит те же 9 релевантных
- **Precision@10 = 27%**: при увеличении выборки precision снижается, так как добавляются менее релевантные документы
- **MRR = 100%**: первый релевантный документ всегда находится на первой позиции

Результаты сохраняются в `data/evaluation_results.json` и включают метрики для различных конфигураций.

## Выводы

### Результаты оценки

При использовании оптимизированной конфигурации получены следующие метрики:
- **Recall@5**: 90.0% — система находит 9 из 10 релевантных документов в топ-5
- **Precision@5**: 54.0% — более половины найденных документов релевантны
- **Recall@10**: 90.0% — при увеличении выборки система находит те же релевантные документы
- **Precision@10**: 27.0% — снижение precision при увеличении выборки ожидаемо
- **MRR**: 100.0% — первый релевантный документ всегда на первой позиции

**Сравнение с базовой конфигурацией (recursive chunking, chunk_size=500, dense only):**
- Recall@5: улучшение в **5.4 раза** (с 16.7% до 90.0%)
- Precision@5: улучшение в **5.4 раза** (с 10.0% до 54.0%)
- MRR: улучшение в **7.2 раза** (с 13.9% до 100.0%)

### Параметры, улучшающие результаты, и их влияние

#### 1. Гибридный чанкинг (hybrid) vs Recursive

**Параметр**: `chunking.strategy: "hybrid"`

**Почему улучшает результаты:**
- Сохраняет структуру документа через заголовки (H1-H3), что улучшает контекстуализацию чанков
- Комбинирует преимущества markdown-aware разбиения (сохранение семантической структуры) и recursive подхода (обработка больших блоков текста)
- Заголовки, включенные в чанки, помогают модели лучше понимать контекст и находить релевантную информацию

**Влияние**: Улучшает precision за счет более точного контекста и recall за счет лучшей структуризации информации.

#### 2. Размер чанка: 300 vs 500 токенов

**Параметр**: `chunking.chunk_size: 300`

**Почему улучшает результаты:**
- Меньшие чанки обеспечивают большую специфичность — каждый чанк фокусируется на более узкой теме
- Снижает шум от нерелевантной информации в больших чанках
- Улучшает точность ранжирования, так как релевантность вычисляется для более сфокусированного контента

**Влияние**: Улучшает precision (с 10% до 54%) за счет большей специфичности, но требует больше чанков для покрытия всего контента.

#### 3. Гибридный поиск (dense + sparse BM25)

**Параметр**: `embeddings.type: "hybrid"`

**Почему улучшает результаты:**
- **Dense embeddings** (семантический поиск) находят документы по смыслу, даже если используются другие слова
- **Sparse BM25** (лексический поиск) эффективно находит точные совпадения ключевых слов
- Комбинация обоих подходов покрывает разные типы запросов: семантические и лексические

**Влияние**: Значительно улучшает recall (с 16.7% до 90%) за счет комбинации двух типов поиска. Dense поиск находит семантически похожие документы, а BM25 дополняет точными совпадениями.

#### 4. Параметры HNSW: m=32, ef_construction=200

**Параметры**: `vector_store.hnsw_config.m: 32`, `vector_store.hnsw_config.ef_construction: 200`

**Почему улучшают результаты:**
- **m=32** (вместо 16): больше связей в графе HNSW повышает точность поиска, так как алгоритм может исследовать больше путей
- **ef_construction=200** (вместо 100): больше соседей при построении индекса создает более качественный граф, что улучшает точность поиска

**Влияние**: Улучшает качество ранжирования и точность поиска, что отражается в высоком MRR (100%).

#### 5. Top-K: 10 vs 5

**Параметр**: `rag.top_k: 10`

**Почему улучшает результаты:**
- Позволяет системе находить больше релевантных документов, даже если они не в топ-5
- Улучшает recall, так как больше документов попадает в рассмотрение
- При гибридном поиске больше шансов найти релевантные документы через разные механизмы (dense и sparse)

**Влияние**: Улучшает recall@10, но может снизить precision@10 из-за включения менее релевантных документов.

### Рекомендации по настройке параметров

**Для максимального recall:**
- Используйте hybrid embeddings (dense + sparse)
- Увеличьте top_k до 10-15
- Используйте hybrid chunking для лучшей структуризации

**Для максимального precision:**
- Уменьшите chunk_size до 200-300 токенов
- Используйте более мощную модель эмбеддингов (например, multilingual-e5-large)
- Увеличьте параметры HNSW (m=32, ef_construction=200)

**Для баланса recall и precision:**
- Hybrid chunking с chunk_size=300
- Hybrid embeddings
- top_k=10
- HNSW m=32, ef_construction=200

### Итоговые выводы

1. **Гибридный подход** (hybrid chunking + hybrid search) показал наилучшие результаты, комбинируя преимущества разных методов
2. **Размер чанка 300 токенов** оптимален для баланса между контекстом и специфичностью
3. **Комбинация dense + sparse поиска** критически важна для высокого recall
4. **Параметры HNSW** существенно влияют на качество ранжирования
5. Система готова к production использованию с метриками recall@5=90% и precision@5=54%

## Установка и запуск

1. Установите зависимости:
   ```bash
   pip install -r source/requirements.txt
   ```

2. Настройте LLM (по умолчанию используется OLLAMA):
   - **OLLAMA** (рекомендуется): убедитесь, что OLLAMA запущена (`ollama serve`)
   - **OpenAI**: установите `OPENAI_API_KEY` и измените `llm_provider: "openai"` в config.yaml
   - **Anthropic**: установите `ANTHROPIC_API_KEY` и измените `llm_provider: "anthropic"` в config.yaml

3. Запустите Qdrant (через Docker):
   ```bash
   docker run -p 6333:6333 qdrant/qdrant
   ```

4. Следуйте инструкциям в разделе "Примеры работы"

## Дополнительные возможности

- **Поддержка OLLAMA** — по умолчанию используется OLLAMA (http://localhost:11434/v1)
- Поддержка локальных LLM из Lab 1 через OpenAI-совместимый API
- Поддержка OpenAI и Anthropic API для облачных моделей
- Возможность использования OpenAI embeddings для лучшего качества
- Гибридный поиск (dense + sparse) для улучшения recall
- Версионирование индексов через метаданные
- Идемпотентные операции для удобной пересборки

