# Лабораторная работа 1: Развёртывание и эмпирический анализ LLM

## 1. Развёрнутые LLM

Платформа: **Ollama** (OpenAI-совместимый REST API)

| Семейство | Модель | Размер |
|-----------|--------|--------|
| Mistral | mistral:7b | 7B |
| Qwen | qwen3:4b | 4B |
| Llama | llama3.2:3b | 3B |

## 2. Тестовые промпты

**P1 (Генерация)**: Написать приключенческую историю про минипига, ищущего яблоки на опушке леса. Требования: 500 слов, меланхоличный тон.

**P2 (Классификация)**: Классифицировать 17 заголовков научных статей по категориям. Метки: `general`, `math`, `physics`, `chemistry`, `biology`, `history`, `geography`, `literature`, `art`, `music`, `other`. Формат: JSON.

**P3 (Суммаризация)**: Сделать краткое резюме философского текста (отрывок из Гегеля) на 10 слов.

## 3. Режимы генерации

**Базовый (A)**: Дефолтные параметры Ollama

**Тюнинг (B)**: 
- temperature: 1.2
- max_tokens: 1500
- top_p: 0.95
- top_k: 60
- repetition_penalty: 0.9

## 4. Анализ

### Длина ответа в токенах

| Модель | Промпт | A | B | Δ |
|--------|--------|---|---|---|
| mistral:7b | P1 | 1450 | 1520 | +70 |
| mistral:7b | P2 | 480 | 495 | +15 |
| mistral:7b | P3 | 48 | 68 | +20 |
| qwen3:4b | P1 | 3200 | 2580 | -620 |
| qwen3:4b | P2 | 1850 | 2120 | +270 |
| qwen3:4b | P3 | 1420 | 1890 | +470 |
| llama3.2:3b | P1 | 720 | 680 | -40 |
| llama3.2:3b | P2 | 520 | 535 | +15 |
| llama3.2:3b | P3 | 22 | 35 | +13 |

Повышение temperature и max_tokens увеличивает длину у Mistral (+5-40%) и Llama (+3-60%). У Qwen наблюдается нестабильность из-за вывода CoT-блоков.

### Точность/адекватность

**P1 (Генерация)**:
- mistral:7b A — связная история, соблюдена структура
- mistral:7b B — более креативная, но менее связная
- qwen3:4b A — вывела CoT-блоки (`<think>`), история есть, но формат нарушен
- qwen3:4b B — CoT + обрыв на полуслове
- llama3.2:3b A/B — нечитаемо, ошибки русского языка

**P2 (Классификация)**:
- mistral:7b A — 15/17 (88%), использовала несуществующие метки `geology`, `language`
- mistral:7b B — 16/17 (94%)
- qwen3:4b A — 17/17 (100%), вывела CoT, но итоговый JSON верен
- qwen3:4b B — обрыв генерации, JSON не сформирован
- llama3.2:3b A — 14/17 (82%), использовала `geology`
- llama3.2:3b B — 13/17 (76%), опечатки в заголовках

**P3 (Суммаризация)**:
- mistral:7b A — 27 слов вместо 10, смысл передан
- mistral:7b B — 35 слов вместо 10
- qwen3:4b A — CoT-блок на 1400 слов, затем финальный ответ на 10 слов
- qwen3:4b B — CoT оборван
- llama3.2:3b A — 8 слов, смысл искажён
- llama3.2:3b B — 13 слов, смысл искажён

### Стабильность/детерминизм

**Mistral**: Завершает все задачи в обоих режимах, стабильна.

**Qwen**: В режиме A завершает с CoT-блоками. В режиме B обрывает генерацию на всех трёх промптах.

**Llama**: Завершает все задачи, но качество русского языка критически низкое.

### Повторяемость

Тесты без фиксации seed. При низкой temperature (A) ожидается высокая повторяемость. При high temperature (B, 1.2) — низкая повторяемость. У Qwen при B — непредсказуемое поведение (обрывы).

### Галлюцинации

**Mistral P1**: Несуществующие персонажи (монахи, Архангел Михаил).

**Mistral P2**: Несуществующие метки `geology`, `language`.

**Qwen**: Вывод нежелательных `<think>` блоков на всех промптах.

**Llama P2**: Несуществующая метка `geology`.

### Время отклика и скорость

| Модель | P1 (A/B) | P2 (A/B) | P3 (A/B) | Средняя скорость |
|--------|----------|----------|----------|------------------|
| mistral:7b | 187с / 199с | 61с / 64с | 6с / 9с | ~7.8 tok/s |
| qwen3:4b | 413с / 332с | 239с / 273с | 183с / 244с | ~7.8 tok/s |
| llama3.2:3b | 103с / 97с | 74с / 76с | 3с / 5с | ~7.0 tok/s |

Скорость зависит от модели, не от гиперпараметров. Mistral и Qwen одинаковы (~7.8 tok/s), Llama медленнее (~7.0 tok/s).

### Токены ввода/вывода

Ollama не возвращает детальную статистику токенов через API.

**Ввод** (оценка):
- P1: ~150 токенов
- P2: ~380 токенов
- P3: ~290 токенов

**Вывод**: см. таблицу выше (раздел "Длина ответа").

### Дефолты движка

Используется Ollama для всех моделей, дефолты одинаковы:

| Параметр | Default | Режим B |
|----------|---------|---------|
| temperature | ~0.7 | 1.2 |
| max_tokens | ~512 | 1500 |
| top_p | ~0.9 | 0.95 |
| top_k | ~40 | 60 |
| repetition_penalty | 1.0 | 0.9 |

## Выводы

**Влияние гиперпараметров**:
- Повышение temperature (0.7→1.2) увеличивает креативность и длину ответов, но снижает стабильность (обрывы у Qwen)
- Увеличение max_tokens (512→1500) позволяет генерировать длинные тексты
- top_p, top_k, repetition_penalty имеют минимальное влияние

**Сравнение моделей**:
- **Mistral** — стабильная, лучшая для генерации текста, склонна к галлюцинациям
- **Qwen** — лучшая точность на классификации (100%), но нестабильна при тюнинге, выводит нежелательный CoT
- **Llama** — непригодна для русского языка

**Общее**: Дефолтные параметры стабильнее агрессивного тюнинга. Все модели галлюцинируют.

## Файлы

- `source/scripts/prompts.yaml` — промпты
- `assistant_responses.json` — все ответы (18 прогонов: 3 модели × 3 промпта × 2 режима)
