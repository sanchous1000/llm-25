import argparse
import uuid

from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.models import Distance, VectorParams, PointStruct

try:
    from build_index import split_documents, load_documents
except ImportError:

    print("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å split_documents –∏–∑ build_index.py.")
    print("‚ö†Ô∏è  –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ build_index.py –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ç–æ–π –∂–µ –ø–∞–ø–∫–µ –∏–ª–∏ PYTHONPATH.")
    exit(1)


def setup_qdrant_collection(client: QdrantClient, collection_name: str, vector_size: int, args):
    if client.collection_exists(collection_name):
        if args.rebuild:
            print(f"üóëÔ∏è  –£–¥–∞–ª—è—é —Å—Ç–∞—Ä—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é '{collection_name}' (--rebuild)...")
            client.delete_collection(collection_name)
        else:
            print(f"‚ÑπÔ∏è  –ö–æ–ª–ª–µ–∫—Ü–∏—è '{collection_name}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ.")
            return

    print(f"üî® –°–æ–∑–¥–∞–µ—Ç—Å—è –∫–æ–ª–ª–µ–∫—Ü–∏—è '{collection_name}'...")
    print(f"   ‚öôÔ∏è  –í–µ–∫—Ç–æ—Ä: {vector_size} (Cosine)")
    print(f"   ‚öôÔ∏è  HNSW: M={args.hnsw_m}, ef_construction={args.hnsw_ef}")

    quantization_config = None
    if args.quantization:
        print("   ‚öôÔ∏è  –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ: Scalar (Int8)")
        quantization_config = models.ScalarQuantization(
            scalar=models.ScalarQuantizationConfig(
                type=models.ScalarType.INT8,
                quantile=0.99,
                always_ram=True
            )
        )

    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),
        hnsw_config=models.HnswConfigDiff(
            m=args.hnsw_m,
            ef_construct=args.hnsw_ef, 
        ),
        quantization_config=quantization_config
    )

def main():
    parser = argparse.ArgumentParser(description="–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –≤ Qdrant")
    
    parser.add_argument("--source_dir", default="verl_sources", help="–ü–∞–ø–∫–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
    parser.add_argument("--chunk_size", type=int, default=512)
    parser.add_argument("--overlap", type=int, default=100)
    parser.add_argument("--add_header", action="store_true", help="–î–æ–±–∞–≤–ª—è—Ç—å –∏–º—è —Ñ–∞–π–ª–∞ –≤ —Ç–µ–∫—Å—Ç")
    
    parser.add_argument("--emb_model", default="intfloat/e5-large-v2")
    
    parser.add_argument("--url", default="http://localhost:6333", help="URL Qdrant")
    parser.add_argument("--collection", default="verl_rag", help="–ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏")
    parser.add_argument("--rebuild", action="store_true", help="–ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é")
    parser.add_argument("--hnsw_m", type=int, default=16)
    parser.add_argument("--hnsw_ef", type=int, default=100)
    parser.add_argument("--quantization", action="store_true", help="–í–∫–ª—é—á–∏—Ç—å —Å–∂–∞—Ç–∏–µ")

    args = parser.parse_args()  

    print("üîÑ –≠—Ç–∞–ø 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    docs = load_documents(args.source_dir)
    chunks = split_documents(docs, args.chunk_size, args.overlap, args.add_header)
    
    if not chunks:
        print("‚ùå –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏.")
        return

    print(f"üß† –≠—Ç–∞–ø 2: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ {args.emb_model}...")
    embeddings_model = HuggingFaceEmbeddings(model_name=args.emb_model)
    
    test_vec = embeddings_model.embed_query("test")
    vector_size = len(test_vec)

    client = QdrantClient(url=args.url)
    setup_qdrant_collection(client, args.collection, vector_size, args)

    batch_size = 64
    total_chunks = len(chunks)
    print(f"üöÄ –≠—Ç–∞–ø 3: –ó–∞–≥—Ä—É–∑–∫–∞ {total_chunks} —á–∞–Ω–∫–æ–≤ –≤ Qdrant...")

    for i in range(0, total_chunks, batch_size):
        batch = chunks[i : i + batch_size]
        
        texts = [doc.page_content for doc in batch]
        vectors = embeddings_model.embed_documents(texts)
        
        points = []
        for j, (text, vector) in enumerate(zip(texts, vectors)):
            payload = batch[j].metadata.copy()
            payload["page_content"] = text
            
            point_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, text))
            points.append(PointStruct(
                id=point_id,
                vector=vector,
                payload=payload
            ))
            
        client.upsert(
            collection_name=args.collection,
            points=points
        )
        print(f"   üì¶ –ü—Ä–æ–≥—Ä–µ—Å—Å: {min(i + batch_size, total_chunks)} / {total_chunks}")

    print(f"\n‚úÖ –£—Å–ø–µ—à–Ω–æ! –ö–æ–ª–ª–µ–∫—Ü–∏—è '{args.collection}' –æ–±–Ω–æ–≤–ª–µ–Ω–∞.")
    print(f"   üìä –¢–µ–ø–µ—Ä—å –≤ –±–∞–∑–µ: {client.count(args.collection).count} –≤–µ–∫—Ç–æ—Ä–æ–≤")

if __name__ == "__main__":
    main()