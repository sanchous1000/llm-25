# Лабораторная работа №1: Развёртывание и эмпирический анализ LLM
## Описание задания
В рамках лабораторной работы было выполнено развёртывание трёх моделей больших языковых моделей (LLM) из разных семейств с использованием фреймворка Ollama. Проведен эксперимент по оценке качества генерации текста моделями на трёх типах задач: генерация текста, классификация и извлечение информации. Для каждой модели и каждого промпта выполнено по два прогона — с базовыми параметрами и с настроенными гиперпараметрами.

## Использованные технологии и модели
- **Фреймворк для развёртывания LLM:** Ollama (версия с поддержкой OpenAI-совместимого REST API)
- **Модели LLM:**
    - Qwen (семейство Qwen) — модель: qwen2.5:7b
    - Llama (семейство Meta) — модель: llama3:8b
    - Mistral (семейство Mistral AI) — модель: mistral:7b

- **Язык программирования:** Python 3.8+
- **Библиотеки:**
    - *requests* — для отправки HTTP-запросов к API Ollama
    - *json* — для работы с JSON-данными
    - *time* — для замера времени выполнения запросов
    - *datetime* — для формирования временных меток

## Результаты работы
Было проведено 18 экспериментов (3 модели × 3 промпта × 2 режима). Все результаты сохранены в JSON-файле.

## Ключевые наблюдения:
**1. Генерация вежливого письма (P1)**
- *Qwen (base)*: Произошло значительное отклонение от темы — модель начала генерировать текст о вирусных заболеваниях вместо извинений за задержку доставки, причем не на русском языке. Это пример галлюцинации.
- *Qwen (tuned)*: После настройки параметров модель корректно сгенерировала письмо с извинениями и предложением компенсации.
- *Llama и Mistral*: Обе модели в обоих режимах успешно сгенерировали письма с извинениями, хотя стилистика различалась.

**2. Классификация обращений (P2)**
- Все модели успешно классифицировали все четыре обращения как в базовом, так и в настроенном режиме.
- Ответы соответствовали требуемому JSON-формату.
- Время выполнения классификации было самым коротким среди всех задач (0.93-1.20 секунд).

**3. Извлечение ключевой информации (P3)**
- Все модели корректно извлекли структурированную информацию из текста вакансии.
- Qwen и Mistral допустили незначительные ошибки в формулировках в базовом режиме, которые были исправлены в настроенном режиме.
- Llama показала наиболее стабильные результаты в обоих режимах.

## Влияние гиперпараметров:
- *Temperature (0.6)*: Снижение температуры по сравнению с дефолтной (обычно 0.8-1.0) сделало ответы более детерминированными и структурированными.
- *Top_p (0.95)*: Позволило сохранить разнообразие ответов при уменьшении вероятности выбора маловероятных токенов.
- *Max_tokens (500)*: Ограничение длины ответа предотвратило избыточную генерацию.
- *Repeat_penalty (1.2)*: Снизило вероятность повторения фраз и слов в ответах.

## Метрики производительности:
- Время ответа: Варьировалось от 0.93 до 3.56 секунд в зависимости от модели и сложности задачи.
- Длина ответа: Составляла от 554 до 1313 символов.
- Стабильность: Настроенные параметры повысили стабильность и предсказуемость ответов, особенно для задачи генерации.

## Выводы
1. Разные модели демонстрируют различное поведение даже на одинаковых промптах, что подчёркивает важность выбора подходящей модели для конкретной задачи.
2. Настройка гиперпараметров существенно влияет на качество и стабильность генерации, особенно для творческих задач (генерация текста).
3. Классификация и извлечение информации — более стабильные задачи, где даже базовые настройки дают приемлемые результаты.
4. Галлюцинации (как в случае Qwen в базовом режиме) остаются серьёзной проблемой, которую можно частично решить через настройку параметров и улучшение промптов.
5. Ollama предоставляет удобный интерфейс для быстрого развёртывания и тестирования различных моделей LLM с возможностью тонкой настройки параметров генерации.

## Инструкция по запуску
### Предварительные требования
- Python 3.8 или выше
- Установленный Ollama (скачать с официального сайта: https://ollama.com/)

## Шаги для воспроизведения
1. Установка Ollama и загрузка моделей:
``` bash
# Скачать и установить Ollama согласно инструкциям на сайте
# Загрузить необходимые модели
ollama pull qwen2.5:7b
ollama pull llama3:8b
ollama pull mistral:7b
```

2. Запуск сервера Ollama:
``` bash
# Сервер запустится автоматически после установки
# Проверить статус сервера
curl http://127.0.0.1:11434/api/tags
```

3. Настройка окружения Python:
```bash
# Клонировать репозиторий с кодом
# Перейти в директорию проекта
cd /path/to/project

# Установить зависимости
pip install -r requirements.txt
```

4. Запуск эксперимента:
``` bash
# Запустить основной скрипт
python main.py
```

5. Анализ результатов:
- Результаты эксперимента будут сохранены в JSON-файле с временной меткой в названии
- Для просмотра результатов можно использовать любой JSON-редактор или анализатор
